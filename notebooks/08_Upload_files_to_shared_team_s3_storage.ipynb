{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d45742c-f0b7-4633-a4b7-148b2348a397",
   "metadata": {},
   "source": [
    "## 08 - How to upload files to shared team S3 storage\n",
    "### A DeepESDL example notebook \n",
    "\n",
    "This notebook demonstrates how to upload files to shared team s3 storage and how to access them using xcube.\n",
    "\n",
    "Please, also refer to the [DeepESDL documentation](https://deepesdl.readthedocs.io/en/latest/guide/jupyterlab/) and visit the platform's [website](https://www.earthsystemdatalab.net/) for further information!\n",
    "\n",
    "Brockmann Consult, 2024\n",
    "\n",
    "-----------------\n",
    "\n",
    "**This notebook runs with the python environment `deepesdl-xcube-1.3.1`, please checkout the documentation for [help on changing the environment](https://deepesdl.readthedocs.io/en/latest/guide/jupyterlab/#python-environment-selection-of-the-jupyter-kerne).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d42ade-82c0-4792-87f5-d19cc9121b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# needed for uploading files to s3 storage\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "# needed for access of uploaded files\n",
    "from xcube.core.store import new_data_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d0103-62c4-425a-b048-b9738f38c21b",
   "metadata": {},
   "source": [
    "Get the environment variables, which are necessary for later specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800c0028-0ca5-411a-bbc0-27c165312a06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"]\n",
    "S3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"]\n",
    "S3_USER_STORAGE_BUCKET = os.environ[\"endpoint\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476618d-0fc0-488b-9056-902053b375a2",
   "metadata": {},
   "source": [
    "Connect to your team storage in S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca30974a-620b-4e18-accb-c9bd6e12af2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note:If you use a prefix when uploading the data so you need the parameter max_depth=2\n",
    "store = new_data_store(\"s3\",\n",
    "                       root=S3_USER_STORAGE_BUCKET,\n",
    "                       storage_options=dict(anon=False,\n",
    "                                            key=S3_USER_STORAGE_KEY,\n",
    "                                            secret=S3_USER_STORAGE_SECRET))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710eafe-8eff-40f4-9a94-9c631548f91e",
   "metadata": {},
   "source": [
    "You can check which dataformats are supported in xcube s3 store. This way you can find out which files you could easily store and then access by xcube from the s3 team storage space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a163fe93-34a7-47ec-b007-ce2f52d334b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dataset:netcdf:s3',\n",
       " 'dataset:zarr:s3',\n",
       " 'dataset:levels:s3',\n",
       " 'mldataset:levels:s3',\n",
       " 'dataset:geotiff:s3',\n",
       " 'mldataset:geotiff:s3',\n",
       " 'geodataframe:shapefile:s3',\n",
       " 'geodataframe:geojson:s3')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.get_data_opener_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a8557-f3e2-4ce0-8870-86c339136175",
   "metadata": {},
   "source": [
    "To upload files from your workspace to the s3 team shared storage, you must specify where your input files are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1dddad5-b48d-4041-9509-7a1e07864f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_datasets_dir = os.path.expanduser(\"~/<path-to-your-files>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c358c3d-da87-4e73-ac2c-022a509f3ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_store = new_data_store(\"file\",\n",
    "                             root=input_datasets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53570477-0c4d-4f2f-b1c2-2dc9cecf1a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample02-geotiff.tif', 'sample01-geotiff.tif']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(local_store.get_data_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b531fb2-2a0f-485c-9838-de5ebd1aeab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function used to upload data to s3 storage\n",
    "def upload_to_team_s3_bucket(local_file, bucket, s3_file):\n",
    "    s3 = boto3.client('s3', \n",
    "                      aws_access_key_id=S3_USER_STORAGE_KEY,\n",
    "                      aws_secret_access_key=S3_USER_STORAGE_SECRET)\n",
    "\n",
    "    try:\n",
    "        s3.upload_file(local_file, bucket, s3_file)\n",
    "        print(f\"Upload Successful of file {local_file}\")\n",
    "        return True\n",
    "    except NoCredentialsError:\n",
    "        print(\"Credentials not available\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "918a5d38-7cee-4f95-a30b-c2bb07275f44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter only for files in the directory\n",
    "data_files = [file for file in os.listdir(input_datasets_dir) if os.path.isfile(os.path.join(input_datasets_dir, file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c4351d2-cd6c-4ed1-a515-1c3b7a03f964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample02-geotiff.tif', 'sample01-geotiff.tif']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "323afbb7-43a2-4f95-8f29-9be03e972ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"input-datasets\" # giving a prefix, so a direcory like structure is created in s3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f42ccc57-ff5a-47fe-812a-9dd93bc81603",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload Successful of file\n",
      "Upload Successful of file\n"
     ]
    }
   ],
   "source": [
    "# looping through datasets and uploading them to s3 \n",
    "for data_file in data_files:\n",
    "    path = os.path.join(input_datasets_dir, data_file)\n",
    "    target_path = f\"{prefix}/{data_file}\"\n",
    "    upload_to_team_s3_bucket(path, S3_USER_STORAGE_BUCKET, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b95fed4-8c00-49c6-98ca-7f06ee65e4d4",
   "metadata": {},
   "source": [
    "Now lets check for the data: You need to instantiate a s3 datastore pointing to the deep-esdl-output bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b24f1f8-83a1-4040-a727-da7103382ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: If you use a prefix when uploading the data so you need the parameter max_depth=2\n",
    "store = new_data_store(\"s3\",\n",
    "                       max_depth=2,\n",
    "                       root=S3_USER_STORAGE_BUCKET,\n",
    "                       storage_options=dict(anon=False,\n",
    "                                            key=S3_USER_STORAGE_KEY,\n",
    "                                            secret=S3_USER_STORAGE_SECRET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f637302-8c5c-4b55-9e22-a57d818b4e59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "attrs": {
        "source": "s3://hub-deepesdl2/input-datasets/sample01-geotiff.tif"
       },
       "bbox": [
        8.999998905086892,
        52.999956335516245,
        30.221627376651387,
        65.91610888054821
       ],
       "coords": {
        "spatial_ref": {
         "attrs": {
          "GeoTransform": "8.999998905086892 0.027777000617231016 0.0 52.999956335516245 0.0 0.016666003283912205"
         },
         "dims": [],
         "dtype": "int64",
         "name": "spatial_ref"
        },
        "x": {
         "dims": [
          "x"
         ],
         "dtype": "float64",
         "name": "x"
        },
        "y": {
         "dims": [
          "y"
         ],
         "dtype": "float64",
         "name": "y"
        }
       },
       "data_id": "input-datasets/sample01-geotiff.tif",
       "data_type": "mldataset",
       "data_vars": {
        "band_1": {
         "attrs": {
          "_FillValue": -9999999,
          "add_offset": 0,
          "cell_methods": "time: mean",
          "grid_mapping": "spatial_ref",
          "long_name": "Spectral significant wave height (Hm0)",
          "scale_factor": 1,
          "standard_name": "sea_surface_wave_significant_height",
          "units": "m",
          "valid_max": 20,
          "valid_min": 0
         },
         "chunks": [
          512,
          512
         ],
         "dims": [
          "y",
          "x"
         ],
         "dtype": "float32",
         "name": "band_1"
        }
       },
       "dims": {
        "x": 764,
        "y": 775
       },
       "num_levels": 1,
       "spatial_res": 0.016666003283908992,
       "time_range": [
        null,
        null
       ]
      },
      "text/plain": [
       "<xcube.core.store.descriptor.MultiLevelDatasetDescriptor at 0x7fe984e8c7d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.describe_data('input-datasets/sample01-geotiff.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6e8c6-7bf4-4b02-ab19-05dd294d04e6",
   "metadata": {},
   "source": [
    "In case you wish to delete data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95faa54b-8d16-40f3-846e-ce2a3669599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.delete_data('input-datasets/sample01-geotiff.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcbfd50f-fe9b-4630-861f-4e57fcee062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.delete_data('input-datasets/sample02-geotiff.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "962c8af6-6734-45f1-bf10-87f128462c18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(store.get_data_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c509a3-ae11-4c19-af8c-8e4ea452aa6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepesdl-xcube-1.1.2",
   "language": "python",
   "name": "conda-env-deepesdl-xcube-1.1.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
