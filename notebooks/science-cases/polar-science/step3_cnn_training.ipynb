{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e108704d-c4c3-4642-8682-45a0fe70d7bc",
   "metadata": {},
   "source": [
    "# Science Demonstration Case: Polar Science\n",
    "# Automatic ice damage detection from Sentinel-1 radar imagery\n",
    "## Step 3: Train the U-Net neural network.\n",
    "### For more info about this science case, please check the documentation [at this link](https://earthsystemdatalab.net/science_cases/polar_science/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebdcbd7-3eb1-4fd1-9ca6-5abf6271f3bf",
   "metadata": {},
   "source": [
    "This notebook shows how to train a Neural Network in order to infer the ice damage from Sentinel-1 imagery.<br>\n",
    "The ice damage is an key variable when modeling the mass loss of the Antarctic Ice Sheet, as the presence or absence of crevasses and fractures influences the resistance of the ice to the natural gravitational flow into the ocean, that the rising temperatures of the current climate crisis are enhancing.\n",
    "<br>\n",
    "The datacube used in this notebook contains Sentinel-1 imagery over the Amery Ice Shelf (East Antarctica) and a model of the ice damage over the same area.\n",
    "<br>\n",
    "The notebook employs a modified U-Net Convolutional Neural Network and it is largely based on [this Tensorflow tutorial](https://www.tensorflow.org/tutorials/images/segmentation).\n",
    "\n",
    "**This notebook runs with the python environment `polar-science-use-case`, please checkout the documentation for [help on changing the environment](https://deepesdl.readthedocs.io/en/latest/guide/jupyterlab/#python-environment-selection-of-the-jupyter-kerne).**\n",
    "\n",
    "\n",
    "<b>Bibliography</b>\n",
    "- Recognising ice damage from satellite imagery: [Lhermitte et al. (2020)](https://www.pnas.org/doi/abs/10.1073/pnas.1912890117) & [Lai et al. (2020)](https://www.nature.com/articles/s41586-020-2627-8)\n",
    "- U-Net: [Ronneberger et al. (2015)](https://arxiv.org/abs/1505.04597)\n",
    "- MobileNetV2 encoder: [Sandler et al. (2018)](https://arxiv.org/abs/1801.04381)\n",
    "- pix2pix cGAN: [Isola et al. (2017)](https://arxiv.org/abs/1611.07004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import gdown\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "from xcube.core.store import new_data_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8fd98a",
   "metadata": {},
   "source": [
    "Define the parameters needed to access the S3 storage. They are saved as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa1655-c5df-41fb-86e5-19105c4f8fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"]\n",
    "S3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"]\n",
    "S3_USER_STORAGE_BUCKET = os.environ[\"S3_USER_STORAGE_BUCKET\"]\n",
    "\n",
    "team_store = new_data_store(\n",
    "    data_store_id=\"s3\",\n",
    "    root=S3_USER_STORAGE_BUCKET,\n",
    "    storage_options=dict(\n",
    "        anon=False, key=S3_USER_STORAGE_KEY,\n",
    "        secret=S3_USER_STORAGE_SECRET\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db3ce9b-3f0c-4224-b85e-36ffeb669f5d",
   "metadata": {},
   "source": [
    "Define a plotting function to quickly show the Sentinel-1 imagery, the true ice damage model and the predicted damage model inferred by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89126144-f558-4b0c-b572-19944c1d570b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_imagery_and_damage(tiles_list: list[np.ndarray]) -> None:\n",
    "    \"\"\"\n",
    "    Create a figure to show imagery and damage model next to each other.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tiles_list : list[np.ndarray]\n",
    "        A list containing a number of arrays, maximum three: the Sentinel-1 imagery, the true damage model and the damage model predicted by the NN.\n",
    "    \"\"\"\n",
    "    title_list = [\"Input S1 imagery\", \"Input damage mask\", \"Predicted damage mask\"]\n",
    "    cmap_list = [\"Greys_r\", \"viridis\", \"viridis\"]\n",
    "    _, axs = plt.subplots(ncols=len(tiles_list), figsize=(15, 15))\n",
    "    for i in range(len(tiles_list)):\n",
    "        ax = axs[i]\n",
    "        ax.imshow(tiles_list[i][:, :, 0], cmap=cmap_list[i], vmin=0, vmax=2)\n",
    "        ax.set_title(title_list[i])\n",
    "        ax.set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc857bf4",
   "metadata": {},
   "source": [
    "Define the directories where to save the plots and the trained neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c6b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = Path.cwd() / \"plots\"\n",
    "plot_dir.mkdir(exist_ok=True)\n",
    "trained_nn_dir = Path.cwd() / \"trained_nn\"\n",
    "trained_nn_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360473cb-7c4a-4c45-ab18-be7129f6be11",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba214bd3-f780-4cef-ad5b-2266c9e8c76f",
   "metadata": {},
   "source": [
    "Load the data cube, containing the Sentinel-1 imagery, the ice damage mask and the grounded ice  (ice sheet) and ocean masks.\n",
    "\n",
    "Note that the step2 Jupyter Notebook retrieves only the imagery for a very narrow time window (5 days in October 2015), to provide with an illustrative example without consuming excessive computational resources. During the development of this use case, however, four larger datacubes were prepared, containing all the Sentinel-1 imagery data available during Q4 (Oct-Dec) of 2015 and 2016.\n",
    "\n",
    "In this example we mix data acquired during ascending and descending orbits. A potential straightforward expansion of this work would be to train the NN over the two datasets separately, to investigate whether there is a difference in performance depending on the orbit type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb725a2e-af57-44d5-88a1-c877586f4675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iceshelf_name = \"amery\"\n",
    "polarization_mode = \"SH\"\n",
    "polarization_type = \"HH\"\n",
    "\n",
    "masks_storage_path = f\"datacubes/{iceshelf_name}/damage_models_res_500m.zarr\"\n",
    "masks_xr = team_store.open_data(masks_storage_path)\n",
    "masks_xr = masks_xr.reindex(x=masks_xr.x[::-1])\n",
    "\n",
    "orbit_type = \"ascending\"\n",
    "time_period = \"2015_Q4\"\n",
    "imagery_ascending_2015Q4_storage_path = f\"datacubes/{iceshelf_name}/s1_imagery_res_500m_{polarization_mode}_{polarization_type}_{orbit_type.capitalize()}_{time_period}.zarr\"\n",
    "imagery_ascending_2015Q4_xr = team_store.open_data(imagery_ascending_2015Q4_storage_path)\n",
    "imagery_ascending_2015Q4_xr = imagery_ascending_2015Q4_xr.reindex(x=imagery_ascending_2015Q4_xr.x[::-1])\n",
    "\n",
    "orbit_type = \"descending\"\n",
    "time_period = \"2015_Q4\"\n",
    "imagery_descending_2015Q4_storage_path = f\"datacubes/{iceshelf_name}/s1_imagery_res_500m_{polarization_mode}_{polarization_type}_{orbit_type.capitalize()}_{time_period}.zarr\"\n",
    "imagery_descending_2015Q4_xr = team_store.open_data(imagery_descending_2015Q4_storage_path)\n",
    "imagery_descending_2015Q4_xr = imagery_descending_2015Q4_xr.reindex(x=imagery_descending_2015Q4_xr.x[::-1])\n",
    "\n",
    "orbit_type = \"ascending\"\n",
    "time_period = \"2016_Q4\"\n",
    "imagery_ascending_2016Q4_storage_path = f\"datacubes/{iceshelf_name}/s1_imagery_res_500m_{polarization_mode}_{polarization_type}_{orbit_type.capitalize()}_{time_period}.zarr\"\n",
    "imagery_ascending_2016Q4_xr = team_store.open_data(imagery_ascending_2016Q4_storage_path)\n",
    "imagery_ascending_2016Q4_xr = imagery_ascending_2016Q4_xr.reindex(x=imagery_ascending_2016Q4_xr.x[::-1])\n",
    "\n",
    "orbit_type = \"descending\"\n",
    "time_period = \"2016_Q4\"\n",
    "imagery_descending_2016Q4_storage_path = f\"datacubes/{iceshelf_name}/s1_imagery_res_500m_{polarization_mode}_{polarization_type}_{orbit_type.capitalize()}_{time_period}.zarr\"\n",
    "imagery_descending_2016Q4_xr = team_store.open_data(imagery_descending_2016Q4_storage_path)\n",
    "imagery_descending_2016Q4_xr = imagery_descending_2016Q4_xr.reindex(x=imagery_descending_2016Q4_xr.x[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aa6bcc-fe17-4a2b-a276-69e25225c470",
   "metadata": {},
   "source": [
    "Tile the data and keep only the ones within the ice shelf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ee81a-a0c8-478a-9369-0d842865d86a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tile_array(data_xr: xr.DataArray, tile_size: int):\n",
    "    num_tiles_along_y = data_xr.sizes[\"y\"] // tile_size\n",
    "    num_tiles_along_x = data_xr.sizes[\"x\"] // tile_size\n",
    "    num_times = data_xr.sizes[\"time\"]\n",
    "\n",
    "    data_array = data_xr.isel(\n",
    "        x=slice(num_tiles_along_x * tile_size),\n",
    "        y=slice(num_tiles_along_y * tile_size)\n",
    "    ).to_numpy()\n",
    "    tiled_data_array = data_array.reshape(\n",
    "        num_times, num_tiles_along_y, tile_size, num_tiles_along_x, tile_size\n",
    "    ).swapaxes(2, 3)\n",
    "    return tiled_data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10352dfb-4b77-4d3d-ba62-69f379e0527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tiles_to_use(\n",
    "    imagery: np.ndarray,\n",
    "    damage_mask: np.ndarray,\n",
    "    ocean_mask: np.ndarray,\n",
    "    grounded_ice_mask: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    num_scenes = imagery.shape[0]\n",
    "    bool_full_tiles_imagery = np.invert(np.isnan(imagery).any(axis=4).any(axis=3))\n",
    "    bool_tiles_overlapping_ice_shelf = damage_mask.sum(axis=4).sum(axis=3).astype(bool)\n",
    "    bool_tiles_overlapping_ice_shelf = np.repeat(\n",
    "        bool_tiles_overlapping_ice_shelf, num_scenes, axis=0)\n",
    "    bool_combined = bool_tiles_overlapping_ice_shelf & bool_full_tiles_imagery\n",
    "\n",
    "    filtered_imagery = imagery[bool_combined]\n",
    "    filtered_damage_mask = np.repeat(damage_mask, num_scenes, axis=0)[bool_combined]\n",
    "    filtered_ocean_mask = np.repeat(ocean_mask, num_scenes, axis=0)[bool_combined]\n",
    "    filtered_grounded_ice_mask = np.repeat(grounded_ice_mask, num_scenes, axis=0)[bool_combined]\n",
    "\n",
    "    return np.stack((\n",
    "        filtered_imagery, filtered_ocean_mask,\n",
    "        filtered_grounded_ice_mask, filtered_damage_mask),\n",
    "        axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e36da-8de4-4002-a290-66f9fc0032f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NB: this step can take up to a few minutes to complete\n",
    "\n",
    "tile_size = 128\n",
    "tiled_damage_mask_surface_temp = tile_array(masks_xr.damage_class_surface_temp, tile_size)\n",
    "tiled_damage_mask_depth_temp = tile_array(masks_xr.damage_class_depth_temp, tile_size)\n",
    "tiled_damage_mask_uniform_temp = tile_array(masks_xr.damage_class_uniform_temp, tile_size)\n",
    "tiled_ocean_mask = tile_array(masks_xr.ocean_mask, tile_size)\n",
    "tiled_grounded_ice_mask = tile_array(masks_xr.grounded_ice_mask, tile_size)\n",
    "tiled_imagery_ascending_2015Q4 = tile_array(imagery_ascending_2015Q4_xr.s1_imagery, tile_size)\n",
    "tiled_imagery_descending_2015Q4 = tile_array(imagery_descending_2015Q4_xr.s1_imagery, tile_size)\n",
    "tiled_imagery_ascending_2016Q4 = tile_array(imagery_ascending_2016Q4_xr.s1_imagery, tile_size)\n",
    "tiled_imagery_descending_2016Q4 = tile_array(imagery_descending_2016Q4_xr.s1_imagery, tile_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b52bee",
   "metadata": {},
   "source": [
    "For this example we use the damage model obtained by assuming a constant vertical temperature gradient between the visible surface and the bottom of the ice shelf - the _depth_temp_ mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8495b-7496-4092-8362-2c96cea6a7ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_data_ascending_2015Q4 = select_tiles_to_use(\n",
    "    tiled_imagery_ascending_2015Q4,\n",
    "    tiled_damage_mask_depth_temp,\n",
    "    tiled_ocean_mask,\n",
    "    tiled_grounded_ice_mask)\n",
    "filtered_data_descending_2015Q4 = select_tiles_to_use(\n",
    "    tiled_imagery_descending_2015Q4,\n",
    "    tiled_damage_mask_depth_temp,\n",
    "    tiled_ocean_mask,\n",
    "    tiled_grounded_ice_mask)\n",
    "filtered_data_ascending_2016Q4 = select_tiles_to_use(\n",
    "    tiled_imagery_ascending_2016Q4,\n",
    "    tiled_damage_mask_depth_temp,\n",
    "    tiled_ocean_mask,\n",
    "    tiled_grounded_ice_mask)\n",
    "filtered_data_descending_2016Q4 = select_tiles_to_use(\n",
    "    tiled_imagery_descending_2016Q4,\n",
    "    tiled_damage_mask_depth_temp,\n",
    "    tiled_ocean_mask,\n",
    "    tiled_grounded_ice_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27921ed-d85b-4a26-acc1-6f237ecc6918",
   "metadata": {},
   "source": [
    "Stack imagery and ocean / grounded ice masks together, to create the input with three channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e4327-6ad7-4480-8a75-ca872b9baf29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data = np.concatenate((\n",
    "    filtered_data_ascending_2015Q4, filtered_data_descending_2015Q4,\n",
    "    filtered_data_ascending_2016Q4, filtered_data_descending_2016Q4),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "input_data = all_data[:3]\n",
    "input_data = np.swapaxes(input_data, 0, 1)\n",
    "input_data = np.swapaxes(input_data, 1, 3)\n",
    "\n",
    "damage_model = all_data[3].astype(np.int8)\n",
    "damage_model = np.expand_dims(damage_model, axis=0)\n",
    "damage_model = np.swapaxes(damage_model, 0, 1)\n",
    "damage_model = np.swapaxes(damage_model, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f6a8a0-bd20-41c4-b77d-146f90d5bb98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(input_data.shape)\n",
    "print(damage_model.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597b987",
   "metadata": {},
   "source": [
    "Let's have a look at how these tiles look like. Let's plot the three datasets passed as input data (imagery and binary masks) and the corresponding damage model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341378a-3c94-4052-a123-4883b8b828f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = 510\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True)\n",
    "axes[0, 0].imshow(input_data[index, :, :, 0], origin=\"lower\", cmap=\"Greys_r\")\n",
    "axes[0, 1].imshow(damage_model[index], origin=\"lower\", cmap=\"viridis\")\n",
    "axes[1, 0].imshow(input_data[index, :, :, 1], origin=\"lower\", cmap=\"viridis\")\n",
    "axes[1, 1].imshow(input_data[index, :, :, 2], origin=\"lower\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ed4b3-5a52-4133-aa34-81cb3d10922a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = 130\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True)\n",
    "axes[0, 0].imshow(input_data[index, :, :, 0], origin=\"lower\", cmap=\"Greys_r\")\n",
    "axes[0, 1].imshow(damage_model[index], origin=\"lower\", cmap=\"viridis\")\n",
    "axes[1, 0].imshow(input_data[index, :, :, 1], origin=\"lower\", cmap=\"viridis\")\n",
    "axes[1, 1].imshow(input_data[index, :, :, 2], origin=\"lower\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dea26f-6e71-40f0-9888-a71a693f045e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Split data between training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a901608-4129-4975-b0de-aa29cf955418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)    # for reproducibility\n",
    "\n",
    "number_tiles = input_data.shape[0]\n",
    "percentage_split_for_training = 0.8\n",
    "boolean_split_train_test = np.random.rand(number_tiles) < percentage_split_for_training\n",
    "\n",
    "train_input_data = input_data[boolean_split_train_test]\n",
    "train_damage_model = damage_model[boolean_split_train_test]\n",
    "test_input_data = input_data[np.invert(boolean_split_train_test)]\n",
    "test_damage_model = damage_model[np.invert(boolean_split_train_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d78e9f-d046-4fae-a462-eaa8b0a77c70",
   "metadata": {},
   "source": [
    "Convert the data into tensorflow format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280ad51-5e5b-45c6-a712-6961c2bc22a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_input_data, train_damage_model))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_input_data, test_damage_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850e661-0348-44dd-8e69-2265f930c0b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Input pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b20bad-3e04-450d-8f65-2a3d33db8871",
   "metadata": {},
   "source": [
    "Define the data augmentation: reduce overfitting and, in general, improve the performance of the CNN by flipping/deforming the original tiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7cfe15-6ec1-4c10-9592-9b12d55b3413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Augment(tf.keras.layers.Layer):\n",
    "    def __init__(self, seed=42):\n",
    "        super().__init__()\n",
    "        # both use the same seed, so they\"ll make the same random changes\n",
    "        self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "        self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "\n",
    "    def call(self, inputs, labels):\n",
    "        inputs = self.augment_inputs(inputs)\n",
    "        labels = self.augment_labels(labels)\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3631b673-f596-4317-b7e4-ab0de4a9a2c8",
   "metadata": {},
   "source": [
    "Set the parameters for training.\n",
    "<br>\n",
    "- ```TRAIN_LENGTH``` is the number of tiles used for the training.\n",
    "- ```BATCH_SIZE``` is the number of tiles used in each batch: when ```TRAIN_LENGTH``` is very high, or the images are very large, the batch needs to set as a fraction of the total training length (e.g. 1/4).\n",
    "- ```BUFFER_SIZE``` sets the extent to which that training data is shuffled between epochs and should be at least equal to ```TRAIN_LENGTH```.\n",
    "- ```STEPS_PER_EPOCH``` is the number of steps needed to pass over the entire training dataset, given ```BATCH_SIZE```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a1e20-2f91-4240-8215-71d2e4cb0e75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = boolean_split_train_test.sum()\n",
    "BATCH_SIZE = TRAIN_LENGTH // 3\n",
    "BUFFER_SIZE = boolean_split_train_test.sum() * 2\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ee42e-2ea1-4b41-9882-d72cbdb7fc38",
   "metadata": {},
   "source": [
    "Combine everything together to set the input pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13572f1-8aea-44a3-99b6-592789095d61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_batches = (\n",
    "    train_dataset\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .repeat()\n",
    "    .map(Augment())\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "test_batches = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9b1a88-047c-473e-81e7-cb40de9ce25b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for images, masks in train_batches.take(1):\n",
    "    sample_image, sample_mask = images[0], masks[0]\n",
    "    display_list = [sample_image, sample_mask]\n",
    "    show_imagery_and_damage(display_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c560196-116b-4482-984f-f15c1d9d2e19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for images, masks in test_batches.take(1):\n",
    "    sample_image, sample_mask = images[3], masks[3]\n",
    "    display_list = [sample_image, sample_mask]\n",
    "    show_imagery_and_damage(display_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be33d6b2-3246-4dc0-b8d5-a14bcb3a61e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for images, masks in train_batches.take(1):\n",
    "    sample_image, sample_mask = images[0], masks[0]\n",
    "    display_list = [sample_image, sample_mask]\n",
    "    show_imagery_and_damage(display_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb0675-dbf2-49d6-812d-e6fbfef20534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for images, masks in test_batches.take(1):\n",
    "    sample_image, sample_mask = images[3], masks[3]\n",
    "    display_list = [sample_image, sample_mask]\n",
    "    show_imagery_and_damage(display_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d57af1-de0c-4b07-a2b6-01f3a0f667d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for images, masks in train_batches.take(1):\n",
    "    sample_image, sample_mask = images[0], masks[0]\n",
    "    display_list = [sample_image, sample_mask]\n",
    "    show_imagery_and_damage(display_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b6b07-8b19-41a3-bd3e-6e5cb773f6a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for images, masks in test_batches.take(1):\n",
    "    sample_image, sample_mask = images[3], masks[3]\n",
    "    display_list = [sample_image, sample_mask]\n",
    "    show_imagery_and_damage(display_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac2c99-d51a-4cdf-ad95-fdf4aca27107",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e68fc-e3b7-4069-9ba9-7c80f78fecdf",
   "metadata": {},
   "source": [
    "The encoder (downsampler) uses a pretrained MobileNetV2 model to reduce the number of trainable parameters. In particular, the activations of some intermediate layers are employed. See [Sandler et al. (2018)](https://arxiv.org/abs/1801.04381) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ea843-b326-46be-95ca-3f67c5db8c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=[128, 128, 3],\n",
    "    include_top=False)\n",
    "\n",
    "layer_names = [\n",
    "    \"block_1_expand_relu\",   # 64x64\n",
    "    \"block_3_expand_relu\",   # 32x32\n",
    "    \"block_6_expand_relu\",   # 16x16\n",
    "    \"block_13_expand_relu\",  # 8x8\n",
    "    \"block_16_project\",      # 4x4\n",
    "]\n",
    "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "down_stack = tf.keras.Model(\n",
    "    inputs=base_model.input,\n",
    "    outputs=base_model_outputs)\n",
    "\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cbd29c-55da-4b54-8c62-ed3d2d0ccf5c",
   "metadata": {},
   "source": [
    "The decoder (upsampler) uses the blocks already implemented in the pix2pix Tensorflow example. See [Isola et al. (2017)](https://arxiv.org/abs/1611.07004) for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b7f26-ac1c-4125-9872-eaa8ebb7287e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f02cb-626e-4b94-a982-f54459930f41",
   "metadata": {},
   "source": [
    "The two parts are combined together into the final U-Net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa6c5b-1353-4afa-8362-a58ccd9a1f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unet_model(output_channels: int):\n",
    "    inputs = tf.keras.layers.Input(shape=[128, 128, output_channels])\n",
    "\n",
    "    # downsampling through the model\n",
    "    skips = down_stack(inputs)\n",
    "    x = skips[-1]\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # upsampling\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        concat = tf.keras.layers.Concatenate()\n",
    "        x = concat([x, skip])\n",
    "\n",
    "    # last layer of the model\n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=output_channels, kernel_size=3, strides=2,\n",
    "        padding=\"same\")  #64x64 -> 128x128\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66eee3-da5a-4875-aff2-0a60726611b8",
   "metadata": {},
   "source": [
    "Set the final parameters and compile the model.\n",
    "<br>\n",
    "```OUTPUT_CLASSES``` represents the number of classes in the damage mask, currently three: (0) outside the ice-shelf, (1) damaged ice, (2) intact ice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d51bb02-3779-4d7e-add0-3b4b53eb1901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_CLASSES = 3\n",
    "\n",
    "model = unet_model(output_channels=OUTPUT_CLASSES)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6bf7df-af0e-4ca5-964a-e0909f7d6a0b",
   "metadata": {},
   "source": [
    "Visualise the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ab13f-908b-43fa-ba8e-992c5bb69270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_model_scheme_plot = plot_dir / \"unet_model_128x128.png\"\n",
    "tf.keras.utils.plot_model(\n",
    "    model, show_shapes=True, expand_nested=False,\n",
    "    to_file=path_to_model_scheme_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419773d-e895-4db2-962c-88c2886541a2",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9793601-a227-4351-8edf-a90b51f3b561",
   "metadata": {
    "tags": []
   },
   "source": [
    "Functions that show the learning progress, while the model is being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c007ee-2ccf-48a9-b2d1-0ec9c9d5aad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# determine predicted damage mask\n",
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.math.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[0]\n",
    "\n",
    "# display predicted mask\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            show_imagery_and_damage([image[0], mask[0], create_mask(pred_mask)])\n",
    "    else:\n",
    "        show_imagery_and_damage([\n",
    "            sample_image, sample_mask,\n",
    "            create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165a669-11a7-4dea-88ca-9b48386660c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is before starting any model training\n",
    "show_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d059f77-443f-4951-8b91-b5d7b3441694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        show_predictions()\n",
    "        print (\"\\nSample Prediction after epoch {}\\n\".format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b459c7b-a373-46fe-ac85-508dc6182715",
   "metadata": {},
   "source": [
    "Run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15b35e-0dd7-4213-8652-e22c4de8d206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100    # (currently no further improvements after 100 epochs)\n",
    "VAL_SUBSPLITS = 1\n",
    "# VALIDATION_STEPS = 23//BATCH_SIZE//VAL_SUBSPLITS\n",
    "VALIDATION_STEPS = 1\n",
    "\n",
    "model_history = model.fit(train_batches, epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=test_batches,\n",
    "                          callbacks=[DisplayCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bf5e7-92b6-43b7-b7ca-bfd8bdb4f026",
   "metadata": {},
   "source": [
    "### Plot the learning curves of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e27f74-9929-4fcf-b9ce-fdbeb898d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_array = np.array(model_history.epoch)[1::2]\n",
    "loss_array = np.array(model_history.history[\"loss\"])[1::2]\n",
    "validation_loss_array = np.array(model_history.history[\"val_loss\"])[1::2]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epoch_array, loss_array, ls=\"-\", c=\"C0\", lw=3, label=\"Training loss\")\n",
    "ax.plot(epoch_array, validation_loss_array, ls=\"--\", c=\"C1\", lw=3, label=\"Validation loss\")\n",
    "ax.set_title(\"Training and Validation Loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss Value\")\n",
    "ax.set_ylim([0, 2])\n",
    "leg = ax.legend()\n",
    "leg.get_frame().set_edgecolor(\"k\")\n",
    "leg.get_frame().set_linewidth(2)\n",
    "plt.savefig(plot_dir / \"depth_temp_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89b146-3b48-475e-a585-aeffd380d0df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch_array = np.array(model_history.epoch)[1::2]\n",
    "accuracy_array = np.array(model_history.history[\"accuracy\"])[1::2]\n",
    "validation_accuracy_array = np.array(model_history.history[\"val_accuracy\"])[1::2]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epoch_array, accuracy_array, ls=\"-\", c=\"C0\", lw=3, label=\"Training accuracy\")\n",
    "ax.plot(epoch_array, validation_accuracy_array, ls=\"--\", c=\"C1\", lw=3, label=\"Validation accuracy\")\n",
    "ax.set_title(\"Training and Validation Accuracy\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Accuracy Value\")\n",
    "ax.set_ylim([0, 1])\n",
    "leg = ax.legend()\n",
    "leg.get_frame().set_edgecolor(\"k\")\n",
    "leg.get_frame().set_linewidth(2)\n",
    "plt.savefig(plot_dir / \"depth_temp_accuracy.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0529989-f312-4f92-a491-b1050a390791",
   "metadata": {},
   "source": [
    "Save the trained model (in two alternative formats), so that it can be used without the need to rerun the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3a457-a8fc-4a9a-8a17-e47a6c1e7f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(trained_nn_dir / \"depth_temp.keras\")\n",
    "model.save(trained_nn_dir / \"depth_temp.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37fb598-70e3-445b-ad19-9dc1a183691e",
   "metadata": {},
   "source": [
    "# Use trained model to make predictions\n",
    "\n",
    "In this section we briefly show how to use the trained Neural Networks models.<br>\n",
    "This section can be run without the section above, as the models have been pre-trained and saved to Google Drive.<br>\n",
    "If the section above has already been run, then the one saved on Google Drive is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2127a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = trained_nn_dir / \"pretrained_models.zip\"\n",
    "\n",
    "# download the zip from Google Drive\n",
    "gdown.download(id=os.environ[\"AMERY_TRAINED_NNS_GDRIVE_ID\"], output=zip_file_path.as_posix())\n",
    "# unzip the compressed file\n",
    "with zipfile.ZipFile(zip_file_path, \"r\") as zip_content:\n",
    "    zip_content.extractall(trained_nn_dir / zip_file_path.stem)\n",
    "# remove the compressed file\n",
    "zip_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff069e-e1c0-4da8-955d-87cd5785c579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model(\n",
    "    trained_nn_dir / zip_file_path.stem / \"depth_temp.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fd7c57-879e-450c-abac-d1f4367fc9ae",
   "metadata": {},
   "source": [
    "Apply it to some imagery data that was not used during the training phase. This allows for example to infer an ice damage model for a different time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e49896-7539-4495-98dd-0f562b985bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for image, mask in test_batches.take(1):\n",
    "    index = int(np.random.rand() * len(image))\n",
    "    pred_mask = reconstructed_model.predict(image[index][tf.newaxis, ...])\n",
    "    show_imagery_and_damage([image[index], mask[index], create_mask(pred_mask)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef13fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
