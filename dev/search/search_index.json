{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"about/","title":"Overview","text":""},{"location":"about/#about-deepesdl","title":"About DeepESDL","text":"<p>Welcome to the online documentation of DeepESDL \u2013 ESA\u2019s Deep Earth System Data Laboratory, a platform providing analysis-ready data cube in a powerful, virtual laboratory to the Earth Science research community. DeepESDL offers a full suite of services to facilitate data exploitation, share data and source code, and publish results. Special emphasize is put on improving the support for machine learning and artificial intelligence approaches, which includes the preparation of AI-ready datasets, providing a programming environment with relevant libraries and packages, and the resources to execute processing pipelines. For more information and access to the lab please visit the DeepESDL website.</p> <p>The DeepESDL documentation contains:</p> <ul> <li>The User Guide for all DeepESDL services.   This is the starting point for new users.</li> <li>An overview of public, pre-generated data cubes   available in DeepESDL with detailed metadata and specifications.</li> <li>A description of DeepESDL\u2019s architecture.</li> </ul>"},{"location":"become_user/","title":"Become a User","text":""},{"location":"become_user/#via-esa-sponsorship","title":"via ESA Sponsorship","text":"<p>To become a user of DeepESD apply for an ESA sponsorship via the Network of Ressources (NoR).</p> <p>Take the following steps for the application process:</p> <ol> <li>Search for DeepESDL in the NoR portfolio.</li> <li>Read the provided information about available VMs and other services.</li> <li>Use the Prizing Wizard to configure your application.</li> <li>Apply.</li> </ol> <p>Detailed information about the application process can be found in the NoR Portal. For more support write an email to <code>esdl-support@brockmann-consult.de</code>.</p>"},{"location":"become_user/#without-esa-sponsorship","title":"without ESA Sponsorship","text":"<p>To use DeepESDL without an ESA Sponsorship write an email to <code>esdl-support@brockmann-consult.de</code> with your requirements and the colleagues from Brockmann Consult GmbH will come back to you with an offer.</p>"},{"location":"datasets/ESDC/","title":"Earth System Data Cube","text":""},{"location":"datasets/ESDC/#earth-system-data-cube-esdc-v301","title":"Earth System Data Cube (ESDC) v3.0.1","text":""},{"location":"datasets/ESDC/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('esdc-8d-0.25deg-1x720x1440-3.0.1.zarr')\n</code></pre>"},{"location":"datasets/ESDC/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/ESDC/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180.0 to 180.0 Bounding box latitude (\u00b0) -90.0 to 90.0 Time range 1979-01-05 to 2021-12-31 Publisher DeepESDL Team <p>Click here for full dataset metadata.</p>"},{"location":"datasets/ESDC/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units aerosol_optical_thickness_550 Aerosol Optical Thickness at 550 nm 1 air_temperature_2m Mean Air Temperature at 2 m \u00b0C bare_soil_evaporation Bare Soil Evaporation mm d^-1 burnt_area Monthly Burnt Area hectares cot Cloud Optical Thickness 1 cth Cloud Top Height km ctt Cloud Top Temperature K evaporation Actual Evaporation mm d^-1 evaporation_era5 Evaporation mm d^-1 evaporative_stress Evaporative Stress 1 gross_primary_productivity Gross Primary Productivity g C m^-2 d^-1 interception_loss Interception Loss mm d^-1 kndvi Kernel Normalized Difference Vegetation Index 1 latent_energy Latent Energy MJ m^-2 d^-1 max_air_temperature_2m Maximum Air Temperature at 2 m \u00b0C min_air_temperature_2m Minimum Air Temperature at 2 m \u00b0C nbar_blue Nadir BRDF Adjusted Reflectance of Band 3 (blue) 1 nbar_green Nadir BRDF Adjusted Reflectance of Band 4 (green) 1 nbar_nir Nadir BRDF Adjusted Reflectance of Band 2 (NIR) 1 nbar_red Nadir BRDF Adjusted Reflectance of Band 1 (red) 1 nbar_swir1 Nadir BRDF Adjusted Reflectance of Band 5 (SWIR1) 1 nbar_swir2 Nadir BRDF Adjusted Reflectance of Band 6 (SWIR2) 1 nbar_swir3 Nadir BRDF Adjusted Reflectance of Band 7 (SWIR3) 1 ndvi Normalized Difference Vegetation Index 1 net_ecosystem_exchange Net Ecosystem Exchange g C m^-2 d^-1 net_radiation Net Radiation MJ m^-2 d^-1 nirv Near Infrared Reflectance of Vegetation 1 open_water_evaporation Open-water Evaporation mm d^-1 potential_evaporation Potential Evaporation mm d^-1 precipitation_era5 Total Precipitation mm d^-1 radiation_era5 Surface Net Solar Radiation J m^-2 root_moisture Root-zone Soil Moisture mm d^-1 sensible_heat Sensible Heat MJ m^-2 d^-1 sif_gome2_jj Downscaled Daily Corrected Sun-Induced Chlorophyll Fluorescence at 740 nm m W m^-2 sr^-1 nm^-1 sif_gome2_pk Downscaled Daily Corrected Sun-Induced Chlorophyll Fluorescence at 740 nm m W m^-2 sr^-1 nm^-1 sif_gosif Sun-Induced Chlorophyll Fluorescence at 757 nm W m^-2 sr^-1 um^-1 sif_rtsif Sun-Induced Chlorophyll Fluorescence at 740 nm m W m^-2 sr^-1 um^-1 sm Volumetric Soil Moisture m^3 m^-3 snow_sublimation Snow Sublimation mm d^-1 surface_moisture Surface Soil Moisture mm d^-1 terrestrial_ecosystem_respiration Terrestrial Ecosystem Respiration g C m^-2 d^-1 transpiration Transpiration mm d^-1"},{"location":"datasets/ESDC/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/ESDC/#aerosol_optical_thickness_550","title":"aerosol_optical_thickness_550","text":"Field Value acknowledgment ESA Aerosol Climate Change Initiative (Aerosol_cci) date_modified 2022-10-13 03:15:18.312024 description ESA Aerosol Climate Change Initiative (Aerosol_cci): Level 3 aerosol products from AATSR (ensemble product), Version 2.6 geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Aerosol Optical Thickness at 550 nm original_add_offset 0.0 original_name AOD550_mean original_scale_factor 1.0 processing_steps Loading data using the cciodp xcube data store, Resampling by 8-day mean, Upsampling to 0.25 degrees using nearest neighbor project DeepESDL references https://doi.org/10.5194/amt-6-1919-2013 reported_day 5.0 source https://catalogue.ceda.ac.uk/uuid/c183044b88734442b6d37f5c4f6b0092 standard_name atmosphere_optical_thickness_due_to_ambient_aerosol temporal_resolution 8D time_coverage_end 2012-04-10T00:00:00.000000000 time_coverage_start 2002-05-21T00:00:00.000000000 time_period 8D units 1"},{"location":"datasets/ESDC/#air_temperature_2m","title":"air_temperature_2m","text":"Field Value acknowledgment ERA5 hourly data on single levels from 1959 to present date_modified 2022-11-04 15:41:36.233472 description ERA5 Reanalysis Products geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Mean Air Temperature at 2 m original_add_offset 0.0 original_name t2m original_scale_factor 1.0 processing_steps Merging nc files, Resampling by daily mean, Converting to \u00b0C from K, Resampling by 8-day mean, Resampling to 0.25 degrees using bilinear interpolation project DeepESDL references https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation reported_day 5.0 source https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels standard_name mean_air_temperature_2m temporal_resolution 8D time_coverage_end 2021-12-27T00:00:00.000000000 time_coverage_start 1979-01-01T00:00:00.000000000 time_period 8D units \u00b0C"},{"location":"datasets/ESDC/#bare_soil_evaporation","title":"bare_soil_evaporation","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Bare Soil Evaporation original_add_offset 0.0 original_name Eb original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name bare_soil_evaporation temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#burnt_area","title":"burnt_area","text":"Field Value acknowledgment https://www.globalfiredata.org/ date_modified 2022-10-13 14:55:35.002779 description Global Fire Emissions Database (GFED) 4 Monthly Burnt Area geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Monthly Burnt Area original_add_offset 0.0 original_name burnt_area original_scale_factor 0.01 processing_steps Merging hdf files, Resampling by 8-day nearest neighbor project DeepESDL references https://doi.org/10.1002/jgrg.20042 reported_day 5.0 source https://www.globalfiredata.org/ standard_name burnt_area temporal_resolution 8D time_coverage_end 2016-12-30T00:00:00.000000000 time_coverage_start 1995-06-06T00:00:00.000000000 time_period 8D units hectares"},{"location":"datasets/ESDC/#cot","title":"cot","text":"Field Value acknowledgment ESA Cloud Climate Change Initiative (Cloud_cci) date_modified 2022-11-04 13:33:18.450458 description ESA Cloud Climate Change Initiative (Cloud_cci): MODIS-TERRA monthly gridded cloud properties, version 2.0 geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Cloud Optical Thickness original_add_offset 0.0 original_name cot original_scale_factor 1.0 processing_steps Loading data using the cciodp xcube data store, Resampling by 8-day nearest neighbor, Upsampling to 0.25 degrees using nearest neighbor project DeepESDL references https://public.satproj.klima.dwd.de/data/ESA_Cloud_CCI/CLD_PRODUCTS/v2.0/DOIs/DOI_ESA_Cloud_cci_MODIS-Terra_v2.0_landingpage.html, https://dap.ceda.ac.uk/neodc/esacci/cloud/docs/DataSet_Desc_ESA_Cloud_cci_CC4CL_1.5.pdf reported_day 5.0 source https://catalogue.ceda.ac.uk/uuid/f1ab07b5292f4813bd3090b51d270aa8 standard_name atmosphere_optical_thickness_due_to_cloud temporal_resolution 8D time_coverage_end 2014-12-15T00:00:00.000000000 time_coverage_start 2000-02-22T00:00:00.000000000 time_period 8D units 1"},{"location":"datasets/ESDC/#cth","title":"cth","text":"Field Value acknowledgment ESA Cloud Climate Change Initiative (Cloud_cci) date_modified 2022-11-04 13:33:18.450458 description ESA Cloud Climate Change Initiative (Cloud_cci): MODIS-TERRA monthly gridded cloud properties, version 2.0 geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Cloud Top Height original_add_offset 0.0 original_name cth original_scale_factor 1.0 processing_steps Loading data using the cciodp xcube data store, Resampling by 8-day nearest neighbor, Upsampling to 0.25 degrees using nearest neighbor project DeepESDL references https://public.satproj.klima.dwd.de/data/ESA_Cloud_CCI/CLD_PRODUCTS/v2.0/DOIs/DOI_ESA_Cloud_cci_MODIS-Terra_v2.0_landingpage.html, https://dap.ceda.ac.uk/neodc/esacci/cloud/docs/DataSet_Desc_ESA_Cloud_cci_CC4CL_1.5.pdf reported_day 5.0 source https://catalogue.ceda.ac.uk/uuid/f1ab07b5292f4813bd3090b51d270aa8 standard_name cloud_top_altitude temporal_resolution 8D time_coverage_end 2014-12-15T00:00:00.000000000 time_coverage_start 2000-02-22T00:00:00.000000000 time_period 8D units km"},{"location":"datasets/ESDC/#ctt","title":"ctt","text":"Field Value acknowledgment ESA Cloud Climate Change Initiative (Cloud_cci) date_modified 2022-11-04 13:33:18.450458 description ESA Cloud Climate Change Initiative (Cloud_cci): MODIS-TERRA monthly gridded cloud properties, version 2.0 geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Cloud Top Temperature original_add_offset 0.0 original_name ctt original_scale_factor 1.0 processing_steps Loading data using the cciodp xcube data store, Resampling by 8-day nearest neighbor, Upsampling to 0.25 degrees using nearest neighbor project DeepESDL references https://public.satproj.klima.dwd.de/data/ESA_Cloud_CCI/CLD_PRODUCTS/v2.0/DOIs/DOI_ESA_Cloud_cci_MODIS-Terra_v2.0_landingpage.html, https://dap.ceda.ac.uk/neodc/esacci/cloud/docs/DataSet_Desc_ESA_Cloud_cci_CC4CL_1.5.pdf reported_day 5.0 source https://catalogue.ceda.ac.uk/uuid/f1ab07b5292f4813bd3090b51d270aa8 standard_name air_temperature_at_cloud_top temporal_resolution 8D time_coverage_end 2014-12-15T00:00:00.000000000 time_coverage_start 2000-02-22T00:00:00.000000000 time_period 8D units K"},{"location":"datasets/ESDC/#evaporation","title":"evaporation","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Actual Evaporation original_add_offset 0.0 original_name E original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name actual_evaporation temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#evaporation_era5","title":"evaporation_era5","text":"Field Value acknowledgment ERA5 hourly data on single levels from 1959 to present date_modified 2022-11-04 15:41:36.233472 description ERA5 Reanalysis Products geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Evaporation original_add_offset 0.0 original_name e original_scale_factor 1.0 processing_steps Merging nc files, Resampling by daily sum, Converting to mm from m, Resampling by 8-day mean, Resampling to 0.25 degrees using bilinear interpolation project DeepESDL references https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation reported_day 5.0 source https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels standard_name lwe_thickness_of_water_evaporation_amount temporal_resolution 8D time_coverage_end 2021-12-27T00:00:00.000000000 time_coverage_start 1979-01-01T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#evaporative_stress","title":"evaporative_stress","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Evaporative Stress original_add_offset 0.0 original_name S original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name evaporative_stress temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units 1"},{"location":"datasets/ESDC/#gross_primary_productivity","title":"gross_primary_productivity","text":"Field Value acknowledgment FLUXCOM date_modified 2022-10-17 22:14:30.401506 description FLUXCOM geospatial_lat_max 89.87499928049999 geospatial_lat_min -89.8750000005 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87499856049996 geospatial_lon_min -179.8750000005 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Gross Primary Productivity original_add_offset 0.0 original_name GPP original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.5194/bg-13-4291-2016, https://doi.org/10.1038/s41597-019-0076-8 reported_day 5.0 source https://www.fluxcom.org/ standard_name gross_primary_productivity_of_carbon temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 2001-01-05T00:00:00.000000000 time_period 8D units g C m^-2 d^-1"},{"location":"datasets/ESDC/#interception_loss","title":"interception_loss","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Interception Loss original_add_offset 0.0 original_name Ei original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name interception_loss temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#kndvi","title":"kndvi","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Kernel Normalized Difference Vegetation Index original_add_offset 0.0 original_name kNDVI original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.1126/sciadv.abc7447, https://github.com/awesome-spectral-indices/awesome-spectral-indices, https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://github.com/awesome-spectral-indices/spyndex, https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name kNDVI temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1"},{"location":"datasets/ESDC/#latent_energy","title":"latent_energy","text":"Field Value acknowledgment FLUXCOM date_modified 2022-10-17 22:14:30.401506 description FLUXCOM geospatial_lat_max 89.87499928049999 geospatial_lat_min -89.8750000005 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87499856049996 geospatial_lon_min -179.8750000005 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Latent Energy original_add_offset 0.0 original_name LE original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.5194/bg-13-4291-2016, https://doi.org/10.1038/s41597-019-0076-8 reported_day 5.0 source https://www.fluxcom.org/ standard_name surface_upward_latent_heat_flux temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 2001-01-05T00:00:00.000000000 time_period 8D units MJ m^-2 d^-1"},{"location":"datasets/ESDC/#max_air_temperature_2m","title":"max_air_temperature_2m","text":"Field Value acknowledgment ERA5 hourly data on single levels from 1959 to present date_modified 2022-11-04 15:41:36.233472 description ERA5 Reanalysis Products geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Maximum Air Temperature at 2 m original_add_offset 0.0 original_name t2m_max original_scale_factor 1.0 processing_steps Merging nc files, Resampling by daily max, Converting to \u00b0C from K, Resampling by 8-day max, Resampling to 0.25 degrees using bilinear interpolation project DeepESDL references https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation reported_day 5.0 source https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels standard_name max_air_temperature_2m temporal_resolution 8D time_coverage_end 2021-12-27T00:00:00.000000000 time_coverage_start 1979-01-01T00:00:00.000000000 time_period 8D units \u00b0C"},{"location":"datasets/ESDC/#min_air_temperature_2m","title":"min_air_temperature_2m","text":"Field Value acknowledgment ERA5 hourly data on single levels from 1959 to present date_modified 2022-11-04 15:41:36.233472 description ERA5 Reanalysis Products geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Minimum Air Temperature at 2 m original_add_offset 0.0 original_name t2m_min original_scale_factor 1.0 processing_steps Merging nc files, Resampling by daily min, Converting to \u00b0C from K, Resampling by 8-day min, Resampling to 0.25 degrees using bilinear interpolation project DeepESDL references https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation reported_day 5.0 source https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels standard_name min_air_temperature_2m temporal_resolution 8D time_coverage_end 2021-12-27T00:00:00.000000000 time_coverage_start 1979-01-01T00:00:00.000000000 time_period 8D units \u00b0C"},{"location":"datasets/ESDC/#nbar_blue","title":"nbar_blue","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ bandwidth 20.0 bandwidth_units nm date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Nadir BRDF Adjusted Reflectance of Band 3 (blue) original_add_offset 0.0 original_name Nadir_Reflectance_Band3 original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name nadir_reflectance_band3 temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1 wavelength 469.0 wavelength_units nm"},{"location":"datasets/ESDC/#nbar_green","title":"nbar_green","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ bandwidth 20.0 bandwidth_units nm date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Nadir BRDF Adjusted Reflectance of Band 4 (green) original_add_offset 0.0 original_name Nadir_Reflectance_Band4 original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name nadir_reflectance_band4 temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1 wavelength 555.0 wavelength_units nm"},{"location":"datasets/ESDC/#nbar_nir","title":"nbar_nir","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ bandwidth 35.0 bandwidth_units nm date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Nadir BRDF Adjusted Reflectance of Band 2 (NIR) original_add_offset 0.0 original_name Nadir_Reflectance_Band2 original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name nadir_reflectance_band2 temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1 wavelength 858.5 wavelength_units nm"},{"location":"datasets/ESDC/#nbar_red","title":"nbar_red","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ bandwidth 50.0 bandwidth_units nm date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Nadir BRDF Adjusted Reflectance of Band 1 (red) original_add_offset 0.0 original_name Nadir_Reflectance_Band1 original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name nadir_reflectance_band1 temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1 wavelength 645.0 wavelength_units nm"},{"location":"datasets/ESDC/#nbar_swir1","title":"nbar_swir1","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ bandwidth 20.0 bandwidth_units nm date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Nadir BRDF Adjusted Reflectance of Band 5 (SWIR1) original_add_offset 0.0 original_name Nadir_Reflectance_Band5 original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name nadir_reflectance_band5 temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1 wavelength 1240.0 wavelength_units nm"},{"location":"datasets/ESDC/#nbar_swir2","title":"nbar_swir2","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ bandwidth 24.0 bandwidth_units nm date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Nadir BRDF Adjusted Reflectance of Band 6 (SWIR2) original_add_offset 0.0 original_name Nadir_Reflectance_Band6 original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name nadir_reflectance_band6 temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1 wavelength 1640.0 wavelength_units nm"},{"location":"datasets/ESDC/#nbar_swir3","title":"nbar_swir3","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ bandwidth 50.0 bandwidth_units nm date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Nadir BRDF Adjusted Reflectance of Band 7 (SWIR3) original_add_offset 0.0 original_name Nadir_Reflectance_Band7 original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name nadir_reflectance_band7 temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1 wavelength 2130.0 wavelength_units nm"},{"location":"datasets/ESDC/#ndvi","title":"ndvi","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Normalized Difference Vegetation Index original_add_offset 0.0 original_name NDVI original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://ntrs.nasa.gov/citations/19740022614, https://github.com/awesome-spectral-indices/awesome-spectral-indices, https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://github.com/awesome-spectral-indices/spyndex, https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name NDVI temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1"},{"location":"datasets/ESDC/#net_ecosystem_exchange","title":"net_ecosystem_exchange","text":"Field Value acknowledgment FLUXCOM date_modified 2022-10-17 22:14:30.401506 description FLUXCOM geospatial_lat_max 89.87499928049999 geospatial_lat_min -89.8750000005 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87499856049996 geospatial_lon_min -179.8750000005 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Net Ecosystem Exchange original_add_offset 0.0 original_name NEE original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.5194/bg-13-4291-2016, https://doi.org/10.1038/s41597-019-0076-8 reported_day 5.0 source https://www.fluxcom.org/ standard_name net_primary_productivity_of_carbon temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 2001-01-05T00:00:00.000000000 time_period 8D units g C m^-2 d^-1"},{"location":"datasets/ESDC/#net_radiation","title":"net_radiation","text":"Field Value acknowledgment FLUXCOM date_modified 2022-10-17 22:14:30.401506 description FLUXCOM geospatial_lat_max 89.87499928049999 geospatial_lat_min -89.8750000005 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87499856049996 geospatial_lon_min -179.8750000005 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Net Radiation original_add_offset 0.0 original_name Rn original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.5194/bg-13-4291-2016, https://doi.org/10.1038/s41597-019-0076-8 reported_day 5.0 source https://www.fluxcom.org/ standard_name surface_net_radiation_flux temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 2001-01-05T00:00:00.000000000 time_period 8D units MJ m^-2 d^-1"},{"location":"datasets/ESDC/#nirv","title":"nirv","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Near Infrared Reflectance of Vegetation original_add_offset 0.0 original_name NIRv original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.1126/sciadv.1602244, https://github.com/awesome-spectral-indices/awesome-spectral-indices, https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://github.com/awesome-spectral-indices/spyndex, https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name NIRv temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1"},{"location":"datasets/ESDC/#open_water_evaporation","title":"open_water_evaporation","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Open-water Evaporation original_add_offset 0.0 original_name Ew original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name open_water_evaporation temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#potential_evaporation","title":"potential_evaporation","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Potential Evaporation original_add_offset 0.0 original_name Ep original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name potential_evaporation temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#precipitation_era5","title":"precipitation_era5","text":"Field Value acknowledgment ERA5 hourly data on single levels from 1959 to present date_modified 2022-11-04 15:41:36.233472 description ERA5 Reanalysis Products geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Total Precipitation original_add_offset 0.0 original_name tp original_scale_factor 1.0 processing_steps Merging nc files, Resampling by daily sum, Converting to mm from m, Resampling by 8-day mean, Resampling to 0.25 degrees using bilinear interpolation project DeepESDL references https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation reported_day 5.0 source https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels standard_name total_precipitation temporal_resolution 8D time_coverage_end 2021-12-27T00:00:00.000000000 time_coverage_start 1979-01-01T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#radiation_era5","title":"radiation_era5","text":"Field Value acknowledgment ERA5 hourly data on single levels from 1959 to present date_modified 2022-11-04 15:41:36.233472 description ERA5 Reanalysis Products geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Surface Net Solar Radiation original_add_offset 0.0 original_name ssr original_scale_factor 1.0 processing_steps Merging nc files, Resampling by daily mean, Resampling by 8-day mean, Resampling to 0.25 degrees using bilinear interpolation project DeepESDL references https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation reported_day 5.0 source https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels standard_name surface_net_downward_shortwave_flux temporal_resolution 8D time_coverage_end 2021-12-27T00:00:00.000000000 time_coverage_start 1979-01-01T00:00:00.000000000 time_period 8D units J m^-2"},{"location":"datasets/ESDC/#root_moisture","title":"root_moisture","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Root-zone Soil Moisture original_add_offset 0.0 original_name SMroot original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name root_zone_soil_moisture temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#sensible_heat","title":"sensible_heat","text":"Field Value acknowledgment FLUXCOM date_modified 2022-10-17 22:14:30.401506 description FLUXCOM geospatial_lat_max 89.87499928049999 geospatial_lat_min -89.8750000005 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87499856049996 geospatial_lon_min -179.8750000005 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Sensible Heat original_add_offset 0.0 original_name H original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.5194/bg-13-4291-2016, https://doi.org/10.1038/s41597-019-0076-8 reported_day 5.0 source https://www.fluxcom.org/ standard_name surface_upward_sensible_heat_flux temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 2001-01-05T00:00:00.000000000 time_period 8D units MJ m^-2 d^-1"},{"location":"datasets/ESDC/#sif_gome2_jj","title":"sif_gome2_jj","text":"Field Value acknowledgment https://doi.org/10.5194/essd-12-1101-2020 date_modified 2022-10-11 22:36:53.583022 description Spatially Downscaled Sun-Induced Fluorescence (JJ Method) geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000000003 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Downscaled Daily Corrected Sun-Induced Chlorophyll Fluorescence at 740 nm original_add_offset 0.0 original_name SIF original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation project DeepESDL references https://doi.org/10.5194/essd-12-1101-2020 reported_day 9.0 source https://data.jrc.ec.europa.eu/dataset/21935ffc-b797-4bee-94da-8fec85b3f9e1 standard_name sif temporal_resolution 16D time_coverage_end 2018-10-04T00:00:00.000000000 time_coverage_start 2007-01-21T00:00:00.000000000 time_period 8D units m W m^-2 sr^-1 nm^-1"},{"location":"datasets/ESDC/#sif_gome2_pk","title":"sif_gome2_pk","text":"Field Value acknowledgment https://doi.org/10.5194/essd-12-1101-2020 date_modified 2022-10-11 22:43:08.258033 description Spatially Downscaled Sun-Induced Fluorescence (PK Method) geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000000003 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Downscaled Daily Corrected Sun-Induced Chlorophyll Fluorescence at 740 nm original_add_offset 0.0 original_name SIF original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation project DeepESDL references https://doi.org/10.5194/essd-12-1101-2020 reported_day 9.0 source https://data.jrc.ec.europa.eu/dataset/21935ffc-b797-4bee-94da-8fec85b3f9e1 standard_name sif temporal_resolution 16D time_coverage_end 2018-12-31T00:00:00.000000000 time_coverage_start 2007-01-21T00:00:00.000000000 time_period 8D units m W m^-2 sr^-1 nm^-1"},{"location":"datasets/ESDC/#sif_gosif","title":"sif_gosif","text":"Field Value acknowledgment https://doi.org/10.3390/rs11050517 date_modified 2022-10-11 22:20:05.841847 description GOSIF Solar-Induced Chlorophyll Fluorescence Derived from OCO-2, MODIS, and Reanalysis Data geospatial_lat_max 89.87499999999999 geospatial_lat_min -89.87500000000001 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000000003 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Sun-Induced Chlorophyll Fluorescence at 757 nm original_add_offset 0.0 original_name sif original_scale_factor 0.0001 processing_steps Merging tif files, Converting water bodies and snow covered areas to NaN, Applying original scale factor, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.3390/rs11050517 reported_day 5.0 source https://globalecology.unh.edu/data.html standard_name sif temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units W m^-2 sr^-1 um^-1"},{"location":"datasets/ESDC/#sif_rtsif","title":"sif_rtsif","text":"Field Value acknowledgment https://doi.org/10.1038/s41597-022-01520-1 date_modified 2022-10-12 14:26:02.972963 description Long-term Reconstructed TROPOMI Solar-Induced Fluorescence (RTSIF) geospatial_lat_max 89.87499999999999 geospatial_lat_min -89.87500000000001 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000000003 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Sun-Induced Chlorophyll Fluorescence at 740 nm original_add_offset 0.0 original_name sif original_scale_factor 1.0 processing_steps Merging tif files, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.1038/s41597-022-01520-1 reported_day 5.0 source https://figshare.com/articles/dataset/RTSIF_dataset/19336346/2 standard_name sif temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 2001-01-05T00:00:00.000000000 time_period 8D units m W m^-2 sr^-1 um^-1"},{"location":"datasets/ESDC/#sm","title":"sm","text":"Field Value acknowledgment ESA Soil Moisture Climate Change Initiative (Soil_Moisture_cci) date_modified 2022-10-13 20:42:41.277132 description ESA Soil Moisture Climate Change Initiative (Soil_Moisture_cci): COMBINED product, Version 06.1 geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Volumetric Soil Moisture original_add_offset 0.0 original_name sm original_scale_factor 1.0 processing_steps Merging nc files, Resampling by 8-day mean project DeepESDL references https://data.cci.ceda.ac.uk/thredds/fileServer/esacci/soil_moisture/docs/v06.1/ESA_CCI_SM_RD_D2.1_v2_ATBD_v06.1_issue_1.1.pdf, https://doi.org/10.5194/essd-11-717-2019, https://doi.org/10.1016/j.rse.2017.07.001 reported_day 5.0 source https://catalogue.ceda.ac.uk/uuid/43d73291472444e6b9c2d2420dbad7d6 standard_name volumetric_soil_moisture temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 1979-01-05T00:00:00.000000000 time_period 8D units m^3 m^-3"},{"location":"datasets/ESDC/#snow_sublimation","title":"snow_sublimation","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Snow Sublimation original_add_offset 0.0 original_name Es original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name snow_sublimation temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#surface_moisture","title":"surface_moisture","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Surface Soil Moisture original_add_offset 0.0 original_name SMsurf original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name surface_soil_moisture temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#terrestrial_ecosystem_respiration","title":"terrestrial_ecosystem_respiration","text":"Field Value acknowledgment FLUXCOM date_modified 2022-10-17 22:14:30.401506 description FLUXCOM geospatial_lat_max 89.87499928049999 geospatial_lat_min -89.8750000005 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87499856049996 geospatial_lon_min -179.8750000005 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Terrestrial Ecosystem Respiration original_add_offset 0.0 original_name TER original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.5194/bg-13-4291-2016, https://doi.org/10.1038/s41597-019-0076-8 reported_day 5.0 source https://www.fluxcom.org/ standard_name ecosystem_respiration_carbon_flux temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 2001-01-05T00:00:00.000000000 time_period 8D units g C m^-2 d^-1"},{"location":"datasets/ESDC/#transpiration","title":"transpiration","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Transpiration original_add_offset 0.0 original_name Et original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name transpiration temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.9 acknowledgment All ESDC data providers are acknowledged inside each variable contributor_name University of Leipzig, Max Planck Institute, Brockmann Consult GmbH contributor_url https://www.uni-leipzig.de/, https://www.mpg.de/en, https://www.brockmann-consult.de/ creator_name University of Leipzig, Brockmann Consult GmbH creator_url https://www.uni-leipzig.de/, https://www.brockmann-consult.de/ date_modified 2022-11-25 23:13:03.350030 geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 id esdc-8d-0.25deg-256x128x128-3.0.1 license Terms and conditions of the DeepESDL data distribution project DeepESDL publisher_name DeepESDL Team publisher_url https://www.earthsystemdatalab.net/ time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1979-01-05T00:00:00.000000000 time_period 8D time_period_reported_day 5.0 title Earth System Data Cube (ESDC) v3.0.1"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/","title":"Land Cover Cube","text":""},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#land-cover-map-of-esa-cci-brokered-by-cds","title":"Land Cover Map of ESA CCI brokered by CDS","text":""},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\n# The cube is saved as a multilevel cube, the level 0 is the base layer with \n# the highest resolution\nml_dataset = store.open_data('LC-1x2025x2025-2.0.0.levels')\n# Chek how many levels are present\nml_dataset.num_levels\n# Open dataset at a certain level, here level 0\nds = ml_dataset.get_dataset(0)\n</code></pre>"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180.0 to 180.0 Bounding box latitude (\u00b0) -90.0 to 90.0 Time range 1992-01-01 to 2022-12-31 <p>Click here for full dataset metadata.</p>"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units change_count number of class changes [none] current_pixel_state LC pixel type mask [none] lat_bounds [none] [none] lccs_class Land cover class defined in LCCS [none] lon_bounds [none] [none] observation_count number of valid observations [none] processed_flag LC map processed area flag [none] time_bounds [none] [none]"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#change_count","title":"change_count","text":"Field Value long_name number of class changes valid_max 100 valid_min 0"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#crs","title":"crs","text":"Field Value i2m 0.002777777777778,0.0,0.0,-0.002777777777778,-180.0,90.0 wkt GEOGCS[\"WGS 84\",    DATUM[\"World Geodetic System 1984\",      SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]],      AUTHORITY[\"EPSG\",\"6326\"]],    PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]],    UNIT[\"degree\", 0.017453292519943295],    AXIS[\"Geodetic longitude\", EAST],    AXIS[\"Geodetic latitude\", NORTH],    AUTHORITY[\"EPSG\",\"4326\"]]"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#current_pixel_state","title":"current_pixel_state","text":"Field Value flag_meanings invalid clear_land clear_water clear_snow_ice cloud cloud_shadow flag_values 0, 1, 2, 3, 4, 5 long_name LC pixel type mask standard_name land_cover_lccs status_flag valid_max 5 valid_min 0"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#lat_bounds","title":"lat_bounds","text":""},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#lccs_class","title":"lccs_class","text":"Field Value ancillary_variables processed_flag current_pixel_state observation_count change_count flag_colors #ffff64 #ffff64 #ffff00 #aaf0f0 #dcf064 #c8c864 #006400 #00a000 #00a000 #aac800 #003c00 #003c00 #005000 #285000 #285000 #286400 #788200 #8ca000 #be9600 #966400 #966400 #966400 #ffb432 #ffdcd2 #ffebaf #ffc864 #ffd278 #ffebaf #00785a #009678 #00dc82 #c31400 #fff5d7 #dcdcdc #fff5d7 #0046c8 #ffffff flag_meanings no_data cropland_rainfed cropland_rainfed_herbaceous_cover cropland_rainfed_tree_or_shrub_cover cropland_irrigated mosaic_cropland mosaic_natural_vegetation tree_broadleaved_evergreen_closed_to_open tree_broadleaved_deciduous_closed_to_open tree_broadleaved_deciduous_closed tree_broadleaved_deciduous_open tree_needleleaved_evergreen_closed_to_open tree_needleleaved_evergreen_closed tree_needleleaved_evergreen_open tree_needleleaved_deciduous_closed_to_open tree_needleleaved_deciduous_closed tree_needleleaved_deciduous_open tree_mixed mosaic_tree_and_shrub mosaic_herbaceous shrubland shrubland_evergreen shrubland_deciduous grassland lichens_and_mosses sparse_vegetation sparse_tree sparse_shrub sparse_herbaceous tree_cover_flooded_fresh_or_brakish_water tree_cover_flooded_saline_water shrub_or_herbaceous_cover_flooded urban bare_areas bare_areas_consolidated bare_areas_unconsolidated water snow_and_ice flag_values 0, 10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100, 110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180, 190, 200, 201, 202, 210, 220 long_name Land cover class defined in LCCS standard_name land_cover_lccs valid_max 220 valid_min 1"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#lon_bounds","title":"lon_bounds","text":""},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#observation_count","title":"observation_count","text":"Field Value long_name number of valid observations standard_name land_cover_lccs number_of_observations valid_max 32767 valid_min 0"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#processed_flag","title":"processed_flag","text":"Field Value flag_meanings not_processed processed flag_values 0, 1 long_name LC map processed area flag standard_name land_cover_lccs status_flag valid_max 1 valid_min 0"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#time_bounds","title":"time_bounds","text":""},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.6 TileSize 2025:2025 cdm_data_type grid comment contact https://www.ecmwf.int/en/about/contact-us/get-support creation_date 20181130T095451Z creator_email landcover-cci@uclouvain.be creator_name UCLouvain creator_url http://www.uclouvain.be/ geospatial_lat_max 90.0 geospatial_lat_min -90.0 geospatial_lat_resolution 0.002778 geospatial_lat_units degrees_north geospatial_lon_max 180 geospatial_lon_min -180 geospatial_lon_resolution 0.002778 geospatial_lon_units degrees_east history amorgos-4,0, lc-sdr-1.0, lc-sr-1.0, lc-classification-1.0,lc-user-tools-3.13,lc-user-tools-4.3 id ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992-v2.0.7cds institution UCLouvain keywords land cover classification,satellite,observation keywords_vocabulary NASA Global Change Master Directory (GCMD) Science Keywords license ESA CCI Data Policy: free and open access naming_authority org.esa-cci product_version 2.0.7cds project Climate Change Initiative - European Space Agency references http://www.esa-landcover-cci.org/ source MERIS FR L1B version 5.05, MERIS RR L1B version 8.0, SPOT VGT P spatial_resolution 300m standard_name_vocabulary NetCDF Climate and Forecast (CF) Standard Names version 21 summary This dataset characterizes the land cover of a particular year (see time_coverage). The land cover was derived from the analysis of satellite data time series of the full period. time_coverage_duration P1Y time_coverage_end 19921231 time_coverage_resolution P1Y time_coverage_start 19920101 title Land Cover Map of ESA CCI brokered by CDS tracking_id 61b96fd7-42c3-4374-9de1-0dc3b0bcae2a type ESACCI-LC-L4-LCCS-Map-300m-P1Y"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/","title":"SMOS L2C OS 20230101 20231231 1W res0 1x1000x1000 levels","text":""},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#smos-ocean-salinity-data-cube","title":"SMOS Ocean Salinity Data Cube","text":""},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000.levels')\n</code></pre>"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180 to 180 Bounding box latitude (\u00b0) -89 to 89 Time range 2023-01-01 to 2024-01-06 Time period 7D Publisher Brockmann Consult GmbH <p>Click here for full dataset metadata.</p>"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units Coast_distance [none] [none] Dg_RFI_X [none] [none] Dg_RFI_Y [none] [none] Dg_chi2_corr [none] [none] Dg_quality_SSS_anom [none] [none] Dg_quality_SSS_corr [none] [none] Mean_acq_time [none] dd SSS_anom [none] psu SSS_corr Sea Surface Salinity psu Sigma_SSS_anom [none] psu Sigma_SSS_corr [none] psu X_swath [none] m"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#coast_distance","title":"Coast_distance","text":"Field Value scale_offset 0.0"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#dg_rfi_x","title":"Dg_RFI_X","text":""},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#dg_rfi_y","title":"Dg_RFI_Y","text":""},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#dg_chi2_corr","title":"Dg_chi2_corr","text":"Field Value scale_offset 0.0"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#dg_quality_sss_anom","title":"Dg_quality_SSS_anom","text":""},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#dg_quality_sss_corr","title":"Dg_quality_SSS_corr","text":""},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#mean_acq_time","title":"Mean_acq_time","text":"Field Value units dd"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#sss_anom","title":"SSS_anom","text":"Field Value units psu"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#sss_corr","title":"SSS_corr","text":"Field Value color_bar_name haline color_value_max 42 color_value_min 0 long_name Sea Surface Salinity units psu"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#sigma_sss_anom","title":"Sigma_SSS_anom","text":"Field Value units psu"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#sigma_sss_corr","title":"Sigma_SSS_corr","text":"Field Value units psu"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#x_swath","title":"X_swath","text":"Field Value units m"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.9 FH-File_Class OPER FH-File_Description L2 Ocean Salinity Output User Data Product. FH-File_Type MIR_OSUDP2 FH-File_Version 0001 FH-Mission SMOS FH-Notes The UDP (User Data Product) is designed for oceanographics and high level centers, it includes geophysical parameters, a theoretical estimate of their accuracy, flags and descriptors of the product quality. FH-Source-Creator L2OP FH-Source-Creator_Version 700 FH-Source-System DPGS VH-MPH-Acquisition_Station SVLD VH-MPH-Logical_Proc_Centre FPC VH-MPH-Processing_Centre ESAC VH-MPH-Product_Confidence NOMINAL VH-MPH-Ref_Doc SO-TN-IDR-GS-0006 acknowledgment ESA SMOS, DeepESDL project creator_email info@brockmann-consult.de creator_name Brockmann Consult GmbH creator_url www.brockmann-consult.de data_id SMOS-L2C-OS-20230101-20231231-1W-res0 date_modified 2024-08-19 16:19:15.359970 description Weekly means SMOS Ocean Salinity 2023 geospatial_lat_max 89.0 geospatial_lat_min -89.0 geospatial_lon_max 180 geospatial_lon_min -180 institution Brockmann Consult GmbH license Creative Commons Attribution 4.0 International (CC BY 4.0) license_url https://creativecommons.org/licenses/by/4.0/ project DeepESDL publisher_email info@brockmann-consult.de publisher_name Brockmann Consult GmbH source ESA SMOS Ocean Salinity temporal_coverage_end 2023-12-31 23:59:59 temporal_coverage_start 2023-01-01 00:00:00 temporal_resolution 1W title SMOS Ocean Salinity Data Cube version 1.0.0"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/","title":"SMOS L2C SM 20230101 20231231 1W res0 1x1000x1000 levels","text":""},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#smos-soil-moisture-data-cube","title":"SMOS Soil Moisture Data Cube","text":""},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000.levels')\n</code></pre>"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180 to 180 Bounding box latitude (\u00b0) -89 to 89 Time range 2023-01-01 to 2024-01-06 Time period 7D Publisher Brockmann Consult GmbH <p>Click here for full dataset metadata.</p>"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units Chi_2 [none] [none] Chi_2_P [none] [none] N_RFI_X [none] [none] N_RFI_Y [none] [none] RFI_Prob [none] [none] Soil_Moisture [none] m3 m-3 Soil_Moisture_DQX [none] m3 m-3"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#chi_2","title":"Chi_2","text":"Field Value scale_offset 0.0"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#chi_2_p","title":"Chi_2_P","text":"Field Value scale_offset 0.0"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#n_rfi_x","title":"N_RFI_X","text":""},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#n_rfi_y","title":"N_RFI_Y","text":""},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#rfi_prob","title":"RFI_Prob","text":"Field Value scale_offset 0.0"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#soil_moisture","title":"Soil_Moisture","text":"Field Value color_bar_name YlGnBu color_value_max 1 color_value_min 0 units m3 m-3"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#soil_moisture_dqx","title":"Soil_Moisture_DQX","text":"Field Value units m3 m-3"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.9 FH-File_Class OPER FH-File_Description L2 Soil Moisture Output User Data Product FH-File_Type MIR_SMUDP2 FH-File_Version 0001 FH-Mission SMOS FH-Source-Creator L2OP FH-Source-Creator_Version 700 FH-Source-System DPGS VH-MPH-Acquisition_Station SVLD VH-MPH-Logical_Proc_Centre FPC VH-MPH-Processing_Centre ESAC VH-MPH-Product_Confidence NOMINAL VH-MPH-Ref_Doc SO-TN-IDR-GS-0006 acknowledgment ESA SMOS, DeepESDL project creator_email info@brockmann-consult.de creator_name Brockmann Consult GmbH creator_url www.brockmann-consult.de data_id SMOS-L2C-SM-20230101-20231231-1W-res0 date_modified 2024-08-19 16:19:15.359970 description Weekly means SMOS Soil Moisture 2023 geospatial_lat_max 89.0 geospatial_lat_min -89.0 geospatial_lon_max 180 geospatial_lon_min -180 institution Brockmann Consult GmbH license Creative Commons Attribution 4.0 International (CC BY 4.0) license_url https://creativecommons.org/licenses/by/4.0/ project DeepESDL publisher_email info@brockmann-consult.de publisher_name Brockmann Consult GmbH source ESA SMOS Soil Moisture temporal_coverage_end 2023-12-31 23:59:59 temporal_coverage_start 2023-01-01 00:00:00 temporal_resolution 1W title SMOS Soil Moisture Data Cube version 1.0.0"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/","title":"SMOS freeze/thaw Cube","text":""},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#smos-freeze-and-thaw-processing-and-dissemination-service","title":"SMOS Freeze and Thaw Processing and Dissemination Service","text":""},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('SMOS-snow-1x720x720-1.0.1.zarr')\n</code></pre>"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180 to 180 Bounding box latitude (\u00b0) 0 to 85 Time range 2010-07-01 to 2023-03-09 <p>Click here for full dataset metadata.</p>"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units L3FT SMOS Level 3 Freeze Thaw Estimates [none] PM Processing Mask [none] quality_flag Quality Flag [none] uncertainty Uncertainty [none]"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#l3ft","title":"L3FT","text":"Field Value flag_meanings Thaw Partial Frozen flag_values 1, 2, 3 grid_mapping crs long_name SMOS Level 3 Freeze Thaw Estimates valid_range 1, 3"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#pm","title":"PM","text":"Field Value grid_mapping crs long_name Processing Mask valid_range 0, 8"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#crs","title":"crs","text":"Field Value GeoTransform -9000000 25000 0 9000000 0 -25000 false_easting 0 false_northing 0 grid_mapping_name lambert_azimuthal_equal_area inverse_flattening 298 latitude_of_projection_origin 90 longitude_of_prime_meridian 0 longitude_of_projection_origin 0 semi_major_axis 6380000.0 spatial_ref PROJCS[\"unnamed\",GEOGCS[\"WGS 84\",DATUM[\"unknown\",SPHEROID[\"WGS84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"latitude_of_center\",90],PARAMETER[\"longitude_of_center\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]]]"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#quality_flag","title":"quality_flag","text":"Field Value grid_mapping crs long_name Quality Flag valid_range 0, 255"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#uncertainty","title":"uncertainty","text":"Field Value grid_mapping crs long_name Uncertainty valid_range 0, 100"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value ancillarydata_2mair Ancillary data for 2m airtemperature: ECMWF ERA Interim reanalysis (1-Jul-2010 - 31-Jul-2018) and ECMWF NRT data (1-Aug-2018 onwards) ancillarydata_snowcover Ancillary data for snow cover: IMS DAILY NORTHERN HEMISPHERE SNOW AND ICE ANALYSIS AT 4 KM contact Kimmo Rautiainen &lt;kimmo.rautiainen@fmi.fi&gt; coordinate_system Equal-Area Scalable Earth Grid 2.0 (EASE-Grid 2.0) - Northern Hemisphere data_date 20100701 incidence_angle_range 50-55 degrees latitude_range 0N - 85N longitude_range 180W - 180E moving_average 20 days orbits_included Currently only descending orbits used processing_date 2019-05-22 processing_organisation Finnish Meteorological Institute processing_software_name FMI SMOS F/T processing sowtware (Rautiainen, Cohen, Hiltunen, Ikonen, Parkkinen, Moisander, Takala 2016-2019) processing_software_version v_2.00 project_id SMOS Freeze and Thaw Processing and Dissemination Service sensor SMOS smosinputdataversion SMOS input data: CATDS v620 spatial_resolution 25 X 25 sq.km title SMOS Freeze and Thaw Processing and Dissemination Service"},{"location":"datasets/SeasFireCube_v3-zarr/","title":"SeasFireCube v3 zarr","text":""},{"location":"datasets/SeasFireCube_v3-zarr/#seasfire-cube-a-global-dataset-for-seasonal-fire-modeling-in-the-earth-system","title":"SeasFire Cube: A Global Dataset for Seasonal Fire Modeling in the Earth System","text":""},{"location":"datasets/SeasFireCube_v3-zarr/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('SeasFireCube_v3.zarr')\n</code></pre>"},{"location":"datasets/SeasFireCube_v3-zarr/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/SeasFireCube_v3-zarr/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180.0 to 180.0 Bounding box latitude (\u00b0) -90.0 to 90.0 Time range 2001-01-01 to 2021-12-27 <p>Click here for full dataset metadata.</p>"},{"location":"datasets/SeasFireCube_v3-zarr/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units area [none] m2 biomes [none] [none] cams_co2fire Wildfire flux of carbon dioxide kg m-2 cams_frpfire Wildfire radiative power W m-2 drought_code_max Drought Code, Maximum Dimensionless drought_code_mean Drought Code, Average Dimensionless fcci_ba Burned Areas from Fire Climate Change Initiative (FCCI) hectares (ha) fcci_ba_valid_mask [none] [none] fcci_fraction_of_burnable_area fraction of burnable area 0 to 1 fcci_fraction_of_observed_area fraction of observed area 0 to 1 fcci_number_of_patches number of burn patches 0 to N fwi_max Fire Weather Index, Maximum Dimensionless fwi_mean Fire Weather Index, Average Dimensionless gfed_ba Burned Areas from GFED hectares (ha) gfed_ba_valid_mask [none] [none] gfed_region GFED basis regions [none] gwis_ba Burned Areas from GWIS hectares (ha) gwis_ba_valid_mask [none] [none] lai Leaf Area Index m\u00b2/m\u00b2 lccs_class_0 Land Cover Class 0 - No data % lccs_class_1 Land Cover Class 1 - Agriculture % lccs_class_2 Land Cover Class 2 - Forest % lccs_class_3 Land Cover Class 3 - Grassland % lccs_class_4 Land Cover Class 4 - Wetland % lccs_class_5 Land Cover Class 5 - Settlement % lccs_class_6 Land Cover Class 6 - Shrubland % lccs_class_7 Land Cover Class 4 - Sparse vegetation, bare areas, permanent snow and ice % lccs_class_8 Land Cover Class 4 - Sparse vegetation, bare areas, permanent snow and ice % lsm Land-sea mask (0 - 1) lst_day Day Land Surface Temperature for Climate Modeling Grid (CMG) Kelvin (K) mslp Mean sea level pressure Pa ndvi CMG 0.05 Deg 16 days NDVI -1 to 1 oci_ao Arctic Oscillation Dimensionless oci_censo Bivariate ENSO Timeseries Dimensionless oci_ea Eastern Asia/Western Russia Dimensionless oci_epo East Pacific/North Pacific Oscillation Dimensionless oci_gmsst Global Mean Land/Ocean Temperature Dimensionless oci_nao North Atlantic Oscillation Dimensionless oci_nina34_anom Ni\u00f1o 3.4 region Dimensionless oci_pdo Pacific Decadal Oscillation Dimensionless oci_pna Pacific North American Index Dimensionless oci_soi Southern Oscillation Index Dimensionless oci_wp Western Pacific Index Dimensionless pop_dens UN WPP-Adjusted Population Density Persons per square kilometer rel_hum Relative humidity % skt Skin Temperature Kelvin (K) ssr Surface net solar radiation MJ m-2 ssrd Surface net solar radiation downwards MJ m-2 sst Sea surface temperature K swvl1 Volumetric soil water layer 1 m3 m-3 swvl2 Volumetric soil water layer 2 m3 m-3 swvl3 Volumetric soil water layer 3 m3 m-3 swvl4 Volumetric soil water layer 4 m3 m-3 t2m_max 2 meters temperature - Maximum value Kelvin (K) t2m_mean 2 meters temperature - Mean value Kelvin (K) t2m_min 2 meters temperature - Minimum Value Kelvin (K) tp Total precipitation mm vpd Vapour Pressure Deficit hPa ws10 Windspeed of the 10m wind [sqrt(u10^2+v10^2)] m*s-2"},{"location":"datasets/SeasFireCube_v3-zarr/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/SeasFireCube_v3-zarr/#area","title":"area","text":"Field Value description Square meters of each grid cell, calculated in crs 8857, WGS 84 / Equal Earth Greenwich units m2"},{"location":"datasets/SeasFireCube_v3-zarr/#biomes","title":"biomes","text":"Field Value 1 Flooded Grasslands &amp; Savannas 10 Tundra 11 Tropical &amp; Subtropical Coniferous Forests 12 Tropical &amp; Subtropical Dry Broadleaf Forests 13 Tropical &amp; Subtropical Moist Broadleaf Forests 14 Montane Grasslands &amp; Shrublandss 15 Temperate Broadleaf &amp; Mixed Forests 2 Tropical &amp; Subtropical Grasslands, Savannas &amp; Shrublands 3 Mediterranean Forests, Woodlands &amp; Scrub 4 Rock &amp; Ice 5 Mangroves 6 Temperate Grasslands, Savannas &amp; Shrublands 7 Temperate Conifer Forests 8 Deserts &amp; Xeric Shrublandss 9 Boreal Forests/Taiga biome_number biome_name description The RESOLVE Ecoregions dataset, updated in 2017, offers a depiction of the 846 terrestrial ecoregions that represent our living planet.Ecoregions, in the simplest definition, are ecosystems of regional extent. Specifically, ecoregions represent distinct assemblages of biodiversity-all taxa, not just vegetation-whose boundaries include the space required to sustain ecological processes. Ecoregions provide a useful basemap for conservation planning in particular because they draw on natural, rather than political, boundaries, define distinct biogeographic assemblages and ecological habitats within biomes, and assist in representation of Earth's biodiversity.This dataset is based on recent advances in biogeography - the science concerning the distribution of plants and animals. The original ecoregions dataset has been widely used since its introduction in 2001, underpinning the most recent analyses of the effects of global climate change on nature by ecologists to the distribution of the world's beetles to modern conservation planning.The 846 terrestrial ecoregions are grouped into 14 biomes provider RESOLVE Biodiversity and Wildlife Solutions"},{"location":"datasets/SeasFireCube_v3-zarr/#cams_co2fire","title":"cams_co2fire","text":"Field Value creator_notes Missing years filled with Nan. To convert to kg multiply by the variable square_meters. description The Global Fire Assimilation System (GFAS) assimilates fire radiative power (FRP) observations from satellite-based sensors to produce daily estimates of biomass burning emissions downloaded from https://confluence.ecmwf.int/display/CKB/CAMS%3A+Global+Fire+Assimilation+System+%28GFAS%29+data+documentation long_name Wildfire flux of carbon dioxide missing_years 2001 &amp; 2002, see Creators Notes provider ECMWF CAMS Global Fire Assimilation System (GFAS) units kg m-2"},{"location":"datasets/SeasFireCube_v3-zarr/#cams_frpfire","title":"cams_frpfire","text":"Field Value creator_notes Missing years filled with Nan. To convert to W multiply by the variable square_meters. description FRP observations currently assimilated in GFAS are the NASA Terra MODIS and Aqua MODIS active fire products (http://modis-fire.umd.edu/) downloaded_from https://confluence.ecmwf.int/display/CKB/CAMS%3A+Global+Fire+Assimilation+System+%28GFAS%29+data+documentation long_name Wildfire radiative power missing_years 2001 &amp; 2002, see creators notes provider ECMWF CAMS Global Fire Assimilation System (GFAS) units W m-2"},{"location":"datasets/SeasFireCube_v3-zarr/#drought_code_max","title":"drought_code_max","text":"Field Value description The Drought code is an indicator of the moisture content in deep compact organic layers. This code represents a fuel layer at approximately 10-20 cm deep. The Drought code fuels have a very slow drying rate, with a time lag of 52 days. The Drought code scale is open-ended, although the maximum value is about 800 downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/cems-fire-historical?tab=overview long_name Drought Code, Maximum provider Copernicus-CEMS units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#drought_code_mean","title":"drought_code_mean","text":"Field Value description The Drought code is an indicator of the moisture content in deep compact organic layers. This code represents a fuel layer at approximately 10-20 cm deep. The Drought code fuels have a very slow drying rate, with a time lag of 52 days. The Drought code scale is open-ended, although the maximum value is about 800 downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/cems-fire-historical?tab=overview long_name Drought Code, Average provider Copernicus CEMS units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#fcci_ba","title":"fcci_ba","text":"Field Value creator_notes Masked ocean with lsm variable. Missing years filled with Nan. USE ONLY for monthly modeling! FireCCI is a monthly product that could not be fairly distributed to weekly time range. Each week of the same month is filled with the same monthly value. To acquire the monthly value take the average of each month description The ESA Fire Disturbance Climate Change Initiative (CCI) project has produced maps of global burned area derived from satellite observations. The MODIS Fire_cci v5.1 grid product described here contains gridded data on global burned area derived from the MODIS instrument onboard the TERRA satellite at 250m resolution for the period 2001 to 2019. This product supercedes the previously available MODIS v5.0 product. The v5.1 dataset was initially published for 2001-2017, and has been periodically extended to include 2018 to 2020. downloaded_from https://catalogue.ceda.ac.uk/uuid/3628cb2fdba443588155e15dee8e5352 long_name Burned Areas from Fire Climate Change Initiative (FCCI) missing_years 2021 provider ESA CCI units hectares (ha)"},{"location":"datasets/SeasFireCube_v3-zarr/#fcci_ba_valid_mask","title":"fcci_ba_valid_mask","text":"Field Value description fcci_ba_valid_mask signifies the time period for which variable fcci_ba is valid. 1 valid, 0 invalid."},{"location":"datasets/SeasFireCube_v3-zarr/#fcci_fraction_of_burnable_area","title":"fcci_fraction_of_burnable_area","text":"Field Value description The fraction of burnable area is the fraction of the cell that corresponds to vegetated land covers that could burn. The land cover classes are those from CCI Land Cover, http://www.esa-landcover-cci.org/ long_name fraction of burnable area units 0 to 1"},{"location":"datasets/SeasFireCube_v3-zarr/#fcci_fraction_of_observed_area","title":"fcci_fraction_of_observed_area","text":"Field Value description The fraction of the total burnable area in the cell (fraction_of_burnable_area variable of this file) that was observed during the time interval, and was not marked as unsuitable/not observable. The latter refers to the area where it was not possible to obtain observational burned area information for the whole time interval because of lack of input data (non-existing data for that location and period). long_name fraction of observed area units 0 to 1"},{"location":"datasets/SeasFireCube_v3-zarr/#fcci_number_of_patches","title":"fcci_number_of_patches","text":"Field Value description Number of contiguous groups of burned pixels. long_name number of burn patches units 0 to N"},{"location":"datasets/SeasFireCube_v3-zarr/#fwi_max","title":"fwi_max","text":"Field Value description The Fire weather index (max and mean) is a combination of Initial spread index and Build-up index, and is a numerical rating of the potential frontal fire intensity. In effect, it indicates fire intensity by combining the rate of fire spread with the amount of fuel being consumed. Fire weather index values are not upper bounded however a value of 50 is considered as extreme in many places. The Fire weather index is used for general public information about fire danger conditions downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/cems-fire-historical?tab=overview long_name Fire Weather Index, Maximum provider Copernicus-CEMS units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#fwi_mean","title":"fwi_mean","text":"Field Value description The Fire weather index (max and mean) is a combination of Initial spread index and Build-up index, and is a numerical rating of the potential frontal fire intensity. In effect, it indicates fire intensity by combining the rate of fire spread with the amount of fuel being consumed. Fire weather index values are not upper bounded however a value of 50 is considered as extreme in many places. The Fire weather index is used for general public information about fire danger conditions downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/cems-fire-historical?tab=overview long_name Fire Weather Index, Average provider Copernicus-CEMS units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#gfed_ba","title":"gfed_ba","text":"Field Value aggregation Temporal creator_notes Masked ocean with lsm variable. Missing years filled with Nan description GFED4 dataset contains information about large fires only downloaded_from http://www.globalfiredata.org/data.html long_name Burned Areas from GFED missing years 2016-2021 provider Global Fire Emissions Database (GFED) units hectares (ha)"},{"location":"datasets/SeasFireCube_v3-zarr/#gfed_ba_valid_mask","title":"gfed_ba_valid_mask","text":"Field Value description gfed_ba_valid_mask signifies the time period for which variable gfed_ba is valid. 1 valid, 0 invalid."},{"location":"datasets/SeasFireCube_v3-zarr/#gfed_region","title":"gfed_region","text":"Field Value description 0-OCEAN, 1-BONA, 2-TENA, 3-CEAM, 4-NHSA, 5-SHSA, 6-EURO, 7-MIDE, 8-NHAF, 9-SHAF, 10-BOAS, 11-CEAS, 12-SEAS, 13-EQAS, 14-AUST. For more information visit http://globalfiredata.org/pages/data/ long_name GFED basis regions"},{"location":"datasets/SeasFireCube_v3-zarr/#gwis_ba","title":"gwis_ba","text":"Field Value aggregation Spatio-Temporal creator_notes Masked ocean with lsm variable. Missing years filled with Nan. Dataset created by using the ignition date of final burned areas description Global dataset of individual fire perimeters for 2001-2020.The dataset is in ESRI shapefile format and is derived from the MCD64A1 burned area product. Each fire shapefile has a unique fire identification code, the initial date, the final date, the geometry and a field specifying if it is a daily burned area or a final burned area. downloaded_from: https://gwis.jrc.ec.europa.eu/apps/country.profile/downloads long_name Burned Areas from GWIS missing_years 2021 provider Global Wildfire Information System (GWIS) units hectares (ha)"},{"location":"datasets/SeasFireCube_v3-zarr/#gwis_ba_valid_mask","title":"gwis_ba_valid_mask","text":"Field Value description gwis_ba_valid_mask signifies the time period for which variable gwis_ba is valid. 1 valid, 0 invalid."},{"location":"datasets/SeasFireCube_v3-zarr/#lai","title":"lai","text":"Field Value aggregation Temporal creator_notes Seasonality in Nan values for high latitudes, due to seasonality in the data availability description The MCD15A2H Version 6 Moderate Resolution Imaging Spectroradiometer (MODIS) Level 4, Combined Fraction of Photosynthetically Active Radiation (FPAR), and Leaf Area Index (LAI) product is an 8-day composite dataset with 500 meter pixel size. The algorithm chooses the best pixel available from all the acquisitions of both MODIS sensors located on NASA\u2019s Terra and Aqua satellites from within the 8-day period.LAI is defined as the one-sided green leaf area per unit ground area in broadleaf canopies and as one-half the total needle surface area per unit ground area in coniferous canopies. FPAR is defined as the fraction of incident photosynthetically active radiation (400-700 nm) absorbed by the green elements of a vegetation canopy. downloaded_from https://lpdaac.usgs.gov/products/mcd15a2hv006/ long_name Leaf Area Index provider NASA units m\u00b2/m\u00b2"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_0","title":"lccs_class_0","text":"Field Value aggregation Spatial description Class 0, contains the percentage of LCCS code [0] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 0 - No data provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_1","title":"lccs_class_1","text":"Field Value aggregation Spatial description Class 1, contains only the percentage of LCCS codes [10,11,12,20,30,40] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 1 - Agriculture provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_2","title":"lccs_class_2","text":"Field Value aggregation Spatial description Class 2, contains only the percentage of LCCS codes  [50,60,61,62,70,71,72,80,81,82,90,100] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 2 - Forest provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_3","title":"lccs_class_3","text":"Field Value aggregation Spatial description Class 3, contains only the percentage of LCCS codes [110,130] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 3 - Grassland provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_4","title":"lccs_class_4","text":"Field Value aggregation Spatial description Class 4, contains only the percentage of LCCS codes [160,170,180] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 4 - Wetland provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_5","title":"lccs_class_5","text":"Field Value aggregation Spatial description Class 5, contains only the percentage with LCCS codes [190] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 5 - Settlement provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_6","title":"lccs_class_6","text":"Field Value aggregation Spatial description Class 6, contains only the percentage of Water Bodies with LCCS codes [120,121,122] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 6 - Shrubland provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_7","title":"lccs_class_7","text":"Field Value aggregation Spatial description Class 7, contains only the percentage with LCCS codes [140,150,151,152,153,200,201,202,220] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 4 - Sparse vegetation, bare areas, permanent snow and ice provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_8","title":"lccs_class_8","text":"Field Value aggregation Spatial description Class 8, contains only the percentage with LCCS codes [210] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 4 - Sparse vegetation, bare areas, permanent snow and ice provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lsm","title":"lsm","text":"Field Value long_name Land-sea mask provider ERA5 units (0 - 1)"},{"location":"datasets/SeasFireCube_v3-zarr/#lst_day","title":"lst_day","text":"Field Value aggregation Temporal creator_notes Seasonality in Nan values for high latitudes, due to seasonality in the data availability description The MOD11C1 Version 6 product provides daily Land Surface Temperature and Emissivity (LST&amp;E) values in a 0.05 degree (5,600 meters at the equator) latitude/longitude Climate Modeling Grid (CMG). The MOD11C1 product is directly derived from the MOD11B1 product. A CMG granule follows a Geographic grid, having 7,200 columns and 3,600 rows, which represent the entire globe. Each MOD11C1 product consists of the following layers for daytime and nighttime observations: LSTs, quality control assessments, observation times, view zenith angles, number of clear-sky observations, and emissivities from bands 20, 22, 23, 29, 31, and 32 (bands 31 and 32 are daytime only) along with the percentage of land in the grid downloaded_from https://lpdaac.usgs.gov/products/mod11c1v006/ long_name Day Land Surface Temperature for Climate Modeling Grid (CMG) provider NASA units Kelvin (K)"},{"location":"datasets/SeasFireCube_v3-zarr/#mslp","title":"mslp","text":"Field Value description This parameter is the pressure (force per unit area) of the atmosphere at the surface of the Earth, adjusted to the height of mean sea level. It is a measure of the weight that all the air in a column vertically above a point on the Earth's surface would have, if the point were located at mean sea level. It is calculated over all surfaces - land, sea and inland water. Maps of mean sea level pressure are used to identify the locations of low and high pressure weather systems, often referred to as cyclones and anticyclones. Contours of mean sea level pressure also indicate the strength of the wind. Tightly packed contours show stronger winds. The units of this parameter are pascals (Pa). Mean sea level pressure is often measured in hPa and sometimes is presented in the old units of millibars, mb (1 hPa = 1 mb = 100 Pa). downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-pressure-levels?tab=overview long_name Mean sea level pressure provider ERA5 units Pa"},{"location":"datasets/SeasFireCube_v3-zarr/#ndvi","title":"ndvi","text":"Field Value aggregation Temporal creator_notes Seasonality in Nan values for high latitudes, due to seasonality in the data availability description The MOD13C1 Version 6 product provides a Vegetation Index (VI) value at a per pixel basis. There are two primary vegetation layers. The first is the Normalized Difference Vegetation Index (NDVI) which is referred to as the continuity index to the existing National Oceanic and Atmospheric Administration-Advanced Very High Resolution Radiometer (NOAA-AVHRR) derived NDVI. The second vegetation layer is the Enhanced Vegetation Index (EVI), which has improved sensitivity over high biomass regions.The Climate Modeling Grid (CMG) consists 3,600 rows and 7,200 columns of 5,600 meter (m) pixels. Global MOD13C1 data are cloud-free spatial composites of the gridded 16-day 1 kilometer MOD13A2 data, and are provided as a Level 3 product projected on a 0.05 degree (5,600 m) geographic CMG. The MOD13C1 has data fields for NDVI, EVI, VI QA, reflectance data, angular information, and spatial statistics such as mean, standard deviation, and number of used input pixels at the 0.05 degree CMG resolution. downloaded_from https://lpdaac.usgs.gov/products/mod13c1v006/ long_name CMG 0.05 Deg 16 days NDVI provider NASA units -1 to 1"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_ao","title":"oci_ao","text":"Field Value creators Notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The Arctic Oscillation (AO) is a large scale mode of climate variability, also referred to as the Northern Hemisphere annular mode. The AO is a climate pattern characterized by winds circulating counterclockwise around the Arctic at around 55\u00b0N latitude. When the AO is in its positive phase, a ring of strong winds circulating around the North Pole acts to confine colder air across polar regions. This belt of winds becomes weaker and more distorted in the negative phase of the AO, which allows an easier southward penetration of colder, arctic airmasses and increased storminess into the mid-latitudes.AO index is obtained by projecting the AO loading pattern to the daily anomaly 1000 millibar height field over 20\u00b0N-90\u00b0N latitude. The AO loading pattern has been chosen as the first mode of EOF analysis using monthly mean 1000 millibar height anomaly data from 1979 to 2000 over 20\u00b0N-90\u00b0N. downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Arctic Oscillation provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_censo","title":"oci_censo","text":"Field Value creators_notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The index was designed to be simple to calculate and to provide a long time period ENSO index for research purposes. Based on 1871-2001 SST and SOI indices downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Bivariate ENSO Timeseries provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_ea","title":"oci_ea","text":"Field Value creators_notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The East Atlantic/ West Russia (EATL/WRUS) pattern is one of three prominent teleconnection patterns that affects Eurasia throughout the year. This pattern has been referred to as the Eurasia-2 pattern by Barnston and Livezey [18]. The East Atlantic/ West Russia pattern consists of four main anomaly centres. The positive phase is associated with positive height anomalies located over Europe and northern China, and negative height anomalies located over the central North Atlantic and north of the Caspian Sea downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Eastern Asia/Western Russia provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_epo","title":"oci_epo","text":"Field Value creators_notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The East Pacific - North Pacific (EP- NP) pattern is a Spring-Summer-Fall pattern with three main anomaly centers.The positive phase of this pattern features positive height anomalies located over Alaska/ Western Canada, and negative anomalies over the central North Pacific and eastern North America fownloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name East Pacific/North Pacific Oscillation provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_gmsst","title":"oci_gmsst","text":"Field Value creators Notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description Data values are from NASA/GISS. Please read and refer to this web page plus the main web page describing various temperature indices at the main NASA/GISTEMP webpage. Note, the index is an anomaly index downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Global Mean Land/Ocean Temperature provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_nao","title":"oci_nao","text":"Field Value creators Notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The North Atlantic Oscillation Index describes changes in the strength of two recurring pressure patterns in the atmosphere over the North Atlantic: a low near Iceland, and a high near the Azores Islands. Positive NAO values indicate these features are strong, creating a big pressure difference between them. Strongly positive values are linked to warm conditions across the U.S. East and Northern Europe, and cold conditions across southern Europe.Negative NAOI indicate these features are relatively weak, and the pressure difference between them is smaller. Strongly negative values are linked to cold conditions in the U.S. East and Northern Europe, and warm conditions in Southern Europe downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name North Atlantic Oscillation provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_nina34_anom","title":"oci_nina34_anom","text":"Field Value creators Notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The Ni\u00f1o 3.4 index typically uses a 5-month running mean, and El Ni\u00f1o or La Ni\u00f1a events are defined when the Ni\u00f1o 3.4 SSTs exceed +/- 0.4C for a period of six months or more downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Ni\u00f1o 3.4 region provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_pdo","title":"oci_pdo","text":"Field Value Provider National Oceanic and Atmospheric Administration (NOAA) creator_notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The Pacific Decadal Oscillation (PDO) is often described as a long-lived El Ni\u00f1o-like pattern of Pacific climate variability. It is a pattern of Pacific climate variability similar to ENSO in character, but which varies over a much longer time scale. The PDO can remain in the same phase for 20 to 30 years, while ENSO cycles typically only last 6 to 18 months. The PDO, like ENSO, consists of a warm and cool phase which alters upper level atmospheric winds. Shifts in the PDO phase can have significant implications for global climate, affecting Pacific and Atlantic hurricane activity, droughts and flooding around the Pacific basin, the productivity of marine ecosystems, and global land temperature patterns. Experts also believe the PDO can intensify or diminish the impacts of ENSO according to its phase. If both ENSO and the PDO are in the same phase, it is believed that El Ni\u00f1o/La Nina impacts may be magnified. Conversely, if ENSO and the PDO are out of phase, it has been proposed that they may offset one another, preventing 'true' ENSO impacts from occurring downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Pacific Decadal Oscillation units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_pna","title":"oci_pna","text":"Field Value creator_notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The Pacific\u2013North American teleconnection pattern (PNA) is a climatological term for a large-scale weather pattern with two modes, denoted positive and negative, and which relates the atmospheric circulation pattern over the North Pacific Ocean with the one over the North American continent downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Pacific North American Index provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_soi","title":"oci_soi","text":"Field Value creator_notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The Southern Oscillation Index (SOI) is a standardised index based on the observed sea level pressure (SLP) differences between Tahiti and Darwin, Australia. The SOI is one measure of the large-scale fluctuations in air pressure occurring between the western and eastern tropical Pacific (i.e., the state of the Southern Oscillation) during El Ni\u00f1o and La Ni\u00f1a episodes downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Southern Oscillation Index provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_wp","title":"oci_wp","text":"Field Value creator_notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The WP pattern is a primary mode of low-frequency variability over the North Pacific in all months, and has been previously described by both Barnston and Livezey and Wallace and Gutzler downloaded from https://psl.noaa.gov/data/climateindices/list/ long_name Western Pacific Index provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#pop_dens","title":"pop_dens","text":"Field Value creator_notes Data available every five years (2000,2005,2010,2015,2020). Each time dimension is filled with the method pad. description The Gridded Population of the World, Version 4 (GPWv4): Population Density Adjusted to Match 2015 Revision of UN WPP Country Totals, Revision 11 consists of estimates of human population density (number of persons per square kilometer) based on counts consistent with national censuses and population registers with respect to relative spatial distribution, but adjusted to match the 2015 Revision of the United Nation's World Population Prospects (UN WPP) country totals, for the years 2000, 2005, 2011, 2015, and 2020. A proportional allocation gridding algorithm, utilizing approximately 13.5 million national and sub-national administrative units, was used to assign UN WPP-adjusted population counts to 30 arc-second grid cells. The density rasters were created by dividing the UN WPP-adjusted population count raster for a given target year by the land area raster. The data files were produced as global rasters at 30 arc-second (~1 km at the equator) resolution. To enable faster global processing, and in support of research communities, the 30 arc-second adjusted count data were aggregated to 2.5 arc-minute, 15 arc-minute, 30 arc-minute and 1 degree resolutions to produce density rasters at these resolutions downloaded_from https://sedac.ciesin.columbia.edu/data/set/gpw-v4-population-density-adjusted-to-2015-unwpp-country-totals-rev11 long_name UN WPP-Adjusted Population Density provider Socioeconomic Data and Applications Center (sedac) units Persons per square kilometer"},{"location":"datasets/SeasFireCube_v3-zarr/#rel_hum","title":"rel_hum","text":"Field Value aggregation Temporal creator_notes This variable is calculated in-house, with Tetens formula for calculation of the saturation vapour pressure of water over liquid and ice(es) and surface pressure (Spressure).eq: Rh=(q \u00d7Spresure)/((0.622 + q)*es)*100 description This parameter is the water vapour pressure as a percentage of the value at which the air becomes saturated (the point at which water vapour begins to condense into liquid water or deposition into ice). For temperatures over 0\u00b0C (273.15 K) it is calculated for saturation over water. At temperatures below -23\u00b0C it is calculated for saturation over ice. Between -23\u00b0C and 0\u00b0C this parameter is calculated by interpolating between the ice and water values using a quadratic function. long_name Relative humidity provider ERA5 units %"},{"location":"datasets/SeasFireCube_v3-zarr/#skt","title":"skt","text":"Field Value aggregation Temporal description This parameter is the temperature of the surface of the Earth. The skin temperature is the theoretical temperature that is required to satisfy the surface energy balance. It represents the temperature of the uppermost surface layer, which has no heat capacity and so can respond instantaneously to changes in surface fluxes. Skin temperature is calculated differently over land and sea. This parameter has units of kelvin (K). Temperature measured in kelvin can be converted to degrees Celsius (\u00b0C) by subtracting 273.15 downloaded from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-pressure-levels?tab=overview long_name Skin Temperature provider ERA5 units Kelvin (K)"},{"location":"datasets/SeasFireCube_v3-zarr/#ssr","title":"ssr","text":"Field Value aggregation Temporal description This parameter is the amount of solar (shortwave) radiation reaching the surface of the Earth (both direct and diffuse) minus the amount reflected by the Earth's surface (which is governed by the albedo), assuming clear-sky (cloudless) conditions. It is the amount of radiation passing through a horizontal plane, not a plane perpendicular to the direction of the Sun downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Surface net solar radiation provider ERA5 units MJ m-2"},{"location":"datasets/SeasFireCube_v3-zarr/#ssrd","title":"ssrd","text":"Field Value aggregation Temporal description This parameter is the amount of solar radiation (also known as shortwave radiation) that reaches a horizontal plane at the surface of the Earth. This parameter comprises both direct and diffuse solar radiation. Radiation from the Sun (solar, or shortwave, radiation) is partly reflected back to space by clouds and particles in the atmosphere (aerosols) and some of it is absorbed. The rest is incident on the Earth's surface (represented by this parameter). To a reasonably good approximation, this parameter is the model equivalent of what would be measured by a pyranometer (an instrument used for measuring solar radiation) at the surface. However, care should be taken when comparing model parameters with observations, because observations are often local to a particular point in space and time, rather than representing averages over a model grid box. This parameter is accumulated over a particular time period which depends on the data extracted. For the reanalysis, the accumulation period is over the 1 hour ending at the validity date and time. For the ensemble members, ensemble mean and ensemble spread, the accumulation period is over the 3 hours ending at the validity date and time. The units are joules per square metre (J m-2 ). To convert to watts per square metre (W m-2 ), the accumulated values should be divided by the accumulation period expressed in seconds. The ECMWF convention for vertical fluxes is positive downwards. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Surface net solar radiation downwards provider ERA5-Land units MJ m-2"},{"location":"datasets/SeasFireCube_v3-zarr/#sst","title":"sst","text":"Field Value description This parameter (SST) is the temperature of sea water near the surface. In ERA5, this parameter is a foundation SST, which means there are no variations due to the daily cycle of the sun (diurnal variations). SST, in ERA5, is given by two external providers. Before September 2007, SST from the HadISST2 dataset is used and from September 2007 onwards, the OSTIA dataset is used. This parameter has units of kelvin (K). Temperature measured in kelvin can be converted to degrees Celsius (\u00b0C) by subtracting 273.15. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Sea surface temperature provider ERA5 units K"},{"location":"datasets/SeasFireCube_v3-zarr/#swvl1","title":"swvl1","text":"Field Value aggregation Temporal description This parameter is the volume of water in soil layer 1 (0 - 7cm, the surface is at 0cm). The ECMWF Integrated Forecasting System (IFS) has a four-layer representation of soil: Layer 1: 0 - 7cm, Layer 2: 7 - 28cm, Layer 3: 28 - 100cm, Layer 4: 100 - 289cm. Soil water is defined over the whole globe, even over ocean. Regions with a water surface can be masked out by only considering grid points where the land-sea mask has a value greater than 0.5 (lsm variable). The volumetric soil water is associated with the soil texture (or classification), soil depth, and the underlying groundwater level. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Volumetric soil water layer 1 provider ERA5-Land units m3 m-3"},{"location":"datasets/SeasFireCube_v3-zarr/#swvl2","title":"swvl2","text":"Field Value aggregation Temporal description This parameter is the volume of water in soil layer 2 (7 - 28cm, the surface is at 0cm). The ECMWF Integrated Forecasting System (IFS) has a four-layer representation of soil: Layer 1: 0 - 7cm, Layer 2: 7 - 28cm, Layer 3: 28 - 100cm, Layer 4: 100 - 289cm. Soil water is defined over the whole globe, even over ocean. Regions with a water surface can be masked out by only considering grid points where the land-sea mask has a value greater than 0.5. The volumetric soil water is associated with the soil texture (or classification), soil depth, and the underlying groundwater level. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Volumetric soil water layer 2 provider ERA5-Land standard_name swvl2 units m3 m-3"},{"location":"datasets/SeasFireCube_v3-zarr/#swvl3","title":"swvl3","text":"Field Value aggregation Temporal description This parameter is the volume of water in soil layer 3 (28 - 100cm, the surface is at 0cm). The ECMWF Integrated Forecasting System (IFS) has a four-layer representation of soil: Layer 1: 0 - 7cm, Layer 2: 7 - 28cm, Layer 3: 28 - 100cm, Layer 4: 100 - 289cm. Soil water is defined over the whole globe, even over ocean. Regions with a water surface can be masked out by only considering grid points where the land-sea mask has a value greater than 0.5. The volumetric soil water is associated with the soil texture (or classification), soil depth, and the underlying groundwater level. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Volumetric soil water layer 3 provider ERA5-Land standard_name swvl3 units m3 m-3"},{"location":"datasets/SeasFireCube_v3-zarr/#swvl4","title":"swvl4","text":"Field Value aggregation Temporal description This parameter is the volume of water in soil layer 4 (100 - 289cm, the surface is at 0cm). The ECMWF Integrated Forecasting System (IFS) has a four-layer representation of soil: Layer 1: 0 - 7cm, Layer 2: 7 - 28cm, Layer 3: 28 - 100cm, Layer 4: 100 - 289cm. Soil water is defined over the whole globe, even over ocean. Regions with a water surface can be masked out by only considering grid points where the land-sea mask has a value greater than 0.5. The volumetric soil water is associated with the soil texture (or classification), soil depth, and the underlying groundwater level. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Volumetric soil water layer 4 provider ERA5-Land standard_name swvl4 units m3 m-3"},{"location":"datasets/SeasFireCube_v3-zarr/#t2m_max","title":"t2m_max","text":"Field Value aggregation Temporal description This parameter is the temperature of air at 2m above the surface of land, sea or inland waters. 2m temperature is calculated by interpolating between the lowest model level and the Earth's surface, taking account of the atmospheric conditions. This parameter has units of kelvin (K). Temperature measured in kelvin can be converted to degrees Celsius (\u00b0C) by subtracting 273.15. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name 2 meters temperature - Maximum value provider ERA5-Land units Kelvin (K)"},{"location":"datasets/SeasFireCube_v3-zarr/#t2m_mean","title":"t2m_mean","text":"Field Value aggregation Temporal description This parameter is the temperature of air at 2m above the surface of land, sea or inland waters. 2m temperature is calculated by interpolating between the lowest model level and the Earth's surface, taking account of the atmospheric conditions. This parameter has units of kelvin (K). Temperature measured in kelvin can be converted to degrees Celsius (\u00b0C) by subtracting 273.15. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name 2 meters temperature - Mean value provider ERA5 units Kelvin (K)"},{"location":"datasets/SeasFireCube_v3-zarr/#t2m_min","title":"t2m_min","text":"Field Value aggregation Temporal description This parameter is the temperature of air at 2m above the surface of land, sea or inland waters. 2m temperature is calculated by interpolating between the lowest model level and the Earth's surface, taking account of the atmospheric conditions. This parameter has units of kelvin (K). Temperature measured in kelvin can be converted to degrees Celsius (\u00b0C) by subtracting 273.15. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name 2 meters temperature - Minimum Value provider ERA5 units Kelvin (K)"},{"location":"datasets/SeasFireCube_v3-zarr/#tp","title":"tp","text":"Field Value aggregation Temporal description This parameter is the accumulated liquid and frozen water, comprising rain and snow, that falls to the Earth's surface. It is the sum of large-scale precipitation and convective precipitation. Large-scale precipitation is generated by the cloud scheme in the ECMWF Integrated Forecasting System (IFS). The cloud scheme represents the formation and dissipation of clouds and large-scale precipitation due to changes in atmospheric quantities (such as pressure, temperature and moisture) predicted directly by the IFS at spatial scales of the grid box or larger. Convective precipitation is generated by the convection scheme in the IFS, which represents convection at spatial scales smaller than the grid box. This parameter does not include fog, dew or the precipitation that evaporates in the atmosphere before it lands at the surface of the Earth. This parameter is accumulated over a particular time period which depends on the data extracted. For the reanalysis, the accumulation period is over the 1 hour ending at the validity date and time. For the ensemble members, ensemble mean and ensemble spread, the accumulation period is over the 3 hours ending at the validity date and time. The units of this parameter are depth in metres of water equivalent. It is the depth the water would have if it were spread evenly over the grid box. Care should be taken when comparing model parameters with observations, because observations are often local to a particular point in space and time, rather than representing averages over a model grid box. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Total precipitation provider ERA5 units mm"},{"location":"datasets/SeasFireCube_v3-zarr/#vpd","title":"vpd","text":"Field Value aggregation Temporal creator_notes This variable is calculated in-house,with Tetens equation (es) and relative humidity(ea = (es*relat_humid)/100). Vapour_pres_def=(es-ea)*10 description Vapour-pressure deficit, or VPD, is the difference (deficit) between the amount of moisture in the air and how much moisture the air can hold when it is saturated. Once air becomes saturated, water will condense out to form clouds, dew or films of water over leaves. long_name Vapour Pressure Deficit units hPa"},{"location":"datasets/SeasFireCube_v3-zarr/#ws10","title":"ws10","text":"Field Value aggregation Temporal description This variable is the combination of the eastward (U) and northward (V) component of wind at 10 meters, and gives the speed and direction of the horizontal wind. A negative sign indicated air moving towards south-west downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Windspeed of the 10m wind [sqrt(u10^2+v10^2)] provider ERA5 units m*s-2"},{"location":"datasets/SeasFireCube_v3-zarr/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value crs EPSG:4326 description The SeasFire Cube is a scientific datacube for seasonal fire forecasting around the globe. It has been created for the SeasFire project, that adresses 'Earth System Deep Learning for Seasonal Fire Forecasting' and is funded by the European Space Agency (ESA)  in the context of ESA Future EO-1 Science for Society Call. It contains almost 20 years of data (2001-2021) in an 8-days time resolution and 0.25 degrees grid resolution. It has a diverse range of seasonal fire drivers. It expands from atmospheric and climatological ones to vegetation variables, socioeconomic and the target variables related to wildfires such as burned areas, fire radiative power, and wildfire-related CO2 emissions. title SeasFire Cube: A Global Dataset for Seasonal Fire Modeling in the Earth System"},{"location":"datasets/black-sea/","title":"Black Sea Cube","text":""},{"location":"datasets/black-sea/#black-sea-data-cube","title":"Black Sea Data Cube","text":""},{"location":"datasets/black-sea/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\n# The cube is saved as a multilevel cube, the level 0 is the base layer with \n# the highest resolution\nml_dataset = store.open_data('black-sea-1x1024x1024.levels')\n# Chek how many levels are present\nml_dataset.num_levels\n# Open dataset at a certain level, here level 0\nds = ml_dataset.get_dataset(0)\n</code></pre>"},{"location":"datasets/black-sea/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/black-sea/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) 26.0 to 41.998999999999725 Bounding box latitude (\u00b0) 39.000000000000156 to 48.0 Time range 2015-12-31 to 2017-12-31 Time period 1D Publisher Brockmann Consult GmbH <p>Click here for full dataset metadata.</p>"},{"location":"datasets/black-sea/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units VHM0 Spectral Significant Wave Height (Hm0) m chl Chlorophyll Concentration mg m^-3 sla Sea Level Anomaly m sss Sea Surface Salinity psu sst Sea Surface Temperature K ugos Absolute Geostrophic Velocity: Zonal Component m s^-1 ugosa Geostrophic Velocity Anomalies: Zonal Component m s^-1 vgos Absolute Geostrophic Velocity: Meridian Component m s^-1 vgosa Geostrophic Velocity Anomalies: Meridian Component m s^-1"},{"location":"datasets/black-sea/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/black-sea/#vhm0","title":"VHM0","text":"Field Value grid_mapping crs long_name Spectral Significant Wave Height (Hm0) processing_level L4 references https://resources.marine.copernicus.eu/product-detail/BLKSEA_MULTIYEAR_WAV_007_006/DOCUMENTATION source CMEMS, Black Sea Waves Reanalysis standard_name sea_surface_wave_significant_height units m valid_max 6.0 valid_min 0.0"},{"location":"datasets/black-sea/#chl","title":"chl","text":"Field Value grid_mapping crs long_name Chlorophyll Concentration processing_level L3 references http://www.eo4sibs.uliege.be/doc/EO4SIBS_DUM_ATBD_OceanColour.pdf source EO4SIBS, Level 3 Chl-a, 300m, daily and monthly standard_name chlorophyll_concentration units mg m^-3 valid_max 31.0 valid_min 0.0"},{"location":"datasets/black-sea/#crs","title":"crs","text":"Field Value crs_wkt GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]] geographic_crs_name WGS 84 grid_mapping_name latitude_longitude inverse_flattening 298.257223563 longitude_of_prime_meridian 0.0 prime_meridian_name Greenwich reference_ellipsoid_name WGS 84 semi_major_axis 6378137.0 semi_minor_axis 6356752.314245179"},{"location":"datasets/black-sea/#sla","title":"sla","text":"Field Value grid_mapping crs long_name Sea Level Anomaly processing_level L4 references http://www.eo4sibs.uliege.be/doc/EO4SIBS_D4.4_AltimetryL4_DUM_v1.1.pdf source EO4SIBS, Level 4, geostrophic currents, multi-mission gridded merged products for a period of 1 year, 0.0625\u00b0*0.0625\u00b0, daily standard_name sea_surface_height_above_sea_level units m valid_max 0.5 valid_min -0.5"},{"location":"datasets/black-sea/#sss","title":"sss","text":"Field Value grid_mapping crs long_name Sea Surface Salinity processing_level L3 references http://www.eo4sibs.uliege.be/doc/EO4SIBS_DUM_ATBD_Salinity.pdf source EO4SIBS, Level 3 SSS, 0.25\u00b0*0.25\u00b0, 9-day averaged produced daily standard_name sea_surface_salinity units psu valid_max 29.0 valid_min 1.0"},{"location":"datasets/black-sea/#sst","title":"sst","text":"Field Value grid_mapping crs long_name Sea Surface Temperature processing_level L3 references https://resources.marine.copernicus.eu/product-detail/SST_BS_SST_L3S_NRT_OBSERVATIONS_010_013/DOCUMENTATION source CMEMS, Black Sea - High Resolution and Ultra High Resolution L3S Sea Surface Temperature standard_name sea_surface_temperature units K valid_max 305.0 valid_min 270.0"},{"location":"datasets/black-sea/#ugos","title":"ugos","text":"Field Value grid_mapping crs long_name Absolute Geostrophic Velocity: Zonal Component processing_level L4 references http://www.eo4sibs.uliege.be/doc/EO4SIBS_D4.4_AltimetryL4_DUM_v1.1.pdf source EO4SIBS, Level 4, geostrophic currents, multi-mission gridded merged products for a period of 1 year, 0.0625\u00b0*0.0625\u00b0, daily standard_name surface_geostrophic_eastward_sea_water_velocity units m s^-1 valid_max 2.0 valid_min -2.0"},{"location":"datasets/black-sea/#ugosa","title":"ugosa","text":"Field Value grid_mapping crs long_name Geostrophic Velocity Anomalies: Zonal Component processing_level L4 references http://www.eo4sibs.uliege.be/doc/EO4SIBS_D4.4_AltimetryL4_DUM_v1.1.pdf source EO4SIBS, Level 4, geostrophic currents, multi-mission gridded merged products for a period of 1 year, 0.0625\u00b0*0.0625\u00b0, daily standard_name surface_geostrophic_eastward_sea_water_velocity_assuming_sea_level_for_geoid units m s^-1 valid_max 2.0 valid_min -2.0"},{"location":"datasets/black-sea/#vgos","title":"vgos","text":"Field Value grid_mapping crs long_name Absolute Geostrophic Velocity: Meridian Component processing_level L4 references http://www.eo4sibs.uliege.be/doc/EO4SIBS_D4.4_AltimetryL4_DUM_v1.1.pdf source EO4SIBS, Level 4, geostrophic currents, multi-mission gridded merged products for a period of 1 year, 0.0625\u00b0*0.0625\u00b0, daily standard_name surface_geostrophic_northward_sea_water_velocity units m s^-1 valid_max 2.0 valid_min -2.0"},{"location":"datasets/black-sea/#vgosa","title":"vgosa","text":"Field Value grid_mapping crs long_name Geostrophic Velocity Anomalies: Meridian Component processing_level L4 references http://www.eo4sibs.uliege.be/doc/EO4SIBS_D4.4_AltimetryL4_DUM_v1.1.pdf source EO4SIBS, Level 4, geostrophic currents, multi-mission gridded merged products for a period of 1 year, 0.0625\u00b0*0.0625\u00b0, daily standard_name surface_geostrophic_northward_sea_water_velocity_assuming_sea_level_for_geoid units m s^-1 valid_max 2.0 valid_min -2.0"},{"location":"datasets/black-sea/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.9 acknowledgment EO4SIBS, CMEMS, DeepESDL project contributor_name Brockmann Geomatics Sweden AB contributor_url www.brockmann-geomatics.se creator_email info@brockmann-consult.de creator_name Brockmann Consult GmbH creator_url www.brockmann-consult.de date_modified 2022-08-19 16:19:15.359970 geospatial_lat_max 47.9985 geospatial_lat_min 39.001500000000156 geospatial_lat_resolution 0.0030000000000001137 geospatial_lon_max 41.997499999999725 geospatial_lon_min 26.0015 geospatial_lon_resolution 0.0030000000000001137 id black-sea-256x256x256 institution Brockmann Consult GmbH license Terms and conditions of the DeepESDL data distribution project DeepESDL publisher_email info@brockmann-consult.de publisher_name Brockmann Consult GmbH publisher_url www.brockmann-consult.de source EO4SIBS, CMEMS time_coverage_end 2017-12-31T00:00:00.000000000 time_coverage_start 2016-01-01T00:00:00.000000000 title Black Sea Data Cube"},{"location":"datasets/datasets/","title":"List of datasets","text":""},{"location":"datasets/datasets/#datasets","title":"Datasets","text":"<p>Data access is possible in two main manners within DeepESDL. xcube  data stores provide on-the-fly access to datasets via the data stores framework. Other datasets are made anaylsis-ready and persisted in object storage for fastest access. </p> <ul> <li>xcube Data Stores</li> <li>Earth System Data Cube</li> <li>Black Sea Cube</li> <li>Land Cover Cube</li> <li>Ocean Cube</li> <li>SMOS freeze/thaw Cube</li> <li>SMOS ocean salinity Cube</li> <li>SMOS soil moistrue Cube</li> <li>Polar Cube</li> <li>Permafrost Cube</li> <li>Hydrology Cube</li> </ul>"},{"location":"datasets/datastores/","title":"xcube Data Stores","text":""},{"location":"datasets/datastores/#xcube-data-stores","title":"xcube Data Stores","text":"<p>The xcube data stores Framework is described in detail in the xcube  documentation. In this section, the currently available data stores are presented with an  example for each of them. For more detailed examples, please refer to the  getting started notebook section of the jupyterlab  user guide.</p> <ul> <li>DeepESDL public datasets</li> <li>xcube CMEMS data store</li> <li>xcube ESA CCI data store</li> <li>xcube Sentinel Hub data store</li> <li>xcube Copernicus Climate Change Service data store</li> </ul>"},{"location":"datasets/datastores/#deepesdl-datasets-in-public-object-storage","title":"DeepESDL datasets in public object storage","text":"<p>The DeepESDL Consortium provides a growing number of datasets which are made  analysis-ready and persisted in public object storage for easy access.  An overview of available persisted datasets is given section  datasets. </p> <p>To access the DeepESDL persisted datasets, please follow the example: </p> <p>Initializing the xcube datastore for s3 object storage: <pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", \n                       root=\"deep-esdl-public\", \n                       storage_options=dict(anon=True))\n</code></pre> List all available datasets: </p> <pre><code>store.list_data_ids()\n</code></pre> <p>To open a certain dataset, please replace the path with the path to the  desired dataset, which was listed in <code>store.list_data_ids()</code></p> <pre><code>dataset = store.open_data('black-sea-1x1024x1024.zarr')\n</code></pre>"},{"location":"datasets/datastores/#contribute-datacubes-to-the-public","title":"Contribute datacubes to the public","text":"<p>If you have created your own dataset and wish to make it available to the  public in the deep-esdl-public cloud bucket, that's fantastic! For onboarding new data cubes to deep-esdl-public, you need to make sure  your dataset creation is documented and reproducible, and that the  documentation is accessible. This can be e.g. your own GitHub repository, or  you could contribute to the DeepESDL cube-gen repository.</p>"},{"location":"datasets/datastores/#xcube-cmems-data-store","title":"xcube CMEMS data store","text":"<p>The xcube Copernicus Marine Data Store (CMEMS) data store allows for reading and  exploring data from the  Copernicus Marine Data Store.</p> <p>A user login is required to access the data provided by CMEMS. If you do not have cmems user yet, you can register for an  account.  For DeepESDL Jupyter Lab default credentials are configured, but due to  bandwidth  limitation by CMEMS performance may impair when used by several people simultaneously. </p> <p>To use your own CMEMS account: <pre><code># replace with your user name and pwd\n#cmemsuser='your-user-name'\n#cmemspwd='your-user-password'\n</code></pre> Initializing the xcube data store for CMEMS:</p> <pre><code>from xcube.core.store import new_data_store\nstore = new_data_store('cmems')\n</code></pre> <p>If you use your own CMEMS account: </p> <pre><code># store = new_data_store('cmems', \n#                        cmems_user=cmemsuser, \n#                        cmems_user_password = cmemspwd)\n# store = new_data_store('cmems')\n</code></pre> <p>List all available datasets: </p> <pre><code>store.list_data_ids()\n</code></pre> <p>To open a certain dataset, please replace the path with the path to the  desired dataset, which was listed in <code>store.list_data_ids()</code></p> <pre><code>dataset = store.open_data('dataset-bal-reanalysis-wav-hourly',\n                          'dataset:zarr:cmems')\n</code></pre>"},{"location":"datasets/datastores/#xcube-esa-cci-data-store","title":"xcube ESA CCI data store","text":"<p>The xcube ESA Climate Change Initiative (CCI) data store allows for reading and  exploring data from the  ESA Climate Change Initiative. More information on the data sets offered can be found in the Open Data Portal.</p> <p>Initializing the xcube data store for CCI:</p> <pre><code>from xcube.core.store import new_data_store\nstore = new_data_store('cciodp')\n</code></pre> <p>The cci data store offers many datasets, therefore listing all available ones  will reveal a long list of results. The store can thus be searched by specific  parameters, which can be listed:</p> <pre><code>store.get_search_params_schema()\n</code></pre> <p>A target data set can then be be opened following the below schema example below:</p> <pre><code>dataset = store.open_data('esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.sst',\n                          variable_names=['analysed_sst'],\n                          time_range=['1981-08-31','2016-12-31'])\n</code></pre>"},{"location":"datasets/datastores/#xcube-sentinel-hub-data-store","title":"xcube Sentinel Hub data store","text":"<p>The xcube Sentinel Hub (SH) data store allows for reading and exploring data provided by the  Sentinel Hub cloud API.</p> <p>Please note: In order to access data from the commercial Sentinel Hub service, you need Sentinel  Hub API credentials. </p> <p>DeepESDL users may apply for sponsored Sentinel Hub subscriptions -  please contact the DeepESDL team or directly appy via the Network of Resources portal.</p> <p>Initializing the xcube data store for Sentinel Hub:</p> <pre><code>from xcube.core.store import new_data_store\nstore = new_data_store('sentinelhub', num_retries=400)\n</code></pre> <p>The Sentinel Hub xcube data store offers different datasets which are  accessible via data ids:</p> <pre><code>list(store.get_data_ids())\n</code></pre> <p>For requesting Sentinel Hub data subsetting is crucial. In the below example  a Sentinel-2 L2A is requested.</p> <pre><code>dataset = store.open_data('S2L2A', \n                          variable_names=['B04'], \n                          bbox=[9.7, 53.4, 10.2, 53.7], \n                          spatial_res=0.00018, \n                          time_range=('2020-08-10','2020-08-20'), \n                          time_period='1D',\n                          tile_size= [1024, 1024])\n</code></pre>"},{"location":"datasets/datastores/#xcube-copernicus-climate-change-service-c3s-data-store","title":"xcube Copernicus Climate Change Service (C3S) data store","text":"<p>The xcube Climate Data Store (CDS) allows to read and explore temperature data from the Copernicus Climate  Change Service (C3S). </p> <p>To access data from the Climate Data Store, you need a CDS API key. You can  obtain the UID and API key as follows:</p> <ol> <li>Create a user account on the CDS Website.</li> <li>Log in to the website with your user name and password.</li> <li>Navigate to your user profile on the website. Your API key is shown        at the bottom of the page.</li> </ol> <p>Then export the <code>CDSAPI_URL</code> and <code>CDSAPI_KEY</code> environment variables,  replacing <code>[UID]</code> and <code>[API-KEY]</code> with the actual values from your account:</p> <pre><code>export CDSAPI_URL=https://cds.climate.copernicus.eu/api/v2\nexport CDSAPI_KEY=[UID]:[API-KEY]\n</code></pre> <p>For DeepESDL Jupyter Lab default credentials are configured, but due to  bandwidth limitation by CDS performance may impair when used by several  people simultaneously. </p> <p>Initializing the xcube data store for C3S:</p> <pre><code>from xcube.core.store import new_data_store\nstore = new_data_store('cds')\n</code></pre> <p>List all available datasets: </p> <pre><code>store.list_data_ids()\n</code></pre> <p>Get more info about a specific dataset. This includes a description of the  possible open formats:</p> <p><pre><code>store.describe_data('reanalysis-era5-single-levels-monthly-means:monthly_averaged_reanalysis')\n</code></pre> There are 4 required parameters, so we need to provide them to open a dataset:</p> <pre><code>dataset = store.open_data('reanalysis-era5-single-levels-monthly-means:monthly_averaged_reanalysis', \n                          variable_names=['2m_temperature'], \n                          bbox=[-10, 45, 40, 65], \n                          spatial_res=0.25, \n                          time_range=['2001-01-01', '2010-12-31'])\n</code></pre>"},{"location":"datasets/datastores/#contribute-a-xcube-plugin-for-a-new-data-store","title":"Contribute a xcube plugin for a new data store","text":"<p>Do you have an API or a data source which you wish to make available via the  xcube data stores framework? We would be very happy if you like to  contribute to the open source xcube software ecosystem!  The xcube data stores framework is described in detail in the xcube  documentation.  There you can have a look at the details of what is mandatory for a data  store. Furthermore, you can get inspiration from existing data store plugins:</p> <ul> <li>xcube-cds</li> <li>xcube-cci</li> <li>xcube-cmems</li> <li>xcube-sh</li> </ul> <p>In case of questions, please feel free to reach out to us! We will give  our best to support you :)</p>"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/","title":"Permafrost Cube","text":""},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#esa-cci-permafrost","title":"ESA CCI PERMAFROST","text":""},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\n# The cube is saved as a multilevel cube, the level 0 is the base layer with \n# the highest resolution\nml_dataset = store.open_data('esa-cci-permafrost-1x1151x1641-0.0.2.levels')\n# Chek how many levels are present\nml_dataset.num_levels\n# Open dataset at a certain level, here level 0\nds = ml_dataset.get_dataset(0)\n</code></pre>"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180 to 180 Bounding box latitude (\u00b0) 0 to 90 Time range 2000-01-01 to 2020-01-01 <p>Click here for full dataset metadata.</p>"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units GST GST surface temperature degrees celsius T10m T10m solid earth subsurface temperature degrees celsius T1m T1m solid earth subsurface temperature degrees celsius T2m T2m solid earth subsurface temperature degrees celsius T5m T5m solid earth subsurface temperature degrees celsius polar_stereographic [none] [none]"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#gst","title":"GST","text":"Field Value color_bar_name coolwarm color_value_max 30 color_value_min -30 grid_mapping polar_stereographic long_name GST surface temperature orig_data_type uint16 standard_name surface_temperature units degrees celsius valid_max 100 valid_min -100"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#t10m","title":"T10m","text":"Field Value color_bar_name coolwarm color_value_max 30 color_value_min -30 grid_mapping polar_stereographic long_name T10m solid earth subsurface temperature orig_data_type uint16 standard_name solid_earth_subsurface_temperature units degrees celsius valid_max 100 valid_min -100"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#t1m","title":"T1m","text":"Field Value color_bar_name coolwarm color_value_max 30 color_value_min -30 grid_mapping polar_stereographic long_name T1m solid earth subsurface temperature orig_data_type uint16 standard_name solid_earth_subsurface_temperature units degrees celsius valid_max 100 valid_min -100"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#t2m","title":"T2m","text":"Field Value color_bar_name coolwarm color_value_max 30 color_value_min -30 grid_mapping polar_stereographic long_name T2m solid earth subsurface temperature orig_data_type uint16 standard_name solid_earth_subsurface_temperature units degrees celsius valid_max 100 valid_min -100"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#t5m","title":"T5m","text":"Field Value color_bar_name coolwarm color_value_max 30 color_value_min -30 grid_mapping polar_stereographic long_name T5m solid earth subsurface temperature orig_data_type uint16 standard_name solid_earth_subsurface_temperature units degrees celsius valid_max 100 valid_min -100"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#polar_stereographic","title":"polar_stereographic","text":"Field Value GeoTransform -8679599.425969256 926.6254331383326 0 4120958.560533902 0 -926.6254331383326 chunk_sizes crs_wkt PROJCRS[\"WGS 84 / Arctic Polar Stereographic\",BASEGEOGCRS[\"WGS 84\",ENSEMBLE[\"World Geodetic System 1984 ensemble\",MEMBER[\"World Geodetic System 1984 (Transit)\"],MEMBER[\"World Geodetic System 1984 (G730)\"],MEMBER[\"World Geodetic System 1984 (G873)\"],MEMBER[\"World Geodetic System 1984 (G1150)\"],MEMBER[\"World Geodetic System 1984 (G1674)\"],MEMBER[\"World Geodetic System 1984 (G1762)\"],MEMBER[\"World Geodetic System 1984 (G2139)\"],ELLIPSOID[\"WGS 84\",6378137,298.257223563,LENGTHUNIT[\"metre\",1]],ENSEMBLEACCURACY[2.0]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433]],ID[\"EPSG\",4326]],CONVERSION[\"Arctic Polar Stereographic\",METHOD[\"Polar Stereographic (variant B)\",ID[\"EPSG\",9829]],PARAMETER[\"Latitude of standard parallel\",71,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8832]],PARAMETER[\"Longitude of origin\",0,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8833]],PARAMETER[\"False easting\",0,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",0,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]]],CS[Cartesian,2],AXIS[\"easting (X)\",south,MERIDIAN[90,ANGLEUNIT[\"degree\",0.0174532925199433]],ORDER[1],LENGTHUNIT[\"metre\",1]],AXIS[\"northing (Y)\",south,MERIDIAN[180,ANGLEUNIT[\"degree\",0.0174532925199433]],ORDER[2],LENGTHUNIT[\"metre\",1]],USAGE[SCOPE[\"Polar research.\"],AREA[\"Northern hemisphere - north of 60\u00b0N onshore and offshore, including Arctic.\"],BBOX[60,-180,90,180]],ID[\"EPSG\",3995]] data_type int64 dimensions esri_wkt PROJCS[\\\"WGS_84_Arctic_Polar_Stereographic\\\",GEOGCS[\\\"GCS_WGS_1984\\\",DATUM[\\\"D_WGS_1984\\\",SPHEROID[\\\"WGS_1984\\\",6378137,298.257223563]],PRIMEM[\\\"Greenwich\\\",0],UNIT[\\\"Degree\\\",0.017453292519943295]],PROJECTION[\\\"Stereographic_North_Pole\\\"],PARAMETER[\\\"standard_parallel_1\\\",71],PARAMETER[\\\"central_meridian\\\",0],PARAMETER[\\\"false_easting\\\",0],PARAMETER[\\\"false_northing\\\",0],UNIT[\\\"Meter\\\",1]] false_easting 0.0 false_northing 0.0 file_chunk_sizes 1 file_dimensions fill_value 9223372036854775807 geographic_crs_name WGS 84 grid_mapping_name polar_stereographic horizontal_datum_name World Geodetic System 1984 ensemble inverse_flattening 298.257223563 latitude_of_projection_origin 90.0 longitude_of_prime_meridian 0.0 orig_data_type int32 prime_meridian_name Greenwich projected_crs_name WGS 84 / Arctic Polar Stereographic reference_ellipsoid_name WGS 84 semi_major_axis 6378137.0 semi_minor_axis 6356752.314245179 shape size 1 spatial_ref GEOGCS[\\\"WGS 84\\\",DATUM[\\\"WGS_1984\\\",SPHEROID[\\\"WGS 84\\\",6378137,298.257223563,AUTHORITY[\\\"EPSG\\\",\\\"7030\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"6326\\\"]],PRIMEM[\\\"Greenwich\\\",0],UNIT[\\\"degree\\\",0.0174532925199433],AUTHORITY[\\\"EPSG\\\",\\\"4326\\\"]],PROJECTION[\\\"Polar_Stereographic\\\"],PARAMETER[\\\"latitude_of_origin\\\",45],PARAMETER[\\\"central_meridian\\\",-170],PARAMETER[\\\"scale_factor\\\",1],PARAMETER[\\\"false_easting\\\",0],PARAMETER[\\\"false_northing\\\",0],UNIT[\\\"metre\\\",1,AUTHORITY[\\\"EPSG\\\",\\\"9001\\\"]] standard_parallel 71.0 straight_vertical_longitude_from_pole 0.0"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.7 date_created 2023-05-08 history cube_params: time_range: 2019-01-01T00:00:00, 2019-12-31T00:00:00, variable_names: polar_stereographic, GST, T1m, T2m, T5m, T10m, program: xcube_cci.chunkstore.CciChunkStore id esa-cci-permafrost-1x1151x1641-0.0.2 processing_level L4 project DeepESDL source esacci.PERMAFROST.yr.L4.GTD.multi-sensor.multi-platform.ERA5_MODISLST_BIASCORRECTED.03-0.r1, esacci.PERMAFROST.yr.L4.GTD.multi-sensor.multi-platform.MODISLST_CRYOGRID.03-0.r1 time_coverage_duration P365DT0H0M0S time_coverage_end 2019-07-02T12:00:00 time_coverage_start 2000-07-02T12:00:00 title ESA CCI PERMAFROST"},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/","title":"extrAIM merged cube 1x86x179 zarr","text":""},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#merged-extraim-dataset","title":"Merged extrAIM dataset","text":""},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('extrAIM-merged-cube-1x86x179.zarr')\n</code></pre>"},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -6.375 to 38.375 Bounding box latitude (\u00b0) 27.625 to 49.125 Time range 2007-01-01 to 2021-10-01 Time period 1D <p>Click here for full dataset metadata.</p>"},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units precip [none] mm"},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#precip","title":"precip","text":"Field Value color_bar_name coolwarm color_value_max 40 color_value_min 0 units mm"},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value history The extrAIM consortium, Sat Feb 24 12:41:25 2024 institution extrAIM title Merged extrAIM dataset"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/","title":"Hydrology Cube","text":""},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#hydrology-cube","title":"Hydrology Cube","text":""},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('hydrology-1D-0.009deg-100x60x60-3.0.2.zarr')\n</code></pre>"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -5.70080002975464 to 37.76919997024536 Bounding box latitude (\u00b0) 28.339799316406264 to 48.17579931640626 Time range 2014-12-31 to 2022-10-06 Time period 1D Publisher DeepESDL Team <p>Click here for full dataset metadata.</p>"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units E Evaporation mm d^-1 SM Soil Moisture % Relative Saturation SWE Snow Water Equivalent 1 precip Precipitation mm d^-1"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#e","title":"E","text":"Field Value acknowledgement Hydrology 4D color_bar_name plasma color_value_max 10 color_value_min 0 description Evaporation long_name Evaporation original_add_offset 0.0 original_name E original_scale_factor 1.0 processing_steps Gridding nc datasets source 4dmed_data.eodchosting.eu/4dmed_data/GLEAM_openloop_V1.1 standard_name evaporation units mm d^-1"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#sm","title":"SM","text":"Field Value acknowledgement Hydrology 4D color_bar_name plasma_r color_value_max 1 color_value_min 0 description TU Wien RT1-Sentinel-1 soil moisutre retrievals long_name Soil Moisture original_add_offset 0.0 original_name SM original_scale_factor 1.0 processing_steps Gridding nc datasets, daily aggregates source 4dmed_data.eodchosting.eu/4dmed_data/TUWien_RT1_SM standard_name soil_moisture units % Relative Saturation untis % relative saturation"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#swe","title":"SWE","text":"Field Value acknowledgement Hydrology 4D color_bar_name Blues_alpha color_value_max 2000 color_value_min 0 description Snow Water Equivalent long_name Snow Water Equivalent original_add_offset 0.0 original_name SWE original_scale_factor 1.0 processing_steps Gridding nc datasets source 4dmed_data.eodchosting.eu/4dmed_data/SWE/SWE_CPC_GPM_ERA5downT_RadGhent_filter5mm standard_name snow_water_equivalent units 1"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#precip","title":"precip","text":"Field Value acknowledgement Hydrology 4D color_bar_name viridis_alpha color_value_max 100 color_value_min 0 description Precipitation long_name Precipitation original_add_offset 0.0 original_name precip original_scale_factor 1.0 processing_steps Gridding nc datasets source 4dmed_data.eodchosting.eu/4dmed_data/CNR_products/precipitation_GPM_CPC_SM2RAIN-ASCAT standard_name precipitation units mm d^-1"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.10 acknowledgment All data providers are acknowledged inside each variable contributor_name University of Leipzig, Brockmann Consult GmbH contributor_url https://www.uni-leipzig.de/, https://www.brockmann-consult.de/ creator_name University of Leipzig, Brockmann Consult GmbH creator_url https://www.uni-leipzig.de/, https://www.brockmann-consult.de/ date_modified 2023-12-21T11:50:17.830496 geospatial_lat_max 48.17579932 geospatial_lat_min 28.33979932 geospatial_lat_resolution 0.009 geospatial_lat_units degrees_north geospatial_lon_max 37.76919997 geospatial_lon_min -5.70080003 geospatial_lon_resolution 0.009 geospatial_lon_units degrees_east id hydrology-1D-0.009deg-100x60x60-3.0.2.zarr license Terms and conditions of the DeepESDL data distribution project DeepESDL publisher_name DeepESDL Team publisher_url https://www.earthsystemdatalab.net/ time_coverage_end 2022-10-06T12:00:00.000000000 time_coverage_start 2015-01-01T12:00:00.000000000 title Hydrology Cube"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/","title":"Ocean Cube","text":""},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#bicep-pools-and-fluxes-of-the-ocean-biological-carbon-pump","title":"BICEP Pools and Fluxes of the Ocean Biological Carbon Pump","text":""},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\n# The cube is saved as a multilevel cube, the level 0 is the base layer with \n# the highest resolution\nml_dataset = store.open_data('ocean-1M-9km-1x1080x1080-1.4.0.levels')\n# Chek how many levels are present\nml_dataset.num_levels\n# Open dataset at a certain level, here level 0\nds = ml_dataset.get_dataset(0)\n</code></pre>"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180.00001525878906 to 180.00001525878906 Bounding box latitude (\u00b0) -89.99999491138114 to 89.99999491138114 Time range 1997-09-01 to 2020-12-01 Publisher DeepESDL Team <p>Click here for full dataset metadata.</p>"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units C_microphyto Microphytoplankton Carbon mg C m^-3 C_nanophyto Nanophytoplankton Carbon mg C m^-3 C_phyto Total Phytoplankton Carbon mg C m^-3 C_picophyto Picophytoplankton Carbon mg C m^-3 DOC Dissolved Organic Carbon \u00b5mol kg^-1 EP_Dunne Export Production based on Dunne et al 2005 mg C m^-2 d^-1 EP_Henson Export Production based on Henson et al 2011 mg C m^-2 d^-1 EP_Li Export Production based on Li et al 2016 mg C m^-2 d^-1 PIC Particulate Inorganic Carbon \u00b5mol C m^-3 POC Particulate Organic Carbon mg C m^-3 chl_a Chlorophyll-a mg C m^-3 mld Mixed Layer Depth m pp Phytoplankton Primary Production mg C m^-2 d^-1"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#c_microphyto","title":"C_microphyto","text":"Field Value acknowledgement BICEP/NCEO color_bar_name YlGnBu_r color_value_max 200 color_value_min 0 description Microphytoplankton Carbon in Sea Water long_name Microphytoplankton Carbon original_add_offset 0.0 original_name C_microphyto original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/PC_1-month_9km/ standard_name mass_concentration_of_microphytoplankton_expressed_as_carbon_in_sea_water units mg C m^-3"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#c_nanophyto","title":"C_nanophyto","text":"Field Value acknowledgement BICEP/NCEO color_bar_name YlGnBu_r color_value_max 200 color_value_min 0 description Nanophytoplankton Carbon in Sea Water long_name Nanophytoplankton Carbon original_add_offset 0.0 original_name C_nanophyto original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/PC_1-month_9km/ standard_name mass_concentration_of_nanophytoplankton_expressed_as_carbon_in_sea_water units mg C m^-3"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#c_phyto","title":"C_phyto","text":"Field Value acknowledgement BICEP/NCEO color_bar_name YlGnBu_r color_value_max 200 color_value_min 0 description Total Phytoplankton Carbon in Sea Water long_name Total Phytoplankton Carbon original_add_offset 0.0 original_name C_phyto original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/PC_1-month_9km/ standard_name mass_concentration_of_phytoplankton_expressed_as_carbon_in_sea_water units mg C m^-3"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#c_picophyto","title":"C_picophyto","text":"Field Value acknowledgement BICEP/NCEO color_bar_name YlGnBu_r color_value_max 200 color_value_min 0 description Picophytoplankton Carbon in Sea Water long_name Picophytoplankton Carbon original_add_offset 0.0 original_name C_picophyto original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/PC_1-month_9km/ standard_name mass_concentration_of_picophytoplankton_expressed_as_carbon_in_sea_water units mg C m^-3"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#doc","title":"DOC","text":"Field Value Derived from In-situ DOC, Ocean Colour, SST, Primary Production, Salinity acknowledgement BICEP/NCEO color_bar_name RdPu_r color_value_max 150 color_value_min 0 description Dissolved organic carbon estimated using EO data and random forest algorithm long_name Dissolved Organic Carbon original_add_offset 0.0 original_name DOC original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/DOC_1-month_9km/ standard_name dissolved_organic_carbon unit \u00b5mol/kg units \u00b5mol kg^-1"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#ep_dunne","title":"EP_Dunne","text":"Field Value Algorithm description DOI 10.1029/2004gb002390 Derived from SST, Chl, Primary Production, Z_eu acknowledgement BICEP/NCEO color_bar_name Blues color_value_max 300 color_value_min 0 data_bins 3954469 data_maximum 0.0 data_minimum 0.0 description Export Production based on Dunne et al 2005 long_name Export Production based on Dunne et al 2005 original_add_offset 0.0 original_name EP_Dunne original_scale_factor 1.0 processing_steps Gridding nc datasets references https://doi.org/10.1029/2004gb002390, https://catalogue.ceda.ac.uk/uuid/a6fc730d88fd4935b59d64903715d891 source https://data.ceda.ac.uk/neodc/bicep/data/oceanic_export_production/v1.0/monthly/ standard_name export_production_dunne unit mg C m-2 d-1 units mg C m^-2 d^-1"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#ep_henson","title":"EP_Henson","text":"Field Value Algorithm description DOI 10.1029/2011gl046735 Derived from SST, Primary Production acknowledgement BICEP/NCEO color_bar_name Blues color_value_max 300 color_value_min 0 data_bins 3954469 data_maximum 0.0 data_minimum 0.0 description Export Production based on Henson et al 2011 long_name Export Production based on Henson et al 2011 original_add_offset 0.0 original_name EP_Henson original_scale_factor 1.0 processing_steps Gridding nc datasets references https://doi.org/10.1029/2011gl046735, https://catalogue.ceda.ac.uk/uuid/a6fc730d88fd4935b59d64903715d891 source https://data.ceda.ac.uk/neodc/bicep/data/oceanic_export_production/v1.0/monthly/ standard_name export_production_henson unit mg C m-2 d-1 units mg C m^-2 d^-1"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#ep_li","title":"EP_Li","text":"Field Value Algorithm description DOI 10.1002/2015gb005314 Derived from SST, Primary Production acknowledgement BICEP/NCEO color_bar_name Blues color_value_max 300 color_value_min 0 data_bins 3954469 data_maximum 0.0 data_minimum 0.0 description Export Production based on Li et al 2016 long_name Export Production based on Li et al 2016 original_add_offset 0.0 original_name EP_Li original_scale_factor 1.0 processing_steps Gridding nc datasets references https://doi.org/10.1002/2015gb005314, https://catalogue.ceda.ac.uk/uuid/a6fc730d88fd4935b59d64903715d891 source https://data.ceda.ac.uk/neodc/bicep/data/oceanic_export_production/v1.0/monthly/ standard_name export_production_li unit mg C m-2 d-1 units mg C m^-2 d^-1"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#pic","title":"PIC","text":"Field Value acknowledgement BICEP/NCEO color_bar_name Blues_r color_value_max 0.001 color_value_min 0 description Particulate Inorganic Carbon long_name Particulate Inorganic Carbon max 0.0015023552358215274 min 7.649861515675783e-05 original_add_offset 0.0 original_name PIC original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/PIC_1-month_9km/ standard_name particulate_inorganic_carbon units \u00b5mol C m^-3"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#poc","title":"POC","text":"Field Value acknowledgement BICEP/NCEO color_bar_name plasma color_value_max 600 color_value_min 0 description Particulate Organic Carbon long_name Particulate Organic Carbon original_add_offset 0.0 original_name POC original_scale_factor 1.0 processing_steps Gridding nc datasets reference(s) see https://bicep-project.org/Deliverables source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/POC_1-month_9km/ standard_name particulate_organic_carbon units mg C m^-3"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#chl_a","title":"chl_a","text":"Field Value acknowledgement BICEP/NCEO color_bar_name viridis color_value_max 50 color_value_min 0 description Chlorophyll-a long_name Chlorophyll-a original_add_offset 0.0 original_name chl_a original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/PC_1-month_9km/ standard_name mass_concentration_of_chlorophyll_a_in_sea_water units mg C m^-3"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#mld","title":"mld","text":"Field Value acknowledgement BICEP/NCEO color_bar_name viridis color_value_max 500 color_value_min 0 description Mixed Layer Depth long_name Mixed Layer Depth original_add_offset 0.0 original_name mld original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/PC_1-month_9km/ standard_name mixed_layer_depth units m"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#pp","title":"pp","text":"Field Value acknowledgement BICEP/NCEO color_bar_name Spectral_r color_value_max 1000 color_value_min 0 description Phytoplankton Primary Production long_name Phytoplankton Primary Production original_add_offset 0.0 original_name pp original_scale_factor 1.0 processing_steps Gridding nc datasets references https://catalogue.ceda.ac.uk/uuid/69b2c9c6c4714517ba10dab3515e4ee6 source https://data.ceda.ac.uk/neodc/bicep/data/marine_primary_production/v4.2/monthly/ standard_name gross_primary_production_of_carbon units mg C m^-2 d^-1"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.10 acknowledgment All data providers are acknowledged inside each variable contributor_name University of Leipzig, Brockmann Consult GmbH contributor_url https://www.uni-leipzig.de/, https://www.brockmann-consult.de/ creator_name University of Leipzig, Brockmann Consult GmbH creator_url https://www.uni-leipzig.de/, https://www.brockmann-consult.de/ date_modified 2023-04-05 12:02:59.047482 geospatial_lat_max 89.95832824707031 geospatial_lat_min -89.95832824707031 geospatial_lat_resolution 0.0833282470703125 geospatial_lon_max 179.95834350585938 geospatial_lon_min -179.95834350585938 geospatial_lon_resolution 0.083343505859375 id ocean-1M-9km-1x1080x1080-v1.4.0.zarr license Terms and conditions of the DeepESDL data distribution project DeepESDL publisher_name DeepESDL Team publisher_url https://www.earthsystemdatalab.net/ time_coverage_end 2020-12-01T00:00:00.000000000 time_coverage_start 1997-09-01T00:00:00.000000000 title BICEP Pools and Fluxes of the Ocean Biological Carbon Pump"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/","title":"Polar Cube","text":""},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#polar-data-cube-v101","title":"Polar Data Cube v1.0.1","text":""},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('polar-100m-1x2048x2048-1.0.1.zarr')\n</code></pre>"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -109.3 to -100 Bounding box latitude (\u00b0) -70 to -77 Time range 2013-01-01 to 2017-01-01 Time period 1461D Publisher DeepESDL Team <p>Click here for full dataset metadata.</p>"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units curie_depth_200km Curie Depth Estimates 200x200 km m curie_depth_300km Curie Depth Estimates 300x300 km m geothermal_heat_flow_200km Geothermal Heat Flow 200x200 km m geothermal_heat_flow_300km Geothermal Heat Flow 300x300 km m geothermal_heat_flow_uncertainty_200km Geothermal Heat Flow Uncertainty 200x200 km m geothermal_heat_flow_uncertainty_300km Geothermal Heat Flow Uncertainty 300x300 km m ice_thickness Ice Thickness m magnetic_anomaly Magnetic Anomaly nT thw_124 Subglacial Lake Thw 124 1 thw_142 Subglacial Lake Thw 142 1 thw_170 Subglacial Lake Thw 170 1 thw_70 Subglacial Lake Thw 70 1"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#crs","title":"crs","text":"Field Value crs_wkt PROJCS[\"WGS 84 / Antarctic Polar Stereographic\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Polar_Stereographic\"],PARAMETER[\"latitude_of_origin\",-71],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",NORTH],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3031\"]] false_easting 0.0 false_northing 0.0 geographic_crs_name WGS 84 grid_mapping_name polar_stereographic horizontal_datum_name World Geodetic System 1984 inverse_flattening 298.257223563 longitude_of_prime_meridian 0.0 prime_meridian_name Greenwich projected_crs_name WGS 84 / Antarctic Polar Stereographic reference_ellipsoid_name WGS 84 semi_major_axis 6378137.0 semi_minor_axis 6356752.314245179 spatial_ref PROJCS[\"WGS 84 / Antarctic Polar Stereographic\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Polar_Stereographic\"],PARAMETER[\"latitude_of_origin\",-71],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",NORTH],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3031\"]] standard_parallel -71.0 straight_vertical_longitude_from_pole 0.0"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#curie_depth_200km","title":"curie_depth_200km","text":"Field Value acknowledgement 4DAntarctica description Curie depth estimates using a 200x200km window grid_mapping crs long_name Curie Depth Estimates 200x200 km original_add_offset 0.0 original_name z original_scale_factor 1.0 processing_steps Gridding nc dataset, Spatial resampling to 100 m using linear interpolation references https://doi.org/10.1594/PANGAEA.932452, https://doi.org/10.1038/s43247-021-00242-3 source https://download.pangaea.de/dataset/932452/files/200km_Dziadek_etal_CurieDepth_Results.nc standard_name curie_depth_estimates_200km units m"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#curie_depth_300km","title":"curie_depth_300km","text":"Field Value acknowledgement 4DAntarctica description Curie depth estimates using a 300x300km window grid_mapping crs long_name Curie Depth Estimates 300x300 km original_add_offset 0.0 original_name z original_scale_factor 1.0 processing_steps Gridding nc dataset, Spatial resampling to 100 m using linear interpolation references https://doi.org/10.1594/PANGAEA.932452, https://doi.org/10.1038/s43247-021-00242-3 source https://download.pangaea.de/dataset/932452/files/300km_Dziadek_etal_CurieDepth_Results.nc standard_name curie_depth_estimates_300km units m"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#geothermal_heat_flow_200km","title":"geothermal_heat_flow_200km","text":"Field Value acknowledgement 4DAntarctica description Geothermal Heat Flow from a 200x200km window grid_mapping crs long_name Geothermal Heat Flow 200x200 km original_add_offset 0.0 original_name z original_scale_factor 1.0 processing_steps Gridding nc dataset, Spatial resampling to 100 m using linear interpolation references https://doi.org/10.1594/PANGAEA.932452, https://doi.org/10.1038/s43247-021-00242-3 source https://download.pangaea.de/dataset/932452/files/200km_GHF_windowCenters.nc standard_name geothermal_heat_flow_200km units m"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#geothermal_heat_flow_300km","title":"geothermal_heat_flow_300km","text":"Field Value acknowledgement 4DAntarctica description Geothermal Heat Flow from a 300x300km window grid_mapping crs long_name Geothermal Heat Flow 300x300 km original_add_offset 0.0 original_name z original_scale_factor 1.0 processing_steps Gridding nc dataset, Spatial resampling to 100 m using linear interpolation references https://doi.org/10.1594/PANGAEA.932452, https://doi.org/10.1038/s43247-021-00242-3 source https://download.pangaea.de/dataset/932452/files/300km_GHF_windowCenters.nc standard_name geothermal_heat_flow_300km units m"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#geothermal_heat_flow_uncertainty_200km","title":"geothermal_heat_flow_uncertainty_200km","text":"Field Value acknowledgement 4DAntarctica description Geothermal Heat Flow Uncertainty from a 200x200km window grid_mapping crs long_name Geothermal Heat Flow Uncertainty 200x200 km original_add_offset 0.0 original_name z original_scale_factor 1.0 processing_steps Gridding nc dataset, Spatial resampling to 100 m using linear interpolation references https://doi.org/10.1594/PANGAEA.932452, https://doi.org/10.1038/s43247-021-00242-3 source https://download.pangaea.de/dataset/932452/files/200km_GHF_uncertainty.nc standard_name geothermal_heat_flow_uncertainty_200km units m"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#geothermal_heat_flow_uncertainty_300km","title":"geothermal_heat_flow_uncertainty_300km","text":"Field Value acknowledgement 4DAntarctica description Geothermal Heat Flow Uncertainty from a 300x300km window grid_mapping crs long_name Geothermal Heat Flow Uncertainty 300x300 km original_add_offset 0.0 original_name z original_scale_factor 1.0 processing_steps Gridding nc dataset, Spatial resampling to 100 m using linear interpolation references https://doi.org/10.1594/PANGAEA.932452, https://doi.org/10.1038/s43247-021-00242-3 source https://download.pangaea.de/dataset/932452/files/300km_GHF_uncertainty.nc standard_name geothermal_heat_flow_uncertainty_300km units m"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#ice_thickness","title":"ice_thickness","text":"Field Value acknowledgement CryoSMOS description Ice Thickness long_name Ice Thickness original_add_offset 0.0 original_name est original_scale_factor 1.0 processing_steps Masking out no-data values, Slicing dataset to the polar region (&lt; -60 degrees), Reprojecting to EPSG:3031, Spatial resampling to 100 m using linear interpolation references https://opensciencedata.esa.int/products/bedrock-topography-antarctica-cryosmos, http://www.ifac.cnr.it/cryosmos/products/CryoSMOS_D8_EDUM_V2.0.pdf source http://www.ifac.cnr.it/cryosmos/products/SMOS_IceThickness_2015_300.nc standard_name ice_thickness units m"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#magnetic_anomaly","title":"magnetic_anomaly","text":"Field Value acknowledgement 4DAntarctica description Magnetic Anomaly long_name Magnetic Anomaly original_add_offset 0.0 original_name original_scale_factor 1.0 processing_steps Spatial resampling to 100 m using linear interpolation references https://doi.org/10.1594/PANGAEA.932452, https://doi.org/10.1038/s43247-021-00242-3 source https://download.pangaea.de/dataset/932452/files/ASE_MagneticCompilation_Dziadeketal_250m.tif standard_name magnetic_anomaly units nT"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#thw_124","title":"thw_124","text":"Field Value acknowledgement 4DAntarctica color_bar_name GnBu description Subglacial Lake Thw 124 grid_mapping crs long_name Subglacial Lake Thw 124 original_add_offset 0.0 original_name original_scale_factor 1.0 processing_steps Spatial resampling to 100 m using nearest neighbor references https://doi.org/10.1029/2020GL089658 source https://4d-antarctica.org/wp-content/uploads/2021/01/Malczyk_etal_2020_data_v2.tar standard_name thw_124 time_coverage_end 2017-01-01 time_coverage_start 2013-01-01 units 1"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#thw_142","title":"thw_142","text":"Field Value acknowledgement 4DAntarctica color_bar_name GnBu description Subglacial Lake Thw 142 grid_mapping crs long_name Subglacial Lake Thw 142 original_add_offset 0.0 original_name original_scale_factor 1.0 processing_steps Spatial resampling to 100 m using nearest neighbor references https://doi.org/10.1029/2020GL089658 source https://4d-antarctica.org/wp-content/uploads/2021/01/Malczyk_etal_2020_data_v2.tar standard_name thw_142 time_coverage_end 2017-01-01 time_coverage_start 2013-01-01 units 1"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#thw_170","title":"thw_170","text":"Field Value acknowledgement 4DAntarctica color_bar_name GnBu description Subglacial Lake Thw 170 grid_mapping crs long_name Subglacial Lake Thw 170 original_add_offset 0.0 original_name original_scale_factor 1.0 processing_steps Spatial resampling to 100 m using nearest neighbor references https://doi.org/10.1029/2020GL089658 source https://4d-antarctica.org/wp-content/uploads/2021/01/Malczyk_etal_2020_data_v2.tar standard_name thw_170 time_coverage_end 2017-01-01 time_coverage_start 2013-01-01 units 1"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#thw_70","title":"thw_70","text":"Field Value acknowledgement 4DAntarctica color_bar_name GnBu description Subglacial Lake Thw 70 grid_mapping crs long_name Subglacial Lake Thw 70 original_add_offset 0.0 original_name original_scale_factor 1.0 processing_steps Spatial resampling to 100 m using nearest neighbor references https://doi.org/10.1029/2020GL089658 source https://4d-antarctica.org/wp-content/uploads/2021/01/Malczyk_etal_2020_data_v2.tar standard_name thw_70 time_coverage_end 2017-01-01 time_coverage_start 2013-01-01 units 1"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.10 acknowledgment All ESDC data providers are acknowledged inside each variable contributor_name University of Leipzig, Brockmann Consult GmbH contributor_url https://www.uni-leipzig.de/, https://www.brockmann-consult.de/ creator_name University of Leipzig, Brockmann Consult GmbH creator_url https://www.uni-leipzig.de/, https://www.brockmann-consult.de/ id polar-100m-1x2048x2048-1.0.1 license Terms and conditions of the DeepESDL data distribution project DeepESDL publisher_name DeepESDL Team publisher_url https://www.earthsystemdatalab.net/ spatial_ref EPSG:3031 time_coverage_end 2017-01-01 time_coverage_start 2013-01-01 title Polar Data Cube v1.0.1"},{"location":"design/","title":"Overview","text":""},{"location":"design/#deepesdl-design-overview","title":"DeepESDL Design Overview","text":"<ul> <li> Context</li> <li> Architecture</li> <li> Hub</li> <li> Services and Tools</li> </ul>"},{"location":"design/architecture/","title":"Architecture","text":""},{"location":"design/architecture/#deepesdl-architecture","title":"DeepESDL Architecture","text":"<p>The following figure depicts the high-level concepts and the architecture  of the DeepESDL project. It comprises the internal DeepESDL Hub  and user projects and the publicly visible parts. Both are served by a  common infrastructure and common resources.</p> <p></p> <p>Applications available to users within DeepESDL comprise:</p> <ul> <li>An xcube Catalogue to browse available data, including a web page    that lists available datasets as a low-barrier entry point for users.    (In development, will be similar to the    ESA EuroDataCube Collections.) </li> <li>Visualisation tools such as adapted versions of the xcube Viewer    and a 4D Viewer.</li> <li>A Jupyter Notebook (JNB) service so that users can run    Jupyter Notebooks with project-specific Machine Learning (ML)    environments. </li> </ul> <p>Services available to users within DeepESDL comprise:</p> <ul> <li>An xcube Server to browse, access, and publish gridded data cubes.</li> <li>A geoDB instance to browse, access, and publish vector datasets. </li> <li>A Workspace so projects can store and share any other data,    such as ML workflows. </li> <li>Access to all shared and project-specific Data Cubes in object storage.</li> </ul> <p>Each project has access to the DeepESDL common resources that comprise  a cluster of worker nodes as well as a common  xcube ARDC Service, that can access a variety of data  access services and turns provided data into analysis-ready data cubes (ARDC). DeepESDL data cubes share a consistent structure and use a uniform format. </p> <p>To enable efficient machine learning on data cubes, the ARDC Generator  utilizes a configurable ML sampling module for training and validation.  Likewise, the worker nodes offer GPU-acceleration for demanding ML training.  Core libraries include Keras/Tensorflow and PyTorch and advanced  tools for model evaluation like TensorBoard or MLflow are made available. </p> <p>The DeepESDL Published Project Data is backed by a subset of the  applications and services granted to each  project, that is, a common xcube Server and geoDB is provided to serve the public catalogue and visualisation tools;  a public REST API allows accessing the services programmatically.  Public services will later also comprise a Jupyter Book  (JB)  or Notebook Viewer  (NBviewer) service, so that selected Project-JNBs  can be elaborated into published, story-telling books.  This service together with catalogue and visualisations tools  are important parts of the DeepESDL scientific information dissemination.</p> <p>The numerous distributed services and applications of the DeepESDL system  are containerized and are executed in a common cloud environment.  To orchestrate, monitor, maintain, and scale the variety of  DeepESDL services and applications, we use today\u2019s most popular  container orchestrator Kubernetes (K8s).  This supports the idea that the DeepESDL system is deployable on any Cloud  environment that can run a K8s service (this is, AWS, Google,  Microsoft Azure, and many others including all DIAS instances). The  initial system is deployed on Amazon Web Services (AWS) in the EU-Central-1  region, which is physically located in Frankfurt, Germany, Europe using the  managed K8s service EKS, which is critical for cost-efficiency and  reliability of DeepESDL. </p> <p>To be able to meet the ambitious requirements for the service,  we must base the solution mainly on existing technologies.  Many of the DeepESDL system components have been developed in former  (mostly ESA) projects and are thus reused, adapted, extended, and branded  for DeepESDL. Examples are</p> <ul> <li>DeepESDL Hub is based on EOxHub developed by EOX and used for   example in the ESA EuroDataCube project;</li> <li>The xcube cube generator tools, xcube Server, xcube Viewer    are derived from already available components in the   xcube Toolkit    developed and used in several activities by the development team;</li> <li>The Cube 4D Viewer will be an adopted version of Earthwave\u2019s 4D Viewer.</li> </ul>"},{"location":"design/context/","title":"Context","text":""},{"location":"design/context/#context","title":"Context","text":"<p>The design of DeepESDL is focused on the idea of on-boarding new  DeepESDL user projects with the help of the  DeepESDL Hub which can be seen as an implementation of the  ESA Earth Science Hub.  Accordingly, a DeepESDL user project may support specific  ESA EO for Society  research projects and activities that are aggregated in Science Clusters  such as</p> <ul> <li>ESA Atmosphere Science Cluster.</li> <li>ESA Carbon Science Cluster.</li> <li>ESA Ocean Science Cluster.</li> <li>ESA Polar Science Cluster.</li> </ul> <p>The DeepESDL Hub manages user projects and their users.  The Hub provides for each project a workspace or tenant,  assigns project resources, hosts project artifacts, provides a dashboard  for result dissemination, and integrates team collaboration tools.  It also controls visibility of results of individual user projects so  that they can be made part of a DeepESDL public portrayal.</p> <p>A DeepESDL user project comprises numerous own applications, interfaces,  services, and data available in the provided workspace.  Service access and application deployments are managed by the  Control Plane of the DeepESDL Hub. Actual workloads are  scaled on the Worker Plane.</p>"},{"location":"design/hub/","title":"Hub","text":""},{"location":"design/hub/#deepesdl-hub","title":"DeepESDL Hub","text":"<p>The DeepESDL Hub is based on the existing software named EOxHub,  which is also used to power, e.g., the  EuroDataCube (EDC) Marketplace as  well as the  EDC EOxHub Workspace.</p> <p>EOxHub is a platform and workflow management runtime for Earth Observation  services and apps. EOxHub can be branded to provide the DeepESDL Hub &amp;  Marketplace and be deployed on any cloud offering a managed Kubernetes service. The designated cloud for DeepESDL, Amazon Web Services (AWS) in  the Europe Frankfurt region, fulfils this requirement with the managed Elastic Kubernetes Service (EKS). Technically EOxHub is split into the Control Plane and the Worker Plane. The Worker Plane is where all workloads from users are run at runtime.  The Control Plane is configured to provide the following:</p> <ul> <li>User Management</li> <li>Access control</li> <li>User Workspaces (Tenants)</li> <li>Workspace Dashboard</li> <li>Service subscription management</li> <li>Marketplace</li> <li>Allocation functions for cloud resources and Data Services</li> <li>Deployment service</li> <li>Workload management functions</li> <li>Docker Image administration/assignment</li> <li>Example notebook catalogue supporting user contributions</li> <li>Automated verification of example notebooks</li> <li>Accounting and billing (voucher handling)</li> </ul> <p>Deploying user workloads on the Worker Plane is performed on configured  autoscaling groups using the managed Elastic Container Service (ECS) of AWS.  This setup ensures, that only actually required resources are used and  thus need to be paid.</p> <p></p> <p>The figure above shows the App deployment in user Workspaces.  The sequence of steps is: The App or Notebook Developer pushes the  App source code to the Code Management tool, adds the App as Docker image,  and registers the App at the Marketplace. The App Consumer discovers and requests the App and triggers the deployment of the App to use it to their  workspace to be run on the cloud infrastructure. The App is now available to be used by the Consumer within the resources provided in their  workspace subscription.</p> <p>EOxHub is extended in two ways to provide the DeepESDL  Collaborative Development Tools:</p> <ul> <li>Extended support for teams as part of the multi-user plan.    Allow for easier sharing of versioned notebooks and other artifacts    within the team but not necessarily the public.</li> <li>Integrated support for Machine Learning (ML) workflows.    This includes the versioning, sharing, and collaborative using of    all ML artifacts like code, data, models, results, etc.    Based on user feedback, the readily available Open-Source tools    like Data Version Control (DVC), MLflow, Kubeflow, or similar, will   be integrated during the project.</li> </ul>"},{"location":"design/services-and-tools/","title":"Services and Tools","text":""},{"location":"design/services-and-tools/#deepesdl-services-and-tools","title":"DeepESDL Services and Tools","text":"<p>The Python xcube Toolkit plays a core role  in the DeepESDL\u2019s system design. Here we briefly introduce the key  components of xcube to provide an overview.</p> <p>The xcube generators are command-line tools used  to generate analysis-ready data cubes (ARDC) with inputs from one or  more data stores. </p> <p>The xcube Server provides a web service that offers several RESTful APIs.  It publishes collections of ARDCs which are provided by configurable data  stores but also provides an OGC WMTS and time-series service. </p> <p>The xcube Viewer is a simple and very easy-to-use single-page web  application that fully exploits the APIs offered by xcube Server. </p> <p>xcube Data Stores are used by the xcube Generator and the xcube Server  represent different data cube sources. </p> <p>Finally, the xcube Python API  provides various high- and low-level functions for  data analysis that operate on data cubes and comprises programmatic  access to all other xcube Components mentioned above.  Users call the Python API from their own programs, scripts, JNBs,  or from user code executed in the <code>xcube gen2</code> generator.  During the project, generic functions will be added to the Python API  to support machine learning using the data cubes (see ML Toolkit below),  as well as new use case specific functions as desired in the  different DeepESDL user projects. </p>"},{"location":"design/services-and-tools/#xcube-generators","title":"xcube Generators","text":"<p>The command-line tools  <code>xcube gen</code> and  <code>xcube gen2</code> offer flexible data cube generation  that is made available via a dedicated JupyterLab  profile. The tool reads data streams from one or more data stores,  then it resamples and combines them. The merged cube can then be  manipulated by user-provided Python code before the resulting  cube is written into another data store, for example AWS S3.</p> <p></p> <p>The current features of the xcube generation service are:</p> <ul> <li>Read data streams from a variety of xcube data stores   (see xcube Toolkit).</li> <li>Perform spatial resampling of raw and rectified data streams to    a common spatial grid using standard coordinate reference systems, e.g.,    EU LAEA (EPSG:3035).</li> <li>Perform temporal resampling by aggregating observations to equally    sized time periods, e.g., 8-day averages. </li> <li>Merge resampled data streams into a single cube, optionally    passing it to user Python code for further value adding or transformation.</li> <li>Write chunked data cube into remote object storage or the    local file system.</li> </ul>"},{"location":"design/services-and-tools/#xcube-server","title":"xcube Server","text":"<p>The xcube Server  (<code>xcube serve</code>) offers a web service that publishes collections of  data cubes which are provided by configurable xcube data stores.  It offers a RESTful API for browsing the published cubes (catalogue),  for multi-resolution image tiles (OCG-compliant WMTS, later WCS),  for time-series retrieval, and for direct data access.  Moreover, it will be equipped with a STAC-compatible catalogue API that  supports spatial OpenSearch requests. </p> <p>The xcube Server will be extended to fully support any new  xcube Viewer features, for example linking the geoDB with  xcube Server and Viewer.</p>"},{"location":"design/services-and-tools/#xcube-viewer","title":"xcube Viewer","text":"<p>The xcube Viewer  (<code>xcube-viewer</code>) is a simple and very easy-to-use single-page web application  that fully exploits the APIs offered by xcube Server. It displays the  spatial images of the data cubes on a map at given time steps, it can show a variable\u2019s time series with error bars for any geometry and  show the details of a data cubes and its variables.</p> <p></p> <p>The xcube Viewer provides several useful tools to explore data cubes.  For example:</p> <ul> <li>Open any number of data cubes and display spatial images of dataset    variables on a map.</li> <li>Open places (vector data) associated with data cubes and display them.</li> <li>Show detailed information about selected data cubes, data variables    and places.</li> <li>Select places or let users draw shapes and display time series with    error bars.</li> <li>Click into time series to show corresponding images in map.</li> <li>Start a \u201cplayer\u201d that steps through time and animates the map and time   indicator in time series diagrams accordingly.</li> <li>Download extracted time series data.</li> </ul> <p>During the DeepESDL project, the xcube Viewer will be enhanced by  new features such as:</p> <ul> <li>Switching between 2D map display and 3D globe displays.</li> <li>Multiple, possibly synchronized 2D/3D displays and split-view displays.</li> <li>Generate new data variables on-the-fly from user-supplied Python functions.   Code may originate from shared locations on GitHub or may be provided   inline by a simple code editor.</li> </ul>"},{"location":"design/services-and-tools/#xcube-data-stores","title":"xcube Data Stores","text":"<p>Data store implementations are  dynamically registered in the xcube Data Store Framework.  The following data stores are already available</p> <ul> <li>ESA CCI Open Data Portal (from xcube plugin xcube-cci).</li> <li>C3S Climate Data Store (from xcube plugin xcube-cds).</li> <li>CMEMS Data Store (from xcube plugin xcube-cmems).</li> <li>Sentinel Hub (from xcube plugin xcube-sh).</li> <li>Generic data stores such as S3-compatible object storage (<code>s3</code>),    local file system (<code>file</code>), and in-memory (<code>memory</code>).</li> </ul> <p>Its is planned to develop data stores for the geoDB service,  so users can retrieve vector datasets and rasterized vector data  sources as gridded ARDCs.  Other data stores that are needed by DeepESDL Projects will be added  as required by specific use cases during the project.</p>"},{"location":"design/services-and-tools/#ml-toolkit","title":"ML Toolkit","text":"<p>The DeepESDL ML Toolkit is a small and handy  Python package that provides useful functions for machine learning tasks with DeepESDL data cubes.</p> <p>The toolkit is made available as a DeepESDL JupyterLab environment (profile),  and includes popular libraries such as scikit learn, TensorFlow,  Keras, and PyTorch.</p> <p>To support model evaluation during training, the ready-to-use processing environment is also extended by a TensorBoard to support the tracking of individual experiments and training runs. This tool can be used with PyTorch and TensorFlow, and it provides  a state-of-the-art toolset for data scientist to inspect the tuning  and training process and compare metrics.</p> <p>The toolkit offers the following:</p> <ul> <li> <p>Adapters are provided to existing data loading and transformation    mechanisms from Keras and PyTorch (<code>DataGenerator</code>, <code>DataLoader</code>)    to be usable for the DeepESDL data cubes. </p> </li> <li> <p>Implementations of sampling mechanisms and online repartitioning methods    suited for the data cube files which are stored in chunks.    This element is essential, as it enables deep learning that respects    the basic principles of geo data way beyond naive applications of   machine learning in the Earth system context. </p> </li> </ul> <p>To ease adoption by scientist we plan to also develop script templates  for common deep learning tasks such as autoencoder on time-series data,  in particular physics informed autoencoder, transfer learning and change  detection on time series.</p> <p>The toolkit is currently for Python only, but Julia will also be  considered to address a small but growing community of data scientists.</p>"},{"location":"guide/4d-viewer/","title":"4D Viewer","text":""},{"location":"guide/4d-viewer/#4d-viewer","title":"4D Viewer","text":""},{"location":"guide/4d-viewer/#introduction","title":"Introduction","text":"<p>The 4D-Viewer tool was built to enable reconstructions of geo-spatial data and its evolution through time.</p> <p>The publicly hosted instance of the 4D-Viewer tool is at https://4dviewer.com/.</p>"},{"location":"guide/4d-viewer/#the-user-interface","title":"The User Interface","text":"<p>4D worlds are built from a series of layers and potentially child layers. A typically workflow would start by adding a \"Terrain\" layer - i.e. a rendered Digital Elevation Model. E.g.:</p> <p></p> <p>Most layers will depend on the selection of a data source. In such cases, the Data Browser will be opened through which a data source can be selected.</p> <p></p> <p>Data sources are grouped into tree hierarchies, double-click through each \"Directory\" to select the relevant data source, aka \"File Name\" again via a double-click.</p>"},{"location":"guide/4d-viewer/#layer-types","title":"Layer Types","text":"<p>Below is a summary of the useful layer types. Those that are present in the app but not listed below are experimental and excluded from this document on purpose.</p>"},{"location":"guide/4d-viewer/#terrain-dem","title":"Terrain (DEM)","text":"<p>As mentioned above, this is a basic DEM data where each pixel value is interpreted as an elevation. The details of the expected data formats is not covered in detailed here.</p> <p>Once a terrain layer has been selected, basic formatting and colorization of the layer can be achieved via its layer menu options.</p> <p></p> <p>Terrain style exposes some default layer styles to mimic water, rock, grass etc. There are 2 special cases worth mentioning here:</p> <ol> <li>Water 2: this can only render on a flat plane and as you zoom in uses rendering effects to mimic ocean waves</li> <li>White + color Ramp: To color elevations using one of the built-in color ramps (per the example shown above), select the \"White\" terrain style and check \"Use Color Ramp\". Lastly select the desired color ramp from the drop down</li> </ol>"},{"location":"guide/4d-viewer/#heatmap","title":"Heatmap","text":"<p>Child layers can be added to a terrain layer by clicking the plus beside the terrain layer. e.g.</p> <p></p> <p>Again, the data browser is opened and a data source must be selected. Once selected, a set of presentation choices can be applied:</p> <ol> <li>Alpha: Set transparency of the layer</li> <li>Color Ramp: Select a set of colors to apply to the data set</li> <li>Min/Max Value: Set color saturation limits</li> </ol>"},{"location":"guide/4d-viewer/#volume-3dtime-data-cube","title":"Volume  (3D+Time Data Cube)","text":"<p>This layer represents a cube of data in 4 dimensions. Color ramps and saturations can be applied per the \"Heatmap\" layer. Additionally, one can select which dimension slices to show. E.g. the examples below show both an example of both vertical slices and an example of a vertical and horizontal slice.</p> <p></p>"},{"location":"guide/4d-viewer/#2d-text-and-image-overlays","title":"2D (Text and Image Overlays)","text":"<p>These layer types exist to help convey useful information about the data being displayed.</p>"},{"location":"guide/4d-viewer/#text","title":"Text","text":"<p>This adds a simple text box. The \"Text\" field is used to set the text box content. The remaining fields drive basic styling and font changes such as color and opacity. The location of the box can be manually set using the following properties or by clicking the \"+\" button and dragging and dropping the box in the active view.</p> <p></p>"},{"location":"guide/4d-viewer/#image","title":"Image","text":"<p>This allows a public image url to be rendered via the \"Select Image\" button.</p>"},{"location":"guide/4d-viewer/#pre-canned-configurations","title":"Pre-Canned Configurations","text":"<p>Once a valid world has been created, you can export the current configuration to your local downloads directory via:</p> <ul> <li>File -&gt; save</li> </ul> <p>To reload the config in a future session, simply open the application and use:</p> <ul> <li>File -&gt; Open file... -&gt; select local configuration file</li> </ul>"},{"location":"guide/datacube-generation/","title":"Datacube generation","text":""},{"location":"guide/datacube-generation/#deepesdl-datacube-generation","title":"DeepESDL datacube generation","text":"<p>The data cubes already provided by DeepESDL might not be sufficient  for your application. However, this should not stop you from creating the  resources you need from source input data and enable you to do your research.  DeepESDL carefully adheres to the reproducibility of dataset resources. Therefore,  there are two approaches to generate datasets.  In the simpler one, the data is retrieved from an existing datastore without persisting the  dataset, usually using the DeepESDL JupyterLab.  If a dataset shall be persisted, maybe even re-published,and is furthermore based on input data that  needs to be e.g. downloaded beforehand or other preprocessing steps are  performed, then the cube generation recipe approach is recommended. Note that the Cube Gen team follows this approach for all cubes generated  and published by DeepESDL. </p>"},{"location":"guide/datacube-generation/#cube-generation-recipe-approach-for-static-data-cubes","title":"Cube generation recipe approach for static data cubes","text":""},{"location":"guide/datacube-generation/#cube-specification-format","title":"Cube specification format","text":"<p>For each data cube, a unique specification is created that describes  the cubes spatio-temporal dimensions, resolutions, coverages, and the data  sources used to generate the cube\u2019s target data variables. To specify each  cube, a dedicated JSON format is created that fully describes the target  cube\u2019s characteristics. We use a special GeoJSON Feature format for this  purpose so each cube definition is both human- and machine-readable and can  be ingested and rendered by many existing tools. The GeoJSON  Feature\u2019s geometry represents the geographical coverage of the cube, while the  GeoJSONFeature\u2019s properties provide numerous further details. To describe many  similar cubes, e.g., for using multiple spatial resolutions for same  variables, a GeoJSON FeatureCollection may be used instead. To validate the  JSON cube specification files, we provide a dedicated online JSON Schema  in the DeepESDL dataset-spec GitHub repository.</p>"},{"location":"guide/datacube-generation/#cube-generation-recipe","title":"Cube generation recipe","text":"<p>The cube generation recipe ensures, that an existing dataset provided by  DeepESDL can be reproduced by following the documented steps and specifications. Here we describe the  recipe structure of the provided datasets within DeepESDL.</p> <p>Each predefined DeepESDL data cube is fully described in a transparent and  comprehensive way by a dedicated sub-folder in the DeepESDL GitHub repository  cube-gen.  Such sub-folder is what we call a cube generation recipe: <code>cube-gen/${cube-name}/</code></p> <p>It contains the machine-readable GeoJSON file that fully specifies the data  cube <code>cube-gen/${cube-name}/cube.geojson</code> and provides the  detailed human-readable information about the cube including how to generate  it from sources in <code>cube-gen/${cube-name}/README.md</code>. After a cube has been released and published, we record the changes in  <code>cube-gen/${cube-name}/CHANGES.md</code>. In each data cube sub-folder, further sub-folders may exist that contain resources and sources such as  configuration files and Python modules. We have defined the following common sub-folder structure, but others may be used  too:</p> <ul> <li> <p><code>cube-gen/${cube-name}/input-collect/</code>        Fetch, download,or copy inputs.</p> </li> <li> <p><code>cube-gen/${cube-name}/input-preprocess/</code>        Transform, concatenate, convert to interm. Zarr.</p> </li> <li> <p><code>cube-gen/${cube-name}/output-merge/</code>       Merge interm. Zarrs to target cube.</p> </li> <li> <p><code>cube-gen/${cube-name}/output-postprocess/</code>        Apply any postprocessing.  </p> </li> </ul> <p>Cube generation recipes are designed to be comprehensive, transparent,  reproducible, and relocatable. That is, with very little configuration  changes, they should be executable in different environments.</p>"},{"location":"guide/datacube-generation/#dynamic-data-cubes-from-data-stores","title":"Dynamic data cubes from Data Stores","text":"<p>Persistently stored data cubes have the advantage that they have a physical  representation, i.e. they \u201cexist\u201d at a given storage location. That way  the data can be \u201cfrozen\u201d and can be assigned a version and/or a DOI. In  addition, access to static data cubes persisted in cloud object storage  is usually fast and scalable. </p> <p>However, there are potential issues and challenges with static cubes. Data  sources of data cubes can become outdated, or are simply updated, like it is the case for new EO data observations  added to an existing product archive. In such cases, related data cubes should  be updated too, hereby creating considerable maintenance effort and risks for the integrity of the data cube. In addition, the generation of static data cubes is in many cases a plain duplication of  data that is actually defined and described elsewhere. The data cube must  ensure to stay in sync with original data sources and metadata. Finally, static cubes can only satisfy requirements of one user or use  case in an optimal way. By definition, they do not allow for streamlined and  tailored datasets. </p> <p>A possible solution to mitigate these issues and address  the challenges are dynamic data cubes. Dynamic data cubes exist in-memory  only and will provide data in a \u201clazy\u201d way. That is, chunks of a data cube  are fetched on-demand, hence computed on-the-fly, including all the  required transformation steps starting with the ingestion of source data.  Dynamic cubes are generated for a given configuration that describes the  data cube to be generated. </p> <p>Dynamic data cubes for a given single data source can be easily retrieved  using xcube data stores. The following Python code opens a data cube  representing a Sentinel-2 L2A data cube with the SentinelHubAPI as data source:</p> <pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"sentinelhub\", **credentials)\ncube = store.open_data(\"S2L2A\", \n                       variable_names=[\"B03\",\"B06\",\"B8A\"],\n                       bbox=..., \n                       spatial_res=...,\n                       time_range=..., \n                       time_period=...)\n</code></pre> <p>Dynamic cubes are application-specific and configured by individual users.</p> <p>Below is an overview of the possible xcube data stores that can be  used to create dynamic cubes together with the title of an example notebook,  if there is one available. A description of how the example notebooks can be  accessed is in section DeepESDL JupyterLab.</p> Data store ID Content Example Notebook Access s3 Any Zarr dataset on AWS S3 or similar 01 Access public cubes Depends on permissions cmems CMEMS datasets 02 Generate CMEMS cubes Requires registration, free cciodp All ESA CCI datasets 03 Generate CCI Cubes Free cds Climate data store Requires registration, free sentinelhub Sentinel 1 to 3, Landsat, ... Requires registration, with costs"},{"location":"guide/datacubes/","title":"Datacubes","text":""},{"location":"guide/datacubes/#deepesdl-datacubes","title":"DeepESDL datacubes","text":"<p>DeepESDL provides a growing list of relevant variables for Earth System Science.  Most of them have been derived from Earth Observation, but the compilation also  includes model or re-analysis data if deemed useful. DeepESDL is very grateful to all data owners for kindly providing the data sets  and allowing us to process, and redistribute them free of charge.  All datacubes generated and distributed by DeepESDL come without any warranty,  neither from the owners, from the DeepESDL, nor from ESA.</p> <p>During ingestion into the DeepESDL, data sets are typically transformed in  space and  time to fit to the common grid of the data cube, a process that necessarily  modifies the original data. If you are looking for the original data, please  follow the links within the dataset attributes for each variable and contact  the data owners.</p> <p>To access the documentation of available datasets, please have a look in the  datasets section.</p>"},{"location":"guide/further-information/","title":"Further information","text":""},{"location":"guide/further-information/#further-information","title":"Further information","text":"<ul> <li>The JupyterLab documentation:    an in-depth user guide for the JupyterLab interface.</li> <li>How to Use JupyterLab:    a short introductory video tutorial.</li> <li>The xcube documentation: user    guide and API reference for the xcube libraries.</li> </ul>"},{"location":"guide/ml-toolkits/","title":"Overview","text":""},{"location":"guide/ml-toolkits/#ml-toolkits","title":"ML Toolkits","text":"<p>The ML Toolkits provide an introduction to  a set of best practice Python-based Jupyter Notebooks that showcase the  implementation of the three start-of-the-art Machine Learning libraries (1)  scikit-learn, (2) PyTorch and (3) TensorFlow based on the Earth System Data  Cube. </p>"},{"location":"guide/overview/","title":"Overview","text":""},{"location":"guide/overview/#user-guide","title":"User guide","text":"<p>This user guide helps you to get started with the main components of  DeepESDL.</p> <p>The growing set of DeepESDL's general functionalities comprises:  </p> <ol> <li>DeepESDL JupyterLab </li> <li>DeepESDL xcube Viewer </li> <li>DeepESDL datacubes </li> <li>DeepESDL datacube generation </li> <li>ML Toolkits </li> </ol>"},{"location":"guide/xcube-viewer/","title":"xcube Viewer","text":""},{"location":"guide/xcube-viewer/#deepesdl-xcube-viewer","title":"DeepESDL xcube Viewer","text":"<p>The DeepESDL xcube Viewer is reachable at viewer.earthsystemdatalab.net.</p> <p></p> <p>The viewer contains public datasets only, but will later also provide user/team  cubes when logged in. The login will be the same as for the DeepESDL JupyterLab.</p> <p>For a more detailed description of the viewer functionality, please refer to a dedicated section in the xcube viewer documentation.</p>"},{"location":"guide/xcube-viewer/#publish-a-dataset-in-a-public-deepesdl-viewer","title":"Publish a dataset in a public DeepESDL Viewer","text":"<p>In order to publish a data cube in a public DeepESDL Viewer take the following steps:</p> <ul> <li> <p>Check if the data cube is ready for publication with the following tools</p> <ul> <li>xrlint: to validate the <code>xrarray.Dataset</code> with a set of recommended rules </li> <li>xcube Viewer extension: in the JuypterLab use the dedicated Jupyter Notebook to test the presentation of    the data in the xcube Viewer (see <code>Visualize_data_with_xcube_viewer.ipynb</code>)</li> </ul> </li> <li> <p>Store the data cube in team storage on S3 </p> <ul> <li>for more information see <code>Save_cube_to_team_storage.ipynb</code></li> </ul> </li> <li> <p>For the publication contact the DeepESDL team (<code>esdl-support@brockmann-consult.de</code>) and communicate whether </p> <ul> <li>the data cube should be published in the Viewer and stored in a public DeepESDL bucket, or</li> <li>the data cube should be published in the Viewer (visualisation only)</li> </ul> </li> </ul>"},{"location":"guide/jupyterlab/","title":"Overview","text":""},{"location":"guide/jupyterlab/#deepesdl-jupyterlab","title":"DeepESDL JupyterLab","text":""},{"location":"guide/jupyterlab/#basic-usage","title":"Basic usage","text":"<p>This section provides a brief introduction for users to the basic features of the JupyterLab environment as offered by DeepESDL. For more in-depth documentation on the various components, see the links in the section Further Information.</p>"},{"location":"guide/jupyterlab/#logging-in-and-starting-the-jupyterlab-profile","title":"Logging in and starting the JupyterLab profile","text":"<p>To use the DeepESDL JupyterLab environment, navigate to https://deep.earthsystemdatalab.net/ with a web browser (a recent version of Firefox, Chrome, or Safari is recommended).</p> <p>Before first usage, we will have to register you with the system. Currently, we are not operational yet and still in testing phase. There is the possibility to register already as an Early Adopter. To this, we kindly ask you to write as an email at <code>esdl-support@brockmann-consult.de</code> and we will see if we can already onboard you.</p> <p>DeepESDL uses a GitHub to authenticate, so if you are already registered as a DeepESDL user, please use your GitHub account to log in. If your Jupyter server is not already running, you may be presented with a menu of user JupyterLab profiles to use for your session; there might be one or more JupyterLab profiles to choose from, depending on the computational resources needs of your team. Please select a suitable profile for your current task; it might not always require the profile with the strongest computational resources available. After choosing your environment, you will see a progress bar appearing for a few moments while it is started for you. The JupyterLab interface will then appear in your web browser, ready for use.</p>"},{"location":"guide/jupyterlab/#changing-a-jupyterlab-profile","title":"Changing a JupyterLab profile","text":"<p>If you have already started your session and need to change the JupyterLab profile, you can do this by selecting Hub control panel from the File menu within JupyterLab. Then click the <code>Stop my server</code> button and wait for your current server to shut down. When the <code>Start my server</code> button appears, you can click on it to return to the user JupyterLab profiles menu.</p>"},{"location":"guide/jupyterlab/#logging-out","title":"Logging out","text":"<p>To log out, select Log out from the File menu within JupyterLab.</p> <p>Note that your JupyterLab session will continue in the background even after you have logged out, but will eventually be terminated due to inactivity. If you wish to stop your session explicitly, you can use the hub control panel as described in the Changing a JupyterLab profile section above.</p>"},{"location":"guide/jupyterlab/#python-environment-selection-of-the-jupyter-kernel","title":"Python environment selection of the Jupyter Kernel","text":"<p>If you wish to use a special set of python packages, you can adjust it in the top right corner of the notebook. Next, a drop-down menu will appear, and you can select the desired kernel environment from it.</p> <p></p> <p>If the selected kernel seems not to load, it could be due to caching of kernels which do not exist anymore. To remove cached non-existing environment kernels,  follow these steps:</p> <ol> <li>Open the terminal within the jupyter lab</li> <li><code>$ rm -r .local/*</code></li> <li>it is alright to get a message like: \"rm: cannot remove     '.local/share/jupyter': Directory not empty\" because you might have      notebooks open, which are in the cache. Make therefor sure not to force     the <code>rm</code> command!</li> <li>Restart your jupyterlab server by selecting Hub control panel from the    File menu within JupyterLab. Then click the <code>Stop my server</code> button and     wait for your current server to shut down. Select the <code>Start my server</code>    button once it appears to return to the user JupyterLab profiles menu and     restart your session. </li> </ol> <p>To get a custom environment which suits your needs, please contact the DeepESDL team directly. </p>"},{"location":"guide/jupyterlab/#creating-custom-team-python-environment","title":"Creating custom team python environment","text":"<p>Up to two team members may create a custom python conda  environment for a team. Please inform the DeepESDL Team  who should be granted these permissions.</p> <p>Steps to create custom team conda environments:</p> <ol> <li>Head over to https://deep.earthsystemdatalab.net/conda-store</li> <li>Login with your GitHub Account which you also use to access the DeepESDL     JupyterLab</li> <li>If you have never created a custom environment, there will be none listed.</li> <li>Click on the Plus-sign next to Environments</li> <li>In the top section, select the namespace for which to create the custom     environment. There might be more than one if you are part of several     teams. If you are unsure which namespace you should use, have a look     at the Server Options overview of the DeepESDL JupyterLab.</li> <li> <p>You may either choose an environment.yml file to upload or paste your     environment configuration     into the window directly.     It should look something like this example: </p> <pre><code>channels:\n    - conda-forge\ndependencies:\n    - xcube=1.1.1\n    - xcube-cds\n    - xcube-sh\n    - xcube-cmems\n    - xcube-cci\n    - xcube_geodb\n    - boto3\n    - rasterio&gt;=1.3.6\n    - cartopy\n    - ipykernel\nname: xcube-1.1.1\n</code></pre> </li> <li> <p>Make sure to set a meaningful value to the environment's <code>name</code> property,     so also your teammates will know what it is about.</p> </li> <li>Once you are happy with your environment hit submit and grab a coffee. It     will take some time to create your custom environment.</li> <li>After submission, it will appear in the overview on     https://deep.earthsystemdatalab.net/conda-store/</li> <li>You can click on the name of your newly created environment and see its     status. There are three different statuses: Building, Completed, Failed</li> <li>If you click on the build number you can see the logs. This might be     useful if the build has failed.</li> <li>Once the build is completed, you need to refresh your browser window     to make it available in the kernel selection. Instructions how to change     the kernel to your custom environment are provided     in the section python environment selection of the jupyter kernel.</li> </ol> <p>You can also modify an existing environment and rebuild it; the conda-store  will keep all the builds' logs. It will look similar to the screenshot below.  </p> <p>The conda-build highlighted in green is the one, which you will use in you  DeepESDL JupyterLab, per default it is the latest successful build. If you  wish to make a different build the one to be used in DeepESDL JupyterLab,  select the checkmark in the blue button panel of the desired conda-build.  The reload button in the blue button panel will trigger a rebuild of the  conda-environment specification of the selected conda-build. The bin button  deletes the conda-build of the selected conda-build. </p>"},{"location":"guide/jupyterlab/#getting-started-notebooks","title":"Getting-started notebooks","text":"<p>You can find example notebooks in DeepESDL JupyterLab to help you to get  started. </p> <p>To access them:</p> <ol> <li>Head to the JupyterLab <code>Launcher</code>      If your <code>Launcher</code> is not visible right away, you can open it via the <code>plus</code>      button in the top left corner, which is highlighted in blue in the      screenshot. </li> <li>On the bottom of the Launcher you see a tile called <code>CATALOG DeeESDL</code>.      Please select this tile.  </li> <li>Once selected you see several example notebooks:      </li> <li>Select one of them, and you will see a preview of the notebook, to execute      the selected notebook click on <code>EXECUTE NOTEBOOK</code> in the top right corner.       </li> <li>The notebook is copied into your workspace, and you can run it and adjust      it according to your needs.</li> </ol>"},{"location":"guide/jupyterlab/#team-resources","title":"Team resources","text":"<p>If you are using DeepESDL Jupyter Lab with others in a team, you can share  content with each other and access the team S3 bucket for data storage.</p>"},{"location":"guide/jupyterlab/#team-shared-directory","title":"Team shared directory","text":"<p>To access the team shared directory, you can mount it into your workspace.  This is a one-time action, and once mounted, it will persist in your workspace  unless you choose to remove the mount. If you are part of multiple teams, you  only need to mount the directory once; there\u2019s no need to do so for each team  separately. </p> <p>To mount the team shared directory:</p> <ol> <li>Open the terminal in the DeepESDL Jupyter Lab via the Launcher.</li> <li><code>$ ln -s /extra ~/team-shared</code></li> </ol> <p>That\u2019s it! You will now see the team shared directory in your workspace.  Please be aware that anyone in the team can see, add, modify, or delete  content in this directory.</p>"},{"location":"guide/jupyterlab/#team-shared-s3-bucket","title":"Team shared S3 bucket","text":"<p>How to make use of the DeepESDL Team shared bucket is demonstrated in the example notebook Save_cube_to_team_storage.ipynb. The notebook is located in the DeepESDL notebook catalog in the \"team-shared\" category.</p>"},{"location":"guide/jupyterlab/#proxy-server-processes-like-dashboards-webserver-uis","title":"Proxy server processes like Dashboards, WebServer UIs,...","text":"<p>If you start a server process on a specific port (e.g. 8080) within your JupyterLab session, you can access it via the url  <code>https://deep.earthsystemdatalab.net/user/&lt;your-username&gt;/proxy/&lt;port&gt;/</code>, e.g. user <code>joe</code> can access his server process on port 8080 at  <code>https://deep.earthsystemdatalab.net/user/joe/proxy/8080/</code>.</p> <p>This url is only accessible as long as the JupyterLab session is running.</p>"},{"location":"guide/jupyterlab/notebooks/","title":"Overview","text":""},{"location":"guide/jupyterlab/notebooks/#example-notebooks","title":"Example Notebooks","text":"<p>This section holds a variety of example Jupyter Notebooks. For more specialised notebooks, e.g. from seminars, see here.</p>"},{"location":"guide/jupyterlab/notebooks/Access_collections_from_xcube_geodb/","title":"Access collections from xcube geodb","text":"<p>This notebook demonstrates how to make use of feature data stored in xcube geoDB.</p> <p>Please, also refer to the DeepESDL documentation and visit the platform's website for further information!</p> <p>Brockmann Consult, 2024</p> <p>This notebook runs with the python environment <code>deepesdl-xcube-1.7.1</code>, please checkout the documentation for help on changing the environment.</p> In\u00a0[1]: Copied! <pre># import xcube geoDB library\nfrom xcube_geodb.core.geodb import GeoDBClient\n</pre> # import xcube geoDB library from xcube_geodb.core.geodb import GeoDBClient In\u00a0[2]: Copied! <pre># utility needed for plotting\nimport matplotlib.pyplot as plt\n</pre> # utility needed for plotting import matplotlib.pyplot as plt <p>Please note: In order to access data from xcube geoDB, you need credentials. If you want to store feature data in xcubes geoDB or access existing data, please contact the DeepESDL team :) If the DeepESDL team has created access for you already, your credentials are saved as env variables.</p> In\u00a0[3]: Copied! <pre># Connect to the xcube geodatabase. \ngeodb = GeoDBClient()\n</pre> # Connect to the xcube geodatabase.  geodb = GeoDBClient() <p>Access available databases for deep-esdl to your xcube geoDB user. The below cell filters all databases for deep-esdl. Currently no data is shared publicly for all DeepESDL users, so the output will be empty if you don't belong to a team that has included data into xcube geoDB.</p> In\u00a0[4]: Copied! <pre>df = geodb.get_my_collections()\ndeepesdl_collections = df[df[\"database\"].str.contains(\"deep-esdl\")==True]\n</pre> df = geodb.get_my_collections() deepesdl_collections = df[df[\"database\"].str.contains(\"deep-esdl\")==True] In\u00a0[5]: Copied! <pre>deepesdl_collections\n</pre> deepesdl_collections Out[5]: owner database collection <p>However, lets have look at a publicly accessible collection to explore the functionality of xcube geodb.</p> The full statistical validation of Urban Atlas 2018 hasn't been performed yet. The European Environment Agency accepts no responsibility or liability whatsoever with regard to use of the non-validated Urban Atlas 2018 data. <p>To view other publicly available data stored in xcube geoDB:</p> In\u00a0[6]: Copied! <pre>df = geodb.get_my_collections()\ndf\n</pre> df = geodb.get_my_collections() df Out[6]: owner database collection 0 geodb_49a05d04-5d72-4c0f-9065-6e6827fd1871 anja E1 1 geodb_49a05d04-5d72-4c0f-9065-6e6827fd1871 anja E10a1 2 geodb_49a05d04-5d72-4c0f-9065-6e6827fd1871 anja E10a2 3 geodb_49a05d04-5d72-4c0f-9065-6e6827fd1871 anja E11 4 geodb_49a05d04-5d72-4c0f-9065-6e6827fd1871 anja E1a ... ... ... ... 935 geodb_b34bfae7-9265-4a3e-b921-06549d3c6035 stac_test ge_train_tier_2_labels 936 geodb_b34bfae7-9265-4a3e-b921-06549d3c6035 stac_test ge_train_tier_2_source 937 geodb_b34bfae7-9265-4a3e-b921-06549d3c6035 stac_test ties_ai_challenge_test 938 geodb_d2c4722a-cc19-4ec1-b575-0cdb6876d4a7 test_duplicate test 939 None None None <p>940 rows \u00d7 3 columns</p> <p>Access a collection which is concerning EEA Urban Atlas data:</p> In\u00a0[7]: Copied! <pre>ds = geodb.get_my_collections(database='eea-urban-atlas')\nds\n</pre> ds = geodb.get_my_collections(database='eea-urban-atlas') ds Out[7]: owner database collection 0 geodb_admin eea-urban-atlas AL001L1_TIRANA_UA2018 1 geodb_admin eea-urban-atlas AL003L1_ELBASAN_UA2018 2 geodb_admin eea-urban-atlas AL004L1_SHKODER_UA2018 3 geodb_admin eea-urban-atlas AL005L0_VLORE_UA2018 4 geodb_admin eea-urban-atlas AT001L3_WIEN_UA2018 ... ... ... ... 784 geodb_admin eea-urban-atlas UK569L2_IPSWICH_UA2018 785 geodb_admin eea-urban-atlas UK571L1_CHELTENHAM_UA2018 786 geodb_admin eea-urban-atlas XK001L1_PRISTINA_UA2018 787 geodb_admin eea-urban-atlas XK002L1_PRIZREN_UA2018 788 geodb_admin eea-urban-atlas XK003L1_MITROVICE_UA2018 <p>789 rows \u00d7 3 columns</p> <p>In the table 'METADATA' you can find a list of all collections with a link to the corresponding metadata xml. A METADATA table might not be available for other databases.</p> In\u00a0[8]: Copied! <pre>metadata = geodb.get_collection('METADATA', database='eea-urban-atlas')\nmetadata\n</pre> metadata = geodb.get_collection('METADATA', database='eea-urban-atlas') metadata Out[8]: id created_at modified_at geometry table_name disclaimer metadata_url 0 1 2021-03-10T11:35:20.29555+00:00 None POINT (0 0) EL004L1_IRAKLEIO_UA2018 The full statistical validation of Urban Atlas... https://edc-eea-urban-atlas.s3.eu-central-1.am... 1 2 2021-03-10T11:35:20.29555+00:00 None POINT (0 0) RO509L1_SATU_MARE_UA2018 The full statistical validation of Urban Atlas... https://edc-eea-urban-atlas.s3.eu-central-1.am... 2 3 2021-03-10T11:35:20.29555+00:00 None POINT (0 0) PL018L2_ZIELONA_GORA_UA2018 The full statistical validation of Urban Atlas... https://edc-eea-urban-atlas.s3.eu-central-1.am... 3 4 2021-03-10T11:35:20.29555+00:00 None POINT (0 0) UK009L1_CARDIFF_UA2018 The full statistical validation of Urban Atlas... https://edc-eea-urban-atlas.s3.eu-central-1.am... 4 5 2021-03-10T11:35:20.29555+00:00 None POINT (0 0) TR046L1_RIZE_UA2018 The full statistical validation of Urban Atlas... https://edc-eea-urban-atlas.s3.eu-central-1.am... ... ... ... ... ... ... ... ... 783 781 2021-03-10T11:35:20.29555+00:00 None POINT (0 0) SE505L1_BORAS_UA2018 The full statistical validation of Urban Atlas... https://edc-eea-urban-atlas.s3.eu-central-1.am... 784 782 2021-03-10T11:35:20.29555+00:00 None POINT (0 0) HU006L2_SZEGED_UA2018 The full statistical validation of Urban Atlas... https://edc-eea-urban-atlas.s3.eu-central-1.am... 785 783 2021-03-10T11:35:20.29555+00:00 None POINT (0 0) BG002L2_PLOVDIV_UA2018 The full statistical validation of Urban Atlas... https://edc-eea-urban-atlas.s3.eu-central-1.am... 786 787 2021-03-10T11:35:20.29555+00:00 None POINT (0 0) ES003L3_VALENCIA_UA2018 The full statistical validation of Urban Atlas... https://edc-eea-urban-atlas.s3.eu-central-1.am... 787 788 2021-03-10T11:35:20.29555+00:00 None POINT (0 0) RO022L1_BISTRITA_UA2018 The full statistical validation of Urban Atlas... https://edc-eea-urban-atlas.s3.eu-central-1.am... <p>788 rows \u00d7 7 columns</p> <p>Let's checkout the metadata for Hamburg:</p> In\u00a0[9]: Copied! <pre>metadata.loc[metadata['table_name'] == 'DE002L1_HAMBURG_UA2018']\n</pre> metadata.loc[metadata['table_name'] == 'DE002L1_HAMBURG_UA2018'] Out[9]: id created_at modified_at geometry table_name disclaimer metadata_url 669 668 2021-03-10T11:35:20.29555+00:00 None POINT (0 0) DE002L1_HAMBURG_UA2018 The full statistical validation of Urban Atlas... https://edc-eea-urban-atlas.s3.eu-central-1.am... In\u00a0[10]: Copied! <pre>data_hh = geodb.get_collection_pg('DE002L1_HAMBURG_UA2018', database='eea-urban-atlas')\ndata_hh\n</pre> data_hh = geodb.get_collection_pg('DE002L1_HAMBURG_UA2018', database='eea-urban-atlas') data_hh Out[10]: id created_at modified_at geometry country fua_name fua_code code_2018 class_2018 prod_date identifier perimeter area comment 0 1 2021-03-02T09:42:42.07546+00:00 None MULTIPOLYGON (((4313681.147 3358339.107, 43136... DE Hamburg DE002L1 11210 Discontinuous dense urban fabric (S.L. : 50% -... 2020-06 8738-DE002L1 136.705206 1053.511488 None 1 2 2021-03-02T09:42:42.07546+00:00 None MULTIPOLYGON (((4286000 3386090.175, 4285988.6... DE Hamburg DE002L1 11210 Discontinuous dense urban fabric (S.L. : 50% -... 2020-06 14614-DE002L1 3036.494939 78551.793270 None 2 3 2021-03-02T09:42:42.07546+00:00 None MULTIPOLYGON (((4313000 3385378.38, 4313079.61... DE Hamburg DE002L1 12100 Industrial, commercial, public, military and p... 2020-06 41718-DE002L1 2902.369992 128039.625423 None 3 4 2021-03-02T09:42:42.07546+00:00 None MULTIPOLYGON (((4336445.628 3378746.092, 43364... DE Hamburg DE002L1 11210 Discontinuous dense urban fabric (S.L. : 50% -... 2020-06 12579-DE002L1 3491.080254 106401.321126 None 4 5 2021-03-02T09:42:42.07546+00:00 None MULTIPOLYGON (((4327413.933 3382235.972, 43273... DE Hamburg DE002L1 11100 Continuous urban fabric (S.L. : &gt; 80%) 2020-06 2191-DE002L1 171.049974 1721.083818 None ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 75878 75879 2021-03-02T09:46:22.184492+00:00 None MULTIPOLYGON (((4349391.84 3384947.19, 4349381... DE Hamburg DE002L1 31000 Forests 2020-06 74168-DE002L1 1236.112475 33920.248741 None 75879 75880 2021-03-02T09:46:22.184492+00:00 None MULTIPOLYGON (((4341241.425 3387281.552, 43412... DE Hamburg DE002L1 31000 Forests 2020-06 74482-DE002L1 4935.325690 452405.287895 None 75880 75881 2021-03-02T09:46:22.184492+00:00 None MULTIPOLYGON (((4359519.682 3395155.954, 43594... DE Hamburg DE002L1 40000 Wetlands 2020-06 74611-DE002L1 1009.438947 23861.015193 None 75881 75882 2021-03-02T09:46:22.184492+00:00 None MULTIPOLYGON (((4321000 3382076.364, 4320988.6... DE Hamburg DE002L1 50000 Water 2020-06 75629-DE002L1 848.674936 21765.647478 None 75882 75883 2021-03-02T09:46:22.184492+00:00 None MULTIPOLYGON (((4363754.913 3380634.246, 43638... DE Hamburg DE002L1 50000 Water 2020-06 75797-DE002L1 874.689690 30695.895634 None <p>75883 rows \u00d7 14 columns</p> <p>Are you eager to vizualise the data? Here we go, import matplotlib first and then you can go ahead and plot the data. Again - it takes a few moments, after all there are 75883 polygons in the data :)</p> In\u00a0[11]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(20,18))\ndata_hh.plot(column=\"code_2018\", cmap=\"tab20\", ax=ax, legend=True)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(20,18)) data_hh.plot(column=\"code_2018\", cmap=\"tab20\", ax=ax, legend=True) Out[11]: <pre>&lt;Axes: &gt;</pre> <p>For more recently added collections you can view the history of a collection, to see what has happened to it so far. Unfortunatly, EEA Urban Atlas data was added before the history feature appeared in xcube geoDB. It also has not been modified since then, therefor the output is empty.</p> In\u00a0[12]: Copied! <pre>geodb.get_event_log('DE002L1_HAMBURG_UA2018', database='eea-urban-atlas')\n</pre> geodb.get_event_log('DE002L1_HAMBURG_UA2018', database='eea-urban-atlas') Out[12]: event_type message username date In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/Access_collections_from_xcube_geodb/#how-to-use-xcube-geodb-and-explore-available-deepesdl-data","title":"How to use xcube geoDB and explore available DeepESDL data\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Access_collections_from_xcube_geodb/#how-to-accesss-eea-urban-atlas-data","title":"How to accesss EEA Urban Atlas Data\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Access_public_cubes/","title":"Access public cubes","text":"In\u00a0[1]: Copied! <pre>import xcube\n</pre> import xcube In\u00a0[2]: Copied! <pre>xcube.__version__\n</pre> xcube.__version__ Out[2]: <pre>'1.6.0'</pre> In\u00a0[3]: Copied! <pre>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nstore.list_data_ids()\n</pre> from xcube.core.store import new_data_store store = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True)) store.list_data_ids() Out[3]: <pre>['LC-1x2160x2160-1.0.0.levels',\n 'SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000.levels',\n 'SMOS-L2C-OS-20230101-20231231-1W-res0-53x120x120.zarr',\n 'SMOS-L2C-OS-20230101-20231231-1W-res0.zarr',\n 'SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000.levels',\n 'SMOS-L2C-SM-20230101-20231231-1W-res0-53x120x120.zarr',\n 'SMOS-L2C-SM-20230101-20231231-1W-res0.zarr',\n 'SMOS-freezethaw-1x720x720-1.0.1.zarr',\n 'SMOS-freezethaw-4267x10x10-1.0.1.zarr',\n 'SeasFireCube_v3.zarr',\n 'black-sea-1x1024x1024.levels',\n 'black-sea-256x128x128.zarr',\n 'esa-cci-permafrost-1x1151x1641-0.0.2.levels',\n 'esdc-8d-0.25deg-1x720x1440-3.0.1.zarr',\n 'esdc-8d-0.25deg-256x128x128-3.0.1.zarr',\n 'hydrology-1D-0.009deg-100x60x60-3.0.2.zarr',\n 'hydrology-1D-0.009deg-1418x70x76-2.0.0.zarr',\n 'hydrology-1D-0.009deg-1x1102x2415-2.0.0.levels',\n 'hydrology-1D-0.009deg-1x1102x966-3.0.2.levels',\n 'ocean-1M-9km-1x1080x1080-1.4.0.levels',\n 'ocean-1M-9km-64x256x256-1.4.0.zarr',\n 'polar-100m-1x2048x2048-1.0.1.zarr']</pre> In\u00a0[4]: Copied! <pre>cube = store.open_data('esdc-8d-0.25deg-1x720x1440-3.0.1.zarr')\n</pre> cube = store.open_data('esdc-8d-0.25deg-1x720x1440-3.0.1.zarr') In\u00a0[5]: Copied! <pre>cube\n</pre> cube Out[5]: <pre>&lt;xarray.Dataset&gt; Size: 353GB\nDimensions:                            (time: 1978, lat: 720, lon: 1440)\nCoordinates:\n  * lat                                (lat) float64 6kB -89.88 -89.62 ... 89.88\n  * lon                                (lon) float64 12kB -179.9 ... 179.9\n  * time                               (time) datetime64[ns] 16kB 1979-01-05 ...\nData variables: (12/42)\n    aerosol_optical_thickness_550      (time, lat, lon) float32 8GB dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;\n    air_temperature_2m                 (time, lat, lon) float32 8GB dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;\n    bare_soil_evaporation              (time, lat, lon) float32 8GB dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;\n    burnt_area                         (time, lat, lon) float64 16GB dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;\n    cot                                (time, lat, lon) float32 8GB dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;\n    cth                                (time, lat, lon) float32 8GB dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;\n    ...                                 ...\n    sif_rtsif                          (time, lat, lon) float32 8GB dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;\n    sm                                 (time, lat, lon) float32 8GB dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;\n    snow_sublimation                   (time, lat, lon) float32 8GB dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;\n    surface_moisture                   (time, lat, lon) float32 8GB dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;\n    terrestrial_ecosystem_respiration  (time, lat, lon) float32 8GB dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;\n    transpiration                      (time, lat, lon) float32 8GB dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;\nAttributes: (12/23)\n    Conventions:                CF-1.9\n    acknowledgment:             All ESDC data providers are acknowledged insi...\n    contributor_name:           ['University of Leipzig', 'Max Planck Institu...\n    contributor_url:            ['https://www.uni-leipzig.de/', 'https://www....\n    creator_name:               ['University of Leipzig', 'Brockmann Consult ...\n    creator_url:                ['https://www.uni-leipzig.de/', 'https://www....\n    ...                         ...\n    publisher_url:              https://www.earthsystemdatalab.net/\n    time_coverage_end:          2021-12-31T00:00:00.000000000\n    time_coverage_start:        1979-01-05T00:00:00.000000000\n    time_period:                8D\n    time_period_reported_day:   5.0\n    title:                      Earth System Data Cube (ESDC) v3.0.1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 1978</li><li>lat: 720</li><li>lon: 1440</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float64-89.88 -89.62 ... 89.62 89.88long_name :latitudestandard_name :latitudeunits :degrees_north<pre>array([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875])</pre></li><li>lon(lon)float64-179.9 -179.6 ... 179.6 179.9long_name :longitudestandard_name :longitudeunits :degrees_east<pre>array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])</pre></li><li>time(time)datetime64[ns]1979-01-05 ... 2021-12-31long_name :timestandard_name :time<pre>array(['1979-01-05T00:00:00.000000000', '1979-01-13T00:00:00.000000000',\n       '1979-01-21T00:00:00.000000000', ..., '2021-12-15T00:00:00.000000000',\n       '2021-12-23T00:00:00.000000000', '2021-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (42)<ul><li>aerosol_optical_thickness_550(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :ESA Aerosol Climate Change Initiative (Aerosol_cci)date_modified :2022-10-13 03:15:18.312024description :ESA Aerosol Climate Change Initiative (Aerosol_cci): Level 3 aerosol products from AATSR (ensemble product), Version 2.6geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Aerosol Optical Thickness at 550 nmoriginal_add_offset :0.0original_name :AOD550_meanoriginal_scale_factor :1.0processing_steps :['Loading data using the cciodp xcube data store', 'Resampling by 8-day mean', 'Upsampling to 0.25 degrees using nearest neighbor']project :DeepESDLreferences :['https://doi.org/10.5194/amt-6-1919-2013']reported_day :5.0source :['https://catalogue.ceda.ac.uk/uuid/c183044b88734442b6d37f5c4f6b0092']standard_name :atmosphere_optical_thickness_due_to_ambient_aerosoltemporal_resolution :8Dtime_coverage_end :2012-04-10T00:00:00.000000000time_coverage_start :2002-05-21T00:00:00.000000000time_period :8Dunits :1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>air_temperature_2m(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Mean Air Temperature at 2 moriginal_add_offset :0.0original_name :t2moriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily mean', 'Converting to \u00b0C from K', 'Resampling by 8-day mean', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :mean_air_temperature_2mtemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :\u00b0C  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>bare_soil_evaporation(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Bare Soil Evaporationoriginal_add_offset :0.0original_name :Eboriginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :bare_soil_evaporationtemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>burnt_area(time, lat, lon)float64dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://www.globalfiredata.org/date_modified :2022-10-13 14:55:35.002779description :Global Fire Emissions Database (GFED) 4 Monthly Burnt Areageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Monthly Burnt Areaoriginal_add_offset :0.0original_name :burnt_areaoriginal_scale_factor :0.01processing_steps :['Merging hdf files', 'Resampling by 8-day nearest neighbor']project :DeepESDLreferences :['https://doi.org/10.1002/jgrg.20042']reported_day :5.0source :['https://www.globalfiredata.org/']standard_name :burnt_areatemporal_resolution :8Dtime_coverage_end :2016-12-30T00:00:00.000000000time_coverage_start :1995-06-06T00:00:00.000000000time_period :8Dunits :hectares  Array   Chunk   Bytes   15.28 GiB   7.91 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float64 numpy.ndarray  1440 720 1978 </li><li>cot(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :ESA Cloud Climate Change Initiative (Cloud_cci)date_modified :2022-11-04 13:33:18.450458description :ESA Cloud Climate Change Initiative (Cloud_cci): MODIS-TERRA monthly gridded cloud properties, version 2.0geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Cloud Optical Thicknessoriginal_add_offset :0.0original_name :cotoriginal_scale_factor :1.0processing_steps :['Loading data using the cciodp xcube data store', 'Resampling by 8-day nearest neighbor', 'Upsampling to 0.25 degrees using nearest neighbor']project :DeepESDLreferences :['https://public.satproj.klima.dwd.de/data/ESA_Cloud_CCI/CLD_PRODUCTS/v2.0/DOIs/DOI_ESA_Cloud_cci_MODIS-Terra_v2.0_landingpage.html', 'https://dap.ceda.ac.uk/neodc/esacci/cloud/docs/DataSet_Desc_ESA_Cloud_cci_CC4CL_1.5.pdf']reported_day :5.0source :['https://catalogue.ceda.ac.uk/uuid/f1ab07b5292f4813bd3090b51d270aa8']standard_name :atmosphere_optical_thickness_due_to_cloudtemporal_resolution :8Dtime_coverage_end :2014-12-15T00:00:00.000000000time_coverage_start :2000-02-22T00:00:00.000000000time_period :8Dunits :1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>cth(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :ESA Cloud Climate Change Initiative (Cloud_cci)date_modified :2022-11-04 13:33:18.450458description :ESA Cloud Climate Change Initiative (Cloud_cci): MODIS-TERRA monthly gridded cloud properties, version 2.0geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Cloud Top Heightoriginal_add_offset :0.0original_name :cthoriginal_scale_factor :1.0processing_steps :['Loading data using the cciodp xcube data store', 'Resampling by 8-day nearest neighbor', 'Upsampling to 0.25 degrees using nearest neighbor']project :DeepESDLreferences :['https://public.satproj.klima.dwd.de/data/ESA_Cloud_CCI/CLD_PRODUCTS/v2.0/DOIs/DOI_ESA_Cloud_cci_MODIS-Terra_v2.0_landingpage.html', 'https://dap.ceda.ac.uk/neodc/esacci/cloud/docs/DataSet_Desc_ESA_Cloud_cci_CC4CL_1.5.pdf']reported_day :5.0source :['https://catalogue.ceda.ac.uk/uuid/f1ab07b5292f4813bd3090b51d270aa8']standard_name :cloud_top_altitudetemporal_resolution :8Dtime_coverage_end :2014-12-15T00:00:00.000000000time_coverage_start :2000-02-22T00:00:00.000000000time_period :8Dunits :km  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>ctt(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :ESA Cloud Climate Change Initiative (Cloud_cci)date_modified :2022-11-04 13:33:18.450458description :ESA Cloud Climate Change Initiative (Cloud_cci): MODIS-TERRA monthly gridded cloud properties, version 2.0geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Cloud Top Temperatureoriginal_add_offset :0.0original_name :cttoriginal_scale_factor :1.0processing_steps :['Loading data using the cciodp xcube data store', 'Resampling by 8-day nearest neighbor', 'Upsampling to 0.25 degrees using nearest neighbor']project :DeepESDLreferences :['https://public.satproj.klima.dwd.de/data/ESA_Cloud_CCI/CLD_PRODUCTS/v2.0/DOIs/DOI_ESA_Cloud_cci_MODIS-Terra_v2.0_landingpage.html', 'https://dap.ceda.ac.uk/neodc/esacci/cloud/docs/DataSet_Desc_ESA_Cloud_cci_CC4CL_1.5.pdf']reported_day :5.0source :['https://catalogue.ceda.ac.uk/uuid/f1ab07b5292f4813bd3090b51d270aa8']standard_name :air_temperature_at_cloud_toptemporal_resolution :8Dtime_coverage_end :2014-12-15T00:00:00.000000000time_coverage_start :2000-02-22T00:00:00.000000000time_period :8Dunits :K  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>evaporation(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Actual Evaporationoriginal_add_offset :0.0original_name :Eoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :actual_evaporationtemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>evaporation_era5(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Evaporationoriginal_add_offset :0.0original_name :eoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily sum', 'Converting to mm from m', 'Resampling by 8-day mean', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :lwe_thickness_of_water_evaporation_amounttemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>evaporative_stress(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Evaporative Stressoriginal_add_offset :0.0original_name :Soriginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :evaporative_stresstemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>gross_primary_productivity(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :FLUXCOMdate_modified :2022-10-17 22:14:30.401506description :FLUXCOMgeospatial_lat_max :89.87499928049999geospatial_lat_min :-89.8750000005geospatial_lat_resolution :0.25geospatial_lon_max :179.87499856049996geospatial_lon_min :-179.8750000005geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Gross Primary Productivityoriginal_add_offset :0.0original_name :GPPoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.5194/bg-13-4291-2016', 'https://doi.org/10.1038/s41597-019-0076-8']reported_day :5.0source :['https://www.fluxcom.org/']standard_name :gross_primary_productivity_of_carbontemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :2001-01-05T00:00:00.000000000time_period :8Dunits :g C m^-2 d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>interception_loss(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Interception Lossoriginal_add_offset :0.0original_name :Eioriginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :interception_losstemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>kndvi(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/date_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Kernel Normalized Difference Vegetation Indexoriginal_add_offset :0.0original_name :kNDVIoriginal_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.1126/sciadv.abc7447', 'https://github.com/awesome-spectral-indices/awesome-spectral-indices', 'https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://github.com/awesome-spectral-indices/spyndex', 'https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :kNDVItemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>latent_energy(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :FLUXCOMdate_modified :2022-10-17 22:14:30.401506description :FLUXCOMgeospatial_lat_max :89.87499928049999geospatial_lat_min :-89.8750000005geospatial_lat_resolution :0.25geospatial_lon_max :179.87499856049996geospatial_lon_min :-179.8750000005geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Latent Energyoriginal_add_offset :0.0original_name :LEoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.5194/bg-13-4291-2016', 'https://doi.org/10.1038/s41597-019-0076-8']reported_day :5.0source :['https://www.fluxcom.org/']standard_name :surface_upward_latent_heat_fluxtemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :2001-01-05T00:00:00.000000000time_period :8Dunits :MJ m^-2 d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>max_air_temperature_2m(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Maximum Air Temperature at 2 moriginal_add_offset :0.0original_name :t2m_maxoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily max', 'Converting to \u00b0C from K', 'Resampling by 8-day max', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :max_air_temperature_2mtemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :\u00b0C  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>min_air_temperature_2m(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Minimum Air Temperature at 2 moriginal_add_offset :0.0original_name :t2m_minoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily min', 'Converting to \u00b0C from K', 'Resampling by 8-day min', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :min_air_temperature_2mtemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :\u00b0C  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nbar_blue(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/bandwidth :20.0bandwidth_units :nmdate_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Nadir BRDF Adjusted Reflectance of Band 3 (blue)original_add_offset :0.0original_name :Nadir_Reflectance_Band3original_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :nadir_reflectance_band3temporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1wavelength :469.0wavelength_units :nm  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nbar_green(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/bandwidth :20.0bandwidth_units :nmdate_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Nadir BRDF Adjusted Reflectance of Band 4 (green)original_add_offset :0.0original_name :Nadir_Reflectance_Band4original_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :nadir_reflectance_band4temporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1wavelength :555.0wavelength_units :nm  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nbar_nir(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/bandwidth :35.0bandwidth_units :nmdate_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Nadir BRDF Adjusted Reflectance of Band 2 (NIR)original_add_offset :0.0original_name :Nadir_Reflectance_Band2original_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :nadir_reflectance_band2temporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1wavelength :858.5wavelength_units :nm  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nbar_red(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/bandwidth :50.0bandwidth_units :nmdate_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Nadir BRDF Adjusted Reflectance of Band 1 (red)original_add_offset :0.0original_name :Nadir_Reflectance_Band1original_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :nadir_reflectance_band1temporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1wavelength :645.0wavelength_units :nm  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nbar_swir1(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/bandwidth :20.0bandwidth_units :nmdate_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Nadir BRDF Adjusted Reflectance of Band 5 (SWIR1)original_add_offset :0.0original_name :Nadir_Reflectance_Band5original_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :nadir_reflectance_band5temporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1wavelength :1240.0wavelength_units :nm  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nbar_swir2(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/bandwidth :24.0bandwidth_units :nmdate_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Nadir BRDF Adjusted Reflectance of Band 6 (SWIR2)original_add_offset :0.0original_name :Nadir_Reflectance_Band6original_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :nadir_reflectance_band6temporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1wavelength :1640.0wavelength_units :nm  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nbar_swir3(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/bandwidth :50.0bandwidth_units :nmdate_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Nadir BRDF Adjusted Reflectance of Band 7 (SWIR3)original_add_offset :0.0original_name :Nadir_Reflectance_Band7original_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :nadir_reflectance_band7temporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1wavelength :2130.0wavelength_units :nm  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>ndvi(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/date_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Normalized Difference Vegetation Indexoriginal_add_offset :0.0original_name :NDVIoriginal_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://ntrs.nasa.gov/citations/19740022614', 'https://github.com/awesome-spectral-indices/awesome-spectral-indices', 'https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://github.com/awesome-spectral-indices/spyndex', 'https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :NDVItemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>net_ecosystem_exchange(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :FLUXCOMdate_modified :2022-10-17 22:14:30.401506description :FLUXCOMgeospatial_lat_max :89.87499928049999geospatial_lat_min :-89.8750000005geospatial_lat_resolution :0.25geospatial_lon_max :179.87499856049996geospatial_lon_min :-179.8750000005geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Net Ecosystem Exchangeoriginal_add_offset :0.0original_name :NEEoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.5194/bg-13-4291-2016', 'https://doi.org/10.1038/s41597-019-0076-8']reported_day :5.0source :['https://www.fluxcom.org/']standard_name :net_primary_productivity_of_carbontemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :2001-01-05T00:00:00.000000000time_period :8Dunits :g C m^-2 d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>net_radiation(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :FLUXCOMdate_modified :2022-10-17 22:14:30.401506description :FLUXCOMgeospatial_lat_max :89.87499928049999geospatial_lat_min :-89.8750000005geospatial_lat_resolution :0.25geospatial_lon_max :179.87499856049996geospatial_lon_min :-179.8750000005geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Net Radiationoriginal_add_offset :0.0original_name :Rnoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.5194/bg-13-4291-2016', 'https://doi.org/10.1038/s41597-019-0076-8']reported_day :5.0source :['https://www.fluxcom.org/']standard_name :surface_net_radiation_fluxtemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :2001-01-05T00:00:00.000000000time_period :8Dunits :MJ m^-2 d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nirv(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/date_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Near Infrared Reflectance of Vegetationoriginal_add_offset :0.0original_name :NIRvoriginal_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.1126/sciadv.1602244', 'https://github.com/awesome-spectral-indices/awesome-spectral-indices', 'https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://github.com/awesome-spectral-indices/spyndex', 'https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :NIRvtemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>open_water_evaporation(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Open-water Evaporationoriginal_add_offset :0.0original_name :Eworiginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :open_water_evaporationtemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>potential_evaporation(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Potential Evaporationoriginal_add_offset :0.0original_name :Eporiginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :potential_evaporationtemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>precipitation_era5(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Total Precipitationoriginal_add_offset :0.0original_name :tporiginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily sum', 'Converting to mm from m', 'Resampling by 8-day mean', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :total_precipitationtemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>radiation_era5(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Surface Net Solar Radiationoriginal_add_offset :0.0original_name :ssroriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily mean', 'Resampling by 8-day mean', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :surface_net_downward_shortwave_fluxtemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :J m^-2  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>root_moisture(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Root-zone Soil Moistureoriginal_add_offset :0.0original_name :SMrootoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :root_zone_soil_moisturetemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>sensible_heat(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :FLUXCOMdate_modified :2022-10-17 22:14:30.401506description :FLUXCOMgeospatial_lat_max :89.87499928049999geospatial_lat_min :-89.8750000005geospatial_lat_resolution :0.25geospatial_lon_max :179.87499856049996geospatial_lon_min :-179.8750000005geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Sensible Heatoriginal_add_offset :0.0original_name :Horiginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.5194/bg-13-4291-2016', 'https://doi.org/10.1038/s41597-019-0076-8']reported_day :5.0source :['https://www.fluxcom.org/']standard_name :surface_upward_sensible_heat_fluxtemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :2001-01-05T00:00:00.000000000time_period :8Dunits :MJ m^-2 d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>sif_gome2_jj(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://doi.org/10.5194/essd-12-1101-2020date_modified :2022-10-11 22:36:53.583022description :Spatially Downscaled Sun-Induced Fluorescence (JJ Method)geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000000003geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Downscaled Daily Corrected Sun-Induced Chlorophyll Fluorescence at 740 nmoriginal_add_offset :0.0original_name :SIForiginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation']project :DeepESDLreferences :['https://doi.org/10.5194/essd-12-1101-2020']reported_day :9.0source :['https://data.jrc.ec.europa.eu/dataset/21935ffc-b797-4bee-94da-8fec85b3f9e1']standard_name :siftemporal_resolution :16Dtime_coverage_end :2018-10-04T00:00:00.000000000time_coverage_start :2007-01-21T00:00:00.000000000time_period :8Dunits :m W m^-2 sr^-1 nm^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>sif_gome2_pk(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://doi.org/10.5194/essd-12-1101-2020date_modified :2022-10-11 22:43:08.258033description :Spatially Downscaled Sun-Induced Fluorescence (PK Method)geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000000003geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Downscaled Daily Corrected Sun-Induced Chlorophyll Fluorescence at 740 nmoriginal_add_offset :0.0original_name :SIForiginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation']project :DeepESDLreferences :['https://doi.org/10.5194/essd-12-1101-2020']reported_day :9.0source :['https://data.jrc.ec.europa.eu/dataset/21935ffc-b797-4bee-94da-8fec85b3f9e1']standard_name :siftemporal_resolution :16Dtime_coverage_end :2018-12-31T00:00:00.000000000time_coverage_start :2007-01-21T00:00:00.000000000time_period :8Dunits :m W m^-2 sr^-1 nm^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>sif_gosif(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://doi.org/10.3390/rs11050517date_modified :2022-10-11 22:20:05.841847description :GOSIF Solar-Induced Chlorophyll Fluorescence Derived from OCO-2, MODIS, and Reanalysis Datageospatial_lat_max :89.87499999999999geospatial_lat_min :-89.87500000000001geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000000003geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Sun-Induced Chlorophyll Fluorescence at 757 nmoriginal_add_offset :0.0original_name :siforiginal_scale_factor :0.0001processing_steps :['Merging tif files', 'Converting water bodies and snow covered areas to NaN', 'Applying original scale factor', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.3390/rs11050517']reported_day :5.0source :['https://globalecology.unh.edu/data.html']standard_name :siftemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :W m^-2 sr^-1 um^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>sif_rtsif(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://doi.org/10.1038/s41597-022-01520-1date_modified :2022-10-12 14:26:02.972963description :Long-term Reconstructed TROPOMI Solar-Induced Fluorescence (RTSIF)geospatial_lat_max :89.87499999999999geospatial_lat_min :-89.87500000000001geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000000003geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Sun-Induced Chlorophyll Fluorescence at 740 nmoriginal_add_offset :0.0original_name :siforiginal_scale_factor :1.0processing_steps :['Merging tif files', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.1038/s41597-022-01520-1']reported_day :5.0source :['https://figshare.com/articles/dataset/RTSIF_dataset/19336346/2']standard_name :siftemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :2001-01-05T00:00:00.000000000time_period :8Dunits :m W m^-2 sr^-1 um^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>sm(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :ESA Soil Moisture Climate Change Initiative (Soil_Moisture_cci)date_modified :2022-10-13 20:42:41.277132description :ESA Soil Moisture Climate Change Initiative (Soil_Moisture_cci): COMBINED product, Version 06.1geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Volumetric Soil Moistureoriginal_add_offset :0.0original_name :smoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by 8-day mean']project :DeepESDLreferences :['https://data.cci.ceda.ac.uk/thredds/fileServer/esacci/soil_moisture/docs/v06.1/ESA_CCI_SM_RD_D2.1_v2_ATBD_v06.1_issue_1.1.pdf', 'https://doi.org/10.5194/essd-11-717-2019', 'https://doi.org/10.1016/j.rse.2017.07.001']reported_day :5.0source :['https://catalogue.ceda.ac.uk/uuid/43d73291472444e6b9c2d2420dbad7d6']standard_name :volumetric_soil_moisturetemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :1979-01-05T00:00:00.000000000time_period :8Dunits :m^3 m^-3  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>snow_sublimation(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Snow Sublimationoriginal_add_offset :0.0original_name :Esoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :snow_sublimationtemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>surface_moisture(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Surface Soil Moistureoriginal_add_offset :0.0original_name :SMsurforiginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :surface_soil_moisturetemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>terrestrial_ecosystem_respiration(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :FLUXCOMdate_modified :2022-10-17 22:14:30.401506description :FLUXCOMgeospatial_lat_max :89.87499928049999geospatial_lat_min :-89.8750000005geospatial_lat_resolution :0.25geospatial_lon_max :179.87499856049996geospatial_lon_min :-179.8750000005geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Terrestrial Ecosystem Respirationoriginal_add_offset :0.0original_name :TERoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.5194/bg-13-4291-2016', 'https://doi.org/10.1038/s41597-019-0076-8']reported_day :5.0source :['https://www.fluxcom.org/']standard_name :ecosystem_respiration_carbon_fluxtemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :2001-01-05T00:00:00.000000000time_period :8Dunits :g C m^-2 d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>transpiration(time, lat, lon)float32dask.array&lt;chunksize=(1, 720, 1440), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Transpirationoriginal_add_offset :0.0original_name :Etoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :transpirationtemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   3.96 MiB   Shape   (1978, 720, 1440)   (1, 720, 1440)   Dask graph   1978 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([-89.875, -89.625, -89.375, -89.125, -88.875, -88.625, -88.375, -88.125,\n       -87.875, -87.625,\n       ...\n        87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,  89.375,\n        89.625,  89.875],\n      dtype='float64', name='lat', length=720))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625, -178.375,\n       -178.125, -177.875, -177.625,\n       ...\n        177.625,  177.875,  178.125,  178.375,  178.625,  178.875,  179.125,\n        179.375,  179.625,  179.875],\n      dtype='float64', name='lon', length=1440))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1979-01-05', '1979-01-13', '1979-01-21', '1979-01-29',\n               '1979-02-06', '1979-02-14', '1979-02-22', '1979-03-02',\n               '1979-03-10', '1979-03-18',\n               ...\n               '2021-10-20', '2021-10-28', '2021-11-05', '2021-11-13',\n               '2021-11-21', '2021-11-29', '2021-12-07', '2021-12-15',\n               '2021-12-23', '2021-12-31'],\n              dtype='datetime64[ns]', name='time', length=1978, freq=None))</pre></li></ul></li><li>Attributes: (23)Conventions :CF-1.9acknowledgment :All ESDC data providers are acknowledged inside each variablecontributor_name :['University of Leipzig', 'Max Planck Institute', 'Brockmann Consult GmbH']contributor_url :['https://www.uni-leipzig.de/', 'https://www.mpg.de/en', 'https://www.brockmann-consult.de/']creator_name :['University of Leipzig', 'Brockmann Consult GmbH']creator_url :['https://www.uni-leipzig.de/', 'https://www.brockmann-consult.de/']date_modified :2022-11-25 23:13:03.350030geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25id :esdc-8d-0.25deg-256x128x128-3.0.1license :Terms and conditions of the DeepESDL data distributionproject :DeepESDLpublisher_name :DeepESDL Teampublisher_url :https://www.earthsystemdatalab.net/time_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1979-01-05T00:00:00.000000000time_period :8Dtime_period_reported_day :5.0title :Earth System Data Cube (ESDC) v3.0.1</li></ul> In\u00a0[6]: Copied! <pre>cube.air_temperature_2m.isel(time=177).plot()\n</pre> cube.air_temperature_2m.isel(time=177).plot() Out[6]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7f0eec503a90&gt;</pre> In\u00a0[7]: Copied! <pre>ml_dataset = store.open_data('LC-1x2160x2160-1.0.0.levels')\n</pre> ml_dataset = store.open_data('LC-1x2160x2160-1.0.0.levels') <p>To find out how many levels are available:#</p> In\u00a0[8]: Copied! <pre>ml_dataset.num_levels\n</pre> ml_dataset.num_levels Out[8]: <pre>6</pre> <p>Display information about all dataset levels, note how the lat and lon shape changes per level:</p> In\u00a0[9]: Copied! <pre>for level in range(ml_dataset.num_levels):\n    dataset_i = ml_dataset.get_dataset(level)\n    display(dataset_i)\n</pre> for level in range(ml_dataset.num_levels):     dataset_i = ml_dataset.get_dataset(level)     display(dataset_i) <pre>&lt;xarray.Dataset&gt; Size: 1TB\nDimensions:              (time: 11, lat: 64800, lon: 129600, bounds: 2)\nCoordinates:\n  * lat                  (lat) float64 518kB 90.0 90.0 89.99 ... -90.0 -90.0\n  * lon                  (lon) float64 1MB -180.0 -180.0 -180.0 ... 180.0 180.0\n  * time                 (time) datetime64[ns] 88B 2010-01-01 ... 2020-01-01\nDimensions without coordinates: bounds\nData variables:\n    change_count         (time, lat, lon) uint8 92GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    crs                  (time) int32 44B dask.array&lt;chunksize=(11,), meta=np.ndarray&gt;\n    current_pixel_state  (time, lat, lon) float32 370GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    lat_bounds           (time, lat, bounds) float64 11MB dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;\n    lccs_class           (time, lat, lon) uint8 92GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    lon_bounds           (time, lon, bounds) float64 23MB dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;\n    observation_count    (time, lat, lon) uint16 185GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    processed_flag       (time, lat, lon) float32 370GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    time_bounds          (time, bounds) datetime64[ns] 176B dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\nAttributes: (12/38)\n    Conventions:                CF-1.6\n    TileSize:                   2025:2025\n    cdm_data_type:              grid\n    comment:                    \n    contact:                    https://www.ecmwf.int/en/about/contact-us/get...\n    creation_date:              20181130T095431Z\n    ...                         ...\n    time_coverage_end:          20101231\n    time_coverage_resolution:   P1Y\n    time_coverage_start:        20100101\n    title:                      Land Cover Map of ESA CCI brokered by CDS\n    tracking_id:                96ac9aca-1ca7-45c6-b4a5-ab448c692646\n    type:                       ESACCI-LC-L4-LCCS-Map-300m-P1Y</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 11</li><li>lat: 64800</li><li>lon: 129600</li><li>bounds: 2</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6490.0 90.0 89.99 ... -90.0 -90.0axis :Ybounds :lat_boundslong_name :latitudestandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([ 89.998611,  89.995833,  89.993056, ..., -89.993056, -89.995833,\n       -89.998611])</pre></li><li>lon(lon)float64-180.0 -180.0 ... 180.0 180.0axis :Xbounds :lon_boundslong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-179.998611, -179.995833, -179.993056, ...,  179.993056,  179.995833,\n        179.998611])</pre></li><li>time(time)datetime64[ns]2010-01-01 ... 2020-01-01axis :Tbounds :time_boundslong_name :timestandard_name :time<pre>array(['2010-01-01T00:00:00.000000000', '2011-01-01T00:00:00.000000000',\n       '2012-01-01T00:00:00.000000000', '2013-01-01T00:00:00.000000000',\n       '2014-01-01T00:00:00.000000000', '2015-01-01T00:00:00.000000000',\n       '2016-01-01T00:00:00.000000000', '2017-01-01T00:00:00.000000000',\n       '2018-01-01T00:00:00.000000000', '2019-01-01T00:00:00.000000000',\n       '2020-01-01T00:00:00.000000000'], dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (9)<ul><li>change_count(time, lat, lon)uint8dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;long_name :number of class changesvalid_max :100valid_min :0  Array   Chunk   Bytes   86.03 GiB   4.45 MiB   Shape   (11, 64800, 129600)   (1, 2160, 2160)   Dask graph   19800 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  129600 64800 11 </li><li>crs(time)int32dask.array&lt;chunksize=(11,), meta=np.ndarray&gt;i2m :0.002777777777778,0.0,0.0,-0.002777777777778,-180.0,90.0wkt :GEOGCS[\"WGS 84\",    DATUM[\"World Geodetic System 1984\",      SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]],      AUTHORITY[\"EPSG\",\"6326\"]],    PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]],    UNIT[\"degree\", 0.017453292519943295],    AXIS[\"Geodetic longitude\", EAST],    AXIS[\"Geodetic latitude\", NORTH],    AUTHORITY[\"EPSG\",\"4326\"]]  Array   Chunk   Bytes   44 B   44 B   Shape   (11,)   (11,)   Dask graph   1 chunks in 2 graph layers   Data type   int32 numpy.ndarray  11 1 </li><li>current_pixel_state(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;flag_meanings :invalid clear_land clear_water clear_snow_ice cloud cloud_shadowflag_values :[0, 1, 2, 3, 4, 5]long_name :LC pixel type maskstandard_name :land_cover_lccs status_flagvalid_max :5valid_min :0  Array   Chunk   Bytes   344.14 GiB   17.80 MiB   Shape   (11, 64800, 129600)   (1, 2160, 2160)   Dask graph   19800 chunks in 2 graph layers   Data type   float32 numpy.ndarray  129600 64800 11 </li><li>lat_bounds(time, lat, bounds)float64dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   10.88 MiB   33.75 kiB   Shape   (11, 64800, 2)   (1, 2160, 2)   Dask graph   330 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 64800 11 </li><li>lccs_class(time, lat, lon)uint8dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;ancillary_variables :processed_flag current_pixel_state observation_count change_countflag_colors :#ffff64 #ffff64 #ffff00 #aaf0f0 #dcf064 #c8c864 #006400 #00a000 #00a000 #aac800 #003c00 #003c00 #005000 #285000 #285000 #286400 #788200 #8ca000 #be9600 #966400 #966400 #966400 #ffb432 #ffdcd2 #ffebaf #ffc864 #ffd278 #ffebaf #00785a #009678 #00dc82 #c31400 #fff5d7 #dcdcdc #fff5d7 #0046c8 #ffffffflag_meanings :no_data cropland_rainfed cropland_rainfed_herbaceous_cover cropland_rainfed_tree_or_shrub_cover cropland_irrigated mosaic_cropland mosaic_natural_vegetation tree_broadleaved_evergreen_closed_to_open tree_broadleaved_deciduous_closed_to_open tree_broadleaved_deciduous_closed tree_broadleaved_deciduous_open tree_needleleaved_evergreen_closed_to_open tree_needleleaved_evergreen_closed tree_needleleaved_evergreen_open tree_needleleaved_deciduous_closed_to_open tree_needleleaved_deciduous_closed tree_needleleaved_deciduous_open tree_mixed mosaic_tree_and_shrub mosaic_herbaceous shrubland shrubland_evergreen shrubland_deciduous grassland lichens_and_mosses sparse_vegetation sparse_tree sparse_shrub sparse_herbaceous tree_cover_flooded_fresh_or_brakish_water tree_cover_flooded_saline_water shrub_or_herbaceous_cover_flooded urban bare_areas bare_areas_consolidated bare_areas_unconsolidated water snow_and_iceflag_values :[0, 10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100, 110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180, 190, 200, 201, 202, 210, 220]long_name :Land cover class defined in LCCSstandard_name :land_cover_lccsvalid_max :220valid_min :1  Array   Chunk   Bytes   86.03 GiB   4.45 MiB   Shape   (11, 64800, 129600)   (1, 2160, 2160)   Dask graph   19800 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  129600 64800 11 </li><li>lon_bounds(time, lon, bounds)float64dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   21.75 MiB   33.75 kiB   Shape   (11, 129600, 2)   (1, 2160, 2)   Dask graph   660 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 129600 11 </li><li>observation_count(time, lat, lon)uint16dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;long_name :number of valid observationsstandard_name :land_cover_lccs number_of_observationsvalid_max :32767valid_min :0  Array   Chunk   Bytes   172.07 GiB   8.90 MiB   Shape   (11, 64800, 129600)   (1, 2160, 2160)   Dask graph   19800 chunks in 2 graph layers   Data type   uint16 numpy.ndarray  129600 64800 11 </li><li>processed_flag(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;flag_meanings :not_processed processedflag_values :[0, 1]long_name :LC map processed area flagstandard_name :land_cover_lccs status_flagvalid_max :1valid_min :0  Array   Chunk   Bytes   344.14 GiB   17.80 MiB   Shape   (11, 64800, 129600)   (1, 2160, 2160)   Dask graph   19800 chunks in 2 graph layers   Data type   float32 numpy.ndarray  129600 64800 11 </li><li>time_bounds(time, bounds)datetime64[ns]dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   176 B   16 B   Shape   (11, 2)   (1, 2)   Dask graph   11 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 11 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.99861111111113,  89.99583333333334,  89.99305555555557,\n        89.99027777777778,  89.98750000000001,  89.98472222222222,\n        89.98194444444445,  89.97916666666669,  89.97638888888889,\n        89.97361111111113,\n       ...\n       -89.97361111111111, -89.97638888888889, -89.97916666666667,\n       -89.98194444444445, -89.98472222222222,           -89.9875,\n       -89.99027777777778, -89.99305555555556, -89.99583333333334,\n       -89.99861111111112],\n      dtype='float64', name='lat', length=64800))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([ -179.9986111111111, -179.99583333333334, -179.99305555555554,\n       -179.99027777777778,           -179.9875, -179.98472222222222,\n       -179.98194444444445, -179.97916666666666,  -179.9763888888889,\n        -179.9736111111111,\n       ...\n         179.9736111111111,   179.9763888888889,  179.97916666666669,\n        179.98194444444448,  179.98472222222222,            179.9875,\n         179.9902777777778,  179.99305555555554,  179.99583333333334,\n        179.99861111111113],\n      dtype='float64', name='lon', length=129600))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2010-01-01', '2011-01-01', '2012-01-01', '2013-01-01',\n               '2014-01-01', '2015-01-01', '2016-01-01', '2017-01-01',\n               '2018-01-01', '2019-01-01', '2020-01-01'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (38)Conventions :CF-1.6TileSize :2025:2025cdm_data_type :gridcomment :contact :https://www.ecmwf.int/en/about/contact-us/get-supportcreation_date :20181130T095431Zcreator_email :landcover-cci@uclouvain.becreator_name :UCLouvaincreator_url :http://www.uclouvain.be/geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.002778geospatial_lat_units :degrees_northgeospatial_lon_max :180geospatial_lon_min :-180geospatial_lon_resolution :0.002778geospatial_lon_units :degrees_easthistory :amorgos-4,0, lc-sdr-1.0, lc-sr-1.0, lc-classification-1.0,lc-user-tools-3.13,lc-user-tools-4.3id :ESACCI-LC-L4-LCCS-Map-300m-P1Y-2010-v2.0.7cdsinstitution :UCLouvainkeywords :land cover classification,satellite,observationkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :ESA CCI Data Policy: free and open accessnaming_authority :org.esa-cciproduct_version :2.0.7cdsproject :Climate Change Initiative - European Space Agencyreferences :http://www.esa-landcover-cci.org/source :MERIS FR L1B version 5.05, MERIS RR L1B version 8.0, SPOT VGT Pspatial_resolution :300mstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Standard Names version 21summary :This dataset characterizes the land cover of a particular year (see time_coverage). The land cover was derived from the analysis of satellite data time series of the full period.time_coverage_duration :P1Ytime_coverage_end :20101231time_coverage_resolution :P1Ytime_coverage_start :20100101title :Land Cover Map of ESA CCI brokered by CDStracking_id :96ac9aca-1ca7-45c6-b4a5-ab448c692646type :ESACCI-LC-L4-LCCS-Map-300m-P1Y</li></ul> <pre>&lt;xarray.Dataset&gt; Size: 277GB\nDimensions:              (time: 11, lat: 32400, lon: 64800, bounds: 2)\nCoordinates:\n  * lat                  (lat) float64 259kB 90.0 89.99 89.98 ... -89.99 -90.0\n  * lon                  (lon) float64 518kB -180.0 -180.0 ... 180.0 180.0\n  * time                 (time) datetime64[ns] 88B 2010-01-01 ... 2020-01-01\nDimensions without coordinates: bounds\nData variables:\n    change_count         (time, lat, lon) uint8 23GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    crs                  (time) int32 44B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n    current_pixel_state  (time, lat, lon) float32 92GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    lat_bounds           (time, lat, bounds) float64 6MB dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;\n    lccs_class           (time, lat, lon) uint8 23GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    lon_bounds           (time, lon, bounds) float64 11MB dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;\n    observation_count    (time, lat, lon) uint16 46GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    processed_flag       (time, lat, lon) float32 92GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    time_bounds          (time, bounds) datetime64[ns] 176B dask.array&lt;chunksize=(11, 2), meta=np.ndarray&gt;\nAttributes: (12/38)\n    Conventions:                CF-1.6\n    TileSize:                   2025:2025\n    cdm_data_type:              grid\n    comment:                    \n    contact:                    https://www.ecmwf.int/en/about/contact-us/get...\n    creation_date:              20181130T095431Z\n    ...                         ...\n    time_coverage_end:          20101231\n    time_coverage_resolution:   P1Y\n    time_coverage_start:        20100101\n    title:                      Land Cover Map of ESA CCI brokered by CDS\n    tracking_id:                96ac9aca-1ca7-45c6-b4a5-ab448c692646\n    type:                       ESACCI-LC-L4-LCCS-Map-300m-P1Y</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 11</li><li>lat: 32400</li><li>lon: 64800</li><li>bounds: 2</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6490.0 89.99 89.98 ... -89.99 -90.0axis :Ybounds :lat_boundslong_name :latitudestandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([ 89.995833,  89.990278,  89.984722, ..., -89.9875  , -89.993056,\n       -89.998611])</pre></li><li>lon(lon)float64-180.0 -180.0 ... 180.0 180.0axis :Xbounds :lon_boundslong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-179.998611, -179.993056, -179.9875  , ...,  179.984722,  179.990278,\n        179.995833])</pre></li><li>time(time)datetime64[ns]2010-01-01 ... 2020-01-01axis :Tbounds :time_boundslong_name :timestandard_name :time<pre>array(['2010-01-01T00:00:00.000000000', '2011-01-01T00:00:00.000000000',\n       '2012-01-01T00:00:00.000000000', '2013-01-01T00:00:00.000000000',\n       '2014-01-01T00:00:00.000000000', '2015-01-01T00:00:00.000000000',\n       '2016-01-01T00:00:00.000000000', '2017-01-01T00:00:00.000000000',\n       '2018-01-01T00:00:00.000000000', '2019-01-01T00:00:00.000000000',\n       '2020-01-01T00:00:00.000000000'], dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (9)<ul><li>change_count(time, lat, lon)uint8dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;long_name :number of class changesvalid_max :100valid_min :0  Array   Chunk   Bytes   21.51 GiB   4.45 MiB   Shape   (11, 32400, 64800)   (1, 2160, 2160)   Dask graph   4950 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  64800 32400 11 </li><li>crs(time)int32dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;i2m :0.002777777777778,0.0,0.0,-0.002777777777778,-180.0,90.0wkt :GEOGCS[\"WGS 84\",    DATUM[\"World Geodetic System 1984\",      SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]],      AUTHORITY[\"EPSG\",\"6326\"]],    PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]],    UNIT[\"degree\", 0.017453292519943295],    AXIS[\"Geodetic longitude\", EAST],    AXIS[\"Geodetic latitude\", NORTH],    AUTHORITY[\"EPSG\",\"4326\"]]  Array   Chunk   Bytes   44 B   4 B   Shape   (11,)   (1,)   Dask graph   11 chunks in 2 graph layers   Data type   int32 numpy.ndarray  11 1 </li><li>current_pixel_state(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;flag_meanings :invalid clear_land clear_water clear_snow_ice cloud cloud_shadowflag_values :[0, 1, 2, 3, 4, 5]long_name :LC pixel type maskstandard_name :land_cover_lccs status_flagvalid_max :5valid_min :0  Array   Chunk   Bytes   86.03 GiB   17.80 MiB   Shape   (11, 32400, 64800)   (1, 2160, 2160)   Dask graph   4950 chunks in 2 graph layers   Data type   float32 numpy.ndarray  64800 32400 11 </li><li>lat_bounds(time, lat, bounds)float64dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   5.44 MiB   33.75 kiB   Shape   (11, 32400, 2)   (1, 2160, 2)   Dask graph   165 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 32400 11 </li><li>lccs_class(time, lat, lon)uint8dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;ancillary_variables :processed_flag current_pixel_state observation_count change_countflag_colors :#ffff64 #ffff64 #ffff00 #aaf0f0 #dcf064 #c8c864 #006400 #00a000 #00a000 #aac800 #003c00 #003c00 #005000 #285000 #285000 #286400 #788200 #8ca000 #be9600 #966400 #966400 #966400 #ffb432 #ffdcd2 #ffebaf #ffc864 #ffd278 #ffebaf #00785a #009678 #00dc82 #c31400 #fff5d7 #dcdcdc #fff5d7 #0046c8 #ffffffflag_meanings :no_data cropland_rainfed cropland_rainfed_herbaceous_cover cropland_rainfed_tree_or_shrub_cover cropland_irrigated mosaic_cropland mosaic_natural_vegetation tree_broadleaved_evergreen_closed_to_open tree_broadleaved_deciduous_closed_to_open tree_broadleaved_deciduous_closed tree_broadleaved_deciduous_open tree_needleleaved_evergreen_closed_to_open tree_needleleaved_evergreen_closed tree_needleleaved_evergreen_open tree_needleleaved_deciduous_closed_to_open tree_needleleaved_deciduous_closed tree_needleleaved_deciduous_open tree_mixed mosaic_tree_and_shrub mosaic_herbaceous shrubland shrubland_evergreen shrubland_deciduous grassland lichens_and_mosses sparse_vegetation sparse_tree sparse_shrub sparse_herbaceous tree_cover_flooded_fresh_or_brakish_water tree_cover_flooded_saline_water shrub_or_herbaceous_cover_flooded urban bare_areas bare_areas_consolidated bare_areas_unconsolidated water snow_and_iceflag_values :[0, 10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100, 110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180, 190, 200, 201, 202, 210, 220]long_name :Land cover class defined in LCCSstandard_name :land_cover_lccsvalid_max :220valid_min :1  Array   Chunk   Bytes   21.51 GiB   4.45 MiB   Shape   (11, 32400, 64800)   (1, 2160, 2160)   Dask graph   4950 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  64800 32400 11 </li><li>lon_bounds(time, lon, bounds)float64dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   10.88 MiB   33.75 kiB   Shape   (11, 64800, 2)   (1, 2160, 2)   Dask graph   330 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 64800 11 </li><li>observation_count(time, lat, lon)uint16dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;long_name :number of valid observationsstandard_name :land_cover_lccs number_of_observationsvalid_max :32767valid_min :0  Array   Chunk   Bytes   43.02 GiB   8.90 MiB   Shape   (11, 32400, 64800)   (1, 2160, 2160)   Dask graph   4950 chunks in 2 graph layers   Data type   uint16 numpy.ndarray  64800 32400 11 </li><li>processed_flag(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;flag_meanings :not_processed processedflag_values :[0, 1]long_name :LC map processed area flagstandard_name :land_cover_lccs status_flagvalid_max :1valid_min :0  Array   Chunk   Bytes   86.03 GiB   17.80 MiB   Shape   (11, 32400, 64800)   (1, 2160, 2160)   Dask graph   4950 chunks in 2 graph layers   Data type   float32 numpy.ndarray  64800 32400 11 </li><li>time_bounds(time, bounds)datetime64[ns]dask.array&lt;chunksize=(11, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   176 B   176 B   Shape   (11, 2)   (11, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 11 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.99583333333334,  89.99027777777778,  89.98472222222222,\n        89.97916666666669,  89.97361111111113,  89.96805555555557,\n                  89.9625,  89.95694444444445,  89.95138888888889,\n        89.94583333333335,\n       ...\n        -89.9486111111111, -89.95416666666667, -89.95972222222223,\n       -89.96527777777777, -89.97083333333333, -89.97638888888889,\n       -89.98194444444445,           -89.9875, -89.99305555555556,\n       -89.99861111111112],\n      dtype='float64', name='lat', length=32400))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([ -179.9986111111111, -179.99305555555554,           -179.9875,\n       -179.98194444444445,  -179.9763888888889, -179.97083333333333,\n       -179.96527777777777,  -179.9597222222222, -179.95416666666668,\n       -179.94861111111112,\n       ...\n        179.94583333333333,   179.9513888888889,  179.95694444444445,\n        179.96250000000003,  179.96805555555557,   179.9736111111111,\n        179.97916666666669,  179.98472222222222,   179.9902777777778,\n        179.99583333333334],\n      dtype='float64', name='lon', length=64800))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2010-01-01', '2011-01-01', '2012-01-01', '2013-01-01',\n               '2014-01-01', '2015-01-01', '2016-01-01', '2017-01-01',\n               '2018-01-01', '2019-01-01', '2020-01-01'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (38)Conventions :CF-1.6TileSize :2025:2025cdm_data_type :gridcomment :contact :https://www.ecmwf.int/en/about/contact-us/get-supportcreation_date :20181130T095431Zcreator_email :landcover-cci@uclouvain.becreator_name :UCLouvaincreator_url :http://www.uclouvain.be/geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.002778geospatial_lat_units :degrees_northgeospatial_lon_max :180geospatial_lon_min :-180geospatial_lon_resolution :0.002778geospatial_lon_units :degrees_easthistory :amorgos-4,0, lc-sdr-1.0, lc-sr-1.0, lc-classification-1.0,lc-user-tools-3.13,lc-user-tools-4.3id :ESACCI-LC-L4-LCCS-Map-300m-P1Y-2010-v2.0.7cdsinstitution :UCLouvainkeywords :land cover classification,satellite,observationkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :ESA CCI Data Policy: free and open accessnaming_authority :org.esa-cciproduct_version :2.0.7cdsproject :Climate Change Initiative - European Space Agencyreferences :http://www.esa-landcover-cci.org/source :MERIS FR L1B version 5.05, MERIS RR L1B version 8.0, SPOT VGT Pspatial_resolution :300mstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Standard Names version 21summary :This dataset characterizes the land cover of a particular year (see time_coverage). The land cover was derived from the analysis of satellite data time series of the full period.time_coverage_duration :P1Ytime_coverage_end :20101231time_coverage_resolution :P1Ytime_coverage_start :20100101title :Land Cover Map of ESA CCI brokered by CDStracking_id :96ac9aca-1ca7-45c6-b4a5-ab448c692646type :ESACCI-LC-L4-LCCS-Map-300m-P1Y</li></ul> <pre>&lt;xarray.Dataset&gt; Size: 69GB\nDimensions:              (time: 11, lat: 16200, lon: 32400, bounds: 2)\nCoordinates:\n  * lat                  (lat) float64 130kB 89.99 89.98 89.97 ... -89.99 -90.0\n  * lon                  (lon) float64 259kB -180.0 -180.0 ... 180.0 180.0\n  * time                 (time) datetime64[ns] 88B 2010-01-01 ... 2020-01-01\nDimensions without coordinates: bounds\nData variables:\n    change_count         (time, lat, lon) uint8 6GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    crs                  (time) int32 44B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n    current_pixel_state  (time, lat, lon) float32 23GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    lat_bounds           (time, lat, bounds) float64 3MB dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;\n    lccs_class           (time, lat, lon) uint8 6GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    lon_bounds           (time, lon, bounds) float64 6MB dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;\n    observation_count    (time, lat, lon) uint16 12GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    processed_flag       (time, lat, lon) float32 23GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    time_bounds          (time, bounds) datetime64[ns] 176B dask.array&lt;chunksize=(11, 2), meta=np.ndarray&gt;\nAttributes: (12/38)\n    Conventions:                CF-1.6\n    TileSize:                   2025:2025\n    cdm_data_type:              grid\n    comment:                    \n    contact:                    https://www.ecmwf.int/en/about/contact-us/get...\n    creation_date:              20181130T095431Z\n    ...                         ...\n    time_coverage_end:          20101231\n    time_coverage_resolution:   P1Y\n    time_coverage_start:        20100101\n    title:                      Land Cover Map of ESA CCI brokered by CDS\n    tracking_id:                96ac9aca-1ca7-45c6-b4a5-ab448c692646\n    type:                       ESACCI-LC-L4-LCCS-Map-300m-P1Y</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 11</li><li>lat: 16200</li><li>lon: 32400</li><li>bounds: 2</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6489.99 89.98 89.97 ... -89.99 -90.0axis :Ybounds :lat_boundslong_name :latitudestandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([ 89.990278,  89.979167,  89.968056, ..., -89.976389, -89.9875  ,\n       -89.998611])</pre></li><li>lon(lon)float64-180.0 -180.0 ... 180.0 180.0axis :Xbounds :lon_boundslong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-179.998611, -179.9875  , -179.976389, ...,  179.968056,  179.979167,\n        179.990278])</pre></li><li>time(time)datetime64[ns]2010-01-01 ... 2020-01-01axis :Tbounds :time_boundslong_name :timestandard_name :time<pre>array(['2010-01-01T00:00:00.000000000', '2011-01-01T00:00:00.000000000',\n       '2012-01-01T00:00:00.000000000', '2013-01-01T00:00:00.000000000',\n       '2014-01-01T00:00:00.000000000', '2015-01-01T00:00:00.000000000',\n       '2016-01-01T00:00:00.000000000', '2017-01-01T00:00:00.000000000',\n       '2018-01-01T00:00:00.000000000', '2019-01-01T00:00:00.000000000',\n       '2020-01-01T00:00:00.000000000'], dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (9)<ul><li>change_count(time, lat, lon)uint8dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;long_name :number of class changesvalid_max :100valid_min :0  Array   Chunk   Bytes   5.38 GiB   4.45 MiB   Shape   (11, 16200, 32400)   (1, 2160, 2160)   Dask graph   1320 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  32400 16200 11 </li><li>crs(time)int32dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;i2m :0.002777777777778,0.0,0.0,-0.002777777777778,-180.0,90.0wkt :GEOGCS[\"WGS 84\",    DATUM[\"World Geodetic System 1984\",      SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]],      AUTHORITY[\"EPSG\",\"6326\"]],    PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]],    UNIT[\"degree\", 0.017453292519943295],    AXIS[\"Geodetic longitude\", EAST],    AXIS[\"Geodetic latitude\", NORTH],    AUTHORITY[\"EPSG\",\"4326\"]]  Array   Chunk   Bytes   44 B   4 B   Shape   (11,)   (1,)   Dask graph   11 chunks in 2 graph layers   Data type   int32 numpy.ndarray  11 1 </li><li>current_pixel_state(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;flag_meanings :invalid clear_land clear_water clear_snow_ice cloud cloud_shadowflag_values :[0, 1, 2, 3, 4, 5]long_name :LC pixel type maskstandard_name :land_cover_lccs status_flagvalid_max :5valid_min :0  Array   Chunk   Bytes   21.51 GiB   17.80 MiB   Shape   (11, 16200, 32400)   (1, 2160, 2160)   Dask graph   1320 chunks in 2 graph layers   Data type   float32 numpy.ndarray  32400 16200 11 </li><li>lat_bounds(time, lat, bounds)float64dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   2.72 MiB   33.75 kiB   Shape   (11, 16200, 2)   (1, 2160, 2)   Dask graph   88 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 16200 11 </li><li>lccs_class(time, lat, lon)uint8dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;ancillary_variables :processed_flag current_pixel_state observation_count change_countflag_colors :#ffff64 #ffff64 #ffff00 #aaf0f0 #dcf064 #c8c864 #006400 #00a000 #00a000 #aac800 #003c00 #003c00 #005000 #285000 #285000 #286400 #788200 #8ca000 #be9600 #966400 #966400 #966400 #ffb432 #ffdcd2 #ffebaf #ffc864 #ffd278 #ffebaf #00785a #009678 #00dc82 #c31400 #fff5d7 #dcdcdc #fff5d7 #0046c8 #ffffffflag_meanings :no_data cropland_rainfed cropland_rainfed_herbaceous_cover cropland_rainfed_tree_or_shrub_cover cropland_irrigated mosaic_cropland mosaic_natural_vegetation tree_broadleaved_evergreen_closed_to_open tree_broadleaved_deciduous_closed_to_open tree_broadleaved_deciduous_closed tree_broadleaved_deciduous_open tree_needleleaved_evergreen_closed_to_open tree_needleleaved_evergreen_closed tree_needleleaved_evergreen_open tree_needleleaved_deciduous_closed_to_open tree_needleleaved_deciduous_closed tree_needleleaved_deciduous_open tree_mixed mosaic_tree_and_shrub mosaic_herbaceous shrubland shrubland_evergreen shrubland_deciduous grassland lichens_and_mosses sparse_vegetation sparse_tree sparse_shrub sparse_herbaceous tree_cover_flooded_fresh_or_brakish_water tree_cover_flooded_saline_water shrub_or_herbaceous_cover_flooded urban bare_areas bare_areas_consolidated bare_areas_unconsolidated water snow_and_iceflag_values :[0, 10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100, 110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180, 190, 200, 201, 202, 210, 220]long_name :Land cover class defined in LCCSstandard_name :land_cover_lccsvalid_max :220valid_min :1  Array   Chunk   Bytes   5.38 GiB   4.45 MiB   Shape   (11, 16200, 32400)   (1, 2160, 2160)   Dask graph   1320 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  32400 16200 11 </li><li>lon_bounds(time, lon, bounds)float64dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   5.44 MiB   33.75 kiB   Shape   (11, 32400, 2)   (1, 2160, 2)   Dask graph   165 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 32400 11 </li><li>observation_count(time, lat, lon)uint16dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;long_name :number of valid observationsstandard_name :land_cover_lccs number_of_observationsvalid_max :32767valid_min :0  Array   Chunk   Bytes   10.75 GiB   8.90 MiB   Shape   (11, 16200, 32400)   (1, 2160, 2160)   Dask graph   1320 chunks in 2 graph layers   Data type   uint16 numpy.ndarray  32400 16200 11 </li><li>processed_flag(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;flag_meanings :not_processed processedflag_values :[0, 1]long_name :LC map processed area flagstandard_name :land_cover_lccs status_flagvalid_max :1valid_min :0  Array   Chunk   Bytes   21.51 GiB   17.80 MiB   Shape   (11, 16200, 32400)   (1, 2160, 2160)   Dask graph   1320 chunks in 2 graph layers   Data type   float32 numpy.ndarray  32400 16200 11 </li><li>time_bounds(time, bounds)datetime64[ns]dask.array&lt;chunksize=(11, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   176 B   176 B   Shape   (11, 2)   (11, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 11 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.99027777777778,  89.97916666666669,  89.96805555555557,\n        89.95694444444445,  89.94583333333335,  89.93472222222223,\n        89.92361111111111,            89.9125,   89.9013888888889,\n        89.89027777777778,\n       ...\n       -89.89861111111111, -89.90972222222223, -89.92083333333333,\n       -89.93194444444444, -89.94305555555556, -89.95416666666667,\n       -89.96527777777777, -89.97638888888889,           -89.9875,\n       -89.99861111111112],\n      dtype='float64', name='lat', length=16200))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([ -179.9986111111111,           -179.9875,  -179.9763888888889,\n       -179.96527777777777, -179.95416666666668, -179.94305555555556,\n       -179.93194444444444, -179.92083333333332, -179.90972222222223,\n        -179.8986111111111,\n       ...\n        179.89027777777778,   179.9013888888889,  179.91250000000002,\n        179.92361111111114,  179.93472222222226,  179.94583333333333,\n        179.95694444444445,  179.96805555555557,  179.97916666666669,\n         179.9902777777778],\n      dtype='float64', name='lon', length=32400))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2010-01-01', '2011-01-01', '2012-01-01', '2013-01-01',\n               '2014-01-01', '2015-01-01', '2016-01-01', '2017-01-01',\n               '2018-01-01', '2019-01-01', '2020-01-01'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (38)Conventions :CF-1.6TileSize :2025:2025cdm_data_type :gridcomment :contact :https://www.ecmwf.int/en/about/contact-us/get-supportcreation_date :20181130T095431Zcreator_email :landcover-cci@uclouvain.becreator_name :UCLouvaincreator_url :http://www.uclouvain.be/geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.002778geospatial_lat_units :degrees_northgeospatial_lon_max :180geospatial_lon_min :-180geospatial_lon_resolution :0.002778geospatial_lon_units :degrees_easthistory :amorgos-4,0, lc-sdr-1.0, lc-sr-1.0, lc-classification-1.0,lc-user-tools-3.13,lc-user-tools-4.3id :ESACCI-LC-L4-LCCS-Map-300m-P1Y-2010-v2.0.7cdsinstitution :UCLouvainkeywords :land cover classification,satellite,observationkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :ESA CCI Data Policy: free and open accessnaming_authority :org.esa-cciproduct_version :2.0.7cdsproject :Climate Change Initiative - European Space Agencyreferences :http://www.esa-landcover-cci.org/source :MERIS FR L1B version 5.05, MERIS RR L1B version 8.0, SPOT VGT Pspatial_resolution :300mstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Standard Names version 21summary :This dataset characterizes the land cover of a particular year (see time_coverage). The land cover was derived from the analysis of satellite data time series of the full period.time_coverage_duration :P1Ytime_coverage_end :20101231time_coverage_resolution :P1Ytime_coverage_start :20100101title :Land Cover Map of ESA CCI brokered by CDStracking_id :96ac9aca-1ca7-45c6-b4a5-ab448c692646type :ESACCI-LC-L4-LCCS-Map-300m-P1Y</li></ul> <pre>&lt;xarray.Dataset&gt; Size: 17GB\nDimensions:              (time: 11, lat: 8100, lon: 16200, bounds: 2)\nCoordinates:\n  * lat                  (lat) float64 65kB 89.98 89.96 89.93 ... -89.98 -90.0\n  * lon                  (lon) float64 130kB -180.0 -180.0 ... 180.0 180.0\n  * time                 (time) datetime64[ns] 88B 2010-01-01 ... 2020-01-01\nDimensions without coordinates: bounds\nData variables:\n    change_count         (time, lat, lon) uint8 1GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    crs                  (time) int32 44B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n    current_pixel_state  (time, lat, lon) float32 6GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    lat_bounds           (time, lat, bounds) float64 1MB dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;\n    lccs_class           (time, lat, lon) uint8 1GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    lon_bounds           (time, lon, bounds) float64 3MB dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;\n    observation_count    (time, lat, lon) uint16 3GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    processed_flag       (time, lat, lon) float32 6GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    time_bounds          (time, bounds) datetime64[ns] 176B dask.array&lt;chunksize=(11, 2), meta=np.ndarray&gt;\nAttributes: (12/38)\n    Conventions:                CF-1.6\n    TileSize:                   2025:2025\n    cdm_data_type:              grid\n    comment:                    \n    contact:                    https://www.ecmwf.int/en/about/contact-us/get...\n    creation_date:              20181130T095431Z\n    ...                         ...\n    time_coverage_end:          20101231\n    time_coverage_resolution:   P1Y\n    time_coverage_start:        20100101\n    title:                      Land Cover Map of ESA CCI brokered by CDS\n    tracking_id:                96ac9aca-1ca7-45c6-b4a5-ab448c692646\n    type:                       ESACCI-LC-L4-LCCS-Map-300m-P1Y</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 11</li><li>lat: 8100</li><li>lon: 16200</li><li>bounds: 2</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6489.98 89.96 89.93 ... -89.98 -90.0axis :Ybounds :lat_boundslong_name :latitudestandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([ 89.979167,  89.956944,  89.934722, ..., -89.954167, -89.976389,\n       -89.998611])</pre></li><li>lon(lon)float64-180.0 -180.0 ... 180.0 180.0axis :Xbounds :lon_boundslong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-179.998611, -179.976389, -179.954167, ...,  179.934722,  179.956944,\n        179.979167])</pre></li><li>time(time)datetime64[ns]2010-01-01 ... 2020-01-01axis :Tbounds :time_boundslong_name :timestandard_name :time<pre>array(['2010-01-01T00:00:00.000000000', '2011-01-01T00:00:00.000000000',\n       '2012-01-01T00:00:00.000000000', '2013-01-01T00:00:00.000000000',\n       '2014-01-01T00:00:00.000000000', '2015-01-01T00:00:00.000000000',\n       '2016-01-01T00:00:00.000000000', '2017-01-01T00:00:00.000000000',\n       '2018-01-01T00:00:00.000000000', '2019-01-01T00:00:00.000000000',\n       '2020-01-01T00:00:00.000000000'], dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (9)<ul><li>change_count(time, lat, lon)uint8dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;long_name :number of class changesvalid_max :100valid_min :0  Array   Chunk   Bytes   1.34 GiB   4.45 MiB   Shape   (11, 8100, 16200)   (1, 2160, 2160)   Dask graph   352 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  16200 8100 11 </li><li>crs(time)int32dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;i2m :0.002777777777778,0.0,0.0,-0.002777777777778,-180.0,90.0wkt :GEOGCS[\"WGS 84\",    DATUM[\"World Geodetic System 1984\",      SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]],      AUTHORITY[\"EPSG\",\"6326\"]],    PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]],    UNIT[\"degree\", 0.017453292519943295],    AXIS[\"Geodetic longitude\", EAST],    AXIS[\"Geodetic latitude\", NORTH],    AUTHORITY[\"EPSG\",\"4326\"]]  Array   Chunk   Bytes   44 B   4 B   Shape   (11,)   (1,)   Dask graph   11 chunks in 2 graph layers   Data type   int32 numpy.ndarray  11 1 </li><li>current_pixel_state(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;flag_meanings :invalid clear_land clear_water clear_snow_ice cloud cloud_shadowflag_values :[0, 1, 2, 3, 4, 5]long_name :LC pixel type maskstandard_name :land_cover_lccs status_flagvalid_max :5valid_min :0  Array   Chunk   Bytes   5.38 GiB   17.80 MiB   Shape   (11, 8100, 16200)   (1, 2160, 2160)   Dask graph   352 chunks in 2 graph layers   Data type   float32 numpy.ndarray  16200 8100 11 </li><li>lat_bounds(time, lat, bounds)float64dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   1.36 MiB   33.75 kiB   Shape   (11, 8100, 2)   (1, 2160, 2)   Dask graph   44 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 8100 11 </li><li>lccs_class(time, lat, lon)uint8dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;ancillary_variables :processed_flag current_pixel_state observation_count change_countflag_colors :#ffff64 #ffff64 #ffff00 #aaf0f0 #dcf064 #c8c864 #006400 #00a000 #00a000 #aac800 #003c00 #003c00 #005000 #285000 #285000 #286400 #788200 #8ca000 #be9600 #966400 #966400 #966400 #ffb432 #ffdcd2 #ffebaf #ffc864 #ffd278 #ffebaf #00785a #009678 #00dc82 #c31400 #fff5d7 #dcdcdc #fff5d7 #0046c8 #ffffffflag_meanings :no_data cropland_rainfed cropland_rainfed_herbaceous_cover cropland_rainfed_tree_or_shrub_cover cropland_irrigated mosaic_cropland mosaic_natural_vegetation tree_broadleaved_evergreen_closed_to_open tree_broadleaved_deciduous_closed_to_open tree_broadleaved_deciduous_closed tree_broadleaved_deciduous_open tree_needleleaved_evergreen_closed_to_open tree_needleleaved_evergreen_closed tree_needleleaved_evergreen_open tree_needleleaved_deciduous_closed_to_open tree_needleleaved_deciduous_closed tree_needleleaved_deciduous_open tree_mixed mosaic_tree_and_shrub mosaic_herbaceous shrubland shrubland_evergreen shrubland_deciduous grassland lichens_and_mosses sparse_vegetation sparse_tree sparse_shrub sparse_herbaceous tree_cover_flooded_fresh_or_brakish_water tree_cover_flooded_saline_water shrub_or_herbaceous_cover_flooded urban bare_areas bare_areas_consolidated bare_areas_unconsolidated water snow_and_iceflag_values :[0, 10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100, 110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180, 190, 200, 201, 202, 210, 220]long_name :Land cover class defined in LCCSstandard_name :land_cover_lccsvalid_max :220valid_min :1  Array   Chunk   Bytes   1.34 GiB   4.45 MiB   Shape   (11, 8100, 16200)   (1, 2160, 2160)   Dask graph   352 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  16200 8100 11 </li><li>lon_bounds(time, lon, bounds)float64dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   2.72 MiB   33.75 kiB   Shape   (11, 16200, 2)   (1, 2160, 2)   Dask graph   88 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 16200 11 </li><li>observation_count(time, lat, lon)uint16dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;long_name :number of valid observationsstandard_name :land_cover_lccs number_of_observationsvalid_max :32767valid_min :0  Array   Chunk   Bytes   2.69 GiB   8.90 MiB   Shape   (11, 8100, 16200)   (1, 2160, 2160)   Dask graph   352 chunks in 2 graph layers   Data type   uint16 numpy.ndarray  16200 8100 11 </li><li>processed_flag(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;flag_meanings :not_processed processedflag_values :[0, 1]long_name :LC map processed area flagstandard_name :land_cover_lccs status_flagvalid_max :1valid_min :0  Array   Chunk   Bytes   5.38 GiB   17.80 MiB   Shape   (11, 8100, 16200)   (1, 2160, 2160)   Dask graph   352 chunks in 2 graph layers   Data type   float32 numpy.ndarray  16200 8100 11 </li><li>time_bounds(time, bounds)datetime64[ns]dask.array&lt;chunksize=(11, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   176 B   176 B   Shape   (11, 2)   (11, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 11 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.97916666666669,  89.95694444444445,  89.93472222222223,\n                  89.9125,  89.89027777777778,  89.86805555555557,\n        89.84583333333333,  89.82361111111112,  89.80138888888891,\n        89.77916666666667,\n       ...\n       -89.79861111111111, -89.82083333333334, -89.84305555555555,\n       -89.86527777777778,           -89.8875, -89.90972222222223,\n       -89.93194444444444, -89.95416666666667, -89.97638888888889,\n       -89.99861111111112],\n      dtype='float64', name='lat', length=8100))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([ -179.9986111111111,  -179.9763888888889, -179.95416666666668,\n       -179.93194444444444, -179.90972222222223,           -179.8875,\n       -179.86527777777778, -179.84305555555557, -179.82083333333333,\n       -179.79861111111111,\n       ...\n         179.7791666666667,  179.80138888888888,  179.82361111111112,\n        179.84583333333336,  179.86805555555554,  179.89027777777778,\n        179.91250000000002,  179.93472222222226,  179.95694444444445,\n        179.97916666666669],\n      dtype='float64', name='lon', length=16200))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2010-01-01', '2011-01-01', '2012-01-01', '2013-01-01',\n               '2014-01-01', '2015-01-01', '2016-01-01', '2017-01-01',\n               '2018-01-01', '2019-01-01', '2020-01-01'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (38)Conventions :CF-1.6TileSize :2025:2025cdm_data_type :gridcomment :contact :https://www.ecmwf.int/en/about/contact-us/get-supportcreation_date :20181130T095431Zcreator_email :landcover-cci@uclouvain.becreator_name :UCLouvaincreator_url :http://www.uclouvain.be/geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.002778geospatial_lat_units :degrees_northgeospatial_lon_max :180geospatial_lon_min :-180geospatial_lon_resolution :0.002778geospatial_lon_units :degrees_easthistory :amorgos-4,0, lc-sdr-1.0, lc-sr-1.0, lc-classification-1.0,lc-user-tools-3.13,lc-user-tools-4.3id :ESACCI-LC-L4-LCCS-Map-300m-P1Y-2010-v2.0.7cdsinstitution :UCLouvainkeywords :land cover classification,satellite,observationkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :ESA CCI Data Policy: free and open accessnaming_authority :org.esa-cciproduct_version :2.0.7cdsproject :Climate Change Initiative - European Space Agencyreferences :http://www.esa-landcover-cci.org/source :MERIS FR L1B version 5.05, MERIS RR L1B version 8.0, SPOT VGT Pspatial_resolution :300mstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Standard Names version 21summary :This dataset characterizes the land cover of a particular year (see time_coverage). The land cover was derived from the analysis of satellite data time series of the full period.time_coverage_duration :P1Ytime_coverage_end :20101231time_coverage_resolution :P1Ytime_coverage_start :20100101title :Land Cover Map of ESA CCI brokered by CDStracking_id :96ac9aca-1ca7-45c6-b4a5-ab448c692646type :ESACCI-LC-L4-LCCS-Map-300m-P1Y</li></ul> <pre>&lt;xarray.Dataset&gt; Size: 4GB\nDimensions:              (time: 11, lat: 4050, lon: 8100, bounds: 2)\nCoordinates:\n  * lat                  (lat) float64 32kB 89.96 89.91 89.87 ... -89.95 -90.0\n  * lon                  (lon) float64 65kB -180.0 -180.0 -179.9 ... 179.9 180.0\n  * time                 (time) datetime64[ns] 88B 2010-01-01 ... 2020-01-01\nDimensions without coordinates: bounds\nData variables:\n    change_count         (time, lat, lon) uint8 361MB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    crs                  (time) int32 44B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n    current_pixel_state  (time, lat, lon) float32 1GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    lat_bounds           (time, lat, bounds) float64 713kB dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;\n    lccs_class           (time, lat, lon) uint8 361MB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    lon_bounds           (time, lon, bounds) float64 1MB dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;\n    observation_count    (time, lat, lon) uint16 722MB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    processed_flag       (time, lat, lon) float32 1GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    time_bounds          (time, bounds) datetime64[ns] 176B dask.array&lt;chunksize=(11, 2), meta=np.ndarray&gt;\nAttributes: (12/38)\n    Conventions:                CF-1.6\n    TileSize:                   2025:2025\n    cdm_data_type:              grid\n    comment:                    \n    contact:                    https://www.ecmwf.int/en/about/contact-us/get...\n    creation_date:              20181130T095431Z\n    ...                         ...\n    time_coverage_end:          20101231\n    time_coverage_resolution:   P1Y\n    time_coverage_start:        20100101\n    title:                      Land Cover Map of ESA CCI brokered by CDS\n    tracking_id:                96ac9aca-1ca7-45c6-b4a5-ab448c692646\n    type:                       ESACCI-LC-L4-LCCS-Map-300m-P1Y</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 11</li><li>lat: 4050</li><li>lon: 8100</li><li>bounds: 2</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6489.96 89.91 89.87 ... -89.95 -90.0axis :Ybounds :lat_boundslong_name :latitudestandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([ 89.956944,  89.9125  ,  89.868056, ..., -89.909722, -89.954167,\n       -89.998611])</pre></li><li>lon(lon)float64-180.0 -180.0 ... 179.9 180.0axis :Xbounds :lon_boundslong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-179.998611, -179.954167, -179.909722, ...,  179.868056,  179.9125  ,\n        179.956944])</pre></li><li>time(time)datetime64[ns]2010-01-01 ... 2020-01-01axis :Tbounds :time_boundslong_name :timestandard_name :time<pre>array(['2010-01-01T00:00:00.000000000', '2011-01-01T00:00:00.000000000',\n       '2012-01-01T00:00:00.000000000', '2013-01-01T00:00:00.000000000',\n       '2014-01-01T00:00:00.000000000', '2015-01-01T00:00:00.000000000',\n       '2016-01-01T00:00:00.000000000', '2017-01-01T00:00:00.000000000',\n       '2018-01-01T00:00:00.000000000', '2019-01-01T00:00:00.000000000',\n       '2020-01-01T00:00:00.000000000'], dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (9)<ul><li>change_count(time, lat, lon)uint8dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;long_name :number of class changesvalid_max :100valid_min :0  Array   Chunk   Bytes   344.14 MiB   4.45 MiB   Shape   (11, 4050, 8100)   (1, 2160, 2160)   Dask graph   88 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  8100 4050 11 </li><li>crs(time)int32dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;i2m :0.002777777777778,0.0,0.0,-0.002777777777778,-180.0,90.0wkt :GEOGCS[\"WGS 84\",    DATUM[\"World Geodetic System 1984\",      SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]],      AUTHORITY[\"EPSG\",\"6326\"]],    PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]],    UNIT[\"degree\", 0.017453292519943295],    AXIS[\"Geodetic longitude\", EAST],    AXIS[\"Geodetic latitude\", NORTH],    AUTHORITY[\"EPSG\",\"4326\"]]  Array   Chunk   Bytes   44 B   4 B   Shape   (11,)   (1,)   Dask graph   11 chunks in 2 graph layers   Data type   int32 numpy.ndarray  11 1 </li><li>current_pixel_state(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;flag_meanings :invalid clear_land clear_water clear_snow_ice cloud cloud_shadowflag_values :[0, 1, 2, 3, 4, 5]long_name :LC pixel type maskstandard_name :land_cover_lccs status_flagvalid_max :5valid_min :0  Array   Chunk   Bytes   1.34 GiB   17.80 MiB   Shape   (11, 4050, 8100)   (1, 2160, 2160)   Dask graph   88 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8100 4050 11 </li><li>lat_bounds(time, lat, bounds)float64dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   696.09 kiB   33.75 kiB   Shape   (11, 4050, 2)   (1, 2160, 2)   Dask graph   22 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 4050 11 </li><li>lccs_class(time, lat, lon)uint8dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;ancillary_variables :processed_flag current_pixel_state observation_count change_countflag_colors :#ffff64 #ffff64 #ffff00 #aaf0f0 #dcf064 #c8c864 #006400 #00a000 #00a000 #aac800 #003c00 #003c00 #005000 #285000 #285000 #286400 #788200 #8ca000 #be9600 #966400 #966400 #966400 #ffb432 #ffdcd2 #ffebaf #ffc864 #ffd278 #ffebaf #00785a #009678 #00dc82 #c31400 #fff5d7 #dcdcdc #fff5d7 #0046c8 #ffffffflag_meanings :no_data cropland_rainfed cropland_rainfed_herbaceous_cover cropland_rainfed_tree_or_shrub_cover cropland_irrigated mosaic_cropland mosaic_natural_vegetation tree_broadleaved_evergreen_closed_to_open tree_broadleaved_deciduous_closed_to_open tree_broadleaved_deciduous_closed tree_broadleaved_deciduous_open tree_needleleaved_evergreen_closed_to_open tree_needleleaved_evergreen_closed tree_needleleaved_evergreen_open tree_needleleaved_deciduous_closed_to_open tree_needleleaved_deciduous_closed tree_needleleaved_deciduous_open tree_mixed mosaic_tree_and_shrub mosaic_herbaceous shrubland shrubland_evergreen shrubland_deciduous grassland lichens_and_mosses sparse_vegetation sparse_tree sparse_shrub sparse_herbaceous tree_cover_flooded_fresh_or_brakish_water tree_cover_flooded_saline_water shrub_or_herbaceous_cover_flooded urban bare_areas bare_areas_consolidated bare_areas_unconsolidated water snow_and_iceflag_values :[0, 10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100, 110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180, 190, 200, 201, 202, 210, 220]long_name :Land cover class defined in LCCSstandard_name :land_cover_lccsvalid_max :220valid_min :1  Array   Chunk   Bytes   344.14 MiB   4.45 MiB   Shape   (11, 4050, 8100)   (1, 2160, 2160)   Dask graph   88 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  8100 4050 11 </li><li>lon_bounds(time, lon, bounds)float64dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   1.36 MiB   33.75 kiB   Shape   (11, 8100, 2)   (1, 2160, 2)   Dask graph   44 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 8100 11 </li><li>observation_count(time, lat, lon)uint16dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;long_name :number of valid observationsstandard_name :land_cover_lccs number_of_observationsvalid_max :32767valid_min :0  Array   Chunk   Bytes   688.28 MiB   8.90 MiB   Shape   (11, 4050, 8100)   (1, 2160, 2160)   Dask graph   88 chunks in 2 graph layers   Data type   uint16 numpy.ndarray  8100 4050 11 </li><li>processed_flag(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;flag_meanings :not_processed processedflag_values :[0, 1]long_name :LC map processed area flagstandard_name :land_cover_lccs status_flagvalid_max :1valid_min :0  Array   Chunk   Bytes   1.34 GiB   17.80 MiB   Shape   (11, 4050, 8100)   (1, 2160, 2160)   Dask graph   88 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8100 4050 11 </li><li>time_bounds(time, bounds)datetime64[ns]dask.array&lt;chunksize=(11, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   176 B   176 B   Shape   (11, 2)   (11, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 11 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.95694444444445,            89.9125,  89.86805555555557,\n        89.82361111111112,  89.77916666666667,  89.73472222222222,\n         89.6902777777778,  89.64583333333334,  89.60138888888889,\n        89.55694444444444,\n       ...\n       -89.59861111111111, -89.64305555555555,           -89.6875,\n       -89.73194444444445, -89.77638888888889, -89.82083333333334,\n       -89.86527777777778, -89.90972222222223, -89.95416666666667,\n       -89.99861111111112],\n      dtype='float64', name='lat', length=4050))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([ -179.9986111111111, -179.95416666666668, -179.90972222222223,\n       -179.86527777777778, -179.82083333333333,  -179.7763888888889,\n       -179.73194444444445,           -179.6875, -179.64305555555555,\n        -179.5986111111111,\n       ...\n        179.55694444444447,   179.6013888888889,  179.64583333333337,\n         179.6902777777778,  179.73472222222222,   179.7791666666667,\n        179.82361111111112,  179.86805555555554,  179.91250000000002,\n        179.95694444444445],\n      dtype='float64', name='lon', length=8100))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2010-01-01', '2011-01-01', '2012-01-01', '2013-01-01',\n               '2014-01-01', '2015-01-01', '2016-01-01', '2017-01-01',\n               '2018-01-01', '2019-01-01', '2020-01-01'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (38)Conventions :CF-1.6TileSize :2025:2025cdm_data_type :gridcomment :contact :https://www.ecmwf.int/en/about/contact-us/get-supportcreation_date :20181130T095431Zcreator_email :landcover-cci@uclouvain.becreator_name :UCLouvaincreator_url :http://www.uclouvain.be/geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.002778geospatial_lat_units :degrees_northgeospatial_lon_max :180geospatial_lon_min :-180geospatial_lon_resolution :0.002778geospatial_lon_units :degrees_easthistory :amorgos-4,0, lc-sdr-1.0, lc-sr-1.0, lc-classification-1.0,lc-user-tools-3.13,lc-user-tools-4.3id :ESACCI-LC-L4-LCCS-Map-300m-P1Y-2010-v2.0.7cdsinstitution :UCLouvainkeywords :land cover classification,satellite,observationkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :ESA CCI Data Policy: free and open accessnaming_authority :org.esa-cciproduct_version :2.0.7cdsproject :Climate Change Initiative - European Space Agencyreferences :http://www.esa-landcover-cci.org/source :MERIS FR L1B version 5.05, MERIS RR L1B version 8.0, SPOT VGT Pspatial_resolution :300mstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Standard Names version 21summary :This dataset characterizes the land cover of a particular year (see time_coverage). The land cover was derived from the analysis of satellite data time series of the full period.time_coverage_duration :P1Ytime_coverage_end :20101231time_coverage_resolution :P1Ytime_coverage_start :20100101title :Land Cover Map of ESA CCI brokered by CDStracking_id :96ac9aca-1ca7-45c6-b4a5-ab448c692646type :ESACCI-LC-L4-LCCS-Map-300m-P1Y</li></ul> <pre>&lt;xarray.Dataset&gt; Size: 1GB\nDimensions:              (time: 11, lat: 2025, lon: 4050, bounds: 2)\nCoordinates:\n  * lat                  (lat) float64 16kB 89.91 89.82 89.73 ... -89.91 -90.0\n  * lon                  (lon) float64 32kB -180.0 -179.9 -179.8 ... 179.8 179.9\n  * time                 (time) datetime64[ns] 88B 2010-01-01 ... 2020-01-01\nDimensions without coordinates: bounds\nData variables:\n    change_count         (time, lat, lon) uint8 90MB dask.array&lt;chunksize=(1, 2025, 2160), meta=np.ndarray&gt;\n    crs                  (time) int32 44B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n    current_pixel_state  (time, lat, lon) float32 361MB dask.array&lt;chunksize=(1, 2025, 2160), meta=np.ndarray&gt;\n    lat_bounds           (time, lat, bounds) float64 356kB dask.array&lt;chunksize=(1, 2025, 2), meta=np.ndarray&gt;\n    lccs_class           (time, lat, lon) uint8 90MB dask.array&lt;chunksize=(1, 2025, 2160), meta=np.ndarray&gt;\n    lon_bounds           (time, lon, bounds) float64 713kB dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;\n    observation_count    (time, lat, lon) uint16 180MB dask.array&lt;chunksize=(1, 2025, 2160), meta=np.ndarray&gt;\n    processed_flag       (time, lat, lon) float32 361MB dask.array&lt;chunksize=(1, 2025, 2160), meta=np.ndarray&gt;\n    time_bounds          (time, bounds) datetime64[ns] 176B dask.array&lt;chunksize=(11, 2), meta=np.ndarray&gt;\nAttributes: (12/38)\n    Conventions:                CF-1.6\n    TileSize:                   2025:2025\n    cdm_data_type:              grid\n    comment:                    \n    contact:                    https://www.ecmwf.int/en/about/contact-us/get...\n    creation_date:              20181130T095431Z\n    ...                         ...\n    time_coverage_end:          20101231\n    time_coverage_resolution:   P1Y\n    time_coverage_start:        20100101\n    title:                      Land Cover Map of ESA CCI brokered by CDS\n    tracking_id:                96ac9aca-1ca7-45c6-b4a5-ab448c692646\n    type:                       ESACCI-LC-L4-LCCS-Map-300m-P1Y</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 11</li><li>lat: 2025</li><li>lon: 4050</li><li>bounds: 2</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6489.91 89.82 89.73 ... -89.91 -90.0axis :Ybounds :lat_boundslong_name :latitudestandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([ 89.9125  ,  89.823611,  89.734722, ..., -89.820833, -89.909722,\n       -89.998611])</pre></li><li>lon(lon)float64-180.0 -179.9 ... 179.8 179.9axis :Xbounds :lon_boundslong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-179.998611, -179.909722, -179.820833, ...,  179.734722,  179.823611,\n        179.9125  ])</pre></li><li>time(time)datetime64[ns]2010-01-01 ... 2020-01-01axis :Tbounds :time_boundslong_name :timestandard_name :time<pre>array(['2010-01-01T00:00:00.000000000', '2011-01-01T00:00:00.000000000',\n       '2012-01-01T00:00:00.000000000', '2013-01-01T00:00:00.000000000',\n       '2014-01-01T00:00:00.000000000', '2015-01-01T00:00:00.000000000',\n       '2016-01-01T00:00:00.000000000', '2017-01-01T00:00:00.000000000',\n       '2018-01-01T00:00:00.000000000', '2019-01-01T00:00:00.000000000',\n       '2020-01-01T00:00:00.000000000'], dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (9)<ul><li>change_count(time, lat, lon)uint8dask.array&lt;chunksize=(1, 2025, 2160), meta=np.ndarray&gt;long_name :number of class changesvalid_max :100valid_min :0  Array   Chunk   Bytes   86.03 MiB   4.17 MiB   Shape   (11, 2025, 4050)   (1, 2025, 2160)   Dask graph   22 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  4050 2025 11 </li><li>crs(time)int32dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;i2m :0.002777777777778,0.0,0.0,-0.002777777777778,-180.0,90.0wkt :GEOGCS[\"WGS 84\",    DATUM[\"World Geodetic System 1984\",      SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]],      AUTHORITY[\"EPSG\",\"6326\"]],    PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]],    UNIT[\"degree\", 0.017453292519943295],    AXIS[\"Geodetic longitude\", EAST],    AXIS[\"Geodetic latitude\", NORTH],    AUTHORITY[\"EPSG\",\"4326\"]]  Array   Chunk   Bytes   44 B   4 B   Shape   (11,)   (1,)   Dask graph   11 chunks in 2 graph layers   Data type   int32 numpy.ndarray  11 1 </li><li>current_pixel_state(time, lat, lon)float32dask.array&lt;chunksize=(1, 2025, 2160), meta=np.ndarray&gt;flag_meanings :invalid clear_land clear_water clear_snow_ice cloud cloud_shadowflag_values :[0, 1, 2, 3, 4, 5]long_name :LC pixel type maskstandard_name :land_cover_lccs status_flagvalid_max :5valid_min :0  Array   Chunk   Bytes   344.14 MiB   16.69 MiB   Shape   (11, 2025, 4050)   (1, 2025, 2160)   Dask graph   22 chunks in 2 graph layers   Data type   float32 numpy.ndarray  4050 2025 11 </li><li>lat_bounds(time, lat, bounds)float64dask.array&lt;chunksize=(1, 2025, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   348.05 kiB   31.64 kiB   Shape   (11, 2025, 2)   (1, 2025, 2)   Dask graph   11 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 2025 11 </li><li>lccs_class(time, lat, lon)uint8dask.array&lt;chunksize=(1, 2025, 2160), meta=np.ndarray&gt;ancillary_variables :processed_flag current_pixel_state observation_count change_countflag_colors :#ffff64 #ffff64 #ffff00 #aaf0f0 #dcf064 #c8c864 #006400 #00a000 #00a000 #aac800 #003c00 #003c00 #005000 #285000 #285000 #286400 #788200 #8ca000 #be9600 #966400 #966400 #966400 #ffb432 #ffdcd2 #ffebaf #ffc864 #ffd278 #ffebaf #00785a #009678 #00dc82 #c31400 #fff5d7 #dcdcdc #fff5d7 #0046c8 #ffffffflag_meanings :no_data cropland_rainfed cropland_rainfed_herbaceous_cover cropland_rainfed_tree_or_shrub_cover cropland_irrigated mosaic_cropland mosaic_natural_vegetation tree_broadleaved_evergreen_closed_to_open tree_broadleaved_deciduous_closed_to_open tree_broadleaved_deciduous_closed tree_broadleaved_deciduous_open tree_needleleaved_evergreen_closed_to_open tree_needleleaved_evergreen_closed tree_needleleaved_evergreen_open tree_needleleaved_deciduous_closed_to_open tree_needleleaved_deciduous_closed tree_needleleaved_deciduous_open tree_mixed mosaic_tree_and_shrub mosaic_herbaceous shrubland shrubland_evergreen shrubland_deciduous grassland lichens_and_mosses sparse_vegetation sparse_tree sparse_shrub sparse_herbaceous tree_cover_flooded_fresh_or_brakish_water tree_cover_flooded_saline_water shrub_or_herbaceous_cover_flooded urban bare_areas bare_areas_consolidated bare_areas_unconsolidated water snow_and_iceflag_values :[0, 10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100, 110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180, 190, 200, 201, 202, 210, 220]long_name :Land cover class defined in LCCSstandard_name :land_cover_lccsvalid_max :220valid_min :1  Array   Chunk   Bytes   86.03 MiB   4.17 MiB   Shape   (11, 2025, 4050)   (1, 2025, 2160)   Dask graph   22 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  4050 2025 11 </li><li>lon_bounds(time, lon, bounds)float64dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   696.09 kiB   33.75 kiB   Shape   (11, 4050, 2)   (1, 2160, 2)   Dask graph   22 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 4050 11 </li><li>observation_count(time, lat, lon)uint16dask.array&lt;chunksize=(1, 2025, 2160), meta=np.ndarray&gt;long_name :number of valid observationsstandard_name :land_cover_lccs number_of_observationsvalid_max :32767valid_min :0  Array   Chunk   Bytes   172.07 MiB   8.34 MiB   Shape   (11, 2025, 4050)   (1, 2025, 2160)   Dask graph   22 chunks in 2 graph layers   Data type   uint16 numpy.ndarray  4050 2025 11 </li><li>processed_flag(time, lat, lon)float32dask.array&lt;chunksize=(1, 2025, 2160), meta=np.ndarray&gt;flag_meanings :not_processed processedflag_values :[0, 1]long_name :LC map processed area flagstandard_name :land_cover_lccs status_flagvalid_max :1valid_min :0  Array   Chunk   Bytes   344.14 MiB   16.69 MiB   Shape   (11, 2025, 4050)   (1, 2025, 2160)   Dask graph   22 chunks in 2 graph layers   Data type   float32 numpy.ndarray  4050 2025 11 </li><li>time_bounds(time, bounds)datetime64[ns]dask.array&lt;chunksize=(11, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   176 B   176 B   Shape   (11, 2)   (11, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 11 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([           89.9125,  89.82361111111112,  89.73472222222222,\n        89.64583333333334,  89.55694444444444,  89.46805555555557,\n        89.37916666666666,  89.29027777777779,  89.20138888888889,\n        89.11250000000001,\n       ...\n        -89.1986111111111,           -89.2875, -89.37638888888888,\n       -89.46527777777777, -89.55416666666666, -89.64305555555555,\n       -89.73194444444445, -89.82083333333334, -89.90972222222223,\n       -89.99861111111112],\n      dtype='float64', name='lat', length=2025))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([ -179.9986111111111, -179.90972222222223, -179.82083333333333,\n       -179.73194444444445, -179.64305555555555, -179.55416666666667,\n       -179.46527777777777,  -179.3763888888889,           -179.2875,\n       -179.19861111111112,\n       ...\n                  179.1125,   179.2013888888889,  179.29027777777782,\n        179.37916666666666,  179.46805555555557,  179.55694444444447,\n        179.64583333333337,  179.73472222222222,  179.82361111111112,\n        179.91250000000002],\n      dtype='float64', name='lon', length=4050))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2010-01-01', '2011-01-01', '2012-01-01', '2013-01-01',\n               '2014-01-01', '2015-01-01', '2016-01-01', '2017-01-01',\n               '2018-01-01', '2019-01-01', '2020-01-01'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (38)Conventions :CF-1.6TileSize :2025:2025cdm_data_type :gridcomment :contact :https://www.ecmwf.int/en/about/contact-us/get-supportcreation_date :20181130T095431Zcreator_email :landcover-cci@uclouvain.becreator_name :UCLouvaincreator_url :http://www.uclouvain.be/geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.002778geospatial_lat_units :degrees_northgeospatial_lon_max :180geospatial_lon_min :-180geospatial_lon_resolution :0.002778geospatial_lon_units :degrees_easthistory :amorgos-4,0, lc-sdr-1.0, lc-sr-1.0, lc-classification-1.0,lc-user-tools-3.13,lc-user-tools-4.3id :ESACCI-LC-L4-LCCS-Map-300m-P1Y-2010-v2.0.7cdsinstitution :UCLouvainkeywords :land cover classification,satellite,observationkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :ESA CCI Data Policy: free and open accessnaming_authority :org.esa-cciproduct_version :2.0.7cdsproject :Climate Change Initiative - European Space Agencyreferences :http://www.esa-landcover-cci.org/source :MERIS FR L1B version 5.05, MERIS RR L1B version 8.0, SPOT VGT Pspatial_resolution :300mstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Standard Names version 21summary :This dataset characterizes the land cover of a particular year (see time_coverage). The land cover was derived from the analysis of satellite data time series of the full period.time_coverage_duration :P1Ytime_coverage_end :20101231time_coverage_resolution :P1Ytime_coverage_start :20100101title :Land Cover Map of ESA CCI brokered by CDStracking_id :96ac9aca-1ca7-45c6-b4a5-ab448c692646type :ESACCI-LC-L4-LCCS-Map-300m-P1Y</li></ul> <p>Get a dataset at a certain level to continue analysis with it.</p> In\u00a0[10]: Copied! <pre>dataset_i = ml_dataset.get_dataset(2)\n</pre> dataset_i = ml_dataset.get_dataset(2) In\u00a0[11]: Copied! <pre>dataset_i\n</pre> dataset_i Out[11]: <pre>&lt;xarray.Dataset&gt; Size: 69GB\nDimensions:              (time: 11, lat: 16200, lon: 32400, bounds: 2)\nCoordinates:\n  * lat                  (lat) float64 130kB 89.99 89.98 89.97 ... -89.99 -90.0\n  * lon                  (lon) float64 259kB -180.0 -180.0 ... 180.0 180.0\n  * time                 (time) datetime64[ns] 88B 2010-01-01 ... 2020-01-01\nDimensions without coordinates: bounds\nData variables:\n    change_count         (time, lat, lon) uint8 6GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    crs                  (time) int32 44B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n    current_pixel_state  (time, lat, lon) float32 23GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    lat_bounds           (time, lat, bounds) float64 3MB dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;\n    lccs_class           (time, lat, lon) uint8 6GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    lon_bounds           (time, lon, bounds) float64 6MB dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;\n    observation_count    (time, lat, lon) uint16 12GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    processed_flag       (time, lat, lon) float32 23GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    time_bounds          (time, bounds) datetime64[ns] 176B dask.array&lt;chunksize=(11, 2), meta=np.ndarray&gt;\nAttributes: (12/38)\n    Conventions:                CF-1.6\n    TileSize:                   2025:2025\n    cdm_data_type:              grid\n    comment:                    \n    contact:                    https://www.ecmwf.int/en/about/contact-us/get...\n    creation_date:              20181130T095431Z\n    ...                         ...\n    time_coverage_end:          20101231\n    time_coverage_resolution:   P1Y\n    time_coverage_start:        20100101\n    title:                      Land Cover Map of ESA CCI brokered by CDS\n    tracking_id:                96ac9aca-1ca7-45c6-b4a5-ab448c692646\n    type:                       ESACCI-LC-L4-LCCS-Map-300m-P1Y</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 11</li><li>lat: 16200</li><li>lon: 32400</li><li>bounds: 2</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6489.99 89.98 89.97 ... -89.99 -90.0axis :Ybounds :lat_boundslong_name :latitudestandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([ 89.990278,  89.979167,  89.968056, ..., -89.976389, -89.9875  ,\n       -89.998611])</pre></li><li>lon(lon)float64-180.0 -180.0 ... 180.0 180.0axis :Xbounds :lon_boundslong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-179.998611, -179.9875  , -179.976389, ...,  179.968056,  179.979167,\n        179.990278])</pre></li><li>time(time)datetime64[ns]2010-01-01 ... 2020-01-01axis :Tbounds :time_boundslong_name :timestandard_name :time<pre>array(['2010-01-01T00:00:00.000000000', '2011-01-01T00:00:00.000000000',\n       '2012-01-01T00:00:00.000000000', '2013-01-01T00:00:00.000000000',\n       '2014-01-01T00:00:00.000000000', '2015-01-01T00:00:00.000000000',\n       '2016-01-01T00:00:00.000000000', '2017-01-01T00:00:00.000000000',\n       '2018-01-01T00:00:00.000000000', '2019-01-01T00:00:00.000000000',\n       '2020-01-01T00:00:00.000000000'], dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (9)<ul><li>change_count(time, lat, lon)uint8dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;long_name :number of class changesvalid_max :100valid_min :0  Array   Chunk   Bytes   5.38 GiB   4.45 MiB   Shape   (11, 16200, 32400)   (1, 2160, 2160)   Dask graph   1320 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  32400 16200 11 </li><li>crs(time)int32dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;i2m :0.002777777777778,0.0,0.0,-0.002777777777778,-180.0,90.0wkt :GEOGCS[\"WGS 84\",    DATUM[\"World Geodetic System 1984\",      SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]],      AUTHORITY[\"EPSG\",\"6326\"]],    PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]],    UNIT[\"degree\", 0.017453292519943295],    AXIS[\"Geodetic longitude\", EAST],    AXIS[\"Geodetic latitude\", NORTH],    AUTHORITY[\"EPSG\",\"4326\"]]  Array   Chunk   Bytes   44 B   4 B   Shape   (11,)   (1,)   Dask graph   11 chunks in 2 graph layers   Data type   int32 numpy.ndarray  11 1 </li><li>current_pixel_state(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;flag_meanings :invalid clear_land clear_water clear_snow_ice cloud cloud_shadowflag_values :[0, 1, 2, 3, 4, 5]long_name :LC pixel type maskstandard_name :land_cover_lccs status_flagvalid_max :5valid_min :0  Array   Chunk   Bytes   21.51 GiB   17.80 MiB   Shape   (11, 16200, 32400)   (1, 2160, 2160)   Dask graph   1320 chunks in 2 graph layers   Data type   float32 numpy.ndarray  32400 16200 11 </li><li>lat_bounds(time, lat, bounds)float64dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   2.72 MiB   33.75 kiB   Shape   (11, 16200, 2)   (1, 2160, 2)   Dask graph   88 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 16200 11 </li><li>lccs_class(time, lat, lon)uint8dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;ancillary_variables :processed_flag current_pixel_state observation_count change_countflag_colors :#ffff64 #ffff64 #ffff00 #aaf0f0 #dcf064 #c8c864 #006400 #00a000 #00a000 #aac800 #003c00 #003c00 #005000 #285000 #285000 #286400 #788200 #8ca000 #be9600 #966400 #966400 #966400 #ffb432 #ffdcd2 #ffebaf #ffc864 #ffd278 #ffebaf #00785a #009678 #00dc82 #c31400 #fff5d7 #dcdcdc #fff5d7 #0046c8 #ffffffflag_meanings :no_data cropland_rainfed cropland_rainfed_herbaceous_cover cropland_rainfed_tree_or_shrub_cover cropland_irrigated mosaic_cropland mosaic_natural_vegetation tree_broadleaved_evergreen_closed_to_open tree_broadleaved_deciduous_closed_to_open tree_broadleaved_deciduous_closed tree_broadleaved_deciduous_open tree_needleleaved_evergreen_closed_to_open tree_needleleaved_evergreen_closed tree_needleleaved_evergreen_open tree_needleleaved_deciduous_closed_to_open tree_needleleaved_deciduous_closed tree_needleleaved_deciduous_open tree_mixed mosaic_tree_and_shrub mosaic_herbaceous shrubland shrubland_evergreen shrubland_deciduous grassland lichens_and_mosses sparse_vegetation sparse_tree sparse_shrub sparse_herbaceous tree_cover_flooded_fresh_or_brakish_water tree_cover_flooded_saline_water shrub_or_herbaceous_cover_flooded urban bare_areas bare_areas_consolidated bare_areas_unconsolidated water snow_and_iceflag_values :[0, 10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100, 110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180, 190, 200, 201, 202, 210, 220]long_name :Land cover class defined in LCCSstandard_name :land_cover_lccsvalid_max :220valid_min :1  Array   Chunk   Bytes   5.38 GiB   4.45 MiB   Shape   (11, 16200, 32400)   (1, 2160, 2160)   Dask graph   1320 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  32400 16200 11 </li><li>lon_bounds(time, lon, bounds)float64dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   5.44 MiB   33.75 kiB   Shape   (11, 32400, 2)   (1, 2160, 2)   Dask graph   165 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 32400 11 </li><li>observation_count(time, lat, lon)uint16dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;long_name :number of valid observationsstandard_name :land_cover_lccs number_of_observationsvalid_max :32767valid_min :0  Array   Chunk   Bytes   10.75 GiB   8.90 MiB   Shape   (11, 16200, 32400)   (1, 2160, 2160)   Dask graph   1320 chunks in 2 graph layers   Data type   uint16 numpy.ndarray  32400 16200 11 </li><li>processed_flag(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;flag_meanings :not_processed processedflag_values :[0, 1]long_name :LC map processed area flagstandard_name :land_cover_lccs status_flagvalid_max :1valid_min :0  Array   Chunk   Bytes   21.51 GiB   17.80 MiB   Shape   (11, 16200, 32400)   (1, 2160, 2160)   Dask graph   1320 chunks in 2 graph layers   Data type   float32 numpy.ndarray  32400 16200 11 </li><li>time_bounds(time, bounds)datetime64[ns]dask.array&lt;chunksize=(11, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   176 B   176 B   Shape   (11, 2)   (11, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 11 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.99027777777778,  89.97916666666669,  89.96805555555557,\n        89.95694444444445,  89.94583333333335,  89.93472222222223,\n        89.92361111111111,            89.9125,   89.9013888888889,\n        89.89027777777778,\n       ...\n       -89.89861111111111, -89.90972222222223, -89.92083333333333,\n       -89.93194444444444, -89.94305555555556, -89.95416666666667,\n       -89.96527777777777, -89.97638888888889,           -89.9875,\n       -89.99861111111112],\n      dtype='float64', name='lat', length=16200))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([ -179.9986111111111,           -179.9875,  -179.9763888888889,\n       -179.96527777777777, -179.95416666666668, -179.94305555555556,\n       -179.93194444444444, -179.92083333333332, -179.90972222222223,\n        -179.8986111111111,\n       ...\n        179.89027777777778,   179.9013888888889,  179.91250000000002,\n        179.92361111111114,  179.93472222222226,  179.94583333333333,\n        179.95694444444445,  179.96805555555557,  179.97916666666669,\n         179.9902777777778],\n      dtype='float64', name='lon', length=32400))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2010-01-01', '2011-01-01', '2012-01-01', '2013-01-01',\n               '2014-01-01', '2015-01-01', '2016-01-01', '2017-01-01',\n               '2018-01-01', '2019-01-01', '2020-01-01'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (38)Conventions :CF-1.6TileSize :2025:2025cdm_data_type :gridcomment :contact :https://www.ecmwf.int/en/about/contact-us/get-supportcreation_date :20181130T095431Zcreator_email :landcover-cci@uclouvain.becreator_name :UCLouvaincreator_url :http://www.uclouvain.be/geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.002778geospatial_lat_units :degrees_northgeospatial_lon_max :180geospatial_lon_min :-180geospatial_lon_resolution :0.002778geospatial_lon_units :degrees_easthistory :amorgos-4,0, lc-sdr-1.0, lc-sr-1.0, lc-classification-1.0,lc-user-tools-3.13,lc-user-tools-4.3id :ESACCI-LC-L4-LCCS-Map-300m-P1Y-2010-v2.0.7cdsinstitution :UCLouvainkeywords :land cover classification,satellite,observationkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :ESA CCI Data Policy: free and open accessnaming_authority :org.esa-cciproduct_version :2.0.7cdsproject :Climate Change Initiative - European Space Agencyreferences :http://www.esa-landcover-cci.org/source :MERIS FR L1B version 5.05, MERIS RR L1B version 8.0, SPOT VGT Pspatial_resolution :300mstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Standard Names version 21summary :This dataset characterizes the land cover of a particular year (see time_coverage). The land cover was derived from the analysis of satellite data time series of the full period.time_coverage_duration :P1Ytime_coverage_end :20101231time_coverage_resolution :P1Ytime_coverage_start :20100101title :Land Cover Map of ESA CCI brokered by CDStracking_id :96ac9aca-1ca7-45c6-b4a5-ab448c692646type :ESACCI-LC-L4-LCCS-Map-300m-P1Y</li></ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/Access_public_cubes/#access-data-cubes-in-deepesdl-public-object-storage","title":"Access data cubes in DeepESDL public object storage\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Access_public_cubes/#a-deepesdl-example-notebook","title":"A DeepESDL example notebook\u00b6","text":"<p>This concise notebook demonstrates how xcubes data stores provide convenient access to the published data cubes, generated by the Cube Gen team and persisted on object storage. The recipes used in the generation process are publicly available at in the Cube Gen repository.</p> <p>Please, also refer to the DeepESDL documentation and visit the platform's website for further information!</p> <p>Brockmann Consult, 2024</p> <p>This notebook runs with the python environment <code>deepesdl-xcube-1.7.0</code>, please checkout the documentation for help on changing the environment.</p>"},{"location":"guide/jupyterlab/notebooks/Access_public_cubes/#how-to-open-zarr-from-the-datastore","title":"How to open .zarr from the datastore\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Access_public_cubes/#how-to-open-a-levels-from-the-datastore","title":"How to open a .levels from the datastore\u00b6","text":"<p>.levels are xcube Multi-Resolution Datasets which store different resolutions of a dataset as a pyramid for fast visualisation e.g. in xcube viewer. If you see in an s3 store a dataset name with both extesions, zarr and levels, then the zarr dataset is the base layer with the higehst resolution and used in the .levels as a link.</p>"},{"location":"guide/jupyterlab/notebooks/Append_to_existing_cube/","title":"Append to existing cube","text":"<p>First, lets create a small cube, which we can later on append data to. We will use ESA CCI data for this. Please head over to 3 - Generate CCI data cubes to get more details about the xcube-cci data store :)</p> In\u00a0[1]: Copied! <pre>import datetime\nimport os\n\nfrom xcube.core.store import new_data_store\n</pre> import datetime import os  from xcube.core.store import new_data_store In\u00a0[2]: Copied! <pre>store = new_data_store(\"ccizarr\")\n</pre> store = new_data_store(\"ccizarr\") <p>Next, we create a cube containing only 2 months of data:</p> In\u00a0[3]: Copied! <pre>def open_zarrstore(filename, time_range, variables):\n    ds = store.open_data(filename)\n    subset = ds.sel(time=slice(time_range[0], time_range[1]))\n    subset = subset[variables]\n\n    return subset\n</pre> def open_zarrstore(filename, time_range, variables):     ds = store.open_data(filename)     subset = ds.sel(time=slice(time_range[0], time_range[1]))     subset = subset[variables]      return subset In\u00a0[4]: Copied! <pre>dataset = open_zarrstore(\n    \"ESACCI-OC-L3S-RRS-MERGED-1M_MONTHLY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr\",\n    time_range=[datetime.datetime(1998, 3, 1), datetime.datetime(1998, 4, 30)],\n    variables=[\"Rrs_412\"],\n)\n</pre> dataset = open_zarrstore(     \"ESACCI-OC-L3S-RRS-MERGED-1M_MONTHLY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr\",     time_range=[datetime.datetime(1998, 3, 1), datetime.datetime(1998, 4, 30)],     variables=[\"Rrs_412\"], ) In\u00a0[5]: Copied! <pre>dataset\n</pre> dataset Out[5]: <pre>&lt;xarray.Dataset&gt; Size: 299MB\nDimensions:  (time: 2, lat: 4320, lon: 8640)\nCoordinates:\n  * lat      (lat) float64 35kB 89.98 89.94 89.9 89.85 ... -89.9 -89.94 -89.98\n  * lon      (lon) float64 69kB -180.0 -179.9 -179.9 ... 179.9 179.9 180.0\n  * time     (time) datetime64[ns] 16B 1998-03-01 1998-04-01\nData variables:\n    Rrs_412  (time, lat, lon) float32 299MB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\nAttributes: (12/52)\n    Conventions:                       CF-1.7\n    Metadata_Conventions:              Unidata Dataset Discovery v1.0\n    NCO:                               netCDF Operators version 4.7.5 (Homepa...\n    cdm_data_type:                     Grid\n    comment:                           See summary attribute\n    creation_date:                     Tue Jan 31 12:15:15 2023\n    ...                                ...\n    time_coverage_end:                 20221201T000000Z\n    time_coverage_resolution:          P1M\n    time_coverage_start:               19970904T000000Z\n    title:                             ESA CCI Ocean Colour Product\n    tracking_id:                       f7c46c10-67de-460c-bdce-6de91ad32075\n    catalogue_url:                     https://catalogue.ceda.ac.uk/uuid/a078...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 2</li><li>lat: 4320</li><li>lon: 8640</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6489.98 89.94 89.9 ... -89.94 -89.98axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northvalid_max :89.97916666666667valid_min :-89.97916666666666<pre>array([ 89.979167,  89.9375  ,  89.895833, ..., -89.895833, -89.9375  ,\n       -89.979167])</pre></li><li>lon(lon)float64-180.0 -179.9 ... 179.9 180.0axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_max :179.97916666666663valid_min :-179.97916666666666<pre>array([-179.979167, -179.9375  , -179.895833, ...,  179.895833,  179.9375  ,\n        179.979167])</pre></li><li>time(time)datetime64[ns]1998-03-01 1998-04-01axis :Tstandard_name :time<pre>array(['1998-03-01T00:00:00.000000000', '1998-04-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>Rrs_412(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;ancillary_variables :Rrs_412_rmsd Rrs_412_biaslong_name :Sea surface reflectance defined as the ratio of water-leaving radiance to surface irradiance at 412 nm.parameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFV13N26standard_name :surface_ratio_of_upwelling_radiance_emerging_from_sea_water_to_downwelling_radiative_flux_in_airunits :sr-1units_nonstandard :sr^-1wavelength :412  Array   Chunk   Bytes   284.77 MiB   17.80 MiB   Shape   (2, 4320, 8640)   (1, 2160, 2160)   Dask graph   16 chunks in 3 graph layers   Data type   float32 numpy.ndarray  8640 4320 2 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.97916666666667,            89.9375,  89.89583333333333,\n        89.85416666666667,            89.8125,  89.77083333333333,\n        89.72916666666667,            89.6875,  89.64583333333333,\n        89.60416666666667,\n       ...\n       -89.60416666666666, -89.64583333333331,           -89.6875,\n       -89.72916666666666, -89.77083333333331,           -89.8125,\n       -89.85416666666666, -89.89583333333331,           -89.9375,\n       -89.97916666666666],\n      dtype='float64', name='lat', length=4320))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.97916666666666,           -179.9375, -179.89583333333334,\n       -179.85416666666666,           -179.8125, -179.77083333333334,\n       -179.72916666666666,           -179.6875, -179.64583333333334,\n       -179.60416666666666,\n       ...\n        179.60416666666663,  179.64583333333331,            179.6875,\n        179.72916666666663,  179.77083333333331,            179.8125,\n        179.85416666666663,  179.89583333333331,            179.9375,\n        179.97916666666663],\n      dtype='float64', name='lon', length=8640))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1998-03-01', '1998-04-01'], dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (52)Conventions :CF-1.7Metadata_Conventions :Unidata Dataset Discovery v1.0NCO :netCDF Operators version 4.7.5 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco)cdm_data_type :Gridcomment :See summary attributecreation_date :Tue Jan 31 12:15:15 2023creator_email :help@esa-oceancolour-cci.orgcreator_name :Plymouth Marine Laboratorycreator_url :http://esa-oceancolour-cci.orgdate_created :Tue Jan 31 12:15:15 2023geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :.04166666666666666666geospatial_lat_units :decimal degrees northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :.04166666666666666666geospatial_lon_units :decimal degrees eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0git_commit_hash :ef9b6f099ea56e816faba0c254a71b178180897fhistory :Source data were: ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221201-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221202-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221203-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221204-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221205-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221206-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221207-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221208-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221209-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221210-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221211-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221212-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221213-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221214-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221215-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221216-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221217-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221218-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221219-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221220-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221221-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221222-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221223-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221224-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221225-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221226-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221227-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221228-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221229-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221230-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221231-fv6.0.nc; netcdf_compositor_cci composites  Rrs_412, Rrs_412_bias, Rrs_443, Rrs_443_bias, Rrs_490, Rrs_490_bias, Rrs_510, Rrs_510_bias, Rrs_560, Rrs_560_bias, Rrs_665, Rrs_665_bias, adg_412, adg_412_bias, adg_443, adg_443_bias, adg_490, adg_490_bias, adg_510, adg_510_bias, adg_560, adg_560_bias, adg_665, adg_665_bias, aph_412, aph_412_bias, aph_443, aph_443_bias, aph_490, aph_490_bias, aph_510, aph_510_bias, aph_560, aph_560_bias, aph_665, aph_665_bias, atot_412, atot_443, atot_490, atot_510, atot_560, atot_665, bbp_412, bbp_443, bbp_490, bbp_510, bbp_560, bbp_665, chlor_a, chlor_a_log10_bias, kd_490, kd_490_bias, water_class1, water_class10, water_class11, water_class12, water_class13, water_class14, water_class2, water_class3, water_class4, water_class5, water_class6, water_class7, water_class8, water_class9 with --mean,  Rrs_412_rmsd, Rrs_443_rmsd, Rrs_490_rmsd, Rrs_510_rmsd, Rrs_560_rmsd, Rrs_665_rmsd, adg_412_rmsd, adg_443_rmsd, adg_490_rmsd, adg_510_rmsd, adg_560_rmsd, adg_665_rmsd, aph_412_rmsd, aph_443_rmsd, aph_490_rmsd, aph_510_rmsd, aph_560_rmsd, aph_665_rmsd, chlor_a_log10_rmsd, kd_490_rmsd with --root-mean-square, and  MERIS_nobs, MODISA_nobs, OLCI-A_nobs, OLCI-B_nobs, SeaWiFS_nobs, VIIRS_nobs, total_nobs - with --total 1675169553 Subsetted from standardised_geo/ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-202212-fv6.0.nc to only include variables MERIS_nobs_sum,MODISA_nobs_sum,OLCI-A_nobs_sum,OLCI-B_nobs_sum,Rrs_412,Rrs_412_bias,Rrs_412_rmsd,Rrs_443,Rrs_443_bias,Rrs_443_rmsd,Rrs_490,Rrs_490_bias,Rrs_490_rmsd,Rrs_510,Rrs_510_bias,Rrs_510_rmsd,Rrs_560,Rrs_560_bias,Rrs_560_rmsd,Rrs_665,Rrs_665_bias,Rrs_665_rmsd,SeaWiFS_nobs_sum,VIIRS_nobs_sum,crs,lat,lon,time,total_nobs_sum,water_class1,water_class10,water_class11,water_class12,water_class13,water_class14,water_class2,water_class3,water_class4,water_class5,water_class6,water_class7,water_class8,water_class9id :ESACCI-OC-L3S-RRS-MERGED-1M_MONTHLY_4km_GEO_PML_RRS-202212-fv6.0.ncinstitution :Plymouth Marine Laboratorykeywords :satellite,observation,ocean,ocean colourkeywords_vocabulary :nonelicense :ESA CCI Data Policy: free and open access.  When referencing, please use: Ocean Colour Climate Change Initiative dataset, Version &lt;Version Number&gt;, European Space Agency, available online at http://www.esa-oceancolour-cci.org.  We would also appreciate being notified of publications so that we can list them on the project website at http://www.esa-oceancolour-cci.org/?q=publicationsnaming_authority :uk.ac.pmlnetcdf_file_type :NETCDF4_CLASSICnumber_of_bands_used_to_classify :4number_of_files_composited :31number_of_optical_water_types :14platform :Orbview-2,Aqua,Envisat,Suomi-NPP, Sentinel-3a, Sentinel-3bprocessing_level :Level-3product_version :6.0project :Climate Change Initiative - European Space Agencyreferences :http://www.esa-oceancolour-cci.org/sensor :SeaWiFS,MODIS,MERIS,VIIRS,OLCIsensors_present : OLCIa OLCIbsource :NASA SeaWiFS  L1A and L2 R2018.0 LAC and GAC, MODIS-Aqua L1A and L2 R2018.0, MERIS L1B 3rd reprocessing inc OCL corrections, NASA VIIRS L1A and L2 R2018.0, OLCI L1Bspatial_resolution :4km nominal at equatorstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventions Version 1.7start_date :01-DEC-2022 00:00:00.000000stop_date :31-DEC-2022 23:59:00.000000summary :Data products generated by the Ocean Colour component of the European Space Agency Climate Change Initiative project. These files are monthly composites of merged sensor (MERIS, MODIS Aqua, SeaWiFS LAC &amp; GAC, VIIRS, OLCI) products.  MODIS Aqua and SeaWiFS were band-shifted and bias-corrected to MERIS bands and values using a temporally and spatially varying scheme based on the overlap years of 2003-2007.  VIIRS was band-shifted and bias-corrected in a second stage against the MODIS Rrs that had already been corrected to MERIS levels, for the overlap period 2012-2013; and at the third stage OLCI was bias corrected against already corrected MODIS, for overlap period 2016-07-01 to 2019-06-30.  VIIRS, MODIS, SeaWiFS and MERIS Rrs were derived from a combination of NASA's l2gen (for basic sensor geometry corrections, etc) and HYGEOS Polymer v4.12 (for atmospheric correction). OLCI Rrs were sourced at L1b (already geometrically corrected) and processed with polymer.  The Rrs were binned to a sinusoidal 4km level-3 grid, and later to 4km geographic projection, by Brockmann Consult's SNAP.  Derived products were generally computed with the standard algorithmsthrough SeaDAS.  QAA IOPs were derived using the standard SeaDAS algorithm but with a modified backscattering table to match that used in the bandshifting.  The final chlorophyll is a combination of OCI, OCI2, OC2 and OCx, depending on the water class memberships.  Uncertainty estimates were added using the fuzzy water classifier and uncertainty estimation algorithm of Tim Moore as documented in Jackson et al (2017). and updated accorsing to Jackson et al. (in prep).time_coverage_duration :P1Mtime_coverage_end :20221201T000000Ztime_coverage_resolution :P1Mtime_coverage_start :19970904T000000Ztitle :ESA CCI Ocean Colour Producttracking_id :f7c46c10-67de-460c-bdce-6de91ad32075catalogue_url :https://catalogue.ceda.ac.uk/uuid/a0782135bcd04d77a1dae4aa71fba47c</li></ul> <p>Next, save it to the team s3 storage:</p> <p>To store the cube in your teams user space, please first retrieve the details from your environment variables as the following:</p> In\u00a0[6]: Copied! <pre>S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"]\nS3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"]\nS3_USER_STORAGE_BUCKET = os.environ[\"S3_USER_STORAGE_BUCKET\"]\n</pre> S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"] S3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"] S3_USER_STORAGE_BUCKET = os.environ[\"S3_USER_STORAGE_BUCKET\"] <p>You need to instantiate a s3 datastore pointing to the team bucket:</p> In\u00a0[7]: Copied! <pre>team_store = new_data_store(\n    \"s3\",\n    root=S3_USER_STORAGE_BUCKET,\n    storage_options=dict(\n        anon=False, key=S3_USER_STORAGE_KEY, secret=S3_USER_STORAGE_SECRET\n    ),\n)\n</pre> team_store = new_data_store(     \"s3\",     root=S3_USER_STORAGE_BUCKET,     storage_options=dict(         anon=False, key=S3_USER_STORAGE_KEY, secret=S3_USER_STORAGE_SECRET     ), ) <p>If you have stored no data to your user space, the returned list will be empty:</p> In\u00a0[8]: Copied! <pre>list(team_store.get_data_ids())\n</pre> list(team_store.get_data_ids()) Out[8]: <pre>['SST.levels',\n 'amazonas_v8.zarr',\n 'amazonas_v9.zarr',\n 'noise_trajectory.zarr',\n 'reanalysis-era5-single-levels-monthly-means-subset-2001-2010_TMH.zarr',\n 's2-demo-cube.zarr']</pre> <p>Appending data currently only works with .zarr format and is not supported in .levels yet.</p> In\u00a0[9]: Copied! <pre>output_id = \"ocean_color.zarr\"\n</pre> output_id = \"ocean_color.zarr\" In\u00a0[10]: Copied! <pre>team_store.write_data(dataset, output_id, replace=True)\n</pre> team_store.write_data(dataset, output_id, replace=True) Out[10]: <pre>'ocean_color.zarr'</pre> <p>If you list the content of you datastore again, you will now see the newly written dataset in the list:</p> In\u00a0[11]: Copied! <pre>list(team_store.get_data_ids())\n</pre> list(team_store.get_data_ids()) Out[11]: <pre>['SST.levels',\n 'amazonas_v8.zarr',\n 'amazonas_v9.zarr',\n 'noise_trajectory.zarr',\n 'ocean_color.zarr',\n 'reanalysis-era5-single-levels-monthly-means-subset-2001-2010_TMH.zarr',\n 's2-demo-cube.zarr']</pre> <p>Now, to append new time stamps, xcube cannot be used but there is a workaround :)</p> In\u00a0[12]: Copied! <pre># needed for appending data to an existing cube saved in s3 storage\nimport s3fs\n</pre> # needed for appending data to an existing cube saved in s3 storage import s3fs <p>Connect to your team storage in S3</p> In\u00a0[13]: Copied! <pre># Connect to AWS S3 storage\nfs = s3fs.S3FileSystem(\n    anon=False, key=S3_USER_STORAGE_KEY, secret=S3_USER_STORAGE_SECRET\n)\n</pre> # Connect to AWS S3 storage fs = s3fs.S3FileSystem(     anon=False, key=S3_USER_STORAGE_KEY, secret=S3_USER_STORAGE_SECRET ) In\u00a0[14]: Copied! <pre>s3_client_kwargs = {\"endpoint_url\": \"https://s3.eu-central-1.amazonaws.com\"}\ntarget_bucket_path = f\"s3://{S3_USER_STORAGE_BUCKET}\"\n</pre> s3_client_kwargs = {\"endpoint_url\": \"https://s3.eu-central-1.amazonaws.com\"} target_bucket_path = f\"s3://{S3_USER_STORAGE_BUCKET}\" <p>We create a new dataset, with different time stamps, which we want to append to the existing cube:</p> In\u00a0[15]: Copied! <pre>dataset = open_zarrstore(\n    \"ESACCI-OC-L3S-RRS-MERGED-1M_MONTHLY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr\",\n    time_range=[datetime.datetime(1998, 5, 1), datetime.datetime(1998, 6, 30)],\n    variables=[\"Rrs_412\"],\n)\n</pre> dataset = open_zarrstore(     \"ESACCI-OC-L3S-RRS-MERGED-1M_MONTHLY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr\",     time_range=[datetime.datetime(1998, 5, 1), datetime.datetime(1998, 6, 30)],     variables=[\"Rrs_412\"], ) In\u00a0[16]: Copied! <pre>dataset\n</pre> dataset Out[16]: <pre>&lt;xarray.Dataset&gt; Size: 299MB\nDimensions:  (time: 2, lat: 4320, lon: 8640)\nCoordinates:\n  * lat      (lat) float64 35kB 89.98 89.94 89.9 89.85 ... -89.9 -89.94 -89.98\n  * lon      (lon) float64 69kB -180.0 -179.9 -179.9 ... 179.9 179.9 180.0\n  * time     (time) datetime64[ns] 16B 1998-05-01 1998-06-01\nData variables:\n    Rrs_412  (time, lat, lon) float32 299MB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\nAttributes: (12/52)\n    Conventions:                       CF-1.7\n    Metadata_Conventions:              Unidata Dataset Discovery v1.0\n    NCO:                               netCDF Operators version 4.7.5 (Homepa...\n    cdm_data_type:                     Grid\n    comment:                           See summary attribute\n    creation_date:                     Tue Jan 31 12:15:15 2023\n    ...                                ...\n    time_coverage_end:                 20221201T000000Z\n    time_coverage_resolution:          P1M\n    time_coverage_start:               19970904T000000Z\n    title:                             ESA CCI Ocean Colour Product\n    tracking_id:                       f7c46c10-67de-460c-bdce-6de91ad32075\n    catalogue_url:                     https://catalogue.ceda.ac.uk/uuid/a078...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 2</li><li>lat: 4320</li><li>lon: 8640</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6489.98 89.94 89.9 ... -89.94 -89.98axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northvalid_max :89.97916666666667valid_min :-89.97916666666666<pre>array([ 89.979167,  89.9375  ,  89.895833, ..., -89.895833, -89.9375  ,\n       -89.979167])</pre></li><li>lon(lon)float64-180.0 -179.9 ... 179.9 180.0axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_max :179.97916666666663valid_min :-179.97916666666666<pre>array([-179.979167, -179.9375  , -179.895833, ...,  179.895833,  179.9375  ,\n        179.979167])</pre></li><li>time(time)datetime64[ns]1998-05-01 1998-06-01axis :Tstandard_name :time<pre>array(['1998-05-01T00:00:00.000000000', '1998-06-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>Rrs_412(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;ancillary_variables :Rrs_412_rmsd Rrs_412_biaslong_name :Sea surface reflectance defined as the ratio of water-leaving radiance to surface irradiance at 412 nm.parameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFV13N26standard_name :surface_ratio_of_upwelling_radiance_emerging_from_sea_water_to_downwelling_radiative_flux_in_airunits :sr-1units_nonstandard :sr^-1wavelength :412  Array   Chunk   Bytes   284.77 MiB   17.80 MiB   Shape   (2, 4320, 8640)   (1, 2160, 2160)   Dask graph   16 chunks in 3 graph layers   Data type   float32 numpy.ndarray  8640 4320 2 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.97916666666667,            89.9375,  89.89583333333333,\n        89.85416666666667,            89.8125,  89.77083333333333,\n        89.72916666666667,            89.6875,  89.64583333333333,\n        89.60416666666667,\n       ...\n       -89.60416666666666, -89.64583333333331,           -89.6875,\n       -89.72916666666666, -89.77083333333331,           -89.8125,\n       -89.85416666666666, -89.89583333333331,           -89.9375,\n       -89.97916666666666],\n      dtype='float64', name='lat', length=4320))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.97916666666666,           -179.9375, -179.89583333333334,\n       -179.85416666666666,           -179.8125, -179.77083333333334,\n       -179.72916666666666,           -179.6875, -179.64583333333334,\n       -179.60416666666666,\n       ...\n        179.60416666666663,  179.64583333333331,            179.6875,\n        179.72916666666663,  179.77083333333331,            179.8125,\n        179.85416666666663,  179.89583333333331,            179.9375,\n        179.97916666666663],\n      dtype='float64', name='lon', length=8640))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1998-05-01', '1998-06-01'], dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (52)Conventions :CF-1.7Metadata_Conventions :Unidata Dataset Discovery v1.0NCO :netCDF Operators version 4.7.5 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco)cdm_data_type :Gridcomment :See summary attributecreation_date :Tue Jan 31 12:15:15 2023creator_email :help@esa-oceancolour-cci.orgcreator_name :Plymouth Marine Laboratorycreator_url :http://esa-oceancolour-cci.orgdate_created :Tue Jan 31 12:15:15 2023geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :.04166666666666666666geospatial_lat_units :decimal degrees northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :.04166666666666666666geospatial_lon_units :decimal degrees eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0git_commit_hash :ef9b6f099ea56e816faba0c254a71b178180897fhistory :Source data were: ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221201-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221202-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221203-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221204-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221205-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221206-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221207-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221208-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221209-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221210-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221211-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221212-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221213-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221214-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221215-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221216-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221217-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221218-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221219-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221220-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221221-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221222-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221223-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221224-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221225-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221226-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221227-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221228-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221229-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221230-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221231-fv6.0.nc; netcdf_compositor_cci composites  Rrs_412, Rrs_412_bias, Rrs_443, Rrs_443_bias, Rrs_490, Rrs_490_bias, Rrs_510, Rrs_510_bias, Rrs_560, Rrs_560_bias, Rrs_665, Rrs_665_bias, adg_412, adg_412_bias, adg_443, adg_443_bias, adg_490, adg_490_bias, adg_510, adg_510_bias, adg_560, adg_560_bias, adg_665, adg_665_bias, aph_412, aph_412_bias, aph_443, aph_443_bias, aph_490, aph_490_bias, aph_510, aph_510_bias, aph_560, aph_560_bias, aph_665, aph_665_bias, atot_412, atot_443, atot_490, atot_510, atot_560, atot_665, bbp_412, bbp_443, bbp_490, bbp_510, bbp_560, bbp_665, chlor_a, chlor_a_log10_bias, kd_490, kd_490_bias, water_class1, water_class10, water_class11, water_class12, water_class13, water_class14, water_class2, water_class3, water_class4, water_class5, water_class6, water_class7, water_class8, water_class9 with --mean,  Rrs_412_rmsd, Rrs_443_rmsd, Rrs_490_rmsd, Rrs_510_rmsd, Rrs_560_rmsd, Rrs_665_rmsd, adg_412_rmsd, adg_443_rmsd, adg_490_rmsd, adg_510_rmsd, adg_560_rmsd, adg_665_rmsd, aph_412_rmsd, aph_443_rmsd, aph_490_rmsd, aph_510_rmsd, aph_560_rmsd, aph_665_rmsd, chlor_a_log10_rmsd, kd_490_rmsd with --root-mean-square, and  MERIS_nobs, MODISA_nobs, OLCI-A_nobs, OLCI-B_nobs, SeaWiFS_nobs, VIIRS_nobs, total_nobs - with --total 1675169553 Subsetted from standardised_geo/ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-202212-fv6.0.nc to only include variables MERIS_nobs_sum,MODISA_nobs_sum,OLCI-A_nobs_sum,OLCI-B_nobs_sum,Rrs_412,Rrs_412_bias,Rrs_412_rmsd,Rrs_443,Rrs_443_bias,Rrs_443_rmsd,Rrs_490,Rrs_490_bias,Rrs_490_rmsd,Rrs_510,Rrs_510_bias,Rrs_510_rmsd,Rrs_560,Rrs_560_bias,Rrs_560_rmsd,Rrs_665,Rrs_665_bias,Rrs_665_rmsd,SeaWiFS_nobs_sum,VIIRS_nobs_sum,crs,lat,lon,time,total_nobs_sum,water_class1,water_class10,water_class11,water_class12,water_class13,water_class14,water_class2,water_class3,water_class4,water_class5,water_class6,water_class7,water_class8,water_class9id :ESACCI-OC-L3S-RRS-MERGED-1M_MONTHLY_4km_GEO_PML_RRS-202212-fv6.0.ncinstitution :Plymouth Marine Laboratorykeywords :satellite,observation,ocean,ocean colourkeywords_vocabulary :nonelicense :ESA CCI Data Policy: free and open access.  When referencing, please use: Ocean Colour Climate Change Initiative dataset, Version &lt;Version Number&gt;, European Space Agency, available online at http://www.esa-oceancolour-cci.org.  We would also appreciate being notified of publications so that we can list them on the project website at http://www.esa-oceancolour-cci.org/?q=publicationsnaming_authority :uk.ac.pmlnetcdf_file_type :NETCDF4_CLASSICnumber_of_bands_used_to_classify :4number_of_files_composited :31number_of_optical_water_types :14platform :Orbview-2,Aqua,Envisat,Suomi-NPP, Sentinel-3a, Sentinel-3bprocessing_level :Level-3product_version :6.0project :Climate Change Initiative - European Space Agencyreferences :http://www.esa-oceancolour-cci.org/sensor :SeaWiFS,MODIS,MERIS,VIIRS,OLCIsensors_present : OLCIa OLCIbsource :NASA SeaWiFS  L1A and L2 R2018.0 LAC and GAC, MODIS-Aqua L1A and L2 R2018.0, MERIS L1B 3rd reprocessing inc OCL corrections, NASA VIIRS L1A and L2 R2018.0, OLCI L1Bspatial_resolution :4km nominal at equatorstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventions Version 1.7start_date :01-DEC-2022 00:00:00.000000stop_date :31-DEC-2022 23:59:00.000000summary :Data products generated by the Ocean Colour component of the European Space Agency Climate Change Initiative project. These files are monthly composites of merged sensor (MERIS, MODIS Aqua, SeaWiFS LAC &amp; GAC, VIIRS, OLCI) products.  MODIS Aqua and SeaWiFS were band-shifted and bias-corrected to MERIS bands and values using a temporally and spatially varying scheme based on the overlap years of 2003-2007.  VIIRS was band-shifted and bias-corrected in a second stage against the MODIS Rrs that had already been corrected to MERIS levels, for the overlap period 2012-2013; and at the third stage OLCI was bias corrected against already corrected MODIS, for overlap period 2016-07-01 to 2019-06-30.  VIIRS, MODIS, SeaWiFS and MERIS Rrs were derived from a combination of NASA's l2gen (for basic sensor geometry corrections, etc) and HYGEOS Polymer v4.12 (for atmospheric correction). OLCI Rrs were sourced at L1b (already geometrically corrected) and processed with polymer.  The Rrs were binned to a sinusoidal 4km level-3 grid, and later to 4km geographic projection, by Brockmann Consult's SNAP.  Derived products were generally computed with the standard algorithmsthrough SeaDAS.  QAA IOPs were derived using the standard SeaDAS algorithm but with a modified backscattering table to match that used in the bandshifting.  The final chlorophyll is a combination of OCI, OCI2, OC2 and OCx, depending on the water class memberships.  Uncertainty estimates were added using the fuzzy water classifier and uncertainty estimation algorithm of Tim Moore as documented in Jackson et al (2017). and updated accorsing to Jackson et al. (in prep).time_coverage_duration :P1Mtime_coverage_end :20221201T000000Ztime_coverage_resolution :P1Mtime_coverage_start :19970904T000000Ztitle :ESA CCI Ocean Colour Producttracking_id :f7c46c10-67de-460c-bdce-6de91ad32075catalogue_url :https://catalogue.ceda.ac.uk/uuid/a0782135bcd04d77a1dae4aa71fba47c</li></ul> In\u00a0[17]: Copied! <pre># we need to create a mapper pointing to the existing cube, stored in the team s3 storage\nmapper = fs.get_mapper(f\"{target_bucket_path}/{output_id}\")\n</pre> # we need to create a mapper pointing to the existing cube, stored in the team s3 storage mapper = fs.get_mapper(f\"{target_bucket_path}/{output_id}\") <p>Now we can append the new dataset to the existing datacube:</p> In\u00a0[18]: Copied! <pre>dataset.to_zarr(mapper, mode=\"a\", append_dim=\"time\", consolidated=True)\n</pre> dataset.to_zarr(mapper, mode=\"a\", append_dim=\"time\", consolidated=True) Out[18]: <pre>&lt;xarray.backends.zarr.ZarrStore at 0x7f211d077240&gt;</pre> In\u00a0[19]: Copied! <pre>list(team_store.get_data_ids())\n</pre> list(team_store.get_data_ids()) Out[19]: <pre>['SST.levels',\n 'amazonas_v8.zarr',\n 'amazonas_v9.zarr',\n 'noise_trajectory.zarr',\n 'ocean_color.zarr',\n 'reanalysis-era5-single-levels-monthly-means-subset-2001-2010_TMH.zarr',\n 's2-demo-cube.zarr']</pre> <p>Check if the cube now contains the expected time stamps:</p> In\u00a0[20]: Copied! <pre>ds = team_store.open_data(output_id)\n</pre> ds = team_store.open_data(output_id) <p>As expected, we now find all four days in the datacube. Please note: you are responsible for passing the time stamps in the right order. If you do not, this might cause trouble later on and you will need to reorder the time dimension.</p> In\u00a0[21]: Copied! <pre>ds\n</pre> ds Out[21]: <pre>&lt;xarray.Dataset&gt; Size: 597MB\nDimensions:  (time: 4, lat: 4320, lon: 8640)\nCoordinates:\n  * lat      (lat) float64 35kB 89.98 89.94 89.9 89.85 ... -89.9 -89.94 -89.98\n  * lon      (lon) float64 69kB -180.0 -179.9 -179.9 ... 179.9 179.9 180.0\n  * time     (time) datetime64[ns] 32B 1998-03-01 1998-04-01 ... 1998-06-01\nData variables:\n    Rrs_412  (time, lat, lon) float32 597MB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\nAttributes: (12/52)\n    Conventions:                       CF-1.7\n    Metadata_Conventions:              Unidata Dataset Discovery v1.0\n    NCO:                               netCDF Operators version 4.7.5 (Homepa...\n    catalogue_url:                     https://catalogue.ceda.ac.uk/uuid/a078...\n    cdm_data_type:                     Grid\n    comment:                           See summary attribute\n    ...                                ...\n    time_coverage_duration:            P1M\n    time_coverage_end:                 20221201T000000Z\n    time_coverage_resolution:          P1M\n    time_coverage_start:               19970904T000000Z\n    title:                             ESA CCI Ocean Colour Product\n    tracking_id:                       f7c46c10-67de-460c-bdce-6de91ad32075</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 4</li><li>lat: 4320</li><li>lon: 8640</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6489.98 89.94 89.9 ... -89.94 -89.98axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northvalid_max :89.97916666666667valid_min :-89.97916666666666<pre>array([ 89.979167,  89.9375  ,  89.895833, ..., -89.895833, -89.9375  ,\n       -89.979167])</pre></li><li>lon(lon)float64-180.0 -179.9 ... 179.9 180.0axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_max :179.97916666666663valid_min :-179.97916666666666<pre>array([-179.979167, -179.9375  , -179.895833, ...,  179.895833,  179.9375  ,\n        179.979167])</pre></li><li>time(time)datetime64[ns]1998-03-01 ... 1998-06-01axis :Tstandard_name :time<pre>array(['1998-03-01T00:00:00.000000000', '1998-04-01T00:00:00.000000000',\n       '1998-05-01T00:00:00.000000000', '1998-06-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>Rrs_412(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;ancillary_variables :Rrs_412_rmsd Rrs_412_biaslong_name :Sea surface reflectance defined as the ratio of water-leaving radiance to surface irradiance at 412 nm.parameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFV13N26standard_name :surface_ratio_of_upwelling_radiance_emerging_from_sea_water_to_downwelling_radiative_flux_in_airunits :sr-1units_nonstandard :sr^-1wavelength :412  Array   Chunk   Bytes   569.53 MiB   17.80 MiB   Shape   (4, 4320, 8640)   (1, 2160, 2160)   Dask graph   32 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 4 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.97916666666667,            89.9375,  89.89583333333333,\n        89.85416666666667,            89.8125,  89.77083333333333,\n        89.72916666666667,            89.6875,  89.64583333333333,\n        89.60416666666667,\n       ...\n       -89.60416666666666, -89.64583333333331,           -89.6875,\n       -89.72916666666666, -89.77083333333331,           -89.8125,\n       -89.85416666666666, -89.89583333333331,           -89.9375,\n       -89.97916666666666],\n      dtype='float64', name='lat', length=4320))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.97916666666666,           -179.9375, -179.89583333333334,\n       -179.85416666666666,           -179.8125, -179.77083333333334,\n       -179.72916666666666,           -179.6875, -179.64583333333334,\n       -179.60416666666666,\n       ...\n        179.60416666666663,  179.64583333333331,            179.6875,\n        179.72916666666663,  179.77083333333331,            179.8125,\n        179.85416666666663,  179.89583333333331,            179.9375,\n        179.97916666666663],\n      dtype='float64', name='lon', length=8640))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1998-03-01', '1998-04-01', '1998-05-01', '1998-06-01'], dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (52)Conventions :CF-1.7Metadata_Conventions :Unidata Dataset Discovery v1.0NCO :netCDF Operators version 4.7.5 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco)catalogue_url :https://catalogue.ceda.ac.uk/uuid/a0782135bcd04d77a1dae4aa71fba47ccdm_data_type :Gridcomment :See summary attributecreation_date :Tue Jan 31 12:15:15 2023creator_email :help@esa-oceancolour-cci.orgcreator_name :Plymouth Marine Laboratorycreator_url :http://esa-oceancolour-cci.orgdate_created :Tue Jan 31 12:15:15 2023geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :.04166666666666666666geospatial_lat_units :decimal degrees northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :.04166666666666666666geospatial_lon_units :decimal degrees eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0git_commit_hash :ef9b6f099ea56e816faba0c254a71b178180897fhistory :Source data were: ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221201-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221202-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221203-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221204-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221205-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221206-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221207-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221208-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221209-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221210-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221211-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221212-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221213-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221214-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221215-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221216-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221217-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221218-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221219-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221220-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221221-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221222-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221223-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221224-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221225-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221226-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221227-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221228-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221229-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221230-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221231-fv6.0.nc; netcdf_compositor_cci composites  Rrs_412, Rrs_412_bias, Rrs_443, Rrs_443_bias, Rrs_490, Rrs_490_bias, Rrs_510, Rrs_510_bias, Rrs_560, Rrs_560_bias, Rrs_665, Rrs_665_bias, adg_412, adg_412_bias, adg_443, adg_443_bias, adg_490, adg_490_bias, adg_510, adg_510_bias, adg_560, adg_560_bias, adg_665, adg_665_bias, aph_412, aph_412_bias, aph_443, aph_443_bias, aph_490, aph_490_bias, aph_510, aph_510_bias, aph_560, aph_560_bias, aph_665, aph_665_bias, atot_412, atot_443, atot_490, atot_510, atot_560, atot_665, bbp_412, bbp_443, bbp_490, bbp_510, bbp_560, bbp_665, chlor_a, chlor_a_log10_bias, kd_490, kd_490_bias, water_class1, water_class10, water_class11, water_class12, water_class13, water_class14, water_class2, water_class3, water_class4, water_class5, water_class6, water_class7, water_class8, water_class9 with --mean,  Rrs_412_rmsd, Rrs_443_rmsd, Rrs_490_rmsd, Rrs_510_rmsd, Rrs_560_rmsd, Rrs_665_rmsd, adg_412_rmsd, adg_443_rmsd, adg_490_rmsd, adg_510_rmsd, adg_560_rmsd, adg_665_rmsd, aph_412_rmsd, aph_443_rmsd, aph_490_rmsd, aph_510_rmsd, aph_560_rmsd, aph_665_rmsd, chlor_a_log10_rmsd, kd_490_rmsd with --root-mean-square, and  MERIS_nobs, MODISA_nobs, OLCI-A_nobs, OLCI-B_nobs, SeaWiFS_nobs, VIIRS_nobs, total_nobs - with --total 1675169553 Subsetted from standardised_geo/ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-202212-fv6.0.nc to only include variables MERIS_nobs_sum,MODISA_nobs_sum,OLCI-A_nobs_sum,OLCI-B_nobs_sum,Rrs_412,Rrs_412_bias,Rrs_412_rmsd,Rrs_443,Rrs_443_bias,Rrs_443_rmsd,Rrs_490,Rrs_490_bias,Rrs_490_rmsd,Rrs_510,Rrs_510_bias,Rrs_510_rmsd,Rrs_560,Rrs_560_bias,Rrs_560_rmsd,Rrs_665,Rrs_665_bias,Rrs_665_rmsd,SeaWiFS_nobs_sum,VIIRS_nobs_sum,crs,lat,lon,time,total_nobs_sum,water_class1,water_class10,water_class11,water_class12,water_class13,water_class14,water_class2,water_class3,water_class4,water_class5,water_class6,water_class7,water_class8,water_class9id :ESACCI-OC-L3S-RRS-MERGED-1M_MONTHLY_4km_GEO_PML_RRS-202212-fv6.0.ncinstitution :Plymouth Marine Laboratorykeywords :satellite,observation,ocean,ocean colourkeywords_vocabulary :nonelicense :ESA CCI Data Policy: free and open access.  When referencing, please use: Ocean Colour Climate Change Initiative dataset, Version &lt;Version Number&gt;, European Space Agency, available online at http://www.esa-oceancolour-cci.org.  We would also appreciate being notified of publications so that we can list them on the project website at http://www.esa-oceancolour-cci.org/?q=publicationsnaming_authority :uk.ac.pmlnetcdf_file_type :NETCDF4_CLASSICnumber_of_bands_used_to_classify :4number_of_files_composited :31number_of_optical_water_types :14platform :Orbview-2,Aqua,Envisat,Suomi-NPP, Sentinel-3a, Sentinel-3bprocessing_level :Level-3product_version :6.0project :Climate Change Initiative - European Space Agencyreferences :http://www.esa-oceancolour-cci.org/sensor :SeaWiFS,MODIS,MERIS,VIIRS,OLCIsensors_present : OLCIa OLCIbsource :NASA SeaWiFS  L1A and L2 R2018.0 LAC and GAC, MODIS-Aqua L1A and L2 R2018.0, MERIS L1B 3rd reprocessing inc OCL corrections, NASA VIIRS L1A and L2 R2018.0, OLCI L1Bspatial_resolution :4km nominal at equatorstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventions Version 1.7start_date :01-DEC-2022 00:00:00.000000stop_date :31-DEC-2022 23:59:00.000000summary :Data products generated by the Ocean Colour component of the European Space Agency Climate Change Initiative project. These files are monthly composites of merged sensor (MERIS, MODIS Aqua, SeaWiFS LAC &amp; GAC, VIIRS, OLCI) products.  MODIS Aqua and SeaWiFS were band-shifted and bias-corrected to MERIS bands and values using a temporally and spatially varying scheme based on the overlap years of 2003-2007.  VIIRS was band-shifted and bias-corrected in a second stage against the MODIS Rrs that had already been corrected to MERIS levels, for the overlap period 2012-2013; and at the third stage OLCI was bias corrected against already corrected MODIS, for overlap period 2016-07-01 to 2019-06-30.  VIIRS, MODIS, SeaWiFS and MERIS Rrs were derived from a combination of NASA's l2gen (for basic sensor geometry corrections, etc) and HYGEOS Polymer v4.12 (for atmospheric correction). OLCI Rrs were sourced at L1b (already geometrically corrected) and processed with polymer.  The Rrs were binned to a sinusoidal 4km level-3 grid, and later to 4km geographic projection, by Brockmann Consult's SNAP.  Derived products were generally computed with the standard algorithmsthrough SeaDAS.  QAA IOPs were derived using the standard SeaDAS algorithm but with a modified backscattering table to match that used in the bandshifting.  The final chlorophyll is a combination of OCI, OCI2, OC2 and OCx, depending on the water class memberships.  Uncertainty estimates were added using the fuzzy water classifier and uncertainty estimation algorithm of Tim Moore as documented in Jackson et al (2017). and updated accorsing to Jackson et al. (in prep).time_coverage_duration :P1Mtime_coverage_end :20221201T000000Ztime_coverage_resolution :P1Mtime_coverage_start :19970904T000000Ztitle :ESA CCI Ocean Colour Producttracking_id :f7c46c10-67de-460c-bdce-6de91ad32075</li></ul> <p>It is also possible to append a variable with the same dimensions to an existing datacube:</p> In\u00a0[22]: Copied! <pre>dataset = open_zarrstore(\n    \"ESACCI-OC-L3S-RRS-MERGED-1M_MONTHLY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr\",\n    time_range=[datetime.datetime(1998, 3, 1), datetime.datetime(1998, 6, 30)],\n    variables=[\"Rrs_443\"],\n)\n</pre> dataset = open_zarrstore(     \"ESACCI-OC-L3S-RRS-MERGED-1M_MONTHLY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr\",     time_range=[datetime.datetime(1998, 3, 1), datetime.datetime(1998, 6, 30)],     variables=[\"Rrs_443\"], ) In\u00a0[23]: Copied! <pre>dataset\n</pre> dataset Out[23]: <pre>&lt;xarray.Dataset&gt; Size: 597MB\nDimensions:  (time: 4, lat: 4320, lon: 8640)\nCoordinates:\n  * lat      (lat) float64 35kB 89.98 89.94 89.9 89.85 ... -89.9 -89.94 -89.98\n  * lon      (lon) float64 69kB -180.0 -179.9 -179.9 ... 179.9 179.9 180.0\n  * time     (time) datetime64[ns] 32B 1998-03-01 1998-04-01 ... 1998-06-01\nData variables:\n    Rrs_443  (time, lat, lon) float32 597MB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\nAttributes: (12/52)\n    Conventions:                       CF-1.7\n    Metadata_Conventions:              Unidata Dataset Discovery v1.0\n    NCO:                               netCDF Operators version 4.7.5 (Homepa...\n    cdm_data_type:                     Grid\n    comment:                           See summary attribute\n    creation_date:                     Tue Jan 31 12:15:15 2023\n    ...                                ...\n    time_coverage_end:                 20221201T000000Z\n    time_coverage_resolution:          P1M\n    time_coverage_start:               19970904T000000Z\n    title:                             ESA CCI Ocean Colour Product\n    tracking_id:                       f7c46c10-67de-460c-bdce-6de91ad32075\n    catalogue_url:                     https://catalogue.ceda.ac.uk/uuid/a078...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 4</li><li>lat: 4320</li><li>lon: 8640</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6489.98 89.94 89.9 ... -89.94 -89.98axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northvalid_max :89.97916666666667valid_min :-89.97916666666666<pre>array([ 89.979167,  89.9375  ,  89.895833, ..., -89.895833, -89.9375  ,\n       -89.979167])</pre></li><li>lon(lon)float64-180.0 -179.9 ... 179.9 180.0axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_max :179.97916666666663valid_min :-179.97916666666666<pre>array([-179.979167, -179.9375  , -179.895833, ...,  179.895833,  179.9375  ,\n        179.979167])</pre></li><li>time(time)datetime64[ns]1998-03-01 ... 1998-06-01axis :Tstandard_name :time<pre>array(['1998-03-01T00:00:00.000000000', '1998-04-01T00:00:00.000000000',\n       '1998-05-01T00:00:00.000000000', '1998-06-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>Rrs_443(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;ancillary_variables :Rrs_443_rmsd Rrs_443_biaslong_name :Sea surface reflectance defined as the ratio of water-leaving radiance to surface irradiance at 443 nm.parameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFV13N26standard_name :surface_ratio_of_upwelling_radiance_emerging_from_sea_water_to_downwelling_radiative_flux_in_airunits :sr-1units_nonstandard :sr^-1wavelength :443  Array   Chunk   Bytes   569.53 MiB   17.80 MiB   Shape   (4, 4320, 8640)   (1, 2160, 2160)   Dask graph   32 chunks in 3 graph layers   Data type   float32 numpy.ndarray  8640 4320 4 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.97916666666667,            89.9375,  89.89583333333333,\n        89.85416666666667,            89.8125,  89.77083333333333,\n        89.72916666666667,            89.6875,  89.64583333333333,\n        89.60416666666667,\n       ...\n       -89.60416666666666, -89.64583333333331,           -89.6875,\n       -89.72916666666666, -89.77083333333331,           -89.8125,\n       -89.85416666666666, -89.89583333333331,           -89.9375,\n       -89.97916666666666],\n      dtype='float64', name='lat', length=4320))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.97916666666666,           -179.9375, -179.89583333333334,\n       -179.85416666666666,           -179.8125, -179.77083333333334,\n       -179.72916666666666,           -179.6875, -179.64583333333334,\n       -179.60416666666666,\n       ...\n        179.60416666666663,  179.64583333333331,            179.6875,\n        179.72916666666663,  179.77083333333331,            179.8125,\n        179.85416666666663,  179.89583333333331,            179.9375,\n        179.97916666666663],\n      dtype='float64', name='lon', length=8640))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1998-03-01', '1998-04-01', '1998-05-01', '1998-06-01'], dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (52)Conventions :CF-1.7Metadata_Conventions :Unidata Dataset Discovery v1.0NCO :netCDF Operators version 4.7.5 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco)cdm_data_type :Gridcomment :See summary attributecreation_date :Tue Jan 31 12:15:15 2023creator_email :help@esa-oceancolour-cci.orgcreator_name :Plymouth Marine Laboratorycreator_url :http://esa-oceancolour-cci.orgdate_created :Tue Jan 31 12:15:15 2023geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :.04166666666666666666geospatial_lat_units :decimal degrees northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :.04166666666666666666geospatial_lon_units :decimal degrees eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0git_commit_hash :ef9b6f099ea56e816faba0c254a71b178180897fhistory :Source data were: ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221201-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221202-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221203-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221204-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221205-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221206-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221207-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221208-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221209-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221210-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221211-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221212-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221213-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221214-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221215-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221216-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221217-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221218-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221219-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221220-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221221-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221222-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221223-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221224-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221225-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221226-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221227-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221228-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221229-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221230-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221231-fv6.0.nc; netcdf_compositor_cci composites  Rrs_412, Rrs_412_bias, Rrs_443, Rrs_443_bias, Rrs_490, Rrs_490_bias, Rrs_510, Rrs_510_bias, Rrs_560, Rrs_560_bias, Rrs_665, Rrs_665_bias, adg_412, adg_412_bias, adg_443, adg_443_bias, adg_490, adg_490_bias, adg_510, adg_510_bias, adg_560, adg_560_bias, adg_665, adg_665_bias, aph_412, aph_412_bias, aph_443, aph_443_bias, aph_490, aph_490_bias, aph_510, aph_510_bias, aph_560, aph_560_bias, aph_665, aph_665_bias, atot_412, atot_443, atot_490, atot_510, atot_560, atot_665, bbp_412, bbp_443, bbp_490, bbp_510, bbp_560, bbp_665, chlor_a, chlor_a_log10_bias, kd_490, kd_490_bias, water_class1, water_class10, water_class11, water_class12, water_class13, water_class14, water_class2, water_class3, water_class4, water_class5, water_class6, water_class7, water_class8, water_class9 with --mean,  Rrs_412_rmsd, Rrs_443_rmsd, Rrs_490_rmsd, Rrs_510_rmsd, Rrs_560_rmsd, Rrs_665_rmsd, adg_412_rmsd, adg_443_rmsd, adg_490_rmsd, adg_510_rmsd, adg_560_rmsd, adg_665_rmsd, aph_412_rmsd, aph_443_rmsd, aph_490_rmsd, aph_510_rmsd, aph_560_rmsd, aph_665_rmsd, chlor_a_log10_rmsd, kd_490_rmsd with --root-mean-square, and  MERIS_nobs, MODISA_nobs, OLCI-A_nobs, OLCI-B_nobs, SeaWiFS_nobs, VIIRS_nobs, total_nobs - with --total 1675169553 Subsetted from standardised_geo/ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-202212-fv6.0.nc to only include variables MERIS_nobs_sum,MODISA_nobs_sum,OLCI-A_nobs_sum,OLCI-B_nobs_sum,Rrs_412,Rrs_412_bias,Rrs_412_rmsd,Rrs_443,Rrs_443_bias,Rrs_443_rmsd,Rrs_490,Rrs_490_bias,Rrs_490_rmsd,Rrs_510,Rrs_510_bias,Rrs_510_rmsd,Rrs_560,Rrs_560_bias,Rrs_560_rmsd,Rrs_665,Rrs_665_bias,Rrs_665_rmsd,SeaWiFS_nobs_sum,VIIRS_nobs_sum,crs,lat,lon,time,total_nobs_sum,water_class1,water_class10,water_class11,water_class12,water_class13,water_class14,water_class2,water_class3,water_class4,water_class5,water_class6,water_class7,water_class8,water_class9id :ESACCI-OC-L3S-RRS-MERGED-1M_MONTHLY_4km_GEO_PML_RRS-202212-fv6.0.ncinstitution :Plymouth Marine Laboratorykeywords :satellite,observation,ocean,ocean colourkeywords_vocabulary :nonelicense :ESA CCI Data Policy: free and open access.  When referencing, please use: Ocean Colour Climate Change Initiative dataset, Version &lt;Version Number&gt;, European Space Agency, available online at http://www.esa-oceancolour-cci.org.  We would also appreciate being notified of publications so that we can list them on the project website at http://www.esa-oceancolour-cci.org/?q=publicationsnaming_authority :uk.ac.pmlnetcdf_file_type :NETCDF4_CLASSICnumber_of_bands_used_to_classify :4number_of_files_composited :31number_of_optical_water_types :14platform :Orbview-2,Aqua,Envisat,Suomi-NPP, Sentinel-3a, Sentinel-3bprocessing_level :Level-3product_version :6.0project :Climate Change Initiative - European Space Agencyreferences :http://www.esa-oceancolour-cci.org/sensor :SeaWiFS,MODIS,MERIS,VIIRS,OLCIsensors_present : OLCIa OLCIbsource :NASA SeaWiFS  L1A and L2 R2018.0 LAC and GAC, MODIS-Aqua L1A and L2 R2018.0, MERIS L1B 3rd reprocessing inc OCL corrections, NASA VIIRS L1A and L2 R2018.0, OLCI L1Bspatial_resolution :4km nominal at equatorstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventions Version 1.7start_date :01-DEC-2022 00:00:00.000000stop_date :31-DEC-2022 23:59:00.000000summary :Data products generated by the Ocean Colour component of the European Space Agency Climate Change Initiative project. These files are monthly composites of merged sensor (MERIS, MODIS Aqua, SeaWiFS LAC &amp; GAC, VIIRS, OLCI) products.  MODIS Aqua and SeaWiFS were band-shifted and bias-corrected to MERIS bands and values using a temporally and spatially varying scheme based on the overlap years of 2003-2007.  VIIRS was band-shifted and bias-corrected in a second stage against the MODIS Rrs that had already been corrected to MERIS levels, for the overlap period 2012-2013; and at the third stage OLCI was bias corrected against already corrected MODIS, for overlap period 2016-07-01 to 2019-06-30.  VIIRS, MODIS, SeaWiFS and MERIS Rrs were derived from a combination of NASA's l2gen (for basic sensor geometry corrections, etc) and HYGEOS Polymer v4.12 (for atmospheric correction). OLCI Rrs were sourced at L1b (already geometrically corrected) and processed with polymer.  The Rrs were binned to a sinusoidal 4km level-3 grid, and later to 4km geographic projection, by Brockmann Consult's SNAP.  Derived products were generally computed with the standard algorithmsthrough SeaDAS.  QAA IOPs were derived using the standard SeaDAS algorithm but with a modified backscattering table to match that used in the bandshifting.  The final chlorophyll is a combination of OCI, OCI2, OC2 and OCx, depending on the water class memberships.  Uncertainty estimates were added using the fuzzy water classifier and uncertainty estimation algorithm of Tim Moore as documented in Jackson et al (2017). and updated accorsing to Jackson et al. (in prep).time_coverage_duration :P1Mtime_coverage_end :20221201T000000Ztime_coverage_resolution :P1Mtime_coverage_start :19970904T000000Ztitle :ESA CCI Ocean Colour Producttracking_id :f7c46c10-67de-460c-bdce-6de91ad32075catalogue_url :https://catalogue.ceda.ac.uk/uuid/a0782135bcd04d77a1dae4aa71fba47c</li></ul> <p>Now we can append the new dataset with the additional variable to the existing datacube:</p> In\u00a0[24]: Copied! <pre>dataset.to_zarr(mapper, mode=\"a\", consolidated=True)\n</pre> dataset.to_zarr(mapper, mode=\"a\", consolidated=True) Out[24]: <pre>&lt;xarray.backends.zarr.ZarrStore at 0x7f211d0aadc0&gt;</pre> <p>Check if the cube now contains the expected new variable:</p> In\u00a0[25]: Copied! <pre>ds = team_store.open_data(output_id)\n</pre> ds = team_store.open_data(output_id) In\u00a0[26]: Copied! <pre>ds\n</pre> ds Out[26]: <pre>&lt;xarray.Dataset&gt; Size: 1GB\nDimensions:  (time: 4, lat: 4320, lon: 8640)\nCoordinates:\n  * lat      (lat) float64 35kB 89.98 89.94 89.9 89.85 ... -89.9 -89.94 -89.98\n  * lon      (lon) float64 69kB -180.0 -179.9 -179.9 ... 179.9 179.9 180.0\n  * time     (time) datetime64[ns] 32B 1998-03-01 1998-04-01 ... 1998-06-01\nData variables:\n    Rrs_412  (time, lat, lon) float32 597MB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    Rrs_443  (time, lat, lon) float32 597MB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\nAttributes: (12/52)\n    Conventions:                       CF-1.7\n    Metadata_Conventions:              Unidata Dataset Discovery v1.0\n    NCO:                               netCDF Operators version 4.7.5 (Homepa...\n    catalogue_url:                     https://catalogue.ceda.ac.uk/uuid/a078...\n    cdm_data_type:                     Grid\n    comment:                           See summary attribute\n    ...                                ...\n    time_coverage_duration:            P1M\n    time_coverage_end:                 20221201T000000Z\n    time_coverage_resolution:          P1M\n    time_coverage_start:               19970904T000000Z\n    title:                             ESA CCI Ocean Colour Product\n    tracking_id:                       f7c46c10-67de-460c-bdce-6de91ad32075</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 4</li><li>lat: 4320</li><li>lon: 8640</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6489.98 89.94 89.9 ... -89.94 -89.98axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northvalid_max :89.97916666666667valid_min :-89.97916666666666<pre>array([ 89.979167,  89.9375  ,  89.895833, ..., -89.895833, -89.9375  ,\n       -89.979167])</pre></li><li>lon(lon)float64-180.0 -179.9 ... 179.9 180.0axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_max :179.97916666666663valid_min :-179.97916666666666<pre>array([-179.979167, -179.9375  , -179.895833, ...,  179.895833,  179.9375  ,\n        179.979167])</pre></li><li>time(time)datetime64[ns]1998-03-01 ... 1998-06-01axis :Tstandard_name :time<pre>array(['1998-03-01T00:00:00.000000000', '1998-04-01T00:00:00.000000000',\n       '1998-05-01T00:00:00.000000000', '1998-06-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (2)<ul><li>Rrs_412(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;ancillary_variables :Rrs_412_rmsd Rrs_412_biaslong_name :Sea surface reflectance defined as the ratio of water-leaving radiance to surface irradiance at 412 nm.parameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFV13N26standard_name :surface_ratio_of_upwelling_radiance_emerging_from_sea_water_to_downwelling_radiative_flux_in_airunits :sr-1units_nonstandard :sr^-1wavelength :412  Array   Chunk   Bytes   569.53 MiB   17.80 MiB   Shape   (4, 4320, 8640)   (1, 2160, 2160)   Dask graph   32 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 4 </li><li>Rrs_443(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;ancillary_variables :Rrs_443_rmsd Rrs_443_biaslong_name :Sea surface reflectance defined as the ratio of water-leaving radiance to surface irradiance at 443 nm.parameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFV13N26standard_name :surface_ratio_of_upwelling_radiance_emerging_from_sea_water_to_downwelling_radiative_flux_in_airunits :sr-1units_nonstandard :sr^-1wavelength :443  Array   Chunk   Bytes   569.53 MiB   17.80 MiB   Shape   (4, 4320, 8640)   (1, 2160, 2160)   Dask graph   32 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 4 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.97916666666667,            89.9375,  89.89583333333333,\n        89.85416666666667,            89.8125,  89.77083333333333,\n        89.72916666666667,            89.6875,  89.64583333333333,\n        89.60416666666667,\n       ...\n       -89.60416666666666, -89.64583333333331,           -89.6875,\n       -89.72916666666666, -89.77083333333331,           -89.8125,\n       -89.85416666666666, -89.89583333333331,           -89.9375,\n       -89.97916666666666],\n      dtype='float64', name='lat', length=4320))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.97916666666666,           -179.9375, -179.89583333333334,\n       -179.85416666666666,           -179.8125, -179.77083333333334,\n       -179.72916666666666,           -179.6875, -179.64583333333334,\n       -179.60416666666666,\n       ...\n        179.60416666666663,  179.64583333333331,            179.6875,\n        179.72916666666663,  179.77083333333331,            179.8125,\n        179.85416666666663,  179.89583333333331,            179.9375,\n        179.97916666666663],\n      dtype='float64', name='lon', length=8640))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1998-03-01', '1998-04-01', '1998-05-01', '1998-06-01'], dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (52)Conventions :CF-1.7Metadata_Conventions :Unidata Dataset Discovery v1.0NCO :netCDF Operators version 4.7.5 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco)catalogue_url :https://catalogue.ceda.ac.uk/uuid/a0782135bcd04d77a1dae4aa71fba47ccdm_data_type :Gridcomment :See summary attributecreation_date :Tue Jan 31 12:15:15 2023creator_email :help@esa-oceancolour-cci.orgcreator_name :Plymouth Marine Laboratorycreator_url :http://esa-oceancolour-cci.orgdate_created :Tue Jan 31 12:15:15 2023geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :.04166666666666666666geospatial_lat_units :decimal degrees northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :.04166666666666666666geospatial_lon_units :decimal degrees eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0git_commit_hash :ef9b6f099ea56e816faba0c254a71b178180897fhistory :Source data were: ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221201-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221202-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221203-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221204-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221205-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221206-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221207-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221208-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221209-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221210-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221211-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221212-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221213-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221214-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221215-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221216-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221217-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221218-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221219-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221220-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221221-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221222-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221223-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221224-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221225-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221226-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221227-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221228-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221229-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221230-fv6.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20221231-fv6.0.nc; netcdf_compositor_cci composites  Rrs_412, Rrs_412_bias, Rrs_443, Rrs_443_bias, Rrs_490, Rrs_490_bias, Rrs_510, Rrs_510_bias, Rrs_560, Rrs_560_bias, Rrs_665, Rrs_665_bias, adg_412, adg_412_bias, adg_443, adg_443_bias, adg_490, adg_490_bias, adg_510, adg_510_bias, adg_560, adg_560_bias, adg_665, adg_665_bias, aph_412, aph_412_bias, aph_443, aph_443_bias, aph_490, aph_490_bias, aph_510, aph_510_bias, aph_560, aph_560_bias, aph_665, aph_665_bias, atot_412, atot_443, atot_490, atot_510, atot_560, atot_665, bbp_412, bbp_443, bbp_490, bbp_510, bbp_560, bbp_665, chlor_a, chlor_a_log10_bias, kd_490, kd_490_bias, water_class1, water_class10, water_class11, water_class12, water_class13, water_class14, water_class2, water_class3, water_class4, water_class5, water_class6, water_class7, water_class8, water_class9 with --mean,  Rrs_412_rmsd, Rrs_443_rmsd, Rrs_490_rmsd, Rrs_510_rmsd, Rrs_560_rmsd, Rrs_665_rmsd, adg_412_rmsd, adg_443_rmsd, adg_490_rmsd, adg_510_rmsd, adg_560_rmsd, adg_665_rmsd, aph_412_rmsd, aph_443_rmsd, aph_490_rmsd, aph_510_rmsd, aph_560_rmsd, aph_665_rmsd, chlor_a_log10_rmsd, kd_490_rmsd with --root-mean-square, and  MERIS_nobs, MODISA_nobs, OLCI-A_nobs, OLCI-B_nobs, SeaWiFS_nobs, VIIRS_nobs, total_nobs - with --total 1675169553 Subsetted from standardised_geo/ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-202212-fv6.0.nc to only include variables MERIS_nobs_sum,MODISA_nobs_sum,OLCI-A_nobs_sum,OLCI-B_nobs_sum,Rrs_412,Rrs_412_bias,Rrs_412_rmsd,Rrs_443,Rrs_443_bias,Rrs_443_rmsd,Rrs_490,Rrs_490_bias,Rrs_490_rmsd,Rrs_510,Rrs_510_bias,Rrs_510_rmsd,Rrs_560,Rrs_560_bias,Rrs_560_rmsd,Rrs_665,Rrs_665_bias,Rrs_665_rmsd,SeaWiFS_nobs_sum,VIIRS_nobs_sum,crs,lat,lon,time,total_nobs_sum,water_class1,water_class10,water_class11,water_class12,water_class13,water_class14,water_class2,water_class3,water_class4,water_class5,water_class6,water_class7,water_class8,water_class9id :ESACCI-OC-L3S-RRS-MERGED-1M_MONTHLY_4km_GEO_PML_RRS-202212-fv6.0.ncinstitution :Plymouth Marine Laboratorykeywords :satellite,observation,ocean,ocean colourkeywords_vocabulary :nonelicense :ESA CCI Data Policy: free and open access.  When referencing, please use: Ocean Colour Climate Change Initiative dataset, Version &lt;Version Number&gt;, European Space Agency, available online at http://www.esa-oceancolour-cci.org.  We would also appreciate being notified of publications so that we can list them on the project website at http://www.esa-oceancolour-cci.org/?q=publicationsnaming_authority :uk.ac.pmlnetcdf_file_type :NETCDF4_CLASSICnumber_of_bands_used_to_classify :4number_of_files_composited :31number_of_optical_water_types :14platform :Orbview-2,Aqua,Envisat,Suomi-NPP, Sentinel-3a, Sentinel-3bprocessing_level :Level-3product_version :6.0project :Climate Change Initiative - European Space Agencyreferences :http://www.esa-oceancolour-cci.org/sensor :SeaWiFS,MODIS,MERIS,VIIRS,OLCIsensors_present : OLCIa OLCIbsource :NASA SeaWiFS  L1A and L2 R2018.0 LAC and GAC, MODIS-Aqua L1A and L2 R2018.0, MERIS L1B 3rd reprocessing inc OCL corrections, NASA VIIRS L1A and L2 R2018.0, OLCI L1Bspatial_resolution :4km nominal at equatorstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventions Version 1.7start_date :01-DEC-2022 00:00:00.000000stop_date :31-DEC-2022 23:59:00.000000summary :Data products generated by the Ocean Colour component of the European Space Agency Climate Change Initiative project. These files are monthly composites of merged sensor (MERIS, MODIS Aqua, SeaWiFS LAC &amp; GAC, VIIRS, OLCI) products.  MODIS Aqua and SeaWiFS were band-shifted and bias-corrected to MERIS bands and values using a temporally and spatially varying scheme based on the overlap years of 2003-2007.  VIIRS was band-shifted and bias-corrected in a second stage against the MODIS Rrs that had already been corrected to MERIS levels, for the overlap period 2012-2013; and at the third stage OLCI was bias corrected against already corrected MODIS, for overlap period 2016-07-01 to 2019-06-30.  VIIRS, MODIS, SeaWiFS and MERIS Rrs were derived from a combination of NASA's l2gen (for basic sensor geometry corrections, etc) and HYGEOS Polymer v4.12 (for atmospheric correction). OLCI Rrs were sourced at L1b (already geometrically corrected) and processed with polymer.  The Rrs were binned to a sinusoidal 4km level-3 grid, and later to 4km geographic projection, by Brockmann Consult's SNAP.  Derived products were generally computed with the standard algorithmsthrough SeaDAS.  QAA IOPs were derived using the standard SeaDAS algorithm but with a modified backscattering table to match that used in the bandshifting.  The final chlorophyll is a combination of OCI, OCI2, OC2 and OCx, depending on the water class memberships.  Uncertainty estimates were added using the fuzzy water classifier and uncertainty estimation algorithm of Tim Moore as documented in Jackson et al (2017). and updated accorsing to Jackson et al. (in prep).time_coverage_duration :P1Mtime_coverage_end :20221201T000000Ztime_coverage_resolution :P1Mtime_coverage_start :19970904T000000Ztitle :ESA CCI Ocean Colour Producttracking_id :f7c46c10-67de-460c-bdce-6de91ad32075</li></ul> <p>In other use cases the chunking could be different for the new variable. You can ensure the desired chunking before writing the data to a cube by specifying it in the encoding. To learn about chunking, head over to the example notebook 10 - Chunking of Datasets</p> <p>Alright, now you know how to append new time stamps or variables to an existing cube - let's clean up our example :)</p> In\u00a0[27]: Copied! <pre>team_store.delete_data(output_id)\n</pre> team_store.delete_data(output_id) In\u00a0[28]: Copied! <pre>list(team_store.get_data_ids())\n</pre> list(team_store.get_data_ids()) Out[28]: <pre>['SST.levels',\n 'amazonas_v8.zarr',\n 'amazonas_v9.zarr',\n 'noise_trajectory.zarr',\n 'reanalysis-era5-single-levels-monthly-means-subset-2001-2010_TMH.zarr',\n 's2-demo-cube.zarr']</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/Append_to_existing_cube/#how-to-append-data-to-existing-datacube-stored-in-team-s3-storage","title":"How to append data to existing datacube stored in team S3 storage\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Append_to_existing_cube/#a-deepesdl-example-notebook","title":"A DeepESDL example notebook\u00b6","text":"<p>This notebook demonstrates how to append new data to an existing datacube. This cannot be done using xcube directly yet.</p> <p>Please, also refer to the DeepESDL documentation and visit the platform's website for further information!</p> <p>Brockmann Consult, 2024</p> <p>This notebook runs with the python environment <code>deepesdl-xcube-1.7.0</code>, please checkout the documentation for help on changing the environment.</p>"},{"location":"guide/jupyterlab/notebooks/Chunking_of_datasets/","title":"Chunking of datasets","text":"<p>First, lets create a small cube, which we can later on append data to. We will use ESA CCI data for this. Please head over to 3 - Generate CCI data cubes to get more details about the xcube-cci data store :)</p> In\u00a0[1]: Copied! <pre>import datetime\nimport os\n\nfrom xcube.core.store import new_data_store\n</pre> import datetime import os  from xcube.core.store import new_data_store In\u00a0[2]: Copied! <pre>store = new_data_store(\"ccizarr\")\n</pre> store = new_data_store(\"ccizarr\") <p>Next, we create a cube containing only 4 days of data:</p> In\u00a0[3]: Copied! <pre>def open_zarrstore(filename, time_range, variables):\n    ds = store.open_data(filename)\n    subset = ds.sel(time=slice(time_range[0], time_range[1]))\n    subset = subset[variables]\n\n    return subset\n\n\ndataset = open_zarrstore(\n    \"ESACCI-L4_GHRSST-SST-GMPE-GLOB_CDR2.0-1981-2016-v02.0-fv01.0.zarr\",\n    time_range=[datetime.datetime(2015, 10, 1), datetime.datetime(2015, 10, 5)],\n    variables=[\"analysed_sst\"],\n)\n</pre> def open_zarrstore(filename, time_range, variables):     ds = store.open_data(filename)     subset = ds.sel(time=slice(time_range[0], time_range[1]))     subset = subset[variables]      return subset   dataset = open_zarrstore(     \"ESACCI-L4_GHRSST-SST-GMPE-GLOB_CDR2.0-1981-2016-v02.0-fv01.0.zarr\",     time_range=[datetime.datetime(2015, 10, 1), datetime.datetime(2015, 10, 5)],     variables=[\"analysed_sst\"], ) In\u00a0[4]: Copied! <pre>dataset\n</pre> dataset Out[4]: <pre>&lt;xarray.Dataset&gt; Size: 33MB\nDimensions:       (time: 4, lat: 720, lon: 1440)\nCoordinates:\n  * lat           (lat) float32 3kB -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n  * lon           (lon) float32 6kB -179.9 -179.6 -179.4 ... 179.4 179.6 179.9\n  * time          (time) datetime64[ns] 32B 2015-10-01T12:00:00 ... 2015-10-0...\nData variables:\n    analysed_sst  (time, lat, lon) float64 33MB dask.array&lt;chunksize=(4, 720, 720), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.4\n    acknowledgment:             Funded by ESA\n    cdm_data_type:              grid\n    comment:                    \n    creator_email:              science.leader@esa-sst-cci.org\n    creator_name:               SST_cci\n    ...                         ...\n    summary:                    An ensemble product with input from a number ...\n    time_coverage_end:          20170101T000000Z\n    time_coverage_start:        20161231T000000Z\n    title:                      Global SST Ensemble, L4 GMPE\n    uuid:                       dc0c5b25-93bf-4943-aba1-7f0de9109620\n    westernmost_longitude:      -180.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 4</li><li>lat: 720</li><li>lon: 1440</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float32-89.88 -89.62 ... 89.62 89.88axis :Yreference_datum :geographical coordinates, WGS84 projectionstandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875],\n      dtype=float32)</pre></li><li>lon(lon)float32-179.9 -179.6 ... 179.6 179.9axis :Xreference_datum :geographical coordinates, WGS84 projectionstandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875],\n      dtype=float32)</pre></li><li>time(time)datetime64[ns]2015-10-01T12:00:00 ... 2015-10-...axis :Tlong_name :reference time of sst fieldstandard_name :time<pre>array(['2015-10-01T12:00:00.000000000', '2015-10-02T12:00:00.000000000',\n       '2015-10-03T12:00:00.000000000', '2015-10-04T12:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>analysed_sst(time, lat, lon)float64dask.array&lt;chunksize=(4, 720, 720), meta=np.ndarray&gt;long_name :median SST from GMPEsource :SST CCI V2, SST CCI V1.1, MyOcean, AVHRR_OI, CMC, HadISST.2.2.0.0_r0, HadISST.2.2.0.0_r1, HadISST.2.2.0.0_r2, HadISST.2.2.0.0_r3, HadISST.2.2.0.0_r4, HadISST.2.2.0.0_r5, HadISST.2.2.0.0_r6, HadISST.2.2.0.0_r7, HadISST.2.2.0.0_r8, HadISST.2.2.0.0_r9, MGDSSTstandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_max :4500valid_min :-300  Array   Chunk   Bytes   31.64 MiB   15.82 MiB   Shape   (4, 720, 1440)   (4, 720, 720)   Dask graph   2 chunks in 3 graph layers   Data type   float64 numpy.ndarray  1440 720 4 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([-89.875, -89.625, -89.375, -89.125, -88.875, -88.625, -88.375, -88.125,\n       -87.875, -87.625,\n       ...\n        87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,  89.375,\n        89.625,  89.875],\n      dtype='float32', name='lat', length=720))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625, -178.375,\n       -178.125, -177.875, -177.625,\n       ...\n        177.625,  177.875,  178.125,  178.375,  178.625,  178.875,  179.125,\n        179.375,  179.625,  179.875],\n      dtype='float32', name='lon', length=1440))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2015-10-01 12:00:00', '2015-10-02 12:00:00',\n               '2015-10-03 12:00:00', '2015-10-04 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (47)Conventions :CF-1.4acknowledgment :Funded by ESAcdm_data_type :gridcomment :creator_email :science.leader@esa-sst-cci.orgcreator_name :SST_ccicreator_url :http://www.esa-sst-cci.orgdate_created :20180711T172223Zeasternmost_longitude :180.0file_quality_level :3gds_version_id :2.0geospatial_lat_resolution :0.25 degreegeospatial_lat_units :degrees northgeospatial_lon_resolution :0.25 degreegeospatial_lon_units :degrees easthistory :NULLid :UKMO-L4LRens-GLOB-GMPEinstitution :UKMOkeywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Creative Commons Licence by attribution (https://creativecommons.org/licenses/by/4.0/metadata_conventions :Unidata Observation Dataset v1.0metadata_link :not availablenaming_authority :org.ghrsstnetcdf_version_id :3.6northernmost_latitude :90.0platform :processing_level :GMPEproduct_version :3.0project :Climate Change Initiative - European Space Agencypublisher_email :science.leader@esa-sst-cci.orgpublisher_name :ESACCIpublisher_url :http://www.esa-sst-cci.orgreferences :Martin et al., Deep Sea Research II, 2011sensor :source :SST CCI V2, SST CCI V1.1, MyOcean, AVHRR_OI, CMC, HadISST.2.2.0.0_r0, HadISST.2.2.0.0_r1, HadISST.2.2.0.0_r2, HadISST.2.2.0.0_r3, HadISST.2.2.0.0_r4, HadISST.2.2.0.0_r5, HadISST.2.2.0.0_r6, HadISST.2.2.0.0_r7, HadISST.2.2.0.0_r8, HadISST.2.2.0.0_r9, MGDSSTsouthernmost_latitude :-90.0spatial_resolution :0.25 degreestandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionstart_time :20161231T000000Zstop_time :20170101T000000Zsummary :An ensemble product with input from a number of L4 SST analysestime_coverage_end :20170101T000000Ztime_coverage_start :20161231T000000Ztitle :Global SST Ensemble, L4 GMPEuuid :dc0c5b25-93bf-4943-aba1-7f0de9109620westernmost_longitude :-180.0</li></ul> <p>In the example above, we can see that the variable analysed_sst is chunked as follows: (4, 720, 720). This means, each chunk contains 4 time values, 720 lat values and 720 lon values per chunk. Variables, which contain 1 time value and many spatial dimensions in one chunk are optimal for visualisation/plotting of one time stamp.</p> <p>For analysing long time series, it is benificial to chunk a dataset accordingly, so the chunks contain more values of the time dimension and less of the spatial dimensions.</p> In\u00a0[5]: Copied! <pre># time optimised chunking - please note, this is just an example\ntime_chunksize = 1\nx_chunksize = 120  # or lon\ny_chunksize = 120  # or lat\n</pre> # time optimised chunking - please note, this is just an example time_chunksize = 1 x_chunksize = 120  # or lon y_chunksize = 120  # or lat <p>Now the chunking is applied to all variables, but skipping crs if present:</p> In\u00a0[6]: Copied! <pre>dataset.data_vars\n</pre> dataset.data_vars Out[6]: <pre>Data variables:\n    analysed_sst  (time, lat, lon) float64 33MB dask.array&lt;chunksize=(4, 720, 720), meta=np.ndarray&gt;</pre> In\u00a0[7]: Copied! <pre>for var_name in dataset.data_vars:\n    if var_name != \"crs\" and \"_bounds\" not in var_name:\n        print(var_name)\n        dataset[var_name] = dataset[var_name].chunk(\n            {\"time\": time_chunksize, \"lat\": y_chunksize, \"lon\": x_chunksize}\n        )\n</pre> for var_name in dataset.data_vars:     if var_name != \"crs\" and \"_bounds\" not in var_name:         print(var_name)         dataset[var_name] = dataset[var_name].chunk(             {\"time\": time_chunksize, \"lat\": y_chunksize, \"lon\": x_chunksize}         ) <pre>analysed_sst\n</pre> <p>To save a copy of a cube with a specific chunking, the encoding must be adjusted acordingly.</p> In\u00a0[8]: Copied! <pre>encoding_dict = dict()\n</pre> encoding_dict = dict() <p>We want to ensure that the coordinate variables are stored in the best performant way, so we ensure that they are not chunked. This can be specified via the encoding:</p> In\u00a0[9]: Copied! <pre>coords_encoding = {k: dict(chunks=v.shape) for k, v in dataset.coords.items()}\n</pre> coords_encoding = {k: dict(chunks=v.shape) for k, v in dataset.coords.items()} In\u00a0[10]: Copied! <pre>coords_encoding\n</pre> coords_encoding Out[10]: <pre>{'lat': {'chunks': (720,)},\n 'lon': {'chunks': (1440,)},\n 'time': {'chunks': (4,)}}</pre> <p>Specify the chunking the data variables encoding and ensuring that empty chunks are not written to disk by adding <code>write_empty_chunks=False</code>. This saves space on disk. Again, skipping crs if present.</p> In\u00a0[11]: Copied! <pre>vars_encoding = {\n    k: dict(chunks=(time_chunksize, y_chunksize, x_chunksize), write_empty_chunks=False)\n    for k, v in dataset.data_vars.items()\n    if k != \"crs\"\n}\n</pre> vars_encoding = {     k: dict(chunks=(time_chunksize, y_chunksize, x_chunksize), write_empty_chunks=False)     for k, v in dataset.data_vars.items()     if k != \"crs\" } In\u00a0[12]: Copied! <pre>vars_encoding\n</pre> vars_encoding Out[12]: <pre>{'analysed_sst': {'chunks': (1, 120, 120), 'write_empty_chunks': False}}</pre> <p>Next, combining both dictionaries to form the encoding for the entire dataset.</p> In\u00a0[13]: Copied! <pre>encoding_dict.update(coords_encoding)\nencoding_dict.update(vars_encoding)\n</pre> encoding_dict.update(coords_encoding) encoding_dict.update(vars_encoding) In\u00a0[14]: Copied! <pre>encoding_dict\n</pre> encoding_dict Out[14]: <pre>{'lat': {'chunks': (720,)},\n 'lon': {'chunks': (1440,)},\n 'time': {'chunks': (4,)},\n 'analysed_sst': {'chunks': (1, 120, 120), 'write_empty_chunks': False}}</pre> <p>Next, save it to the team s3 storage:</p> <p>To store the cube in your teams user space, please first retrieve the details from your environment variables as the following:</p> In\u00a0[15]: Copied! <pre>S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"]\nS3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"]\nS3_USER_STORAGE_BUCKET = os.environ[\"S3_USER_STORAGE_BUCKET\"]\n</pre> S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"] S3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"] S3_USER_STORAGE_BUCKET = os.environ[\"S3_USER_STORAGE_BUCKET\"] <p>You need to instantiate a s3 datastore pointing to the team bucket:</p> In\u00a0[16]: Copied! <pre>team_store = new_data_store(\n    \"s3\",\n    root=S3_USER_STORAGE_BUCKET,\n    storage_options=dict(\n        anon=False, key=S3_USER_STORAGE_KEY, secret=S3_USER_STORAGE_SECRET\n    ),\n)\n</pre> team_store = new_data_store(     \"s3\",     root=S3_USER_STORAGE_BUCKET,     storage_options=dict(         anon=False, key=S3_USER_STORAGE_KEY, secret=S3_USER_STORAGE_SECRET     ), ) <p>If you have stored no data to your user space, the returned list will be empty:</p> In\u00a0[17]: Copied! <pre>list(team_store.get_data_ids())\n</pre> list(team_store.get_data_ids()) Out[17]: <pre>['SST.levels',\n 'amazonas_v8.zarr',\n 'amazonas_v9.zarr',\n 'noise_trajectory.zarr',\n 'reanalysis-era5-single-levels-monthly-means-subset-2001-2010_TMH.zarr',\n 's2-demo-cube.zarr']</pre> In\u00a0[18]: Copied! <pre>output_id = \"analysed_sst.zarr\"\n</pre> output_id = \"analysed_sst.zarr\" <p>Now let's write the data to the team s3 storage and remember to specify the encoding while doing so:</p> In\u00a0[19]: Copied! <pre>team_store.write_data(dataset, output_id, encoding=encoding_dict, replace=True)\n</pre> team_store.write_data(dataset, output_id, encoding=encoding_dict, replace=True) Out[19]: <pre>'analysed_sst.zarr'</pre> <p>If you list the content of you datastore again, you will now see the newly written dataset in the list:</p> In\u00a0[20]: Copied! <pre>list(team_store.get_data_ids())\n</pre> list(team_store.get_data_ids()) Out[20]: <pre>['SST.levels',\n 'amazonas_v8.zarr',\n 'amazonas_v9.zarr',\n 'analysed_sst.zarr',\n 'noise_trajectory.zarr',\n 'reanalysis-era5-single-levels-monthly-means-subset-2001-2010_TMH.zarr',\n 's2-demo-cube.zarr']</pre> <p>Let's verify that our chunking has been applied:</p> In\u00a0[21]: Copied! <pre>ds = team_store.open_data(output_id)\n</pre> ds = team_store.open_data(output_id) In\u00a0[22]: Copied! <pre>ds\n</pre> ds Out[22]: <pre>&lt;xarray.Dataset&gt; Size: 33MB\nDimensions:       (time: 4, lat: 720, lon: 1440)\nCoordinates:\n  * lat           (lat) float32 3kB -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n  * lon           (lon) float32 6kB -179.9 -179.6 -179.4 ... 179.4 179.6 179.9\n  * time          (time) datetime64[ns] 32B 2015-10-01T12:00:00 ... 2015-10-0...\nData variables:\n    analysed_sst  (time, lat, lon) float64 33MB dask.array&lt;chunksize=(1, 120, 120), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.4\n    acknowledgment:             Funded by ESA\n    cdm_data_type:              grid\n    comment:                    \n    creator_email:              science.leader@esa-sst-cci.org\n    creator_name:               SST_cci\n    ...                         ...\n    summary:                    An ensemble product with input from a number ...\n    time_coverage_end:          20170101T000000Z\n    time_coverage_start:        20161231T000000Z\n    title:                      Global SST Ensemble, L4 GMPE\n    uuid:                       dc0c5b25-93bf-4943-aba1-7f0de9109620\n    westernmost_longitude:      -180.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 4</li><li>lat: 720</li><li>lon: 1440</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float32-89.88 -89.62 ... 89.62 89.88axis :Yreference_datum :geographical coordinates, WGS84 projectionstandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875],\n      dtype=float32)</pre></li><li>lon(lon)float32-179.9 -179.6 ... 179.6 179.9axis :Xreference_datum :geographical coordinates, WGS84 projectionstandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875],\n      dtype=float32)</pre></li><li>time(time)datetime64[ns]2015-10-01T12:00:00 ... 2015-10-...axis :Tlong_name :reference time of sst fieldstandard_name :time<pre>array(['2015-10-01T12:00:00.000000000', '2015-10-02T12:00:00.000000000',\n       '2015-10-03T12:00:00.000000000', '2015-10-04T12:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>analysed_sst(time, lat, lon)float64dask.array&lt;chunksize=(1, 120, 120), meta=np.ndarray&gt;long_name :median SST from GMPEsource :SST CCI V2, SST CCI V1.1, MyOcean, AVHRR_OI, CMC, HadISST.2.2.0.0_r0, HadISST.2.2.0.0_r1, HadISST.2.2.0.0_r2, HadISST.2.2.0.0_r3, HadISST.2.2.0.0_r4, HadISST.2.2.0.0_r5, HadISST.2.2.0.0_r6, HadISST.2.2.0.0_r7, HadISST.2.2.0.0_r8, HadISST.2.2.0.0_r9, MGDSSTstandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_max :4500valid_min :-300  Array   Chunk   Bytes   31.64 MiB   112.50 kiB   Shape   (4, 720, 1440)   (1, 120, 120)   Dask graph   288 chunks in 2 graph layers   Data type   float64 numpy.ndarray  1440 720 4 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([-89.875, -89.625, -89.375, -89.125, -88.875, -88.625, -88.375, -88.125,\n       -87.875, -87.625,\n       ...\n        87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,  89.375,\n        89.625,  89.875],\n      dtype='float32', name='lat', length=720))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625, -178.375,\n       -178.125, -177.875, -177.625,\n       ...\n        177.625,  177.875,  178.125,  178.375,  178.625,  178.875,  179.125,\n        179.375,  179.625,  179.875],\n      dtype='float32', name='lon', length=1440))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2015-10-01 12:00:00', '2015-10-02 12:00:00',\n               '2015-10-03 12:00:00', '2015-10-04 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (47)Conventions :CF-1.4acknowledgment :Funded by ESAcdm_data_type :gridcomment :creator_email :science.leader@esa-sst-cci.orgcreator_name :SST_ccicreator_url :http://www.esa-sst-cci.orgdate_created :20180711T172223Zeasternmost_longitude :180.0file_quality_level :3gds_version_id :2.0geospatial_lat_resolution :0.25 degreegeospatial_lat_units :degrees northgeospatial_lon_resolution :0.25 degreegeospatial_lon_units :degrees easthistory :NULLid :UKMO-L4LRens-GLOB-GMPEinstitution :UKMOkeywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Creative Commons Licence by attribution (https://creativecommons.org/licenses/by/4.0/metadata_conventions :Unidata Observation Dataset v1.0metadata_link :not availablenaming_authority :org.ghrsstnetcdf_version_id :3.6northernmost_latitude :90.0platform :processing_level :GMPEproduct_version :3.0project :Climate Change Initiative - European Space Agencypublisher_email :science.leader@esa-sst-cci.orgpublisher_name :ESACCIpublisher_url :http://www.esa-sst-cci.orgreferences :Martin et al., Deep Sea Research II, 2011sensor :source :SST CCI V2, SST CCI V1.1, MyOcean, AVHRR_OI, CMC, HadISST.2.2.0.0_r0, HadISST.2.2.0.0_r1, HadISST.2.2.0.0_r2, HadISST.2.2.0.0_r3, HadISST.2.2.0.0_r4, HadISST.2.2.0.0_r5, HadISST.2.2.0.0_r6, HadISST.2.2.0.0_r7, HadISST.2.2.0.0_r8, HadISST.2.2.0.0_r9, MGDSSTsouthernmost_latitude :-90.0spatial_resolution :0.25 degreestandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionstart_time :20161231T000000Zstop_time :20170101T000000Zsummary :An ensemble product with input from a number of L4 SST analysestime_coverage_end :20170101T000000Ztime_coverage_start :20161231T000000Ztitle :Global SST Ensemble, L4 GMPEuuid :dc0c5b25-93bf-4943-aba1-7f0de9109620westernmost_longitude :-180.0</li></ul> <p>Looks good, now let's clean up the example cube :)</p> In\u00a0[23]: Copied! <pre>team_store.delete_data(output_id)\n</pre> team_store.delete_data(output_id) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/Chunking_of_datasets/#how-to-save-a-data-cube-with-a-desired-chunking","title":"How to save a data cube with a desired chunking\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Chunking_of_datasets/#a-deepesdl-example-notebook","title":"A DeepESDL example notebook\u00b6","text":"<p>This notebook demonstrates how modify the chunking of a dataset before persisting it.</p> <p>Please, also refer to the DeepESDL documentation and visit the platform's website for further information!</p> <p>Brockmann Consult, 2024</p> <p>This notebook runs with the python environment <code>deepesdl-xcube-1.7.0</code>, please checkout the documentation for help on changing the environment.</p>"},{"location":"guide/jupyterlab/notebooks/Create_Atmospheric_Cubes_CDS/","title":"Create Atmospheric Cubes CDS","text":"<p>This notebook demonstrates how to access Climate Data Store (CDS) data via the dedicated xcube store, which provides dynamic data cube views into each gridded data set. Furthermore, an overview will be given on how to write data cubes into a team storage.</p> <p>Please, also refer to the DeepESDL documentation and visit the platform's website for further information!</p> <p>Brockmann Consult, 2024</p> <p>This notebook runs with the python environment <code>deepesdl-xcube-1.7.0</code>, please checkout the documentation for help on changing the environment.</p> <p>Please note:</p> <p>To access data from the Climate Data Store, you need a CDS API key.</p> In\u00a0[30]: Copied! <pre>import os\nos.environ['CDSAPI_URL'] = 'https://cds-beta.climate.copernicus.eu/api'\nos.environ['CDSAPI_KEY'] = '[PERSONAL-ACCESS-TOKEN]'\n</pre> import os os.environ['CDSAPI_URL'] = 'https://cds-beta.climate.copernicus.eu/api' os.environ['CDSAPI_KEY'] = '[PERSONAL-ACCESS-TOKEN]' In\u00a0[2]: Copied! <pre># mandatory imports\nfrom xcube.core.store import find_data_store_extensions\nfrom xcube.core.store import get_data_store_params_schema\nfrom xcube.core.store import new_data_store\n\n# Utilities for notebook visualization\nimport shapely.geometry\nimport IPython.display\nfrom IPython.display import JSON\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n</pre> # mandatory imports from xcube.core.store import find_data_store_extensions from xcube.core.store import get_data_store_params_schema from xcube.core.store import new_data_store  # Utilities for notebook visualization import shapely.geometry import IPython.display from IPython.display import JSON import matplotlib as mpl import matplotlib.pyplot as plt <p>Configure matplotlib to display graphs inline directly in the notebook and set a sensible default figure size.</p> In\u00a0[3]: Copied! <pre>%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = 16,12\n</pre> %matplotlib inline plt.rcParams[\"figure.figsize\"] = 16,12 <p>Check whether the <code>cds</code> store is among the available stores, if not please follow the installation information from the top of this notebook.</p> In\u00a0[4]: Copied! <pre>JSON({e.name: e.metadata for e in find_data_store_extensions()})\n</pre> JSON({e.name: e.metadata for e in find_data_store_extensions()}) Out[4]: <pre>&lt;IPython.core.display.JSON object&gt;</pre> <p>Usually we need more information to get the actual data store object. Which data store parameters are available for <code>cds</code>?</p> In\u00a0[5]: Copied! <pre>get_data_store_params_schema('cds')\n</pre> get_data_store_params_schema('cds') Out[5]: <pre>&lt;xcube.util.jsonschema.JsonObjectSchema at 0x7f85ed082490&gt;</pre> <p>Provide mandatory parameters to instantiate the store class:</p> In\u00a0[6]: Copied! <pre>store = new_data_store('cds')\nstore\n</pre> store = new_data_store('cds') store Out[6]: <pre>&lt;xcube_cds.store.CDSDataStore at 0x7f85ed1bed50&gt;</pre> <p>Which datasets are provided? (the list may contain both gridded and vector datasets):</p> In\u00a0[7]: Copied! <pre>JSON(store.list_data_ids())\n</pre> JSON(store.list_data_ids()) Out[7]: <pre>&lt;IPython.core.display.JSON object&gt;</pre> <p>Get more info about a specific dataset. This includes a description of the possible open formats:</p> <p>There are 4 required parameters, so we need to provide them to open a dataset:</p> In\u00a0[8]: Copied! <pre>store.describe_data('reanalysis-era5-single-levels-monthly-means:monthly_averaged_reanalysis')\n</pre> store.describe_data('reanalysis-era5-single-levels-monthly-means:monthly_averaged_reanalysis') Out[8]: <pre>&lt;xcube.core.store.descriptor.DatasetDescriptor at 0x7f85ecd2a250&gt;</pre> In\u00a0[9]: Copied! <pre>bbox=[-5, 45, 35, 65]\n</pre> bbox=[-5, 45, 35, 65] In\u00a0[10]: Copied! <pre>IPython.display.GeoJSON(shapely.geometry.box(*bbox).__geo_interface__)\n</pre> IPython.display.GeoJSON(shapely.geometry.box(*bbox).__geo_interface__) <pre>&lt;IPython.display.GeoJSON object&gt;</pre> In\u00a0[11]: Copied! <pre>dataset = store.open_data('reanalysis-era5-single-levels-monthly-means:monthly_averaged_reanalysis', \n                          variable_names=['2m_temperature'], \n                          bbox=bbox, \n                          spatial_res=0.25, \n                          time_range=['2006-01-01', '2010-12-31'])\ndataset\n</pre> dataset = store.open_data('reanalysis-era5-single-levels-monthly-means:monthly_averaged_reanalysis',                            variable_names=['2m_temperature'],                            bbox=bbox,                            spatial_res=0.25,                            time_range=['2006-01-01', '2010-12-31']) dataset <pre>xcube-cds version 0.9.3\n2024-09-18 07:54:59,234 INFO Request ID is bbe8cdca-f852-4881-84eb-561efb844200\n2024-09-18 07:54:59,284 INFO status has been updated to accepted\n2024-09-18 07:55:00,844 INFO status has been updated to running\n2024-09-18 07:55:03,157 INFO status has been updated to successful\n</pre> <pre>fb560841cf5bceb6b69d7064c2268332.nc:   0%|          | 0.00/1.50M [00:00&lt;?, ?B/s]</pre> Out[11]: <pre>&lt;xarray.Dataset&gt; Size: 3MB\nDimensions:  (lat: 80, lon: 160, time: 60)\nCoordinates:\n    number   int64 8B ...\n  * lat      (lat) float64 640B 64.88 64.62 64.38 64.12 ... 45.62 45.38 45.12\n  * lon      (lon) float64 1kB -4.875 -4.625 -4.375 -4.125 ... 34.38 34.62 34.88\n    expver   (time) &lt;U4 960B ...\n  * time     (time) datetime64[ns] 480B 2006-01-01 2006-02-01 ... 2010-12-01\nData variables:\n    t2m      (time, lat, lon) float32 3MB ...\nAttributes:\n    GRIB_centre:             ecmf\n    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n    GRIB_subCentre:          0\n    Conventions:             CF-1.7\n    institution:             European Centre for Medium-Range Weather Forecasts\n    history:                 2024-09-18T07:22 GRIB to CDM+CF via cfgrib-0.9.1...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 80</li><li>lon: 160</li><li>time: 60</li></ul></li><li>Coordinates: (5)<ul><li>number()int64...long_name :ensemble member numerical idunits :1standard_name :realization<pre>[1 values with dtype=int64]</pre></li><li>lat(lat)float6464.88 64.62 64.38 ... 45.38 45.12units :degrees_northstandard_name :latitudelong_name :latitudestored_direction :decreasing<pre>array([64.875, 64.625, 64.375, 64.125, 63.875, 63.625, 63.375, 63.125, 62.875,\n       62.625, 62.375, 62.125, 61.875, 61.625, 61.375, 61.125, 60.875, 60.625,\n       60.375, 60.125, 59.875, 59.625, 59.375, 59.125, 58.875, 58.625, 58.375,\n       58.125, 57.875, 57.625, 57.375, 57.125, 56.875, 56.625, 56.375, 56.125,\n       55.875, 55.625, 55.375, 55.125, 54.875, 54.625, 54.375, 54.125, 53.875,\n       53.625, 53.375, 53.125, 52.875, 52.625, 52.375, 52.125, 51.875, 51.625,\n       51.375, 51.125, 50.875, 50.625, 50.375, 50.125, 49.875, 49.625, 49.375,\n       49.125, 48.875, 48.625, 48.375, 48.125, 47.875, 47.625, 47.375, 47.125,\n       46.875, 46.625, 46.375, 46.125, 45.875, 45.625, 45.375, 45.125])</pre></li><li>lon(lon)float64-4.875 -4.625 ... 34.62 34.88units :degrees_eaststandard_name :longitudelong_name :longitude<pre>array([-4.875, -4.625, -4.375, -4.125, -3.875, -3.625, -3.375, -3.125, -2.875,\n       -2.625, -2.375, -2.125, -1.875, -1.625, -1.375, -1.125, -0.875, -0.625,\n       -0.375, -0.125,  0.125,  0.375,  0.625,  0.875,  1.125,  1.375,  1.625,\n        1.875,  2.125,  2.375,  2.625,  2.875,  3.125,  3.375,  3.625,  3.875,\n        4.125,  4.375,  4.625,  4.875,  5.125,  5.375,  5.625,  5.875,  6.125,\n        6.375,  6.625,  6.875,  7.125,  7.375,  7.625,  7.875,  8.125,  8.375,\n        8.625,  8.875,  9.125,  9.375,  9.625,  9.875, 10.125, 10.375, 10.625,\n       10.875, 11.125, 11.375, 11.625, 11.875, 12.125, 12.375, 12.625, 12.875,\n       13.125, 13.375, 13.625, 13.875, 14.125, 14.375, 14.625, 14.875, 15.125,\n       15.375, 15.625, 15.875, 16.125, 16.375, 16.625, 16.875, 17.125, 17.375,\n       17.625, 17.875, 18.125, 18.375, 18.625, 18.875, 19.125, 19.375, 19.625,\n       19.875, 20.125, 20.375, 20.625, 20.875, 21.125, 21.375, 21.625, 21.875,\n       22.125, 22.375, 22.625, 22.875, 23.125, 23.375, 23.625, 23.875, 24.125,\n       24.375, 24.625, 24.875, 25.125, 25.375, 25.625, 25.875, 26.125, 26.375,\n       26.625, 26.875, 27.125, 27.375, 27.625, 27.875, 28.125, 28.375, 28.625,\n       28.875, 29.125, 29.375, 29.625, 29.875, 30.125, 30.375, 30.625, 30.875,\n       31.125, 31.375, 31.625, 31.875, 32.125, 32.375, 32.625, 32.875, 33.125,\n       33.375, 33.625, 33.875, 34.125, 34.375, 34.625, 34.875])</pre></li><li>expver(time)&lt;U4...<pre>[60 values with dtype=&lt;U4]</pre></li><li>time(time)datetime64[ns]2006-01-01 ... 2010-12-01standard_name :time<pre>array(['2006-01-01T00:00:00.000000000', '2006-02-01T00:00:00.000000000',\n       '2006-03-01T00:00:00.000000000', '2006-04-01T00:00:00.000000000',\n       '2006-05-01T00:00:00.000000000', '2006-06-01T00:00:00.000000000',\n       '2006-07-01T00:00:00.000000000', '2006-08-01T00:00:00.000000000',\n       '2006-09-01T00:00:00.000000000', '2006-10-01T00:00:00.000000000',\n       '2006-11-01T00:00:00.000000000', '2006-12-01T00:00:00.000000000',\n       '2007-01-01T00:00:00.000000000', '2007-02-01T00:00:00.000000000',\n       '2007-03-01T00:00:00.000000000', '2007-04-01T00:00:00.000000000',\n       '2007-05-01T00:00:00.000000000', '2007-06-01T00:00:00.000000000',\n       '2007-07-01T00:00:00.000000000', '2007-08-01T00:00:00.000000000',\n       '2007-09-01T00:00:00.000000000', '2007-10-01T00:00:00.000000000',\n       '2007-11-01T00:00:00.000000000', '2007-12-01T00:00:00.000000000',\n       '2008-01-01T00:00:00.000000000', '2008-02-01T00:00:00.000000000',\n       '2008-03-01T00:00:00.000000000', '2008-04-01T00:00:00.000000000',\n       '2008-05-01T00:00:00.000000000', '2008-06-01T00:00:00.000000000',\n       '2008-07-01T00:00:00.000000000', '2008-08-01T00:00:00.000000000',\n       '2008-09-01T00:00:00.000000000', '2008-10-01T00:00:00.000000000',\n       '2008-11-01T00:00:00.000000000', '2008-12-01T00:00:00.000000000',\n       '2009-01-01T00:00:00.000000000', '2009-02-01T00:00:00.000000000',\n       '2009-03-01T00:00:00.000000000', '2009-04-01T00:00:00.000000000',\n       '2009-05-01T00:00:00.000000000', '2009-06-01T00:00:00.000000000',\n       '2009-07-01T00:00:00.000000000', '2009-08-01T00:00:00.000000000',\n       '2009-09-01T00:00:00.000000000', '2009-10-01T00:00:00.000000000',\n       '2009-11-01T00:00:00.000000000', '2009-12-01T00:00:00.000000000',\n       '2010-01-01T00:00:00.000000000', '2010-02-01T00:00:00.000000000',\n       '2010-03-01T00:00:00.000000000', '2010-04-01T00:00:00.000000000',\n       '2010-05-01T00:00:00.000000000', '2010-06-01T00:00:00.000000000',\n       '2010-07-01T00:00:00.000000000', '2010-08-01T00:00:00.000000000',\n       '2010-09-01T00:00:00.000000000', '2010-10-01T00:00:00.000000000',\n       '2010-11-01T00:00:00.000000000', '2010-12-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>t2m(time, lat, lon)float32...GRIB_paramId :167GRIB_dataType :anGRIB_numberOfPoints :12800GRIB_typeOfLevel :surfaceGRIB_stepUnits :1GRIB_stepType :avguaGRIB_gridType :regular_llGRIB_uvRelativeToGrid :0GRIB_NV :0GRIB_Nx :160GRIB_Ny :80GRIB_cfName :unknownGRIB_cfVarName :t2mGRIB_gridDefinitionDescription :Latitude/Longitude GridGRIB_iDirectionIncrementInDegrees :0.25GRIB_iScansNegatively :0GRIB_jDirectionIncrementInDegrees :0.25GRIB_jPointsAreConsecutive :0GRIB_jScansPositively :0GRIB_latitudeOfFirstGridPointInDegrees :64.875GRIB_latitudeOfLastGridPointInDegrees :45.125GRIB_longitudeOfFirstGridPointInDegrees :-4.875GRIB_longitudeOfLastGridPointInDegrees :34.875GRIB_missingValue :3.4028234663852886e+38GRIB_name :2 metre temperatureGRIB_shortName :2tGRIB_totalNumber :0GRIB_units :Klong_name :2 metre temperatureunits :Kstandard_name :unknownGRIB_surface :0.0<pre>[768000 values with dtype=float32]</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([64.875, 64.625, 64.375, 64.125, 63.875, 63.625, 63.375, 63.125, 62.875,\n       62.625, 62.375, 62.125, 61.875, 61.625, 61.375, 61.125, 60.875, 60.625,\n       60.375, 60.125, 59.875, 59.625, 59.375, 59.125, 58.875, 58.625, 58.375,\n       58.125, 57.875, 57.625, 57.375, 57.125, 56.875, 56.625, 56.375, 56.125,\n       55.875, 55.625, 55.375, 55.125, 54.875, 54.625, 54.375, 54.125, 53.875,\n       53.625, 53.375, 53.125, 52.875, 52.625, 52.375, 52.125, 51.875, 51.625,\n       51.375, 51.125, 50.875, 50.625, 50.375, 50.125, 49.875, 49.625, 49.375,\n       49.125, 48.875, 48.625, 48.375, 48.125, 47.875, 47.625, 47.375, 47.125,\n       46.875, 46.625, 46.375, 46.125, 45.875, 45.625, 45.375, 45.125],\n      dtype='float64', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-4.875, -4.625, -4.375, -4.125, -3.875, -3.625, -3.375, -3.125, -2.875,\n       -2.625,\n       ...\n       32.625, 32.875, 33.125, 33.375, 33.625, 33.875, 34.125, 34.375, 34.625,\n       34.875],\n      dtype='float64', name='lon', length=160))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2006-01-01', '2006-02-01', '2006-03-01', '2006-04-01',\n               '2006-05-01', '2006-06-01', '2006-07-01', '2006-08-01',\n               '2006-09-01', '2006-10-01', '2006-11-01', '2006-12-01',\n               '2007-01-01', '2007-02-01', '2007-03-01', '2007-04-01',\n               '2007-05-01', '2007-06-01', '2007-07-01', '2007-08-01',\n               '2007-09-01', '2007-10-01', '2007-11-01', '2007-12-01',\n               '2008-01-01', '2008-02-01', '2008-03-01', '2008-04-01',\n               '2008-05-01', '2008-06-01', '2008-07-01', '2008-08-01',\n               '2008-09-01', '2008-10-01', '2008-11-01', '2008-12-01',\n               '2009-01-01', '2009-02-01', '2009-03-01', '2009-04-01',\n               '2009-05-01', '2009-06-01', '2009-07-01', '2009-08-01',\n               '2009-09-01', '2009-10-01', '2009-11-01', '2009-12-01',\n               '2010-01-01', '2010-02-01', '2010-03-01', '2010-04-01',\n               '2010-05-01', '2010-06-01', '2010-07-01', '2010-08-01',\n               '2010-09-01', '2010-10-01', '2010-11-01', '2010-12-01'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (6)GRIB_centre :ecmfGRIB_centreDescription :European Centre for Medium-Range Weather ForecastsGRIB_subCentre :0Conventions :CF-1.7institution :European Centre for Medium-Range Weather Forecastshistory :2024-09-18T07:22 GRIB to CDM+CF via cfgrib-0.9.14.0/ecCodes-2.36.0 with {\"source\": \"data.grib\", \"filter_by_keys\": {}, \"encode_cf\": [\"parameter\", \"time\", \"geography\", \"vertical\"]}</li></ul> <p>We can explore this dataset by plotting a temperature map for selected time points. First, we select January 2001. Land areas \u2013 and mountain ranges in particular \u2013 show up on the map as colder regions.</p> In\u00a0[12]: Copied! <pre>t2m_2001_jan = dataset.t2m.sel(time='2010-01-01 00:00:00', method='nearest')\nt2m_2001_jan.plot.imshow(vmin=260, vmax=285, figsize=(14, 8), cmap='plasma')\n</pre> t2m_2001_jan = dataset.t2m.sel(time='2010-01-01 00:00:00', method='nearest') t2m_2001_jan.plot.imshow(vmin=260, vmax=285, figsize=(14, 8), cmap='plasma') Out[12]: <pre>&lt;matplotlib.image.AxesImage at 0x7f8634b4f010&gt;</pre> <p>To store the cube in your teams user space, please first retrieve the details from your environment variables as the following:</p> In\u00a0[13]: Copied! <pre>S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"]\nS3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"]\nS3_USER_STORAGE_BUCKET = os.environ[\"S3_USER_STORAGE_BUCKET\"]\n</pre> S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"] S3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"] S3_USER_STORAGE_BUCKET = os.environ[\"S3_USER_STORAGE_BUCKET\"] <p>You need to instantiate a s3 datastore pointing to the team bucket:</p> In\u00a0[14]: Copied! <pre>from xcube.core.store import new_data_store\nteam_store = new_data_store(\"s3\", \n                       root=S3_USER_STORAGE_BUCKET, \n                       storage_options=dict(anon=False, \n                                            key=S3_USER_STORAGE_KEY, \n                                            secret=S3_USER_STORAGE_SECRET))\n</pre> from xcube.core.store import new_data_store team_store = new_data_store(\"s3\",                         root=S3_USER_STORAGE_BUCKET,                         storage_options=dict(anon=False,                                              key=S3_USER_STORAGE_KEY,                                              secret=S3_USER_STORAGE_SECRET)) <p>If you have stored no data to your user space, the returned list will be empty:</p> In\u00a0[15]: Copied! <pre>list(team_store.get_data_ids())\n</pre> list(team_store.get_data_ids()) Out[15]: <pre>['SST.levels',\n 'amazonas_v8.zarr',\n 'amazonas_v9.zarr',\n 'analysed_sst.zarr',\n 'noise_trajectory.zarr',\n 'reanalysis-era5-single-levels-monthly-means-subset-2001-2010_TMH.zarr']</pre> In\u00a0[16]: Copied! <pre>team_store.write_data(dataset,'reanalysis-era5-single-levels-monthly-means-subset-2006-2010_TMH.zarr', replace=True)\n</pre> team_store.write_data(dataset,'reanalysis-era5-single-levels-monthly-means-subset-2006-2010_TMH.zarr', replace=True) Out[16]: <pre>'reanalysis-era5-single-levels-monthly-means-subset-2006-2010_TMH.zarr'</pre> In\u00a0[17]: Copied! <pre>list(team_store.get_data_ids())\n</pre> list(team_store.get_data_ids()) Out[17]: <pre>['SST.levels',\n 'amazonas_v8.zarr',\n 'amazonas_v9.zarr',\n 'analysed_sst.zarr',\n 'noise_trajectory.zarr',\n 'reanalysis-era5-single-levels-monthly-means-subset-2001-2010_TMH.zarr',\n 'reanalysis-era5-single-levels-monthly-means-subset-2006-2010_TMH.zarr']</pre> In\u00a0[18]: Copied! <pre>cds_subset = team_store.open_data('reanalysis-era5-single-levels-monthly-means-subset-2006-2010_TMH.zarr')\ncds_subset\n</pre> cds_subset = team_store.open_data('reanalysis-era5-single-levels-monthly-means-subset-2006-2010_TMH.zarr') cds_subset Out[18]: <pre>&lt;xarray.Dataset&gt; Size: 3MB\nDimensions:  (time: 60, lat: 80, lon: 160)\nCoordinates:\n    expver   (time) &lt;U4 960B dask.array&lt;chunksize=(60,), meta=np.ndarray&gt;\n  * lat      (lat) float64 640B 64.88 64.62 64.38 64.12 ... 45.62 45.38 45.12\n  * lon      (lon) float64 1kB -4.875 -4.625 -4.375 -4.125 ... 34.38 34.62 34.88\n    number   int64 8B ...\n  * time     (time) datetime64[ns] 480B 2006-01-01 2006-02-01 ... 2010-12-01\nData variables:\n    t2m      (time, lat, lon) float32 3MB dask.array&lt;chunksize=(30, 40, 80), meta=np.ndarray&gt;\nAttributes:\n    Conventions:             CF-1.7\n    GRIB_centre:             ecmf\n    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n    GRIB_subCentre:          0\n    history:                 2024-09-18T07:22 GRIB to CDM+CF via cfgrib-0.9.1...\n    institution:             European Centre for Medium-Range Weather Forecasts</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 60</li><li>lat: 80</li><li>lon: 160</li></ul></li><li>Coordinates: (5)<ul><li>expver(time)&lt;U4dask.array&lt;chunksize=(60,), meta=np.ndarray&gt;  Array   Chunk   Bytes   0.94 kiB   0.94 kiB   Shape   (60,)   (60,)   Dask graph   1 chunks in 2 graph layers   Data type  60 1 </li><li>lat(lat)float6464.88 64.62 64.38 ... 45.38 45.12long_name :latitudestandard_name :latitudestored_direction :decreasingunits :degrees_north<pre>array([64.875, 64.625, 64.375, 64.125, 63.875, 63.625, 63.375, 63.125, 62.875,\n       62.625, 62.375, 62.125, 61.875, 61.625, 61.375, 61.125, 60.875, 60.625,\n       60.375, 60.125, 59.875, 59.625, 59.375, 59.125, 58.875, 58.625, 58.375,\n       58.125, 57.875, 57.625, 57.375, 57.125, 56.875, 56.625, 56.375, 56.125,\n       55.875, 55.625, 55.375, 55.125, 54.875, 54.625, 54.375, 54.125, 53.875,\n       53.625, 53.375, 53.125, 52.875, 52.625, 52.375, 52.125, 51.875, 51.625,\n       51.375, 51.125, 50.875, 50.625, 50.375, 50.125, 49.875, 49.625, 49.375,\n       49.125, 48.875, 48.625, 48.375, 48.125, 47.875, 47.625, 47.375, 47.125,\n       46.875, 46.625, 46.375, 46.125, 45.875, 45.625, 45.375, 45.125])</pre></li><li>lon(lon)float64-4.875 -4.625 ... 34.62 34.88long_name :longitudestandard_name :longitudeunits :degrees_east<pre>array([-4.875, -4.625, -4.375, -4.125, -3.875, -3.625, -3.375, -3.125, -2.875,\n       -2.625, -2.375, -2.125, -1.875, -1.625, -1.375, -1.125, -0.875, -0.625,\n       -0.375, -0.125,  0.125,  0.375,  0.625,  0.875,  1.125,  1.375,  1.625,\n        1.875,  2.125,  2.375,  2.625,  2.875,  3.125,  3.375,  3.625,  3.875,\n        4.125,  4.375,  4.625,  4.875,  5.125,  5.375,  5.625,  5.875,  6.125,\n        6.375,  6.625,  6.875,  7.125,  7.375,  7.625,  7.875,  8.125,  8.375,\n        8.625,  8.875,  9.125,  9.375,  9.625,  9.875, 10.125, 10.375, 10.625,\n       10.875, 11.125, 11.375, 11.625, 11.875, 12.125, 12.375, 12.625, 12.875,\n       13.125, 13.375, 13.625, 13.875, 14.125, 14.375, 14.625, 14.875, 15.125,\n       15.375, 15.625, 15.875, 16.125, 16.375, 16.625, 16.875, 17.125, 17.375,\n       17.625, 17.875, 18.125, 18.375, 18.625, 18.875, 19.125, 19.375, 19.625,\n       19.875, 20.125, 20.375, 20.625, 20.875, 21.125, 21.375, 21.625, 21.875,\n       22.125, 22.375, 22.625, 22.875, 23.125, 23.375, 23.625, 23.875, 24.125,\n       24.375, 24.625, 24.875, 25.125, 25.375, 25.625, 25.875, 26.125, 26.375,\n       26.625, 26.875, 27.125, 27.375, 27.625, 27.875, 28.125, 28.375, 28.625,\n       28.875, 29.125, 29.375, 29.625, 29.875, 30.125, 30.375, 30.625, 30.875,\n       31.125, 31.375, 31.625, 31.875, 32.125, 32.375, 32.625, 32.875, 33.125,\n       33.375, 33.625, 33.875, 34.125, 34.375, 34.625, 34.875])</pre></li><li>number()int64...long_name :ensemble member numerical idstandard_name :realizationunits :1<pre>[1 values with dtype=int64]</pre></li><li>time(time)datetime64[ns]2006-01-01 ... 2010-12-01standard_name :time<pre>array(['2006-01-01T00:00:00.000000000', '2006-02-01T00:00:00.000000000',\n       '2006-03-01T00:00:00.000000000', '2006-04-01T00:00:00.000000000',\n       '2006-05-01T00:00:00.000000000', '2006-06-01T00:00:00.000000000',\n       '2006-07-01T00:00:00.000000000', '2006-08-01T00:00:00.000000000',\n       '2006-09-01T00:00:00.000000000', '2006-10-01T00:00:00.000000000',\n       '2006-11-01T00:00:00.000000000', '2006-12-01T00:00:00.000000000',\n       '2007-01-01T00:00:00.000000000', '2007-02-01T00:00:00.000000000',\n       '2007-03-01T00:00:00.000000000', '2007-04-01T00:00:00.000000000',\n       '2007-05-01T00:00:00.000000000', '2007-06-01T00:00:00.000000000',\n       '2007-07-01T00:00:00.000000000', '2007-08-01T00:00:00.000000000',\n       '2007-09-01T00:00:00.000000000', '2007-10-01T00:00:00.000000000',\n       '2007-11-01T00:00:00.000000000', '2007-12-01T00:00:00.000000000',\n       '2008-01-01T00:00:00.000000000', '2008-02-01T00:00:00.000000000',\n       '2008-03-01T00:00:00.000000000', '2008-04-01T00:00:00.000000000',\n       '2008-05-01T00:00:00.000000000', '2008-06-01T00:00:00.000000000',\n       '2008-07-01T00:00:00.000000000', '2008-08-01T00:00:00.000000000',\n       '2008-09-01T00:00:00.000000000', '2008-10-01T00:00:00.000000000',\n       '2008-11-01T00:00:00.000000000', '2008-12-01T00:00:00.000000000',\n       '2009-01-01T00:00:00.000000000', '2009-02-01T00:00:00.000000000',\n       '2009-03-01T00:00:00.000000000', '2009-04-01T00:00:00.000000000',\n       '2009-05-01T00:00:00.000000000', '2009-06-01T00:00:00.000000000',\n       '2009-07-01T00:00:00.000000000', '2009-08-01T00:00:00.000000000',\n       '2009-09-01T00:00:00.000000000', '2009-10-01T00:00:00.000000000',\n       '2009-11-01T00:00:00.000000000', '2009-12-01T00:00:00.000000000',\n       '2010-01-01T00:00:00.000000000', '2010-02-01T00:00:00.000000000',\n       '2010-03-01T00:00:00.000000000', '2010-04-01T00:00:00.000000000',\n       '2010-05-01T00:00:00.000000000', '2010-06-01T00:00:00.000000000',\n       '2010-07-01T00:00:00.000000000', '2010-08-01T00:00:00.000000000',\n       '2010-09-01T00:00:00.000000000', '2010-10-01T00:00:00.000000000',\n       '2010-11-01T00:00:00.000000000', '2010-12-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>t2m(time, lat, lon)float32dask.array&lt;chunksize=(30, 40, 80), meta=np.ndarray&gt;GRIB_NV :0GRIB_Nx :160GRIB_Ny :80GRIB_cfName :unknownGRIB_cfVarName :t2mGRIB_dataType :anGRIB_gridDefinitionDescription :Latitude/Longitude GridGRIB_gridType :regular_llGRIB_iDirectionIncrementInDegrees :0.25GRIB_iScansNegatively :0GRIB_jDirectionIncrementInDegrees :0.25GRIB_jPointsAreConsecutive :0GRIB_jScansPositively :0GRIB_latitudeOfFirstGridPointInDegrees :64.875GRIB_latitudeOfLastGridPointInDegrees :45.125GRIB_longitudeOfFirstGridPointInDegrees :-4.875GRIB_longitudeOfLastGridPointInDegrees :34.875GRIB_missingValue :3.4028234663852886e+38GRIB_name :2 metre temperatureGRIB_numberOfPoints :12800GRIB_paramId :167GRIB_shortName :2tGRIB_stepType :avguaGRIB_stepUnits :1GRIB_surface :0.0GRIB_totalNumber :0GRIB_typeOfLevel :surfaceGRIB_units :KGRIB_uvRelativeToGrid :0long_name :2 metre temperaturestandard_name :unknownunits :K  Array   Chunk   Bytes   2.93 MiB   375.00 kiB   Shape   (60, 80, 160)   (30, 40, 80)   Dask graph   8 chunks in 2 graph layers   Data type   float32 numpy.ndarray  160 80 60 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([64.875, 64.625, 64.375, 64.125, 63.875, 63.625, 63.375, 63.125, 62.875,\n       62.625, 62.375, 62.125, 61.875, 61.625, 61.375, 61.125, 60.875, 60.625,\n       60.375, 60.125, 59.875, 59.625, 59.375, 59.125, 58.875, 58.625, 58.375,\n       58.125, 57.875, 57.625, 57.375, 57.125, 56.875, 56.625, 56.375, 56.125,\n       55.875, 55.625, 55.375, 55.125, 54.875, 54.625, 54.375, 54.125, 53.875,\n       53.625, 53.375, 53.125, 52.875, 52.625, 52.375, 52.125, 51.875, 51.625,\n       51.375, 51.125, 50.875, 50.625, 50.375, 50.125, 49.875, 49.625, 49.375,\n       49.125, 48.875, 48.625, 48.375, 48.125, 47.875, 47.625, 47.375, 47.125,\n       46.875, 46.625, 46.375, 46.125, 45.875, 45.625, 45.375, 45.125],\n      dtype='float64', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-4.875, -4.625, -4.375, -4.125, -3.875, -3.625, -3.375, -3.125, -2.875,\n       -2.625,\n       ...\n       32.625, 32.875, 33.125, 33.375, 33.625, 33.875, 34.125, 34.375, 34.625,\n       34.875],\n      dtype='float64', name='lon', length=160))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2006-01-01', '2006-02-01', '2006-03-01', '2006-04-01',\n               '2006-05-01', '2006-06-01', '2006-07-01', '2006-08-01',\n               '2006-09-01', '2006-10-01', '2006-11-01', '2006-12-01',\n               '2007-01-01', '2007-02-01', '2007-03-01', '2007-04-01',\n               '2007-05-01', '2007-06-01', '2007-07-01', '2007-08-01',\n               '2007-09-01', '2007-10-01', '2007-11-01', '2007-12-01',\n               '2008-01-01', '2008-02-01', '2008-03-01', '2008-04-01',\n               '2008-05-01', '2008-06-01', '2008-07-01', '2008-08-01',\n               '2008-09-01', '2008-10-01', '2008-11-01', '2008-12-01',\n               '2009-01-01', '2009-02-01', '2009-03-01', '2009-04-01',\n               '2009-05-01', '2009-06-01', '2009-07-01', '2009-08-01',\n               '2009-09-01', '2009-10-01', '2009-11-01', '2009-12-01',\n               '2010-01-01', '2010-02-01', '2010-03-01', '2010-04-01',\n               '2010-05-01', '2010-06-01', '2010-07-01', '2010-08-01',\n               '2010-09-01', '2010-10-01', '2010-11-01', '2010-12-01'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (6)Conventions :CF-1.7GRIB_centre :ecmfGRIB_centreDescription :European Centre for Medium-Range Weather ForecastsGRIB_subCentre :0history :2024-09-18T07:22 GRIB to CDM+CF via cfgrib-0.9.14.0/ecCodes-2.36.0 with {\"source\": \"data.grib\", \"filter_by_keys\": {}, \"encode_cf\": [\"parameter\", \"time\", \"geography\", \"vertical\"]}institution :European Centre for Medium-Range Weather Forecasts</li></ul> In\u00a0[19]: Copied! <pre>from xcube.webapi.viewer import Viewer\n</pre> from xcube.webapi.viewer import Viewer <p>Let's set some attributes of the variable, so the colormapping does not fall back on the default</p> In\u00a0[20]: Copied! <pre>cds_subset.t2m.attrs[\"color_value_min\"] = 270\ncds_subset.t2m.attrs[\"color_value_max\"] = 310\ncds_subset.t2m.attrs[\"color_bar_name\"] = \"plasma\"\n</pre> cds_subset.t2m.attrs[\"color_value_min\"] = 270 cds_subset.t2m.attrs[\"color_value_max\"] = 310 cds_subset.t2m.attrs[\"color_bar_name\"] = \"plasma\" In\u00a0[21]: Copied! <pre>cds_subset.attrs[\"title\"] = \"ERA5 2m Temperature\"\n</pre> cds_subset.attrs[\"title\"] = \"ERA5 2m Temperature\" In\u00a0[\u00a0]: Copied! <pre>viewer = Viewer()\n</pre> viewer = Viewer() <pre>404 GET /viewer/config/config.json (127.0.0.1): xcube viewer has not been been configured\n404 GET /viewer/config/config.json (127.0.0.1) 3.75ms\nUncaught exception GET /datasets?details=1 (127.0.0.1)\nHTTPServerRequest(protocol='http', host='deep.earthsystemdatalab.net', method='GET', uri='/datasets?details=1', version='HTTP/1.1', remote_ip='127.0.0.1')\nTraceback (most recent call last):\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/tornado/web.py\", line 1790, in _execute\n    result = await result\n             ^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/server/webservers/tornado.py\", line 335, in get\n    await self._call_method(\"get\", *args, **kwargs)\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/server/webservers/tornado.py\", line 355, in _call_method\n    method(*args, **kwargs)\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/routes.py\", line 109, in get\n    response = get_datasets(\n               ^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/controllers.py\", line 73, in get_datasets\n    dataset_configs = list(ctx.get_dataset_configs())\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 449, in get_dataset_configs\n    assert self._dataset_configs is not None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n500 GET /datasets?details=1 (127.0.0.1) 5.76ms\nUncaught exception GET /tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/4/8?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z (127.0.0.1)\nHTTPServerRequest(protocol='http', host='deep.earthsystemdatalab.net', method='GET', uri='/tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/4/8?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z', version='HTTP/1.1', remote_ip='127.0.0.1')\nTraceback (most recent call last):\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/tornado/web.py\", line 1790, in _execute\n    result = await result\n             ^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/server/webservers/tornado.py\", line 335, in get\n    await self._call_method(\"get\", *args, **kwargs)\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/server/webservers/tornado.py\", line 353, in _call_method\n    await method(*args, **kwargs)\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/routes.py\", line 91, in get\n    tile = await self.ctx.run_in_executor(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/controllers.py\", line 34, in compute_ml_dataset_tile\n    return _compute_ml_dataset_tile(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/controllers.py\", line 66, in _compute_ml_dataset_tile\n    ml_dataset = ctx.datasets_ctx.get_ml_dataset(ds_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 162, in get_ml_dataset\n    ml_dataset, _ = self._get_dataset_entry(ds_id)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 605, in _get_dataset_entry\n    self._set_dataset_entry(self._create_dataset_entry(ds_id))\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 617, in _create_dataset_entry\n    dataset_config = self.get_dataset_config(ds_id)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 440, in get_dataset_config\n    dataset_configs = self.get_dataset_configs()\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 449, in get_dataset_configs\n    assert self._dataset_configs is not None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n500 GET /tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/4/8?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z (127.0.0.1) 5.91ms\nUncaught exception GET /tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/4/9?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z (127.0.0.1)\nHTTPServerRequest(protocol='http', host='deep.earthsystemdatalab.net', method='GET', uri='/tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/4/9?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z', version='HTTP/1.1', remote_ip='127.0.0.1')\nTraceback (most recent call last):\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/tornado/web.py\", line 1790, in _execute\n    result = await result\n             ^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/server/webservers/tornado.py\", line 335, in get\n    await self._call_method(\"get\", *args, **kwargs)\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/server/webservers/tornado.py\", line 353, in _call_method\n    await method(*args, **kwargs)\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/routes.py\", line 91, in get\n    tile = await self.ctx.run_in_executor(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/controllers.py\", line 34, in compute_ml_dataset_tile\n    return _compute_ml_dataset_tile(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/controllers.py\", line 66, in _compute_ml_dataset_tile\n    ml_dataset = ctx.datasets_ctx.get_ml_dataset(ds_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 162, in get_ml_dataset\n    ml_dataset, _ = self._get_dataset_entry(ds_id)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 605, in _get_dataset_entry\n    self._set_dataset_entry(self._create_dataset_entry(ds_id))\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 617, in _create_dataset_entry\n    dataset_config = self.get_dataset_config(ds_id)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 440, in get_dataset_config\n    dataset_configs = self.get_dataset_configs()\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 449, in get_dataset_configs\n    assert self._dataset_configs is not None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n500 GET /tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/4/9?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z (127.0.0.1) 7.86ms\nUncaught exception GET /tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/5/9?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z (127.0.0.1)\nHTTPServerRequest(protocol='http', host='deep.earthsystemdatalab.net', method='GET', uri='/tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/5/9?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z', version='HTTP/1.1', remote_ip='127.0.0.1')\nTraceback (most recent call last):\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/tornado/web.py\", line 1790, in _execute\n    result = await result\n             ^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/server/webservers/tornado.py\", line 335, in get\n    await self._call_method(\"get\", *args, **kwargs)\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/server/webservers/tornado.py\", line 353, in _call_method\n    await method(*args, **kwargs)\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/routes.py\", line 91, in get\n    tile = await self.ctx.run_in_executor(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/controllers.py\", line 34, in compute_ml_dataset_tile\n    return _compute_ml_dataset_tile(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/controllers.py\", line 66, in _compute_ml_dataset_tile\n    ml_dataset = ctx.datasets_ctx.get_ml_dataset(ds_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 162, in get_ml_dataset\n    ml_dataset, _ = self._get_dataset_entry(ds_id)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 605, in _get_dataset_entry\n    self._set_dataset_entry(self._create_dataset_entry(ds_id))\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 617, in _create_dataset_entry\n    dataset_config = self.get_dataset_config(ds_id)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 440, in get_dataset_config\n    dataset_configs = self.get_dataset_configs()\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 449, in get_dataset_configs\n    assert self._dataset_configs is not None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n500 GET /tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/5/9?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z (127.0.0.1) 4.53ms\nUncaught exception GET /tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/5/8?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z (127.0.0.1)\nHTTPServerRequest(protocol='http', host='deep.earthsystemdatalab.net', method='GET', uri='/tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/5/8?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z', version='HTTP/1.1', remote_ip='127.0.0.1')\nTraceback (most recent call last):\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/tornado/web.py\", line 1790, in _execute\n    result = await result\n             ^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/server/webservers/tornado.py\", line 335, in get\n    await self._call_method(\"get\", *args, **kwargs)\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/server/webservers/tornado.py\", line 353, in _call_method\n    await method(*args, **kwargs)\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/routes.py\", line 91, in get\n    tile = await self.ctx.run_in_executor(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/controllers.py\", line 34, in compute_ml_dataset_tile\n    return _compute_ml_dataset_tile(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/controllers.py\", line 66, in _compute_ml_dataset_tile\n    ml_dataset = ctx.datasets_ctx.get_ml_dataset(ds_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 162, in get_ml_dataset\n    ml_dataset, _ = self._get_dataset_entry(ds_id)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 605, in _get_dataset_entry\n    self._set_dataset_entry(self._create_dataset_entry(ds_id))\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 617, in _create_dataset_entry\n    dataset_config = self.get_dataset_config(ds_id)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 440, in get_dataset_config\n    dataset_configs = self.get_dataset_configs()\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 449, in get_dataset_configs\n    assert self._dataset_configs is not None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n500 GET /tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/5/8?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z (127.0.0.1) 6.96ms\nUncaught exception GET /tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/4/7?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z (127.0.0.1)\nHTTPServerRequest(protocol='http', host='deep.earthsystemdatalab.net', method='GET', uri='/tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/4/7?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z', version='HTTP/1.1', remote_ip='127.0.0.1')\nTraceback (most recent call last):\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/tornado/web.py\", line 1790, in _execute\n    result = await result\n             ^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/server/webservers/tornado.py\", line 335, in get\n    await self._call_method(\"get\", *args, **kwargs)\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/server/webservers/tornado.py\", line 353, in _call_method\n    await method(*args, **kwargs)\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/routes.py\", line 91, in get\n    tile = await self.ctx.run_in_executor(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/controllers.py\", line 34, in compute_ml_dataset_tile\n    return _compute_ml_dataset_tile(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/controllers.py\", line 66, in _compute_ml_dataset_tile\n    ml_dataset = ctx.datasets_ctx.get_ml_dataset(ds_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 162, in get_ml_dataset\n    ml_dataset, _ = self._get_dataset_entry(ds_id)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 605, in _get_dataset_entry\n    self._set_dataset_entry(self._create_dataset_entry(ds_id))\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 617, in _create_dataset_entry\n    dataset_config = self.get_dataset_config(ds_id)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 440, in get_dataset_config\n    dataset_configs = self.get_dataset_configs()\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 449, in get_dataset_configs\n    assert self._dataset_configs is not None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n500 GET /tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/4/7?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z (127.0.0.1) 2.75ms\nUncaught exception GET /tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/5/7?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z (127.0.0.1)\nHTTPServerRequest(protocol='http', host='deep.earthsystemdatalab.net', method='GET', uri='/tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/5/7?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z', version='HTTP/1.1', remote_ip='127.0.0.1')\nTraceback (most recent call last):\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/tornado/web.py\", line 1790, in _execute\n    result = await result\n             ^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/server/webservers/tornado.py\", line 335, in get\n    await self._call_method(\"get\", *args, **kwargs)\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/server/webservers/tornado.py\", line 353, in _call_method\n    await method(*args, **kwargs)\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/routes.py\", line 91, in get\n    tile = await self.ctx.run_in_executor(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/controllers.py\", line 34, in compute_ml_dataset_tile\n    return _compute_ml_dataset_tile(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/tiles/controllers.py\", line 66, in _compute_ml_dataset_tile\n    ml_dataset = ctx.datasets_ctx.get_ml_dataset(ds_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 162, in get_ml_dataset\n    ml_dataset, _ = self._get_dataset_entry(ds_id)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 605, in _get_dataset_entry\n    self._set_dataset_entry(self._create_dataset_entry(ds_id))\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 617, in _create_dataset_entry\n    dataset_config = self.get_dataset_config(ds_id)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 440, in get_dataset_config\n    dataset_configs = self.get_dataset_configs()\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/conda/deepesdl/36a2c06c01942b4828818c0c77b0355c0d66b085bd378ed58a97b1f2e589092a-20240918-063730-228122-584-xcube-1.7.0/lib/python3.11/site-packages/xcube/webapi/datasets/context.py\", line 449, in get_dataset_configs\n    assert self._dataset_configs is not None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n500 GET /tiles/a2f214fd-d771-4a78-908b-55f6cb9d4935/t2m/4/5/7?crs=EPSG%3A3857&amp;vmin=270&amp;vmax=310&amp;cmap=plasma&amp;time=2010-12-01T00%3A00%3A00Z (127.0.0.1) 2.14ms\n</pre> In\u00a0[23]: Copied! <pre>viewer.add_dataset(cds_subset)\n</pre> viewer.add_dataset(cds_subset) Out[23]: <pre>'a2f214fd-d771-4a78-908b-55f6cb9d4935'</pre> <p>You can click on the viewer link to open xcube Viewer in a new browser tab:</p> In\u00a0[24]: Copied! <pre>viewer.info()\n</pre> viewer.info() <pre>Server: https://deep.earthsystemdatalab.net/user/alicebalfanz/proxy/8000\nViewer: https://deep.earthsystemdatalab.net/user/alicebalfanz/proxy/8000/viewer/?serverUrl=https://deep.earthsystemdatalab.net/user/alicebalfanz/proxy/8000\n</pre> <p>You can also open xcube Viewer inlined here:</p> In\u00a0[25]: Copied! <pre>viewer.show()\n</pre> viewer.show() Out[25]: In\u00a0[26]: Copied! <pre>viewer.stop_server()\n</pre> viewer.stop_server() <p>In case you wish to delete data:</p> In\u00a0[28]: Copied! <pre>team_store.delete_data('reanalysis-era5-single-levels-monthly-means-subset-2006-2010_TMH.zarr')\n</pre> team_store.delete_data('reanalysis-era5-single-levels-monthly-means-subset-2006-2010_TMH.zarr') In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/Create_Atmospheric_Cubes_CDS/#xcube-data-store-framework-climate-data-store","title":"xcube Data Store Framework \u2013 Climate Data Store\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Create_Atmospheric_Cubes_CDS/#creating-and-writing-data-cubes","title":"Creating and writing data cubes\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Create_Atmospheric_Cubes_CDS/#obtain-a-cds-personal-access-token","title":"Obtain a CDS Personal Access Token\u00b6","text":"<p>You can obtain a CDS Personal Access Token as follows:</p> <ol> <li>Create a user account on the CDS Website.</li> <li>Log in to the website with your username and password.</li> <li>Navigate to your user page, where you can find your Personal Access Token.</li> </ol>"},{"location":"guide/jupyterlab/notebooks/Create_Atmospheric_Cubes_CDS/#configure-cds-api-access","title":"Configure CDS API access\u00b6","text":"<p>Your CDS Personal Access Token must be made available to the CDS API library. You can do this by creating a file named <code>.cdsapirc</code> in your home directory, with the following format:</p> <pre><code>url: https://cds-beta.climate.copernicus.eu/api\nkey: &lt;PERSONAL-ACCESS-TOKEN&gt;\n</code></pre> <p>Replace <code>&lt;PERSONAL-ACCESS-TOKEN&gt;</code> with your Personal Access Token.</p> <p>Then export the <code>CDSAPI_URL</code> and <code>CDSAPI_KEY</code> environment variables:</p> <pre>export CDSAPI_URL=https://cds-beta.climate.copernicus.eu/api\nexport CDSAPI_KEY=[PERSONAL-ACCESS-TOKEN]\n</pre>"},{"location":"guide/jupyterlab/notebooks/Create_Atmospheric_Cubes_CDS/#agree-to-the-terms-of-use-for-the-datasets-you-require","title":"Agree to the terms of use for the datasets you require\u00b6","text":"<p>The datasets available through CDS have associated terms of use. Before accessing a dataset via the API, you must agree to its terms of use, which can only be done via the CDS website, as follows:</p> <ol> <li>Log in to the CDS website, and go to 'Datasets' to find the dataset you require.</li> <li>On the dataset's web page, select the \u2018Download\u2019 tab.</li> <li>Scroll to the bottom of the page, and you will see a section titled \u2018Terms of use\u2019, which will contain either an \u2018Accept terms\u2019 button to allow you to accept the terms, or a confirmation that you have already accepted the terms.</li> </ol> <p>Once you have accepted the terms on the website, the dataset will also be made available to you through the API.</p>"},{"location":"guide/jupyterlab/notebooks/Create_Atmospheric_Cubes_CDS/#connect-to-cds-store","title":"Connect to cds store\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Create_Atmospheric_Cubes_CDS/#retrieve-and-open-requested-dataset","title":"Retrieve and open requested dataset\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Create_Atmospheric_Cubes_CDS/#lets-set-a-bbox","title":"Let's set a bbox\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Create_Atmospheric_Cubes_CDS/#now-set-the-other-parameters-for-opening-the-dataset-from-the-store","title":"Now set the other parameters for opening the dataset from the store:\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Create_Atmospheric_Cubes_CDS/#write-cube-in-team-storage","title":"Write cube in team storage\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Create_Atmospheric_Cubes_CDS/#open-data-from-team-storage","title":"Open data from team storage\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Create_Atmospheric_Cubes_CDS/#visualize-the-dataset-using-xcube-viewer","title":"Visualize the dataset using xcube viewer\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Create_Zarr_With_Slices_SentinelHub/","title":"Create Zarr With Slices SentinelHub","text":"<p>Please note:</p> <p>In order to access data from Sentinel Hub, you need Sentinel Hub API credentials. They may be passed as store parameters (see further below) or exported from environment variables. In case you have not exported them already, you may also set them by uncommenting the cell below and adjusting the content to your access credentials. However, we do not recommend this method!</p> <ul> <li>Within DeepESDL there is the possiblity to apply for SentinelHub access - please contact the DeepESDL team :)</li> <li>This notebook runs with the python environment <code>deepesdl-xcube-1.7.0</code>, please checkout the documentation for help on changing the environment.**</li> </ul> In\u00a0[1]: Copied! <pre>import os\n\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\n\n# mandatory imports\nfrom xcube.core.store import find_data_store_extensions\nfrom xcube.core.store import get_data_store_params_schema\nfrom xcube.core.store import new_data_store\nfrom zappend.api import zappend\n</pre> import os  import numpy as np import pandas as pd import xarray as xr  # mandatory imports from xcube.core.store import find_data_store_extensions from xcube.core.store import get_data_store_params_schema from xcube.core.store import new_data_store from zappend.api import zappend In\u00a0[2]: Copied! <pre>sh_store = new_data_store('sentinelhub', num_retries=400)\n</pre> sh_store = new_data_store('sentinelhub', num_retries=400) <p>Which datasets are supported by this data store?</p> In\u00a0[3]: Copied! <pre>sh_store.list_data_ids()\n</pre> sh_store.list_data_ids() Out[3]: <pre>['S2L1C', 'S1GRD', 'S2L2A', 'DEM']</pre> <p>Describe the S2L2A dataset. The <code>open_params_schema</code> describes the parameters that can be passed to the data store's <code>open_data()</code> method.</p> In\u00a0[4]: Copied! <pre>sh_store.describe_data('S2L2A')\n</pre> sh_store.describe_data('S2L2A') Out[4]: <pre>&lt;xcube.core.store.descriptor.DatasetDescriptor at 0x7fb102d20a50&gt;</pre> <p>Let's open a subset of this dataset to familiarize with its representation as an <code>xarray.Dataset</code>. The dataset is opened lazily, that is, data is loaded only at the time it is needed:</p> In\u00a0[5]: Copied! <pre>s2_cube = sh_store.open_data(\n    'S2L2A',\n    variable_names=['B02','B03','B04'],\n    tile_size=[1024, 1024],\n    spatial_res=0.00018,   # = 20.038 meters in degree\n    bbox=[10.0, 54.27, 11.0, 54.6],\n    time_range=[f'2023-07-08', f'2023-07-12'],\n    time_period='1D'\n)\ns2_cube\n</pre> s2_cube = sh_store.open_data(     'S2L2A',     variable_names=['B02','B03','B04'],     tile_size=[1024, 1024],     spatial_res=0.00018,   # = 20.038 meters in degree     bbox=[10.0, 54.27, 11.0, 54.6],     time_range=[f'2023-07-08', f'2023-07-12'],     time_period='1D' ) s2_cube Out[5]: <pre>&lt;xarray.Dataset&gt; Size: 755MB\nDimensions:    (time: 5, lat: 2048, lon: 6144, bnds: 2)\nCoordinates:\n  * lat        (lat) float64 16kB 54.64 54.64 54.64 54.64 ... 54.27 54.27 54.27\n  * lon        (lon) float64 49kB 10.0 10.0 10.0 10.0 ... 11.11 11.11 11.11\n  * time       (time) datetime64[ns] 40B 2023-07-08T12:00:00 ... 2023-07-12T1...\n    time_bnds  (time, bnds) datetime64[ns] 80B dask.array&lt;chunksize=(5, 2), meta=np.ndarray&gt;\nDimensions without coordinates: bnds\nData variables:\n    B02        (time, lat, lon) float32 252MB dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n    B03        (time, lat, lon) float32 252MB dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n    B04        (time, lat, lon) float32 252MB dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\nAttributes: (12/13)\n    Conventions:               CF-1.7\n    title:                     S2L2A Data Cube Subset\n    history:                   [{'program': 'xcube_sh.chunkstore.SentinelHubC...\n    date_created:              2024-09-11T08:47:04.855222\n    time_coverage_start:       2023-07-08T00:00:00+00:00\n    time_coverage_end:         2023-07-13T00:00:00+00:00\n    ...                        ...\n    time_coverage_resolution:  P1DT0H0M0S\n    geospatial_lon_min:        10.0\n    geospatial_lat_min:        54.27\n    geospatial_lon_max:        11.10592\n    geospatial_lat_max:        54.63864\n    processing_level:          L2A</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 5</li><li>lat: 2048</li><li>lon: 6144</li><li>bnds: 2</li></ul></li><li>Coordinates: (4)<ul><li>lat(lat)float6454.64 54.64 54.64 ... 54.27 54.27units :decimal_degreeslong_name :latitudestandard_name :latitude<pre>array([54.63855, 54.63837, 54.63819, ..., 54.27045, 54.27027, 54.27009])</pre></li><li>lon(lon)float6410.0 10.0 10.0 ... 11.11 11.11units :decimal_degreeslong_name :longitudestandard_name :longitude<pre>array([10.00009, 10.00027, 10.00045, ..., 11.10547, 11.10565, 11.10583])</pre></li><li>time(time)datetime64[ns]2023-07-08T12:00:00 ... 2023-07-...standard_name :timebounds :time_bnds<pre>array(['2023-07-08T12:00:00.000000000', '2023-07-09T12:00:00.000000000',\n       '2023-07-10T12:00:00.000000000', '2023-07-11T12:00:00.000000000',\n       '2023-07-12T12:00:00.000000000'], dtype='datetime64[ns]')</pre></li><li>time_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(5, 2), meta=np.ndarray&gt;standard_name :time  Array   Chunk   Bytes   80 B   80 B   Shape   (5, 2)   (5, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 5 </li></ul></li><li>Data variables: (3)<ul><li>B02(time, lat, lon)float32dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;sample_type :FLOAT32units :reflectancewavelength :492.25wavelength_a :492.4wavelength_b :492.1bandwidth :66.0bandwidth_a :66bandwidth_b :66resolution :10  Array   Chunk   Bytes   240.00 MiB   4.00 MiB   Shape   (5, 2048, 6144)   (1, 1024, 1024)   Dask graph   60 chunks in 2 graph layers   Data type   float32 numpy.ndarray  6144 2048 5 </li><li>B03(time, lat, lon)float32dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;sample_type :FLOAT32units :reflectancewavelength :559.4wavelength_a :559.8wavelength_b :559bandwidth :36.0bandwidth_a :36bandwidth_b :36resolution :10  Array   Chunk   Bytes   240.00 MiB   4.00 MiB   Shape   (5, 2048, 6144)   (1, 1024, 1024)   Dask graph   60 chunks in 2 graph layers   Data type   float32 numpy.ndarray  6144 2048 5 </li><li>B04(time, lat, lon)float32dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;sample_type :FLOAT32units :reflectancewavelength :664.75wavelength_a :664.6wavelength_b :664.9bandwidth :31.0bandwidth_a :31bandwidth_b :31resolution :10  Array   Chunk   Bytes   240.00 MiB   4.00 MiB   Shape   (5, 2048, 6144)   (1, 1024, 1024)   Dask graph   60 chunks in 2 graph layers   Data type   float32 numpy.ndarray  6144 2048 5 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([          54.63855,           54.63837,           54.63819,\n                 54.63801,           54.63783,           54.63765,\n                 54.63747,           54.63729,           54.63711,\n                 54.63693,\n       ...\n       54.271710000000006, 54.271530000000006, 54.271350000000005,\n       54.271170000000005, 54.270990000000005, 54.270810000000004,\n       54.270630000000004, 54.270450000000004, 54.270270000000004,\n                 54.27009],\n      dtype='float64', name='lat', length=2048))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([          10.00009,           10.00027,           10.00045,\n       10.000630000000001,           10.00081,           10.00099,\n                 10.00117,           10.00135,           10.00153,\n       10.001710000000001,\n       ...\n       11.104209999999998, 11.104389999999999, 11.104569999999999,\n                 11.10475,           11.10493,           11.10511,\n                 11.10529, 11.105469999999999, 11.105649999999999,\n                 11.10583],\n      dtype='float64', name='lon', length=6144))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2023-07-08 12:00:00', '2023-07-09 12:00:00',\n               '2023-07-10 12:00:00', '2023-07-11 12:00:00',\n               '2023-07-12 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (13)Conventions :CF-1.7title :S2L2A Data Cube Subsethistory :[{'program': 'xcube_sh.chunkstore.SentinelHubChunkStore', 'cube_config': {'dataset_name': 'S2L2A', 'band_names': ['B02', 'B03', 'B04'], 'band_fill_values': None, 'band_sample_types': None, 'band_units': None, 'tile_size': [1024, 1024], 'bbox': [10.0, 54.27, 11.10592, 54.63864], 'spatial_res': 0.00018, 'crs': 'WGS84', 'upsampling': 'NEAREST', 'downsampling': 'NEAREST', 'mosaicking_order': 'mostRecent', 'time_range': ['2023-07-08T00:00:00+00:00', '2023-07-12T00:00:00+00:00'], 'time_period': '1 days 00:00:00', 'time_tolerance': None, 'collection_id': None, 'four_d': False}}]date_created :2024-09-11T08:47:04.855222time_coverage_start :2023-07-08T00:00:00+00:00time_coverage_end :2023-07-13T00:00:00+00:00time_coverage_duration :P5DT0H0M0Stime_coverage_resolution :P1DT0H0M0Sgeospatial_lon_min :10.0geospatial_lat_min :54.27geospatial_lon_max :11.10592geospatial_lat_max :54.63864processing_level :L2A</li></ul> In\u00a0[6]: Copied! <pre>s2_cube.B03.isel(time=0).plot.imshow(vmax=0.2, cmap=\"bone\")\n</pre> s2_cube.B03.isel(time=0).plot.imshow(vmax=0.2, cmap=\"bone\") Out[6]: <pre>&lt;matplotlib.image.AxesImage at 0x7fb10012f690&gt;</pre> <p>The dataset <code>s2_cube</code> could be written to Zarr by simply using <code>s2_cube.to_zarr(path)</code> but that approach is not appropriate for larger datacubes for several reasons:</p> <ul> <li>Sentinel Hub may lower bandwidth due to the very many concurrent API requests it receives for each written chunk of our datacube.</li> <li>Your Sentinel Hub subscription may run out of credits.</li> <li>Network errors while reading or filesystem errors while writing may occur unexpectedly at any time.</li> <li>Potential, yet undiscovered memory leaks in the involved libraries may lead to out-of-memory conditions.</li> </ul> <p>Therefore we will introduce a new tool zappend that can avoid or at least mitigate some of the issues above. Before that we discuss the team storage where you can persist your datacubes.</p> In\u00a0[7]: Copied! <pre>S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"]\nS3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"]\nS3_USER_STORAGE_BUCKET = os.environ[\"S3_USER_STORAGE_BUCKET\"]\n</pre> S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"] S3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"] S3_USER_STORAGE_BUCKET = os.environ[\"S3_USER_STORAGE_BUCKET\"] In\u00a0[8]: Copied! <pre>team_store = new_data_store(\n    \"s3\", \n    root=S3_USER_STORAGE_BUCKET, \n    storage_options=dict(\n        anon=False, \n        key=S3_USER_STORAGE_KEY, \n        secret=S3_USER_STORAGE_SECRET\n    )\n)\n</pre> team_store = new_data_store(     \"s3\",      root=S3_USER_STORAGE_BUCKET,      storage_options=dict(         anon=False,          key=S3_USER_STORAGE_KEY,          secret=S3_USER_STORAGE_SECRET     ) ) <p>Which datasets are already in the team store?</p> In\u00a0[9]: Copied! <pre>team_store.list_data_ids()\n</pre> team_store.list_data_ids() Out[9]: <pre>['SST.levels',\n 'amazonas_v8.zarr',\n 'amazonas_v9.zarr',\n 'noise_trajectory.zarr',\n 'reanalysis-era5-single-levels-monthly-means-subset-2001-2010_TMH.zarr',\n 's2-demo-cube.zarr']</pre> <p>If you need to delete datasets from team storage, use the <code>has_data()</code> and <code>delete_data()</code> store methods:</p> In\u00a0[10]: Copied! <pre>if team_store.has_data('ERA5_example.levels'):\n    team_store.delete_data('ERA5_example.levels')\n</pre> if team_store.has_data('ERA5_example.levels'):     team_store.delete_data('ERA5_example.levels') <p>Our datacube specification:</p> In\u00a0[11]: Copied! <pre>variables = ['B02', 'B03', 'B04']\n\nx1 = 10.00  # degree\ny1 = 54.27  # degree\nx2 = 11.00  # degree\ny2 = 54.60  # degree\n\nbbox = x1, y1, x2, y2\n\nspatial_res = 0.00018   # = 20.038 meters in degree\n\ntime_range = '2023-06-01', '2023-06-05'  # For testing. adjust!\ntime_period = '1D'\n\ntarget_zarr = 's2-demo-cube.zarr'\n</pre> variables = ['B02', 'B03', 'B04']  x1 = 10.00  # degree y1 = 54.27  # degree x2 = 11.00  # degree y2 = 54.60  # degree  bbox = x1, y1, x2, y2  spatial_res = 0.00018   # = 20.038 meters in degree  time_range = '2023-06-01', '2023-06-05'  # For testing. adjust! time_period = '1D'  target_zarr = 's2-demo-cube.zarr' <p>Delete an existing target datacube:</p> In\u00a0[12]: Copied! <pre>if team_store.has_data(target_zarr):\n    team_store.delete_data(target_zarr)\n</pre> if team_store.has_data(target_zarr):     team_store.delete_data(target_zarr) <p>This is the aforementioned generator function that provides the stream of slice datasets:</p> In\u00a0[13]: Copied! <pre>def generate_slices(sh_store, time_ranges, variables, bbox, spatial_res, time_period):\n    for time_range in time_ranges:\n        print(f'Opening slice dataset for {time_range}')\n        slice_ds = sh_store.open_data(\n            'S2L2A',\n            variable_names=variables,\n            tile_size=[1024, 1024],\n            spatial_res=spatial_res,\n            bbox=bbox,\n            time_range=time_range,\n            time_period=time_period\n        )\n        print(f'Opened slice dataset of size {slice_ds.nbytes / (10 ** 9)} GB')\n\n        # Note, that you can modify slice_ds in any way you want here.\n        # E.g. you could add computed variables or update dataset attributes.\n        \n        yield slice_ds    \n</pre> def generate_slices(sh_store, time_ranges, variables, bbox, spatial_res, time_period):     for time_range in time_ranges:         print(f'Opening slice dataset for {time_range}')         slice_ds = sh_store.open_data(             'S2L2A',             variable_names=variables,             tile_size=[1024, 1024],             spatial_res=spatial_res,             bbox=bbox,             time_range=time_range,             time_period=time_period         )         print(f'Opened slice dataset of size {slice_ds.nbytes / (10 ** 9)} GB')          # Note, that you can modify slice_ds in any way you want here.         # E.g. you could add computed variables or update dataset attributes.                  yield slice_ds     <p>Define a function that creates a list of time ranges each having the duration given by <code>time_period</code>:</p> In\u00a0[14]: Copied! <pre>def get_time_ranges(time_range, time_period):\n    interval_td = pd.Timedelta(time_period)\n    one_sec = pd.Timedelta(\"1s\")\n    start_date, stop_date = pd.to_datetime(time_range)\n    dates = pd.date_range(start_date, stop_date + interval_td, freq=interval_td)\n    def to_date_str(date):\n        return date.strftime(\"%Y-%m-%d\")\n    return [\n        (to_date_str(dates[i]), to_date_str(dates[i + 1] - one_sec))\n        for i in range(len(dates) - 1)\n    ]\n</pre> def get_time_ranges(time_range, time_period):     interval_td = pd.Timedelta(time_period)     one_sec = pd.Timedelta(\"1s\")     start_date, stop_date = pd.to_datetime(time_range)     dates = pd.date_range(start_date, stop_date + interval_td, freq=interval_td)     def to_date_str(date):         return date.strftime(\"%Y-%m-%d\")     return [         (to_date_str(dates[i]), to_date_str(dates[i + 1] - one_sec))         for i in range(len(dates) - 1)     ] In\u00a0[15]: Copied! <pre>time_ranges = get_time_ranges(time_range, time_period)\n\n# Test it\ntime_ranges\n</pre> time_ranges = get_time_ranges(time_range, time_period)  # Test it time_ranges Out[15]: <pre>[('2023-06-01', '2023-06-01'),\n ('2023-06-02', '2023-06-02'),\n ('2023-06-03', '2023-06-03'),\n ('2023-06-04', '2023-06-04'),\n ('2023-06-05', '2023-06-05')]</pre> In\u00a0[16]: Copied! <pre># Remember, no data is read here, the generator produces a lazy slice stream!\nslices = generate_slices(sh_store, time_ranges, variables, bbox, spatial_res, time_period)\n</pre> # Remember, no data is read here, the generator produces a lazy slice stream! slices = generate_slices(sh_store, time_ranges, variables, bbox, spatial_res, time_period) In\u00a0[17]: Copied! <pre>zappend(\n    slices=slices,\n    target_dir=f\"s3://{S3_USER_STORAGE_BUCKET}/{target_zarr}\",\n    target_storage_options={\n        \"anon\": False,\n        \"key\": S3_USER_STORAGE_KEY,\n        \"secret\": S3_USER_STORAGE_SECRET\n    }\n)\n</pre> zappend(     slices=slices,     target_dir=f\"s3://{S3_USER_STORAGE_BUCKET}/{target_zarr}\",     target_storage_options={         \"anon\": False,         \"key\": S3_USER_STORAGE_KEY,         \"secret\": S3_USER_STORAGE_SECRET     } ) <pre>Opening slice dataset for ('2023-06-01', '2023-06-01')\nOpened slice dataset of size 0.151060504 GB\nOpening slice dataset for ('2023-06-02', '2023-06-02')\nOpened slice dataset of size 0.151060504 GB\nOpening slice dataset for ('2023-06-03', '2023-06-03')\nOpened slice dataset of size 0.151060504 GB\nOpening slice dataset for ('2023-06-04', '2023-06-04')\nOpened slice dataset of size 0.151060504 GB\nOpening slice dataset for ('2023-06-05', '2023-06-05')\nOpened slice dataset of size 0.151060504 GB\n</pre> In\u00a0[18]: Copied! <pre>list(team_store.get_data_ids())\n</pre> list(team_store.get_data_ids()) Out[18]: <pre>['SST.levels',\n 'amazonas_v8.zarr',\n 'amazonas_v9.zarr',\n 'noise_trajectory.zarr',\n 'reanalysis-era5-single-levels-monthly-means-subset-2001-2010_TMH.zarr',\n 's2-demo-cube.zarr']</pre> In\u00a0[19]: Copied! <pre>s2_cube = team_store.open_data(target_zarr)\ns2_cube\n</pre> s2_cube = team_store.open_data(target_zarr) s2_cube Out[19]: <pre>&lt;xarray.Dataset&gt; Size: 755MB\nDimensions:    (time: 5, lat: 2048, lon: 6144, bnds: 2)\nCoordinates:\n  * lat        (lat) float64 16kB 54.64 54.64 54.64 54.64 ... 54.27 54.27 54.27\n  * lon        (lon) float64 49kB 10.0 10.0 10.0 10.0 ... 11.11 11.11 11.11\n  * time       (time) datetime64[ns] 40B 2023-06-01T12:00:00 ... 2023-06-05T1...\n    time_bnds  (time, bnds) datetime64[ns] 80B dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\nDimensions without coordinates: bnds\nData variables:\n    B02        (time, lat, lon) float32 252MB dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n    B03        (time, lat, lon) float32 252MB dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n    B04        (time, lat, lon) float32 252MB dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\nAttributes: (12/13)\n    Conventions:               CF-1.7\n    date_created:              2024-09-11T08:47:12.058769\n    geospatial_lat_max:        54.63864\n    geospatial_lat_min:        54.27\n    geospatial_lon_max:        11.10592\n    geospatial_lon_min:        10.0\n    ...                        ...\n    processing_level:          L2A\n    time_coverage_duration:    P1DT0H0M0S\n    time_coverage_end:         2023-06-02T00:00:00+00:00\n    time_coverage_resolution:  P1DT0H0M0S\n    time_coverage_start:       2023-06-01T00:00:00+00:00\n    title:                     S2L2A Data Cube Subset</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 5</li><li>lat: 2048</li><li>lon: 6144</li><li>bnds: 2</li></ul></li><li>Coordinates: (4)<ul><li>lat(lat)float6454.64 54.64 54.64 ... 54.27 54.27long_name :latitudestandard_name :latitudeunits :decimal_degrees<pre>array([54.63855, 54.63837, 54.63819, ..., 54.27045, 54.27027, 54.27009])</pre></li><li>lon(lon)float6410.0 10.0 10.0 ... 11.11 11.11long_name :longitudestandard_name :longitudeunits :decimal_degrees<pre>array([10.00009, 10.00027, 10.00045, ..., 11.10547, 11.10565, 11.10583])</pre></li><li>time(time)datetime64[ns]2023-06-01T12:00:00 ... 2023-06-...bounds :time_bndsstandard_name :time<pre>array(['2023-06-01T12:00:00.000000000', '2023-06-02T12:00:00.000000000',\n       '2023-06-03T12:00:00.000000000', '2023-06-04T12:00:00.000000000',\n       '2023-06-05T12:00:00.000000000'], dtype='datetime64[ns]')</pre></li><li>time_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;standard_name :time  Array   Chunk   Bytes   80 B   16 B   Shape   (5, 2)   (1, 2)   Dask graph   5 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 5 </li></ul></li><li>Data variables: (3)<ul><li>B02(time, lat, lon)float32dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;bandwidth :66.0bandwidth_a :66bandwidth_b :66resolution :10sample_type :FLOAT32units :reflectancewavelength :492.25wavelength_a :492.4wavelength_b :492.1  Array   Chunk   Bytes   240.00 MiB   4.00 MiB   Shape   (5, 2048, 6144)   (1, 1024, 1024)   Dask graph   60 chunks in 2 graph layers   Data type   float32 numpy.ndarray  6144 2048 5 </li><li>B03(time, lat, lon)float32dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;bandwidth :36.0bandwidth_a :36bandwidth_b :36resolution :10sample_type :FLOAT32units :reflectancewavelength :559.4wavelength_a :559.8wavelength_b :559  Array   Chunk   Bytes   240.00 MiB   4.00 MiB   Shape   (5, 2048, 6144)   (1, 1024, 1024)   Dask graph   60 chunks in 2 graph layers   Data type   float32 numpy.ndarray  6144 2048 5 </li><li>B04(time, lat, lon)float32dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;bandwidth :31.0bandwidth_a :31bandwidth_b :31resolution :10sample_type :FLOAT32units :reflectancewavelength :664.75wavelength_a :664.6wavelength_b :664.9  Array   Chunk   Bytes   240.00 MiB   4.00 MiB   Shape   (5, 2048, 6144)   (1, 1024, 1024)   Dask graph   60 chunks in 2 graph layers   Data type   float32 numpy.ndarray  6144 2048 5 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([          54.63855,           54.63837,           54.63819,\n                 54.63801,           54.63783,           54.63765,\n                 54.63747,           54.63729,           54.63711,\n                 54.63693,\n       ...\n       54.271710000000006, 54.271530000000006, 54.271350000000005,\n       54.271170000000005, 54.270990000000005, 54.270810000000004,\n       54.270630000000004, 54.270450000000004, 54.270270000000004,\n                 54.27009],\n      dtype='float64', name='lat', length=2048))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([          10.00009,           10.00027,           10.00045,\n       10.000630000000001,           10.00081,           10.00099,\n                 10.00117,           10.00135,           10.00153,\n       10.001710000000001,\n       ...\n       11.104209999999998, 11.104389999999999, 11.104569999999999,\n                 11.10475,           11.10493,           11.10511,\n                 11.10529, 11.105469999999999, 11.105649999999999,\n                 11.10583],\n      dtype='float64', name='lon', length=6144))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2023-06-01 12:00:00', '2023-06-02 12:00:00',\n               '2023-06-03 12:00:00', '2023-06-04 12:00:00',\n               '2023-06-05 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (13)Conventions :CF-1.7date_created :2024-09-11T08:47:12.058769geospatial_lat_max :54.63864geospatial_lat_min :54.27geospatial_lon_max :11.10592geospatial_lon_min :10.0history :[{'cube_config': {'band_fill_values': None, 'band_names': ['B02', 'B03', 'B04'], 'band_sample_types': None, 'band_units': None, 'bbox': [10.0, 54.27, 11.10592, 54.63864], 'collection_id': None, 'crs': 'WGS84', 'dataset_name': 'S2L2A', 'downsampling': 'NEAREST', 'four_d': False, 'mosaicking_order': 'mostRecent', 'spatial_res': 0.00018, 'tile_size': [1024, 1024], 'time_period': '1 days 00:00:00', 'time_range': ['2023-06-01T00:00:00+00:00', '2023-06-01T00:00:00+00:00'], 'time_tolerance': None, 'upsampling': 'NEAREST'}, 'program': 'xcube_sh.chunkstore.SentinelHubChunkStore'}]processing_level :L2Atime_coverage_duration :P1DT0H0M0Stime_coverage_end :2023-06-02T00:00:00+00:00time_coverage_resolution :P1DT0H0M0Stime_coverage_start :2023-06-01T00:00:00+00:00title :S2L2A Data Cube Subset</li></ul> <p>Looks good, now let's clean up the example cube :)</p> In\u00a0[20]: Copied! <pre>team_store.delete_data(target_zarr)\n</pre> team_store.delete_data(target_zarr) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/Create_Zarr_With_Slices_SentinelHub/#xcube-data-store-framework-sentinel-hub-data-store","title":"xcube Data Store Framework \u2013 Sentinel Hub Data Store\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Create_Zarr_With_Slices_SentinelHub/#generating-a-larger-zarr-datacube-from-sentinel-2-level-2a-data","title":"Generating a larger Zarr datacube from Sentinel 2 Level-2A data\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Create_Zarr_With_Slices_SentinelHub/#access-data-from-sentinel-hub","title":"Access data from Sentinel Hub\u00b6","text":"<p>The xcube plugin <code>xcube-sh</code> provides a data store named <code>sentinelhub</code> that can be used to access Sentinel data in form 3-d datacubes of type <code>xarray.Dataset</code>.</p>"},{"location":"guide/jupyterlab/notebooks/Create_Zarr_With_Slices_SentinelHub/#access-team-storage","title":"Access team storage\u00b6","text":"<p>The DeepESDL team storage is a special writable bucket in AWS S3 object storage that is used by your team to allow for shared access of datasets. The xcube software provides an data store of type <code>s3</code> to access datasets in object storage.</p>"},{"location":"guide/jupyterlab/notebooks/Create_Zarr_With_Slices_SentinelHub/#generate-a-larger-zarr-datacube-from-sentinel-data","title":"Generate a larger Zarr datacube from Sentinel data\u00b6","text":"<p>As anounced, we will utilize the zappend tool for writing larger datacubes into team storage. The objective of zappend is to address recurring memory issues when generating large geospatial datacubes using the Zarr format by subsequently concatenating data slices along an append dimension, e.g., time (the default) for geospatial satellite observations. Each append step is atomic, that is, the append operation is a transaction that is rolled back, in case the append operation fails. This ensures integrity of the target data cube. Find our more:</p> <p>zappend can be used in a number of ways. In the following example we read smaller Sentinel datacubes, dataset slices, from the <code>sh_store</code> as described above. We use a Python generator function that provides a stream of dataset slices. The slice stream will be passed to zappend allowing it to process the slices subsequentially and open a slice dataset only when it is its turn. Another advantage of this approach is that a used slice dataset vanishes from memory, as no references are being kept anywhere.</p>"},{"location":"guide/jupyterlab/notebooks/Create_Zarr_With_Slices_SentinelHub/#open-generated-cube-from-team-storage","title":"Open generated cube from team storage\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_C3S_CDS_cubes/","title":"Generate C3S CDS cubes","text":"<p>This notebook demonstrates how to access Climate Data Store (CDS) data via the dedicated xcube store, which provides dynamic data cube views into each gridded data set.</p> <p>Please, also refer to the DeepESDL documentation and visit the platform's website for further information!</p> <p>Brockmann Consult, 2024</p> <p>This notebook runs with the python environment <code>deepesdl-xcube-1.7.0</code>, please checkout the documentation for help on changing the environment.</p> <p>Please note:</p> <p>To access data from the Climate Data Store, you need a CDS API key.</p> In\u00a0[17]: Copied! <pre>import os\nos.environ['CDSAPI_URL'] = 'https://cds-beta.climate.copernicus.eu/api'\nos.environ['CDSAPI_KEY'] = '[PERSONAL-ACCESS-TOKEN]'\n</pre> import os os.environ['CDSAPI_URL'] = 'https://cds-beta.climate.copernicus.eu/api' os.environ['CDSAPI_KEY'] = '[PERSONAL-ACCESS-TOKEN]' In\u00a0[2]: Copied! <pre># mandatory imports\nfrom xcube.core.store import find_data_store_extensions\nfrom xcube.core.store import get_data_store_params_schema\nfrom xcube.core.store import new_data_store\n\n# Utilities for notebook visualization\nfrom IPython.display import JSON\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n</pre> # mandatory imports from xcube.core.store import find_data_store_extensions from xcube.core.store import get_data_store_params_schema from xcube.core.store import new_data_store  # Utilities for notebook visualization from IPython.display import JSON import matplotlib as mpl import matplotlib.pyplot as plt <p>Configure matplotlib to display graphs inline directly in the notebook and set a sensible default figure size.</p> In\u00a0[3]: Copied! <pre>%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = 16,12\n</pre> %matplotlib inline plt.rcParams[\"figure.figsize\"] = 16,12 <p>Check whether the <code>cds</code> store is among the available stores, if not please follow the installation information from the top of this notebook.</p> In\u00a0[4]: Copied! <pre>JSON({e.name: e.metadata for e in find_data_store_extensions()})\n</pre> JSON({e.name: e.metadata for e in find_data_store_extensions()}) Out[4]: <pre>&lt;IPython.core.display.JSON object&gt;</pre> <p>Usually we need more information to get the actual data store object. Which data store parameters are available for <code>cds</code>?</p> In\u00a0[5]: Copied! <pre>get_data_store_params_schema('cds')\n</pre> get_data_store_params_schema('cds') Out[5]: <pre>&lt;xcube.util.jsonschema.JsonObjectSchema at 0x7fcdf5755790&gt;</pre> <p>Provide mandatory parameters to instantiate the store class:</p> In\u00a0[6]: Copied! <pre>store = new_data_store('cds')\nstore\n</pre> store = new_data_store('cds') store Out[6]: <pre>&lt;xcube_cds.store.CDSDataStore at 0x7fcdf56d8bd0&gt;</pre> <p>Which datasets are provided? (the list may contain both gridded and vector datasets):</p> In\u00a0[7]: Copied! <pre>JSON(store.list_data_ids())\n</pre> JSON(store.list_data_ids()) Out[7]: <pre>&lt;IPython.core.display.JSON object&gt;</pre> <p>Get more info about a specific dataset. This includes a description of the possible open formats:</p> In\u00a0[8]: Copied! <pre>store.describe_data('reanalysis-era5-single-levels-monthly-means:monthly_averaged_reanalysis')\n</pre> store.describe_data('reanalysis-era5-single-levels-monthly-means:monthly_averaged_reanalysis') Out[8]: <pre>&lt;xcube.core.store.descriptor.DatasetDescriptor at 0x7fcdf5415b50&gt;</pre> <p>There are 4 required parameters, so we need to provide them to open a dataset:</p> In\u00a0[14]: Copied! <pre>dataset = store.open_data('reanalysis-era5-single-levels-monthly-means:monthly_averaged_reanalysis', \n                          variable_names=['2m_temperature'], \n                          bbox=[-10, 45, 40, 65], \n                          spatial_res=0.25, \n                          time_range=['2006-01-01', '2010-12-31'])\ndataset\n</pre> dataset = store.open_data('reanalysis-era5-single-levels-monthly-means:monthly_averaged_reanalysis',                            variable_names=['2m_temperature'],                            bbox=[-10, 45, 40, 65],                            spatial_res=0.25,                            time_range=['2006-01-01', '2010-12-31']) dataset <pre>xcube-cds version 0.9.3\n2024-09-18 08:02:01,436 INFO Request ID is 2cf61c9d-916c-4ff4-93a5-5a46c901fa55\n2024-09-18 08:02:01,478 INFO status has been updated to accepted\n2024-09-18 08:02:05,317 INFO status has been updated to running\n2024-09-18 08:02:08,727 INFO Creating download object as as_source with files:\n['data_0.nc']\n2024-09-18 08:02:13,829 INFO status has been updated to successful\n</pre> <pre>1053781ec8bc0f7cd4903487de286a77.nc:   0%|          | 0.00/1.85M [00:00&lt;?, ?B/s]</pre> Out[14]: <pre>&lt;xarray.Dataset&gt; Size: 4MB\nDimensions:  (lat: 80, lon: 200, time: 60)\nCoordinates:\n    number   int64 8B ...\n  * lat      (lat) float64 640B 64.88 64.62 64.38 64.12 ... 45.62 45.38 45.12\n  * lon      (lon) float64 2kB -9.875 -9.625 -9.375 -9.125 ... 39.38 39.62 39.88\n    expver   (time) &lt;U4 960B ...\n  * time     (time) datetime64[ns] 480B 2006-01-01 2006-02-01 ... 2010-12-01\nData variables:\n    t2m      (time, lat, lon) float32 4MB ...\nAttributes:\n    GRIB_centre:             ecmf\n    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n    GRIB_subCentre:          0\n    Conventions:             CF-1.7\n    institution:             European Centre for Medium-Range Weather Forecasts\n    history:                 2024-09-18T08:02 GRIB to CDM+CF via cfgrib-0.9.1...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 80</li><li>lon: 200</li><li>time: 60</li></ul></li><li>Coordinates: (5)<ul><li>number()int64...long_name :ensemble member numerical idunits :1standard_name :realization<pre>[1 values with dtype=int64]</pre></li><li>lat(lat)float6464.88 64.62 64.38 ... 45.38 45.12units :degrees_northstandard_name :latitudelong_name :latitudestored_direction :decreasing<pre>array([64.875, 64.625, 64.375, 64.125, 63.875, 63.625, 63.375, 63.125, 62.875,\n       62.625, 62.375, 62.125, 61.875, 61.625, 61.375, 61.125, 60.875, 60.625,\n       60.375, 60.125, 59.875, 59.625, 59.375, 59.125, 58.875, 58.625, 58.375,\n       58.125, 57.875, 57.625, 57.375, 57.125, 56.875, 56.625, 56.375, 56.125,\n       55.875, 55.625, 55.375, 55.125, 54.875, 54.625, 54.375, 54.125, 53.875,\n       53.625, 53.375, 53.125, 52.875, 52.625, 52.375, 52.125, 51.875, 51.625,\n       51.375, 51.125, 50.875, 50.625, 50.375, 50.125, 49.875, 49.625, 49.375,\n       49.125, 48.875, 48.625, 48.375, 48.125, 47.875, 47.625, 47.375, 47.125,\n       46.875, 46.625, 46.375, 46.125, 45.875, 45.625, 45.375, 45.125])</pre></li><li>lon(lon)float64-9.875 -9.625 ... 39.62 39.88units :degrees_eaststandard_name :longitudelong_name :longitude<pre>array([-9.875, -9.625, -9.375, -9.125, -8.875, -8.625, -8.375, -8.125, -7.875,\n       -7.625, -7.375, -7.125, -6.875, -6.625, -6.375, -6.125, -5.875, -5.625,\n       -5.375, -5.125, -4.875, -4.625, -4.375, -4.125, -3.875, -3.625, -3.375,\n       -3.125, -2.875, -2.625, -2.375, -2.125, -1.875, -1.625, -1.375, -1.125,\n       -0.875, -0.625, -0.375, -0.125,  0.125,  0.375,  0.625,  0.875,  1.125,\n        1.375,  1.625,  1.875,  2.125,  2.375,  2.625,  2.875,  3.125,  3.375,\n        3.625,  3.875,  4.125,  4.375,  4.625,  4.875,  5.125,  5.375,  5.625,\n        5.875,  6.125,  6.375,  6.625,  6.875,  7.125,  7.375,  7.625,  7.875,\n        8.125,  8.375,  8.625,  8.875,  9.125,  9.375,  9.625,  9.875, 10.125,\n       10.375, 10.625, 10.875, 11.125, 11.375, 11.625, 11.875, 12.125, 12.375,\n       12.625, 12.875, 13.125, 13.375, 13.625, 13.875, 14.125, 14.375, 14.625,\n       14.875, 15.125, 15.375, 15.625, 15.875, 16.125, 16.375, 16.625, 16.875,\n       17.125, 17.375, 17.625, 17.875, 18.125, 18.375, 18.625, 18.875, 19.125,\n       19.375, 19.625, 19.875, 20.125, 20.375, 20.625, 20.875, 21.125, 21.375,\n       21.625, 21.875, 22.125, 22.375, 22.625, 22.875, 23.125, 23.375, 23.625,\n       23.875, 24.125, 24.375, 24.625, 24.875, 25.125, 25.375, 25.625, 25.875,\n       26.125, 26.375, 26.625, 26.875, 27.125, 27.375, 27.625, 27.875, 28.125,\n       28.375, 28.625, 28.875, 29.125, 29.375, 29.625, 29.875, 30.125, 30.375,\n       30.625, 30.875, 31.125, 31.375, 31.625, 31.875, 32.125, 32.375, 32.625,\n       32.875, 33.125, 33.375, 33.625, 33.875, 34.125, 34.375, 34.625, 34.875,\n       35.125, 35.375, 35.625, 35.875, 36.125, 36.375, 36.625, 36.875, 37.125,\n       37.375, 37.625, 37.875, 38.125, 38.375, 38.625, 38.875, 39.125, 39.375,\n       39.625, 39.875])</pre></li><li>expver(time)&lt;U4...<pre>[60 values with dtype=&lt;U4]</pre></li><li>time(time)datetime64[ns]2006-01-01 ... 2010-12-01standard_name :time<pre>array(['2006-01-01T00:00:00.000000000', '2006-02-01T00:00:00.000000000',\n       '2006-03-01T00:00:00.000000000', '2006-04-01T00:00:00.000000000',\n       '2006-05-01T00:00:00.000000000', '2006-06-01T00:00:00.000000000',\n       '2006-07-01T00:00:00.000000000', '2006-08-01T00:00:00.000000000',\n       '2006-09-01T00:00:00.000000000', '2006-10-01T00:00:00.000000000',\n       '2006-11-01T00:00:00.000000000', '2006-12-01T00:00:00.000000000',\n       '2007-01-01T00:00:00.000000000', '2007-02-01T00:00:00.000000000',\n       '2007-03-01T00:00:00.000000000', '2007-04-01T00:00:00.000000000',\n       '2007-05-01T00:00:00.000000000', '2007-06-01T00:00:00.000000000',\n       '2007-07-01T00:00:00.000000000', '2007-08-01T00:00:00.000000000',\n       '2007-09-01T00:00:00.000000000', '2007-10-01T00:00:00.000000000',\n       '2007-11-01T00:00:00.000000000', '2007-12-01T00:00:00.000000000',\n       '2008-01-01T00:00:00.000000000', '2008-02-01T00:00:00.000000000',\n       '2008-03-01T00:00:00.000000000', '2008-04-01T00:00:00.000000000',\n       '2008-05-01T00:00:00.000000000', '2008-06-01T00:00:00.000000000',\n       '2008-07-01T00:00:00.000000000', '2008-08-01T00:00:00.000000000',\n       '2008-09-01T00:00:00.000000000', '2008-10-01T00:00:00.000000000',\n       '2008-11-01T00:00:00.000000000', '2008-12-01T00:00:00.000000000',\n       '2009-01-01T00:00:00.000000000', '2009-02-01T00:00:00.000000000',\n       '2009-03-01T00:00:00.000000000', '2009-04-01T00:00:00.000000000',\n       '2009-05-01T00:00:00.000000000', '2009-06-01T00:00:00.000000000',\n       '2009-07-01T00:00:00.000000000', '2009-08-01T00:00:00.000000000',\n       '2009-09-01T00:00:00.000000000', '2009-10-01T00:00:00.000000000',\n       '2009-11-01T00:00:00.000000000', '2009-12-01T00:00:00.000000000',\n       '2010-01-01T00:00:00.000000000', '2010-02-01T00:00:00.000000000',\n       '2010-03-01T00:00:00.000000000', '2010-04-01T00:00:00.000000000',\n       '2010-05-01T00:00:00.000000000', '2010-06-01T00:00:00.000000000',\n       '2010-07-01T00:00:00.000000000', '2010-08-01T00:00:00.000000000',\n       '2010-09-01T00:00:00.000000000', '2010-10-01T00:00:00.000000000',\n       '2010-11-01T00:00:00.000000000', '2010-12-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>t2m(time, lat, lon)float32...GRIB_paramId :167GRIB_dataType :anGRIB_numberOfPoints :16000GRIB_typeOfLevel :surfaceGRIB_stepUnits :1GRIB_stepType :avguaGRIB_gridType :regular_llGRIB_uvRelativeToGrid :0GRIB_NV :0GRIB_Nx :200GRIB_Ny :80GRIB_cfName :unknownGRIB_cfVarName :t2mGRIB_gridDefinitionDescription :Latitude/Longitude GridGRIB_iDirectionIncrementInDegrees :0.25GRIB_iScansNegatively :0GRIB_jDirectionIncrementInDegrees :0.25GRIB_jPointsAreConsecutive :0GRIB_jScansPositively :0GRIB_latitudeOfFirstGridPointInDegrees :64.875GRIB_latitudeOfLastGridPointInDegrees :45.125GRIB_longitudeOfFirstGridPointInDegrees :-9.875GRIB_longitudeOfLastGridPointInDegrees :39.875GRIB_missingValue :3.4028234663852886e+38GRIB_name :2 metre temperatureGRIB_shortName :2tGRIB_totalNumber :0GRIB_units :Klong_name :2 metre temperatureunits :Kstandard_name :unknownGRIB_surface :0.0<pre>[960000 values with dtype=float32]</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([64.875, 64.625, 64.375, 64.125, 63.875, 63.625, 63.375, 63.125, 62.875,\n       62.625, 62.375, 62.125, 61.875, 61.625, 61.375, 61.125, 60.875, 60.625,\n       60.375, 60.125, 59.875, 59.625, 59.375, 59.125, 58.875, 58.625, 58.375,\n       58.125, 57.875, 57.625, 57.375, 57.125, 56.875, 56.625, 56.375, 56.125,\n       55.875, 55.625, 55.375, 55.125, 54.875, 54.625, 54.375, 54.125, 53.875,\n       53.625, 53.375, 53.125, 52.875, 52.625, 52.375, 52.125, 51.875, 51.625,\n       51.375, 51.125, 50.875, 50.625, 50.375, 50.125, 49.875, 49.625, 49.375,\n       49.125, 48.875, 48.625, 48.375, 48.125, 47.875, 47.625, 47.375, 47.125,\n       46.875, 46.625, 46.375, 46.125, 45.875, 45.625, 45.375, 45.125],\n      dtype='float64', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-9.875, -9.625, -9.375, -9.125, -8.875, -8.625, -8.375, -8.125, -7.875,\n       -7.625,\n       ...\n       37.625, 37.875, 38.125, 38.375, 38.625, 38.875, 39.125, 39.375, 39.625,\n       39.875],\n      dtype='float64', name='lon', length=200))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2006-01-01', '2006-02-01', '2006-03-01', '2006-04-01',\n               '2006-05-01', '2006-06-01', '2006-07-01', '2006-08-01',\n               '2006-09-01', '2006-10-01', '2006-11-01', '2006-12-01',\n               '2007-01-01', '2007-02-01', '2007-03-01', '2007-04-01',\n               '2007-05-01', '2007-06-01', '2007-07-01', '2007-08-01',\n               '2007-09-01', '2007-10-01', '2007-11-01', '2007-12-01',\n               '2008-01-01', '2008-02-01', '2008-03-01', '2008-04-01',\n               '2008-05-01', '2008-06-01', '2008-07-01', '2008-08-01',\n               '2008-09-01', '2008-10-01', '2008-11-01', '2008-12-01',\n               '2009-01-01', '2009-02-01', '2009-03-01', '2009-04-01',\n               '2009-05-01', '2009-06-01', '2009-07-01', '2009-08-01',\n               '2009-09-01', '2009-10-01', '2009-11-01', '2009-12-01',\n               '2010-01-01', '2010-02-01', '2010-03-01', '2010-04-01',\n               '2010-05-01', '2010-06-01', '2010-07-01', '2010-08-01',\n               '2010-09-01', '2010-10-01', '2010-11-01', '2010-12-01'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (6)GRIB_centre :ecmfGRIB_centreDescription :European Centre for Medium-Range Weather ForecastsGRIB_subCentre :0Conventions :CF-1.7institution :European Centre for Medium-Range Weather Forecastshistory :2024-09-18T08:02 GRIB to CDM+CF via cfgrib-0.9.14.0/ecCodes-2.36.0 with {\"source\": \"data.grib\", \"filter_by_keys\": {}, \"encode_cf\": [\"parameter\", \"time\", \"geography\", \"vertical\"]}</li></ul> <p>Plot the differences between successive time points in the dataset. We can see that the times are monotonically increasing (all the difference values are positive), but not equally spaced, since months are not all of the same length. The lowest values correspond to February; the four-year leap year cycle can also be discerned.</p> In\u00a0[15]: Copied! <pre>dataset.time.diff(dim='time').plot.line(figsize=(20, 4))\n</pre> dataset.time.diff(dim='time').plot.line(figsize=(20, 4)) Out[15]: <pre>[&lt;matplotlib.lines.Line2D at 0x7fcdf4a10d50&gt;]</pre> <p>We can explore these data by plotting a temperature map for selected time points. First, we select January 2001. Land areas \u2013 and mountain ranges in particular \u2013 show up on the map as colder regions.</p> In\u00a0[16]: Copied! <pre>t2m_2001_jan = dataset.t2m.sel(time='2001-01-01 00:00:00', method='nearest')\nt2m_2001_jan.plot.imshow(vmin=260, vmax=285, figsize=(14, 8), cmap='plasma')\n</pre> t2m_2001_jan = dataset.t2m.sel(time='2001-01-01 00:00:00', method='nearest') t2m_2001_jan.plot.imshow(vmin=260, vmax=285, figsize=(14, 8), cmap='plasma') Out[16]: <pre>&lt;matplotlib.image.AxesImage at 0x7fcdf0fb5890&gt;</pre> <p>For a more elegant and informative map, we define a function to plot a customized orthographic projection with overlaid coastlines and a grid.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/Generate_C3S_CDS_cubes/#xcube-data-store-framework-climate-data-store","title":"xcube Data Store Framework \u2013 Climate Data Store\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_C3S_CDS_cubes/#obtain-a-cds-personal-access-token","title":"Obtain a CDS Personal Access Token\u00b6","text":"<p>You can obtain a CDS Personal Access Token as follows:</p> <ol> <li>Create a user account on the CDS Website.</li> <li>Log in to the website with your username and password.</li> <li>Navigate to your user page, where you can find your Personal Access Token.</li> </ol>"},{"location":"guide/jupyterlab/notebooks/Generate_C3S_CDS_cubes/#configure-cds-api-access","title":"Configure CDS API access\u00b6","text":"<p>Your CDS Personal Access Token must be made available to the CDS API library. You can do this by creating a file named <code>.cdsapirc</code> in your home directory, with the following format:</p> <pre><code>url: https://cds-beta.climate.copernicus.eu/api\nkey: &lt;PERSONAL-ACCESS-TOKEN&gt;\n</code></pre> <p>Replace <code>&lt;PERSONAL-ACCESS-TOKEN&gt;</code> with your Personal Access Token.</p> <p>Then export the <code>CDSAPI_URL</code> and <code>CDSAPI_KEY</code> environment variables:</p> <pre>export CDSAPI_URL=https://cds-beta.climate.copernicus.eu/api\nexport CDSAPI_KEY=[PERSONAL-ACCESS-TOKEN]\n</pre>"},{"location":"guide/jupyterlab/notebooks/Generate_C3S_CDS_cubes/#agree-to-the-terms-of-use-for-the-datasets-you-require","title":"Agree to the terms of use for the datasets you require\u00b6","text":"<p>The datasets available through CDS have associated terms of use. Before accessing a dataset via the API, you must agree to its terms of use, which can only be done via the CDS website, as follows:</p> <ol> <li>Log in to the CDS website, and go to 'Datasets' to find the dataset you require.</li> <li>On the dataset's web page, select the \u2018Download\u2019 tab.</li> <li>Scroll to the bottom of the page, and you will see a section titled \u2018Terms of use\u2019, which will contain either an \u2018Accept terms\u2019 button to allow you to accept the terms, or a confirmation that you have already accepted the terms.</li> </ol> <p>Once you have accepted the terms on the website, the dataset will also be made available to you through the API.</p>"},{"location":"guide/jupyterlab/notebooks/Generate_CCI_cubes/","title":"Generate CCI cubes","text":"In\u00a0[1]: Copied! <pre># mandatory imports\nimport datetime\n\nimport matplotlib.pyplot as plt\n\n# Utilities for notebook visualization\nimport shapely.geometry\nfrom IPython.display import JSON\nfrom xcube.core.store import (\n    find_data_store_extensions,\n    get_data_store_params_schema,\n    new_data_store,\n)\n</pre> # mandatory imports import datetime  import matplotlib.pyplot as plt  # Utilities for notebook visualization import shapely.geometry from IPython.display import JSON from xcube.core.store import (     find_data_store_extensions,     get_data_store_params_schema,     new_data_store, ) <p>Configure matplotlib to display graphs inline directly in the notebook and set a sensible default figure size.</p> In\u00a0[2]: Copied! <pre>%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = 16, 8\n</pre> %matplotlib inline plt.rcParams[\"figure.figsize\"] = 16, 8 <p>Check whether the <code>cciodp</code> store is among the available stores, if not please follow the installation information from the top of this notebook.</p> In\u00a0[3]: Copied! <pre>JSON({e.name: e.metadata for e in find_data_store_extensions()})\n</pre> JSON({e.name: e.metadata for e in find_data_store_extensions()}) Out[3]: <pre>&lt;IPython.core.display.JSON object&gt;</pre> <p>Usually we need more information to get the actual data store object. Which data store parameters are available for <code>cciodp</code>?</p> In\u00a0[4]: Copied! <pre>get_data_store_params_schema(\"cciodp\")\n</pre> get_data_store_params_schema(\"cciodp\") <pre>&lt;frozen abc&gt;:106: FutureWarning: xarray subclass VectorDataCube should explicitly define __slots__\n</pre> Out[4]: <pre>&lt;xcube.util.jsonschema.JsonObjectSchema at 0x7f9ded4eef10&gt;</pre> <p>Provide mandatory parameters to instantiate the store class:</p> In\u00a0[5]: Copied! <pre>odp_store = new_data_store(\"cciodp\")\n</pre> odp_store = new_data_store(\"cciodp\") <p>Which datasets are provided? (the list may contain both gridded and vector datasets):</p> In\u00a0[6]: Copied! <pre>odp_store.list_data_ids()\n</pre> odp_store.list_data_ids() Out[6]: <pre>['esacci.AEROSOL.5-days.L3C.AEX.GOMOS.Envisat.AERGOM.3-00.r1',\n 'esacci.AEROSOL.climatology.L3.AAI.multi-sensor.multi-platform.MSAAI.1-7.r1',\n 'esacci.AEROSOL.day.L3.AAI.multi-sensor.multi-platform.MSAAI.1-7.r1',\n 'esacci.AEROSOL.day.L3C.AER_PRODUCTS.AATSR.Envisat.ADV.2-31.r1',\n 'esacci.AEROSOL.day.L3C.AER_PRODUCTS.AATSR.Envisat.ORAC.04-01-.r1',\n 'esacci.AEROSOL.day.L3C.AER_PRODUCTS.AATSR.Envisat.ORAC.04-01_seg-.r1',\n 'esacci.AEROSOL.day.L3C.AER_PRODUCTS.AATSR.Envisat.SU.4-3.r1',\n 'esacci.AEROSOL.day.L3C.AER_PRODUCTS.ATSR-2.ERS-2.ADV.2-31.r1',\n 'esacci.AEROSOL.day.L3C.AER_PRODUCTS.ATSR-2.ERS-2.ORAC.04-01-.r1',\n 'esacci.AEROSOL.day.L3C.AER_PRODUCTS.ATSR-2.ERS-2.ORAC.04-01_seg-.r1',\n 'esacci.AEROSOL.day.L3C.AER_PRODUCTS.ATSR-2.ERS-2.SU.4-3.r1',\n 'esacci.AEROSOL.day.L3C.AER_PRODUCTS.multi-sensor.multi-platform.AATSR-ENVISAT-ENS_DAILY.v2-6.r1',\n 'esacci.AEROSOL.day.L3C.AER_PRODUCTS.multi-sensor.multi-platform.ATSR2-ENVISAT-ENS_DAILY.v2-6.r1',\n 'esacci.AEROSOL.day.L3C.AOD.MERIS.Envisat.MERIS_ENVISAT.2-2.r1',\n 'esacci.AEROSOL.mon.L3.AAI.multi-sensor.multi-platform.MSAAI.1-7.r1',\n 'esacci.AEROSOL.mon.L3C.AER_PRODUCTS.AATSR.Envisat.ADV.2-31.r1',\n 'esacci.AEROSOL.mon.L3C.AER_PRODUCTS.AATSR.Envisat.ORAC.04-01-.r1',\n 'esacci.AEROSOL.mon.L3C.AER_PRODUCTS.AATSR.Envisat.ORAC.04-01_seg-.r1',\n 'esacci.AEROSOL.mon.L3C.AER_PRODUCTS.AATSR.Envisat.SU.4-3.r1',\n 'esacci.AEROSOL.mon.L3C.AER_PRODUCTS.ATSR-2.ERS-2.ADV.2-31.r1',\n 'esacci.AEROSOL.mon.L3C.AER_PRODUCTS.ATSR-2.ERS-2.ORAC.04-01-.r1',\n 'esacci.AEROSOL.mon.L3C.AER_PRODUCTS.ATSR-2.ERS-2.ORAC.04-01_seg-.r1',\n 'esacci.AEROSOL.mon.L3C.AER_PRODUCTS.ATSR-2.ERS-2.SU.4-3.r1',\n 'esacci.AEROSOL.mon.L3C.AER_PRODUCTS.multi-sensor.multi-platform.AATSR-ENVISAT-ENS_MONTHLY.v2-6.r1',\n 'esacci.AEROSOL.mon.L3C.AER_PRODUCTS.multi-sensor.multi-platform.ATSR2-ENVISAT-ENS_MONTHLY.v2-6.r1',\n 'esacci.AEROSOL.mon.L3C.AOD.MERIS.Envisat.MERIS_ENVISAT.2-2.r1',\n 'esacci.AEROSOL.yr.L3C.AER_PRODUCTS.AATSR.Envisat.AATSR-ENVISAT-ENS_ANNUAL.v2-6.r1',\n 'esacci.AEROSOL.yr.L3C.AER_PRODUCTS.ATSR-2.Envisat.ATSR2-ENVISAT-ENS_ANNUAL.v2-6.r1',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.CHANGE.4-0.r1',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.CHANGE.5-0.2016-2015',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.CHANGE.5-0.2017-2016',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.CHANGE.5-0.2018-2017',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.CHANGE.5-0.2019-2018',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.CHANGE.5-0.2020-2010',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.CHANGE.5-0.2020-2019',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.CHANGE.5-0.2021-2020',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.MERGED.2-0.r1',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.MERGED.3-0.r1',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.MERGED.4-0.r1',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.MERGED.5-0.10000m',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.MERGED.5-0.1000m',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.MERGED.5-0.100m',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.MERGED.5-0.25000m',\n 'esacci.BIOMASS.yr.L4.AGB.multi-sensor.multi-platform.MERGED.5-0.50000m',\n 'esacci.CLOUD.mon.L3C.CLD_PRODUCTS.MODIS.Aqua.MODIS_AQUA.2-0.r1',\n 'esacci.CLOUD.mon.L3C.CLD_PRODUCTS.MODIS.Terra.MODIS_TERRA.2-0.r1',\n 'esacci.CLOUD.mon.L3C.CLD_PRODUCTS.multi-sensor.multi-platform.ATSR2-AATSR.3-0.r1',\n 'esacci.CLOUD.mon.L3C.CLD_PRODUCTS.multi-sensor.multi-platform.AVHRR-AM.3-0.r1',\n 'esacci.CLOUD.mon.L3C.CLD_PRODUCTS.multi-sensor.multi-platform.AVHRR-PM.3-0.r1',\n 'esacci.CLOUD.mon.L3C.CLD_PRODUCTS.multi-sensor.multi-platform.MERIS-AATSR.2-0.r1',\n 'esacci.FIRE.mon.L4.BA.MODIS.Terra.MODIS_TERRA.v5-1.grid',\n 'esacci.FIRE.mon.L4.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.grid',\n 'esacci.FIRE.mon.L4.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.grid',\n 'esacci.FIRE.mon.L4.BA.multi-sensor.multi-platform.SYN.v1-1.grid',\n 'esacci.ICESHEETS.mon.IND.GMB.GRACE-instrument.GRACE.VARIOUS.1-3.greenland_gmb_time_series',\n 'esacci.ICESHEETS.unspecified.L4.IV.SAR-C-(Sentinel-1).multi-platform.UNSPECIFIED.1-1.greenland_s1_250m_20150610_20170321_Helheim',\n 'esacci.ICESHEETS.unspecified.L4.SEC.multi-sensor.multi-platform.UNSPECIFIED.0-1.greenland_sec_saral_altika',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.AMI-SAR.ERS-1.UNSPECIFIED.1-1.greenland_northern_drainage_basin_winter_1991_1992',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.AMI-SAR.ERS-2.UNSPECIFIED.1-1.greenland_margin_1995_1996',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).Sentinel-2A.UNSPECIFIED.1-0.greenland_s2_50m_20160508_20160518_docker_smith',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_seasonal_20170501_20170829_Helheim',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_seasonal_20170501_20170914_Petermann',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_seasonal_20170603_20170908_Jakobshavn',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_seasonal_20170625_20170810_79Fjord',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_seasonal_20170625_20170810_Zachariae',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_seasonal_20170630_20170814_Hagen',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_seasonal_20170715_20170814_Upernavik',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_seasonal_20170721_20170820_Kangerdlugssuaq',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_timeseries_20170501_20170829_Helheim',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_timeseries_20170501_20170914_Petermann',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_timeseries_20170603_20170908_Jakobshavn',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_timeseries_20170625_20170810_79Fjord',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_timeseries_20170625_20170810_Zachariae',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_timeseries_20170630_20170814_Hagen',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_timeseries_20170715_20170814_Upernavik',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.MSI-(Sentinel-2).multi-platform.UNSPECIFIED.1-1.greenland_s2_50m_timeseries_20170721_20170820_Kangerdlugssuaq',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.SAR-(RadarSat-2).RadarSat-2.UNSPECIFIED.1-0.greenland_map_winter_2013_2014',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.SAR-2000.multi-platform.UNSPECIFIED.1-0.greenland_csk_250m_timeseries_20120604_20141223_Jakobshavn',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.SAR-C-(Sentinel-1).Sentinel-1A.UNSPECIFIED.1-0.greenland_map_winter_2014_2015',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.SAR-C-(Sentinel-1).Sentinel-1A.UNSPECIFIED.1-2.greenland_map_winter_2015_2016',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.SAR-C-(Sentinel-1).multi-platform.UNSPECIFIED.1-0.greenland_map_winter_2016_2017',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.SAR-C-(Sentinel-1).multi-platform.UNSPECIFIED.1-0.greenland_map_winter_2017_2018',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.SAR-C-(Sentinel-1).multi-platform.UNSPECIFIED.1-1.greenland_s1_250m_20141010_20170317_Upernavik',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.SAR-C-(Sentinel-1).multi-platform.UNSPECIFIED.1-1.greenland_s1_250m_20141011_20170317_20150122_20170322_Hagen',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.SAR-C-(Sentinel-1).multi-platform.UNSPECIFIED.1-1.greenland_s1_250m_20141011_20170317_Jakobshavn',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.SAR-C-(Sentinel-1).multi-platform.UNSPECIFIED.1-1.greenland_s1_250m_20150118_20170321_Kangerlussuaq',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.SAR-C-(Sentinel-1).multi-platform.UNSPECIFIED.1-1.greenland_s1_250m_20150122_20170319_Petermann',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.SAR-C-(Sentinel-1).multi-platform.UNSPECIFIED.1-1.greenland_s1_250m_20150122_20170322_79-Fjord',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.SAR-C-(Sentinel-1).multi-platform.UNSPECIFIED.1-1.greenland_s1_250m_20150124_20170322_Storstroemmen',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.SAR-C-(Sentinel-1).multi-platform.UNSPECIFIED.1-1.greenland_s1_250m_20150126_20170322_Zachariae',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.multi-sensor.multi-platform.UNSPECIFIED.1-0.greenland_timeseries_Kangerlussuaq',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.multi-sensor.multi-platform.UNSPECIFIED.1-1.greenland_timeseries_2002_2010_Jakobshavn',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.multi-sensor.multi-platform.UNSPECIFIED.1-1.greenland_timeseries_Hagen',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.multi-sensor.multi-platform.UNSPECIFIED.1-1.greenland_timeseries_Helheim',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.multi-sensor.multi-platform.UNSPECIFIED.1-1.greenland_timeseries_Petermann',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.multi-sensor.multi-platform.UNSPECIFIED.1-1.greenland_timeseries_Storstrommen',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.multi-sensor.multi-platform.UNSPECIFIED.1-1.greenland_timeseries_Zachariae_79Fjord',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.multi-sensor.multi-platform.UNSPECIFIED.1-2.greenland_timeseries_1992_2010_Jakobshavn',\n 'esacci.ICESHEETS.unspecified.Unspecified.IV.multi-sensor.multi-platform.UNSPECIFIED.1-2.greenland_timeseries_Upernarvik',\n 'esacci.ICESHEETS.yr.Unspecified.GMB.GRACE-instrument.GRACE.UNSPECIFIED.1-2.greenland_gmb_timeseries',\n 'esacci.ICESHEETS.yr.Unspecified.GMB.GRACE-instrument.GRACE.UNSPECIFIED.1-4.greenland_gmb_time_series',\n 'esacci.ICESHEETS.yr.Unspecified.GMB.GRACE-instrument.GRACE.UNSPECIFIED.1-5.greenland_gmb_time_series',\n 'esacci.ICESHEETS.yr.Unspecified.IV.PALSAR.ALOS.UNSPECIFIED.1-1.greenland_margin_2006_2011',\n 'esacci.ICESHEETS.yr.Unspecified.SEC.SIRAL.CryoSat-2.UNSPECIFIED.2-2.greenland_sec_cryosat_2yr',\n 'esacci.ICESHEETS.yr.Unspecified.SEC.SIRAL.CryoSat-2.UNSPECIFIED.2-2.greenland_sec_cryosat_5yr',\n 'esacci.ICESHEETS.yr.Unspecified.SEC.multi-sensor.multi-platform.UNSPECIFIED.1-2.r1',\n 'esacci.LAKES.day.L3S.LK_PRODUCTS.multi-sensor.multi-platform.MERGED.v1-0.r1',\n 'esacci.LAKES.day.L3S.LK_PRODUCTS.multi-sensor.multi-platform.MERGED.v1-1.r1',\n 'esacci.LAKES.day.L3S.LK_PRODUCTS.multi-sensor.multi-platform.MERGED.v2-0-2.r1',\n 'esacci.LAKES.day.L3S.LK_PRODUCTS.multi-sensor.multi-platform.MERGED.v2-1-0.r1',\n 'esacci.LC.13-yrs.L4.WB.ASAR.Envisat.Map.4-0.r1',\n 'esacci.LC.5-yrs.L4.CHANGE.multi-sensor.multi-platform.HRLCC30-A01.v1-2.Africa',\n 'esacci.LC.5-yrs.L4.CHANGE.multi-sensor.multi-platform.HRLCC30-A02.v1-2.Amazonia',\n 'esacci.LC.5-yrs.L4.CHANGE.multi-sensor.multi-platform.HRLCC30-A03.v1-2.Siberia',\n 'esacci.LC.5-yrs.L4.Map.multi-sensor.multi-platform.HRLC30-A01.v1-2.Africa',\n 'esacci.LC.5-yrs.L4.Map.multi-sensor.multi-platform.HRLC30-A02.v1-2.Amazonia',\n 'esacci.LC.5-yrs.L4.Map.multi-sensor.multi-platform.HRLC30-A03.v1-2.Siberia',\n 'esacci.LC.yr.L4.LCCS.multi-sensor.multi-platform.Map.2-0-7.r1',\n 'esacci.LC.yr.L4.Map.multi-sensor.multi-platform.HRLC10-A01.v1-2.Africa',\n 'esacci.LC.yr.L4.Map.multi-sensor.multi-platform.HRLC10-A02.v1-2.Amazonia',\n 'esacci.LC.yr.L4.Map.multi-sensor.multi-platform.HRLC10-A03.v1-2.Siberia',\n 'esacci.LC.yr.L4.PFT.Unspecified.Unspecified.Map.2-0-8.r1',\n 'esacci.LST.3-hours.L3S.LST.multi-sensor.multi-platform.IRMGP.1-00.r1',\n 'esacci.LST.day.L3C.LST.AATSR.Envisat.ATSR_3.3-00.DAY',\n 'esacci.LST.day.L3C.LST.AATSR.Envisat.ATSR_3.3-00.NIGHT',\n 'esacci.LST.day.L3C.LST.ATSR-2.ERS-2.ATSR_2.3-00.DAY',\n 'esacci.LST.day.L3C.LST.ATSR-2.ERS-2.ATSR_2.3-00.NIGHT',\n 'esacci.LST.day.L3C.LST.MODIS.Aqua.MODISA.3-00.DAY',\n 'esacci.LST.day.L3C.LST.MODIS.Aqua.MODISA.3-00.NIGHT',\n 'esacci.LST.day.L3C.LST.MODIS.Terra.MODIST.3-00.DAY',\n 'esacci.LST.day.L3C.LST.MODIS.Terra.MODIST.3-00.NIGHT',\n 'esacci.LST.day.L3C.LST.SLSTR.Sentinel-3A.SLSTRA.3-00.DAY',\n 'esacci.LST.day.L3C.LST.SLSTR.Sentinel-3A.SLSTRA.3-00.NIGHT',\n 'esacci.LST.day.L3C.LST.SLSTR.Sentinel-3B.SLSTRB.3-00.DAY',\n 'esacci.LST.day.L3C.LST.SLSTR.Sentinel-3B.SLSTRB.3-00.NIGHT',\n 'esacci.LST.day.L3C.LST.multi-sensor.multi-platform.SSMI_SSMIS.v2-33.ASC',\n 'esacci.LST.day.L3C.LST.multi-sensor.multi-platform.SSMI_SSMIS.v2-33.DES',\n 'esacci.LST.day.L3S.LST.multi-sensor.multi-platform.IRCDR.2-00.DAY',\n 'esacci.LST.day.L3S.LST.multi-sensor.multi-platform.IRCDR.2-00.NIGHT',\n 'esacci.LST.mon.L3C.LST.AATSR.Envisat.ATSR_3.3-00.DAY',\n 'esacci.LST.mon.L3C.LST.AATSR.Envisat.ATSR_3.3-00.NIGHT',\n 'esacci.LST.mon.L3C.LST.ATSR-2.ERS-2.ATSR_2.3-00.DAY',\n 'esacci.LST.mon.L3C.LST.ATSR-2.ERS-2.ATSR_2.3-00.NIGHT',\n 'esacci.LST.mon.L3C.LST.MODIS.Aqua.MODISA.3-00.DAY',\n 'esacci.LST.mon.L3C.LST.MODIS.Aqua.MODISA.3-00.NIGHT',\n 'esacci.LST.mon.L3C.LST.MODIS.Terra.MODIST.3-00.DAY',\n 'esacci.LST.mon.L3C.LST.MODIS.Terra.MODIST.3-00.NIGHT',\n 'esacci.LST.mon.L3C.LST.SLSTR.Sentinel-3A.SLSTRA.3-00.DAY',\n 'esacci.LST.mon.L3C.LST.SLSTR.Sentinel-3A.SLSTRA.3-00.NIGHT',\n 'esacci.LST.mon.L3C.LST.SLSTR.Sentinel-3B.SLSTRB.3-00.DAY',\n 'esacci.LST.mon.L3C.LST.SLSTR.Sentinel-3B.SLSTRB.3-00.NIGHT',\n 'esacci.LST.mon.L3C.LST.multi-sensor.multi-platform.SSMI_SSMIS.v2-33.ASC',\n 'esacci.LST.mon.L3C.LST.multi-sensor.multi-platform.SSMI_SSMIS.v2-33.DES',\n 'esacci.LST.mon.L3S.LST.multi-sensor.multi-platform.IRCDR.2-00.DAY',\n 'esacci.LST.mon.L3S.LST.multi-sensor.multi-platform.IRCDR.2-00.NIGHT',\n 'esacci.LST.mon.L3S.LST.multi-sensor.multi-platform.IRMGP.1-00.00:00UTC',\n 'esacci.LST.mon.L3S.LST.multi-sensor.multi-platform.IRMGP.1-00.03:00UTC',\n 'esacci.LST.mon.L3S.LST.multi-sensor.multi-platform.IRMGP.1-00.06:00UTC',\n 'esacci.LST.mon.L3S.LST.multi-sensor.multi-platform.IRMGP.1-00.09:00UTC',\n 'esacci.LST.mon.L3S.LST.multi-sensor.multi-platform.IRMGP.1-00.12:00UTC',\n 'esacci.LST.mon.L3S.LST.multi-sensor.multi-platform.IRMGP.1-00.15:00UTC',\n 'esacci.LST.mon.L3S.LST.multi-sensor.multi-platform.IRMGP.1-00.18:00UTC',\n 'esacci.LST.mon.L3S.LST.multi-sensor.multi-platform.IRMGP.1-00.21:00UTC',\n 'esacci.LST.yr.L3C.LST.multi-sensor.multi-platform.SSMI_SSMIS.v2-33.ASC',\n 'esacci.LST.yr.L3C.LST.multi-sensor.multi-platform.SSMI_SSMIS.v2-33.DES',\n 'esacci.OC.5-days.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.5-days.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.5-days.L3S.IOP.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.5-days.L3S.IOP.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.5-days.L3S.K_490.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.5-days.L3S.K_490.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.5-days.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.5-days.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.5-days.L3S.RRS.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.5-days.L3S.RRS.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.8-days.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.8-days.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.8-days.L3S.IOP.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.8-days.L3S.IOP.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.8-days.L3S.K_490.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.8-days.L3S.K_490.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.8-days.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.8-days.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.8-days.L3S.RRS.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.8-days.L3S.RRS.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.day.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.day.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.day.L3S.IOP.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.day.L3S.IOP.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.day.L3S.K_490.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.day.L3S.K_490.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.day.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.day.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.day.L3S.RRS.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.day.L3S.RRS.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.mon.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.mon.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.mon.L3S.IOP.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.mon.L3S.IOP.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.mon.L3S.K_490.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.mon.L3S.K_490.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.mon.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.mon.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.mon.L3S.RRS.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.mon.L3S.RRS.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.yr.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.yr.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.yr.L3S.IOP.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.yr.L3S.IOP.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.yr.L3S.K_490.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.yr.L3S.K_490.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.yr.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.yr.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OC.yr.L3S.RRS.multi-sensor.multi-platform.MERGED.6-0.geographic',\n 'esacci.OC.yr.L3S.RRS.multi-sensor.multi-platform.MERGED.6-0.sinusoidal',\n 'esacci.OZONE.day.L3S.TC.multi-sensor.multi-platform.MERGED.fv0100.r1',\n 'esacci.OZONE.mon.L3.LP.GOMOS.Envisat.GOMOS_ENVISAT.v0001.r1',\n 'esacci.OZONE.mon.L3.LP.MIPAS.Envisat.MIPAS_ENVISAT.v0001.r1',\n 'esacci.OZONE.mon.L3.LP.OSIRIS.ODIN.OSIRIS_ODIN.v0001.r1',\n 'esacci.OZONE.mon.L3.LP.SCIAMACHY.Envisat.SCIAMACHY_ENVISAT.v0001.r1',\n 'esacci.OZONE.mon.L3.LP.SMR.ODIN.MZM.v0001.r1',\n 'esacci.OZONE.mon.L3.LP.SMR.ODIN.SMR_ODIN.v0001.r1',\n 'esacci.OZONE.mon.L3.NP.multi-sensor.multi-platform.MERGED.fv0002.r1',\n 'esacci.PERMAFROST.yr.L4.ALT.multi-sensor.multi-platform.ERA5_MODISLST_BIASCORRECTED.03-0.r1',\n 'esacci.PERMAFROST.yr.L4.ALT.multi-sensor.multi-platform.ERA5_MODISLST_BIASCORRECTED.04-0.r1',\n 'esacci.PERMAFROST.yr.L4.ALT.multi-sensor.multi-platform.MODISLST_CRYOGRID.03-0.r1',\n 'esacci.PERMAFROST.yr.L4.ALT.multi-sensor.multi-platform.MODISLST_CRYOGRID.04-0.r1',\n 'esacci.PERMAFROST.yr.L4.GTD.multi-sensor.multi-platform.ERA5_MODISLST_BIASCORRECTED.03-0.r1',\n 'esacci.PERMAFROST.yr.L4.GTD.multi-sensor.multi-platform.ERA5_MODISLST_BIASCORRECTED.04-0.r1',\n 'esacci.PERMAFROST.yr.L4.GTD.multi-sensor.multi-platform.MODISLST_CRYOGRID.03-0.r1',\n 'esacci.PERMAFROST.yr.L4.GTD.multi-sensor.multi-platform.MODISLST_CRYOGRID.04-0.r1',\n 'esacci.PERMAFROST.yr.L4.PFR.multi-sensor.multi-platform.ERA5_MODISLST_BIASCORRECTED.03-0.r1',\n 'esacci.PERMAFROST.yr.L4.PFR.multi-sensor.multi-platform.ERA5_MODISLST_BIASCORRECTED.04-0.r1',\n 'esacci.PERMAFROST.yr.L4.PFR.multi-sensor.multi-platform.MODISLST_CRYOGRID.03-0.r1',\n 'esacci.PERMAFROST.yr.L4.PFR.multi-sensor.multi-platform.MODISLST_CRYOGRID.04-0.r1',\n 'esacci.RD.satellite-orbit-frequency.L3S.WL.multi-sensor.multi-platform.MERGED.v1-1.r1',\n 'esacci.SEAICE.day.L3C.SICONC.ESMR-(Nimbus-5).Nimbus-5.NIMBUS5_ESMR-EASE2_NH.1-0.NH',\n 'esacci.SEAICE.day.L3C.SICONC.ESMR-(Nimbus-5).Nimbus-5.NIMBUS5_ESMR-EASE2_SH.1-0.SH',\n 'esacci.SEAICE.day.L4.SICONC.multi-sensor.multi-platform.AMSR_25kmEASE2.2-1.NH',\n 'esacci.SEAICE.day.L4.SICONC.multi-sensor.multi-platform.AMSR_25kmEASE2.2-1.SH',\n 'esacci.SEAICE.day.L4.SICONC.multi-sensor.multi-platform.AMSR_50kmEASE2.2-1.NH',\n 'esacci.SEAICE.day.L4.SICONC.multi-sensor.multi-platform.AMSR_50kmEASE2.2-1.SH',\n 'esacci.SEAICE.day.L4.SICONC.multi-sensor.multi-platform.RE_SSMI_12-5kmEASE2-NH.3-0.NH',\n 'esacci.SEAICE.day.L4.SICONC.multi-sensor.multi-platform.RE_SSMI_12-5kmEASE2-SH.3-0.SH',\n 'esacci.SEAICE.mon.L3C.SITHICK.RA-2.Envisat.NH25KMEASE2.2-0.r1',\n 'esacci.SEAICE.mon.L3C.SITHICK.RA-2.Envisat.NH25KMEASE2.3-0.r1',\n 'esacci.SEAICE.mon.L3C.SITHICK.RA-2.Envisat.SH50KMEASE2.2-0.r1',\n 'esacci.SEAICE.mon.L3C.SITHICK.RA-2.Envisat.SH50KMEASE2.3-0.r1',\n 'esacci.SEAICE.mon.L3C.SITHICK.SIRAL.CryoSat-2.NH25KMEASE2.2-0.r1',\n 'esacci.SEAICE.mon.L3C.SITHICK.SIRAL.CryoSat-2.NH25KMEASE2.3-0.r1',\n 'esacci.SEAICE.mon.L3C.SITHICK.SIRAL.CryoSat-2.SH50KMEASE2.2-0.r1',\n 'esacci.SEAICE.mon.L3C.SITHICK.SIRAL.CryoSat-2.SH50KMEASE2.3-0.r1',\n 'esacci.SEALEVEL.mon.IND.MSL.multi-sensor.multi-platform.MERGED.2-0.r1',\n 'esacci.SEALEVEL.mon.IND.MSLTR.multi-sensor.multi-platform.MERGED.2-0.r1',\n 'esacci.SEALEVEL.mon.L4.MSLA.multi-sensor.multi-platform.MERGED.2-0.r1',\n 'esacci.SEALEVEL.unspecified.IND.MSLAMPH.multi-sensor.multi-platform.MERGED.2-0.r1',\n 'esacci.SEASTATE.mon.L4.SWH.multi-sensor.multi-platform.MULTI_1M.3-0.r1',\n 'esacci.SEASURFACESALINITY.15-days.L4.SSS.multi-sensor.multi-platform.GLOBAL-MERGED_OI_Monthly_CENTRED_15Day_0-25deg.4-41.r1',\n 'esacci.SEASURFACESALINITY.15-days.L4.SSS.multi-sensor.multi-platform.MERGED_OI_Monthly_CENTRED_15Day_25km.2-31.r1',\n 'esacci.SEASURFACESALINITY.15-days.L4.SSS.multi-sensor.multi-platform.MERGED_OI_Monthly_CENTRED_15Day_25km.3-21.r1',\n 'esacci.SEASURFACESALINITY.15-days.L4.SSS.multi-sensor.multi-platform.POLAR-MERGED_OI_Monthly_CENTRED_15Day_25kmEASE2-NH.4-41.r1',\n 'esacci.SEASURFACESALINITY.15-days.L4.SSS.multi-sensor.multi-platform.POLAR-MERGED_OI_Monthly_CENTRED_15Day_25kmEASE2-SH.4-41.r1',\n 'esacci.SEASURFACESALINITY.day.L4.SSS.multi-sensor.multi-platform.GLOBAL-MERGED_OI_7DAY_RUNNINGMEAN_DAILY_0-25deg.4-41.r1',\n 'esacci.SEASURFACESALINITY.day.L4.SSS.multi-sensor.multi-platform.MERGED_OI_7DAY_RUNNINGMEAN_DAILY_25km.2-31.r1',\n 'esacci.SEASURFACESALINITY.day.L4.SSS.multi-sensor.multi-platform.MERGED_OI_7DAY_RUNNINGMEAN_DAILY_25km.3-21.r1',\n 'esacci.SEASURFACESALINITY.day.L4.SSS.multi-sensor.multi-platform.POLAR-MERGED_OI_7DAY_RUNNINGMEAN_DAILY_25kmEASE2-NH.4-41.r1',\n 'esacci.SEASURFACESALINITY.day.L4.SSS.multi-sensor.multi-platform.POLAR-MERGED_OI_7DAY_RUNNINGMEAN_DAILY_25kmEASE2-SH.4-41.r1',\n 'esacci.SNOW.day.L3C.SCFG.AATSR.Envisat.AATSR_ENVISAT.v1-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.ATSR-2.ERS-2.ATSR2_ERS2.v1-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR-2.NOAA-11.AVHRR_NOAA11.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR-2.NOAA-12.AVHRR_NOAA12.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR-2.NOAA-14.AVHRR_NOAA14.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR-2.NOAA-7.AVHRR_NOAA7.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR-2.NOAA-9.AVHRR_NOAA9.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR-3.Metop-A.AVHRR_METOPA.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR-3.Metop-B.AVHRR_METOPB.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR-3.Metop-C.AVHRR_METOPC.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR-3.NOAA-16.AVHRR_NOAA16.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR-3.NOAA-17.AVHRR_NOAA17.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR-3.NOAA-18.AVHRR_NOAA18.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR-3.NOAA-19.AVHRR_NOAA19.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR.NOAA-10.AVHRR_NOAA10.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR.NOAA-6.AVHRR_NOAA6.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR.NOAA-8.AVHRR_NOAA8.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.AVHRR.TIROS-N.AVHRR_TIROSN.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.MODIS.Terra.MODIS_TERRA.2-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.MODIS.Terra.MODIS_TERRA.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFG.multi-sensor.multi-platform.AVHRR_MERGED.2-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AATSR.Envisat.AATSR_ENVISAT.v1-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.ATSR-2.ERS-2.ATSR2_ERS2.v1-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR-2.NOAA-11.AVHRR_NOAA11.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR-2.NOAA-12.AVHRR_NOAA12.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR-2.NOAA-14.AVHRR_NOAA14.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR-2.NOAA-7.AVHRR_NOAA7.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR-2.NOAA-9.AVHRR_NOAA9.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR-3.Metop-A.AVHRR_METOPA.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR-3.Metop-B.AVHRR_METOPB.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR-3.Metop-C.AVHRR_METOPC.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR-3.NOAA-16.AVHRR_NOAA16.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR-3.NOAA-17.AVHRR_NOAA17.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR-3.NOAA-18.AVHRR_NOAA18.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR-3.NOAA-19.AVHRR_NOAA19.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR.NOAA-10.AVHRR_NOAA10.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR.NOAA-6.AVHRR_NOAA6.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR.NOAA-8.AVHRR_NOAA8.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.AVHRR.TIROS-N.AVHRR_TIROSN.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.MODIS.Terra.MODIS_TERRA.2-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.MODIS.Terra.MODIS_TERRA.3-0.r1',\n 'esacci.SNOW.day.L3C.SCFV.multi-sensor.multi-platform.AVHRR_MERGED.2-0.r1',\n 'esacci.SNOW.day.L3C.SWE.multi-sensor.multi-platform.MERGED.2-0.r1',\n 'esacci.SNOW.day.L3C.SWE.multi-sensor.multi-platform.MERGED.3-1.r1',\n 'esacci.SNOW.day.L3S.SCFG.multi-sensor.multi-platform.MERGED.1-0.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMS.multi-sensor.multi-platform.ACTIVE.05-2.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMS.multi-sensor.multi-platform.ACTIVE.v05-3.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMS.multi-sensor.multi-platform.ACTIVE.v06-1.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMS.multi-sensor.multi-platform.ACTIVE.v06-2.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMS.multi-sensor.multi-platform.ACTIVE.v07-1.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMS.multi-sensor.multi-platform.ACTIVE.v08-1.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMS.multi-sensor.multi-platform.ACTIVE.v09-1.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.COMBINED.05-2.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.COMBINED.v05-3.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.COMBINED.v06-1.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.COMBINED.v06-2.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.COMBINED.v07-1.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.COMBINED.v08-1.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.COMBINED.v09-1.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.COMBINED_ADJUSTED.v07-1.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.PASSIVE.05-2.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.PASSIVE.v05-3.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.PASSIVE.v06-1.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.PASSIVE.v06-2.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.PASSIVE.v07-1.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.PASSIVE.v08-1.r1',\n 'esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.PASSIVE.v09-1.r1',\n 'esacci.SST.climatology.L4.SSTdepth.multi-sensor.multi-platform.Climatology.2-2.r1',\n 'esacci.SST.day.L3C.SSTskin.AATSR.Envisat.AATSR.2-1.day',\n 'esacci.SST.day.L3C.SSTskin.AATSR.Envisat.AATSR.2-1.night',\n 'esacci.SST.day.L3C.SSTskin.ATSR-2.ERS-2.ATSR2.2-1.day',\n 'esacci.SST.day.L3C.SSTskin.ATSR-2.ERS-2.ATSR2.2-1.night',\n 'esacci.SST.day.L3C.SSTskin.ATSR.ERS-1.ATSR1.2-1.day',\n 'esacci.SST.day.L3C.SSTskin.ATSR.ERS-1.ATSR1.2-1.night',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-11.AVHRR11_G.2-1.day',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-11.AVHRR11_G.2-1.night',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-12.AVHRR12_G.2-1.day',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-12.AVHRR12_G.2-1.night',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-14.AVHRR14_G.2-1.day',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-14.AVHRR14_G.2-1.night',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-7.AVHRR07_G.2-1.day',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-7.AVHRR07_G.2-1.night',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-9.AVHRR09_G.2-1.day',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-9.AVHRR09_G.2-1.night',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-3.Metop-A.AVHRRMTA_G.2-1.day',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-3.Metop-A.AVHRRMTA_G.2-1.night',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-15.AVHRR15_G.2-1.day',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-15.AVHRR15_G.2-1.night',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-16.AVHRR16_G.2-1.day',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-16.AVHRR16_G.2-1.night',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-17.AVHRR17_G.2-1.day',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-17.AVHRR17_G.2-1.night',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-18.AVHRR18_G.2-1.day',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-18.AVHRR18_G.2-1.night',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-19.AVHRR19_G.2-1.day',\n 'esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-19.AVHRR19_G.2-1.night',\n 'esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.1-1.r1',\n 'esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.anomaly',\n 'esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.sst',\n 'esacci.SST.day.L4.SSTskin.Unspecified.Unspecified.GMPE.2-0.r1',\n 'esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AATSR.Envisat.AATSR.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L2P.SSTskin.ATSR-2.ERS-2.ATSR2.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L2P.SSTskin.ATSR.ERS-1.ATSR1.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-2.NOAA-11.AVHRR11_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-2.NOAA-12.AVHRR12_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-2.NOAA-14.AVHRR14_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-2.NOAA-7.AVHRR07_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-2.NOAA-9.AVHRR09_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-3.Metop-A.AVHRRMTA_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-3.NOAA-15.AVHRR15_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-3.NOAA-16.AVHRR16_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-3.NOAA-17.AVHRR17_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-3.NOAA-18.AVHRR18_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-3.NOAA-19.AVHRR19_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AATSR.Envisat.AATSR.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L3U.SSTskin.ATSR-2.ERS-2.ATSR2.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L3U.SSTskin.ATSR.ERS-1.ATSR1.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-2.NOAA-11.AVHRR11_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-2.NOAA-12.AVHRR12_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-2.NOAA-14.AVHRR14_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-2.NOAA-7.AVHRR07_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-2.NOAA-9.AVHRR09_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-3.Metop-A.AVHRRMTA_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-3.NOAA-15.AVHRR15_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-3.NOAA-16.AVHRR16_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-3.NOAA-17.AVHRR17_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-3.NOAA-18.AVHRR18_G.2-1.r1',\n 'esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-3.NOAA-19.AVHRR19_G.2-1.r1',\n 'esacci.VEGETATION.5-days.L3S.VP_PRODUCTS.VEGETATION.SPOT-5.MERGED.v1-0.r1',\n 'esacci.VEGETATION.5-days.L3S.VP_PRODUCTS.VEGETATION.multi-platform.MERGED.v1-0.r1',\n 'esacci.VEGETATION.5-days.L3S.VP_PRODUCTS.V\u00e9g\u00e9tation-P.PROBA-V.MERGED.v1-0.r1',\n 'esacci.VEGETATION.5-days.L3S.VP_PRODUCTS.multi-sensor.multi-platform.MERGED.v1-0.r1',\n 'esacci.WATERVAPOUR.day.L3S.TCWV.multi-sensor.multi-platform.TCWV_land_005deg.3-2.r1',\n 'esacci.WATERVAPOUR.day.L3S.TCWV.multi-sensor.multi-platform.TCWV_land_05deg.3-2.r1',\n 'esacci.WATERVAPOUR.mon.L3S.TCWV.multi-sensor.multi-platform.TCWV_land_005deg.3-2.r1',\n 'esacci.WATERVAPOUR.mon.L3S.TCWV.multi-sensor.multi-platform.TCWV_land_05deg.3-2.r1',\n 'esacci.FIRE.mon.L3S.BA.MODIS.Terra.MODIS_TERRA.v5-1.pixel~AREA_1',\n 'esacci.FIRE.mon.L3S.BA.MODIS.Terra.MODIS_TERRA.v5-1.pixel~AREA_2',\n 'esacci.FIRE.mon.L3S.BA.MODIS.Terra.MODIS_TERRA.v5-1.pixel~AREA_3',\n 'esacci.FIRE.mon.L3S.BA.MODIS.Terra.MODIS_TERRA.v5-1.pixel~AREA_4',\n 'esacci.FIRE.mon.L3S.BA.MODIS.Terra.MODIS_TERRA.v5-1.pixel~AREA_5',\n 'esacci.FIRE.mon.L3S.BA.MODIS.Terra.MODIS_TERRA.v5-1.pixel~AREA_6',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h32v13-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h32v13-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h32v13-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h32v14-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h32v14-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h32v14-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h32v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h32v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h32v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h33v13-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h33v13-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h33v13-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h33v14-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h33v14-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h33v14-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h33v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h33v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h33v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h33v16-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h33v16-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h33v16-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v13-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v13-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v13-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v14-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v14-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v14-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v16-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v16-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v16-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v17-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v17-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h34v17-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v13-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v13-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v13-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v14-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v14-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v14-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v16-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v16-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v16-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v17-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v17-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h35v17-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h36v13-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h36v13-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h36v13-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h36v14-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h36v14-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h36v14-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h36v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h36v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h36v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h36v16-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h36v16-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h36v16-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v13-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v13-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v13-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v14-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v14-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v14-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v16-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v16-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v16-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v17-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v17-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v17-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v18-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v18-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h37v18-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v13-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v13-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v13-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v14-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v14-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v14-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v16-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v16-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v16-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v17-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v17-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v17-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v18-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v18-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v18-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v19-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v19-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v19-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v20-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v20-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v20-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v21-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v21-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v21-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v22-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v22-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v22-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v23-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v23-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h38v23-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v13-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v13-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v13-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v14-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v14-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v14-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v16-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v16-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v16-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v17-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v17-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v17-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v18-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v18-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v18-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v19-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v19-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v19-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v20-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v20-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v20-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v21-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v21-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v21-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v22-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v22-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v22-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v23-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v23-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v23-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v24-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v24-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h39v24-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v13-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v13-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v13-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v14-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v14-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v14-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v16-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v16-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v16-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v17-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v17-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v17-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v18-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v18-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v18-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v19-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v19-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v19-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v20-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v20-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v20-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v21-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v21-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v21-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v22-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v22-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v22-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v23-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v23-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v23-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v24-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v24-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h40v24-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v13-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v13-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v13-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v14-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v14-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v14-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v16-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v16-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v16-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v17-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v17-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v17-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v18-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v18-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v18-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v19-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v19-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v19-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v20-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v20-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v20-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v21-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v21-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v21-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v22-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v22-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v22-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v23-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v23-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v23-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v24-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v24-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h41v24-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v13-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v13-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v13-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v14-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v14-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v14-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v16-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v16-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v16-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v17-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v17-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v17-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v18-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v18-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v18-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v19-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v19-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v19-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v20-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v20-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v20-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v21-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v21-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v21-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v22-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v22-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v22-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v23-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v23-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v23-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v24-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v24-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h42v24-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v13-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v13-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v13-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v14-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v14-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v14-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v16-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v16-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v16-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v17-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v17-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v17-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v18-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v18-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v18-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v19-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v19-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v19-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v20-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v20-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v20-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v21-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v21-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v21-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v22-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v22-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h43v22-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v14-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v14-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v14-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v16-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v16-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v16-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v17-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v17-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v17-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v18-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v18-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v18-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v20-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v20-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v20-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v21-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v21-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v21-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v22-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v22-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v22-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v23-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v23-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h44v23-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v16-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v16-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v16-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v17-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v17-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v17-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v20-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v20-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v20-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v21-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v21-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v21-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v22-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v22-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v22-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v23-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v23-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h45v23-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h46v15-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h46v15-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h46v15-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h46v16-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h46v16-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h46v16-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h46v20-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h46v20-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h46v20-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h46v21-fv2.0-CL',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h46v21-fv2.0-JD',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.2-0.pixel~h46v21-fv2.0-LC',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h32v13',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h32v14',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h32v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h33v13',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h33v14',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h33v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h33v16',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h34v13',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h34v14',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h34v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h34v16',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h34v17',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h35v13',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h35v14',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h35v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h35v16',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h35v17',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h36v13',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h36v14',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h36v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h36v16',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h37v13',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h37v14',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h37v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h37v16',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h37v17',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h37v18',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h38v13',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h38v14',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h38v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h38v16',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h38v17',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h38v18',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h38v19',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h38v20',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h38v21',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h38v22',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h38v23',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h39v13',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h39v14',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h39v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h39v16',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h39v17',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h39v18',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h39v19',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h39v20',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h39v21',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h39v22',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h39v23',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h39v24',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h40v13',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h40v14',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h40v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h40v16',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h40v17',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h40v18',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h40v19',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h40v20',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h40v21',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h40v22',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h40v23',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h40v24',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h41v13',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h41v14',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h41v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h41v16',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h41v17',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h41v18',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h41v19',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h41v20',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h41v21',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h41v22',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h41v23',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h41v24',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h42v13',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h42v14',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h42v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h42v16',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h42v17',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h42v18',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h42v19',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h42v20',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h42v21',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h42v22',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h42v23',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h42v24',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h43v13',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h43v14',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h43v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h43v16',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h43v17',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h43v18',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h43v19',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h43v20',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h43v21',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h43v22',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h44v14',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h44v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h44v16',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h44v17',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h44v18',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h44v20',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h44v21',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h44v22',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h44v23',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h45v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h45v16',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h45v17',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h45v20',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h45v21',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h45v22',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h45v23',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h46v15',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h46v16',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h46v20',\n 'esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel~h46v21',\n 'esacci.FIRE.mon.L3S.BA.multi-sensor.multi-platform.SYN.v1-1.pixel~AREA_1',\n 'esacci.FIRE.mon.L3S.BA.multi-sensor.multi-platform.SYN.v1-1.pixel~AREA_2',\n 'esacci.FIRE.mon.L3S.BA.multi-sensor.multi-platform.SYN.v1-1.pixel~AREA_3',\n 'esacci.FIRE.mon.L3S.BA.multi-sensor.multi-platform.SYN.v1-1.pixel~AREA_4',\n 'esacci.FIRE.mon.L3S.BA.multi-sensor.multi-platform.SYN.v1-1.pixel~AREA_5',\n 'esacci.FIRE.mon.L3S.BA.multi-sensor.multi-platform.SYN.v1-1.pixel~AREA_6',\n 'esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.AATSR.Envisat.AATSR-ENVISAT-ENS.v2-6.r1',\n 'esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.AATSR.Envisat.ADV.2-31.r1',\n 'esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.AATSR.Envisat.ORAC.04-01.r1',\n 'esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.AATSR.Envisat.SU.4-3.r1',\n 'esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.ATSR-2.ERS-2.ADV.2-31.r1',\n 'esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.ATSR-2.ERS-2.ORAC.04-01.r1',\n 'esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.ATSR-2.ERS-2.SU.4-3.r1',\n 'esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.multi-sensor.multi-platform.ATSR2-ENVISAT-ENS.v2-6.r1',\n 'esacci.AEROSOL.satellite-orbit-frequency.L2P.AOD.MERIS.Envisat.MERIS_ENVISAT.2-2.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CH4.SCIAMACHY.Envisat.IMAP.v7-2.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CH4.SCIAMACHY.Envisat.WFMD.v4-0.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CH4.TANSO-FTS-2.GOSAT-2.SRFP.v2-0-2.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CH4.TANSO-FTS-2.GOSAT-2.SRPR.v2-0-2.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CH4.TANSO-FTS.GOSAT.EMMA.ch4_v1-2.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CH4.TANSO-FTS.GOSAT.OCFP.v2-1.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CH4.TANSO-FTS.GOSAT.OCPR.v7-0.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CH4.TANSO-FTS.GOSAT.SRFP.v2-3-8.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CH4.TANSO-FTS.GOSAT.SRPR.v2-3-8.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CH4.TROPOMI.Sentinel-5P.WFMD.v1-8.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CO2.OCO.OCO-2.FOCAL.v10-1.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CO2.SCIAMACHY.Envisat.BESD.v02-01-02.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CO2.SCIAMACHY.Envisat.WFMD.v4-0.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CO2.TANSO-FTS-2.GOSAT-2.SRFP.v2-0-2.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CO2.TANSO-FTS.GOSAT.EMMA.v2-2c.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CO2.TANSO-FTS.GOSAT.OCFP.v7-0-1.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CO2.TANSO-FTS.GOSAT.SRFP.v2-3-8.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CO2.multi-sensor.multi-platform.EMMA.v2-2a.r1',\n 'esacci.GHG.satellite-orbit-frequency.L2.CO2.multi-sensor.multi-platform.EMMA.v2-2b.r1',\n 'esacci.ICESHEETS.unspecified.Unspecified.CFL.multi-sensor.multi-platform.UNSPECIFIED.v3-0.greenland',\n 'esacci.ICESHEETS.unspecified.Unspecified.GLL.multi-sensor.multi-platform.UNSPECIFIED.v1-3.greenland',\n 'esacci.SEAICE.satellite-orbit-frequency.L2P.SITHICK.RA-2.Envisat.NH.2-0.r1',\n 'esacci.SEAICE.satellite-orbit-frequency.L2P.SITHICK.RA-2.Envisat.NH.3-0.r1',\n 'esacci.SEAICE.satellite-orbit-frequency.L2P.SITHICK.RA-2.Envisat.SH.2-0.r1',\n 'esacci.SEAICE.satellite-orbit-frequency.L2P.SITHICK.RA-2.Envisat.SH.3-0.r1',\n 'esacci.SEAICE.satellite-orbit-frequency.L2P.SITHICK.SIRAL.CryoSat-2.NH.2-0.r1',\n 'esacci.SEAICE.satellite-orbit-frequency.L2P.SITHICK.SIRAL.CryoSat-2.NH.3-0.r1',\n 'esacci.SEAICE.satellite-orbit-frequency.L2P.SITHICK.SIRAL.CryoSat-2.SH.2-0.r1',\n 'esacci.SEAICE.satellite-orbit-frequency.L2P.SITHICK.SIRAL.CryoSat-2.SH.3-0.r1',\n 'esacci.SEALEVEL.satellite-orbit-frequency.L1.UNSPECIFIED.AltiKa.SARAL.UNSPECIFIED.v2-0.r1',\n 'esacci.SEALEVEL.satellite-orbit-frequency.L1.UNSPECIFIED.GFO-RA.GFO.UNSPECIFIED.v2-0.r1',\n 'esacci.SEALEVEL.satellite-orbit-frequency.L1.UNSPECIFIED.Poseidon-2.Jason-1.UNSPECIFIED.v2-0.r1',\n 'esacci.SEALEVEL.satellite-orbit-frequency.L1.UNSPECIFIED.Poseidon-3.Jason-2.UNSPECIFIED.v2-0.r1',\n 'esacci.SEALEVEL.satellite-orbit-frequency.L1.UNSPECIFIED.RA-2.Envisat.UNSPECIFIED.v2-0.r1',\n 'esacci.SEALEVEL.satellite-orbit-frequency.L1.UNSPECIFIED.RA.ERS-1.UNSPECIFIED.v2-0.r1',\n 'esacci.SEALEVEL.satellite-orbit-frequency.L1.UNSPECIFIED.RA.ERS-2.UNSPECIFIED.v2-0.r1',\n 'esacci.SEALEVEL.satellite-orbit-frequency.L1.UNSPECIFIED.SIRAL.CryoSat-2.UNSPECIFIED.v2-0.r1',\n 'esacci.SEALEVEL.satellite-orbit-frequency.L1.UNSPECIFIED.SSALT.Topex-Poseidon.UNSPECIFIED.v2-0.r1',\n 'esacci.ICESHEETS.yr.Unspecified.GMB.GRACE-instrument.GRACE.UNSPECIFIED.1-2.greenland_gmb_mass_trends',\n 'esacci.ICESHEETS.yr.Unspecified.GMB.GRACE-instrument.GRACE.UNSPECIFIED.1-3.greenland_gmb_mass_trends',\n 'esacci.ICESHEETS.yr.Unspecified.GMB.GRACE-instrument.GRACE.UNSPECIFIED.1-4.greenland_gmb_mass_trends',\n 'esacci.ICESHEETS.yr.Unspecified.GMB.GRACE-instrument.GRACE.UNSPECIFIED.1-5.greenland_gmb_mass_trends',\n 'esacci.SEALEVEL.mon.IND.MSLTR.multi-sensor.multi-platform.MERGED.2-2.ASA',\n 'esacci.SEALEVEL.mon.IND.MSLTR.multi-sensor.multi-platform.MERGED.2-2.BENGUELA',\n 'esacci.SEALEVEL.mon.IND.MSLTR.multi-sensor.multi-platform.MERGED.2-2.CARIBBEAN',\n 'esacci.SEALEVEL.mon.IND.MSLTR.multi-sensor.multi-platform.MERGED.2-2.GULFSTREAM',\n 'esacci.SEALEVEL.mon.IND.MSLTR.multi-sensor.multi-platform.MERGED.2-2.HUMBOLDT',\n 'esacci.SEALEVEL.mon.IND.MSLTR.multi-sensor.multi-platform.MERGED.2-2.MED_SEA',\n 'esacci.SEALEVEL.mon.IND.MSLTR.multi-sensor.multi-platform.MERGED.2-2.NE_ATL',\n 'esacci.SEALEVEL.mon.IND.MSLTR.multi-sensor.multi-platform.MERGED.2-2.N_INDIAN',\n 'esacci.SEALEVEL.mon.IND.MSLTR.multi-sensor.multi-platform.MERGED.2-2.SE_AFRICA',\n 'esacci.SEALEVEL.mon.IND.MSLTR.multi-sensor.multi-platform.MERGED.2-2.SE_ASIA',\n 'esacci.SEALEVEL.mon.IND.MSLTR.multi-sensor.multi-platform.MERGED.2-2.S_AUSTRALIA',\n 'esacci.SEALEVEL.mon.IND.MSLTR.multi-sensor.multi-platform.MERGED.2-2.WAFRICA',\n 'esacci.SEALEVEL.mon.IND.MSLTR.multi-sensor.multi-platform.MERGED.2-2.r1']</pre> <p>We may ask for a specific dataset ...</p> In\u00a0[7]: Copied! <pre>odp_store.has_data(\n    \"esacci.OC.5-days.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.3-1.geographic\"\n)\n</pre> odp_store.has_data(     \"esacci.OC.5-days.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.3-1.geographic\" ) Out[7]: <pre>False</pre> <p>... but in many cases we want to query for certain criteria. How can we do that?</p> In\u00a0[8]: Copied! <pre>odp_store.get_search_params_schema()\n</pre> odp_store.get_search_params_schema() Out[8]: <pre>&lt;xcube.util.jsonschema.JsonObjectSchema at 0x7f9dec048690&gt;</pre> <p>Now search, let's search for sea surface temperature data (SST) with a daily frequency:</p> In\u00a0[9]: Copied! <pre>iterator = odp_store.search_data(\n    cci_attrs=dict(ecv=\"SST\", frequency=\"day\", processing_level=\"L4\")\n)\nJSON([item.to_dict() for item in iterator])\n</pre> iterator = odp_store.search_data(     cci_attrs=dict(ecv=\"SST\", frequency=\"day\", processing_level=\"L4\") ) JSON([item.to_dict() for item in iterator]) <pre>/home/conda/deepesdl/909ac6b14a2e5a6c9dce2c44580b179aab714a951a659cea07707d516b61742a-20241209-101855-030681-626-xcube-1.7.1/lib/python3.11/site-packages/xcube_cci/cciodp.py:1960: CciOdpWarning: Variable \"field_name\" has no fill value, cannot set one. For parts where no data is available you will see random values. This is usually the case when data is missing for a time step.\n  warnings.warn(f'Variable \"{fixed_key}\" has no fill value, '\n</pre> Out[9]: <pre>&lt;IPython.core.display.JSON object&gt;</pre> <p>Which parameters must be passsed or are available to open the dataset?</p> In\u00a0[10]: Copied! <pre>odp_store.get_open_data_params_schema(\n    \"esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.sst\"\n)\n</pre> odp_store.get_open_data_params_schema(     \"esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.sst\" ) Out[10]: <pre>&lt;xcube.util.jsonschema.JsonObjectSchema at 0x7f9dec057710&gt;</pre> <p>There are no required parameters, so we can decide what parameters we would like to provide them to open a dataset:</p> In\u00a0[11]: Copied! <pre>odp_dataset = odp_store.open_data(\n    \"esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.sst\",\n    variable_names=[\"analysed_sst\"],\n    time_range=[\"1981-08-31\", \"2016-12-31\"],\n)\n</pre> odp_dataset = odp_store.open_data(     \"esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.sst\",     variable_names=[\"analysed_sst\"],     time_range=[\"1981-08-31\", \"2016-12-31\"], ) <p>Plot one time stamp of the dataset for a analysed_sst in order to take a brief look at the dataset:</p> In\u00a0[12]: Copied! <pre>odp_dataset\n</pre> odp_dataset Out[12]: <pre>&lt;xarray.Dataset&gt; Size: 3TB\nDimensions:       (time: 12907, lat: 3600, lon: 7200, bnds: 2)\nCoordinates:\n  * lat           (lat) float32 14kB -89.97 -89.93 -89.88 ... 89.88 89.93 89.97\n    lat_bnds      (lat, bnds) float32 29kB dask.array&lt;chunksize=(3600, 2), meta=np.ndarray&gt;\n  * lon           (lon) float32 29kB -180.0 -179.9 -179.9 ... 179.9 179.9 180.0\n    lon_bnds      (lon, bnds) float32 58kB dask.array&lt;chunksize=(7200, 2), meta=np.ndarray&gt;\n  * time          (time) datetime64[ns] 103kB 1981-08-31T12:00:00 ... 2016-12...\n    time_bnds     (time, bnds) datetime64[ns] 207kB dask.array&lt;chunksize=(12907, 2), meta=np.ndarray&gt;\nDimensions without coordinates: bnds\nData variables:\n    analysed_sst  (time, lat, lon) float64 3TB dask.array&lt;chunksize=(1, 1200, 2400), meta=np.ndarray&gt;\nAttributes:\n    Conventions:             CF-1.7\n    title:                   esacci.SST.day.L4.SSTdepth.multi-sensor.multi-pl...\n    date_created:            2024-12-10T14:29:32.620686\n    processing_level:        L4\n    time_coverage_start:     1981-08-31T00:00:00\n    time_coverage_end:       2017-01-01T00:00:00\n    time_coverage_duration:  P12907DT0H0M0S\n    history:                 [{'program': 'xcube_cci.chunkstore.CciChunkStore...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 12907</li><li>lat: 3600</li><li>lon: 7200</li><li>bnds: 2</li></ul></li><li>Coordinates: (6)<ul><li>lat(lat)float32-89.97 -89.93 ... 89.93 89.97standard_name :latitudeunits :degrees_northvalid_min :-90.0valid_max :90.0axis :Ycomment : Latitude geographical coordinates,WGS84 projectionlong_name :Latitudereference_datum :geographical coordinates, WGS84 projectionbounds :lat_bndsorig_data_type :float32fill_value :nansize :3600shape :[3600]chunk_sizes :3600file_chunk_sizes :3600data_type :float32dimensions :['lat']file_dimensions :['lat']<pre>array([-89.975, -89.925, -89.875, ...,  89.875,  89.925,  89.975],\n      dtype=float32)</pre></li><li>lat_bnds(lat, bnds)float32dask.array&lt;chunksize=(3600, 2), meta=np.ndarray&gt;units :degrees_northlong_name :Latitude cell boundariesvalid_min :-90.0valid_max :90.0comment :Contains the northern and southern boundaries of the grid cells.reference_datum :geographical coordinates, WGS84 projectionorig_data_type :float32fill_value :nansize :7200shape :[3600, 2]chunk_sizes :[3600, 2]file_chunk_sizes :[3600, 2]data_type :float32dimensions :['lat', 'bnds']file_dimensions :['lat', 'bnds']  Array   Chunk   Bytes   28.12 kiB   28.12 kiB   Shape   (3600, 2)   (3600, 2)   Dask graph   1 chunks in 2 graph layers   Data type   float32 numpy.ndarray  2 3600 </li><li>lon(lon)float32-180.0 -179.9 ... 179.9 180.0standard_name :longitudeunits :degrees_eastvalid_min :-180.0valid_max :180.0axis :Xcomment : Longitude geographical coordinates,WGS84 projectionlong_name :Longitudereference_datum :geographical coordinates, WGS84 projectionbounds :lon_bndsorig_data_type :float32fill_value :nansize :7200shape :[7200]chunk_sizes :7200file_chunk_sizes :7200data_type :float32dimensions :['lon']file_dimensions :['lon']<pre>array([-179.975, -179.925, -179.875, ...,  179.875,  179.925,  179.975],\n      dtype=float32)</pre></li><li>lon_bnds(lon, bnds)float32dask.array&lt;chunksize=(7200, 2), meta=np.ndarray&gt;valid_max :180.0comment :Contains the eastern and western boundaries of the grid cells.reference_datum :geographical coordinates, WGS84 projectionvalid_min :-180.0units :degrees_eastlong_name :Longitude cell boundariesorig_data_type :float32fill_value :nansize :14400shape :[7200, 2]chunk_sizes :[7200, 2]file_chunk_sizes :[7200, 2]data_type :float32dimensions :['lon', 'bnds']file_dimensions :['lon', 'bnds']  Array   Chunk   Bytes   56.25 kiB   56.25 kiB   Shape   (7200, 2)   (7200, 2)   Dask graph   1 chunks in 2 graph layers   Data type   float32 numpy.ndarray  2 7200 </li><li>time(time)datetime64[ns]1981-08-31T12:00:00 ... 2016-12-...standard_name :timebounds :time_bnds<pre>array(['1981-08-31T12:00:00.000000000', '1981-09-01T12:00:00.000000000',\n       '1981-09-02T12:00:00.000000000', ..., '2016-12-29T12:00:00.000000000',\n       '2016-12-30T12:00:00.000000000', '2016-12-31T12:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>time_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(12907, 2), meta=np.ndarray&gt;standard_name :time_bnds  Array   Chunk   Bytes   201.67 kiB   201.67 kiB   Shape   (12907, 2)   (12907, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 12907 </li></ul></li><li>Data variables: (1)<ul><li>analysed_sst(time, lat, lon)float64dask.array&lt;chunksize=(1, 1200, 2400), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_water_temperatureunits :kelvinvalid_min :-300valid_max :4500source :ATSR&lt;1,2&gt;-ESACCI-L3U-v2.0, AATSR-ESACCI-L3U-v2.0, AVHRR&lt;07,09,11,12,14,15,16,17,18,19&gt;_G-ESACCI-L3U-v2.0, AVHRRMTA_G-ESACCI-L3U-v2.0depth :20 cmorig_data_type :int16fill_value :-32768size :334549440000shape :[12907, 3600, 7200]chunk_sizes :[1, 1200, 2400]file_chunk_sizes :[1, 1200, 2400]data_type :int16dimensions :['time', 'lat', 'lon']file_dimensions :['time', 'lat', 'lon']  Array   Chunk   Bytes   2.43 TiB   21.97 MiB   Shape   (12907, 3600, 7200)   (1, 1200, 2400)   Dask graph   116163 chunks in 2 graph layers   Data type   float64 numpy.ndarray  7200 3600 12907 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ -89.9749984741211, -89.92500305175781,            -89.875,\n       -89.82499694824219,  -89.7750015258789,  -89.7249984741211,\n       -89.67500305175781,            -89.625, -89.57499694824219,\n        -89.5250015258789,\n       ...\n         89.5250015258789,  89.57499694824219,             89.625,\n        89.67500305175781,   89.7249984741211,   89.7750015258789,\n        89.82499694824219,             89.875,  89.92500305175781,\n         89.9749984741211],\n      dtype='float32', name='lat', length=3600))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.97500610351562,  -179.9250030517578,            -179.875,\n        -179.8249969482422, -179.77499389648438, -179.72500610351562,\n        -179.6750030517578,            -179.625,  -179.5749969482422,\n       -179.52499389648438,\n       ...\n        179.52499389648438,   179.5749969482422,             179.625,\n         179.6750030517578,  179.72500610351562,  179.77499389648438,\n         179.8249969482422,             179.875,   179.9250030517578,\n        179.97500610351562],\n      dtype='float32', name='lon', length=7200))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1981-08-31 12:00:00', '1981-09-01 12:00:00',\n               '1981-09-02 12:00:00', '1981-09-03 12:00:00',\n               '1981-09-04 12:00:00', '1981-09-05 12:00:00',\n               '1981-09-06 12:00:00', '1981-09-07 12:00:00',\n               '1981-09-08 12:00:00', '1981-09-09 12:00:00',\n               ...\n               '2016-12-22 12:00:00', '2016-12-23 12:00:00',\n               '2016-12-24 12:00:00', '2016-12-25 12:00:00',\n               '2016-12-26 12:00:00', '2016-12-27 12:00:00',\n               '2016-12-28 12:00:00', '2016-12-29 12:00:00',\n               '2016-12-30 12:00:00', '2016-12-31 12:00:00'],\n              dtype='datetime64[ns]', name='time', length=12907, freq=None))</pre></li></ul></li><li>Attributes: (8)Conventions :CF-1.7title :esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.sstdate_created :2024-12-10T14:29:32.620686processing_level :L4time_coverage_start :1981-08-31T00:00:00time_coverage_end :2017-01-01T00:00:00time_coverage_duration :P12907DT0H0M0Shistory :[{'program': 'xcube_cci.chunkstore.CciChunkStore', 'cube_params': {'variable_names': ['analysed_sst'], 'time_range': ['1981-08-31T00:00:00', '2016-12-31T00:00:00']}}]</li></ul> In\u00a0[13]: Copied! <pre>odp_dataset.analysed_sst.isel(time=0).plot.imshow(cmap=\"plasma\")\n</pre> odp_dataset.analysed_sst.isel(time=0).plot.imshow(cmap=\"plasma\") Out[13]: <pre>&lt;matplotlib.image.AxesImage at 0x7f9ddc208d10&gt;</pre> In\u00a0[14]: Copied! <pre>zarr_store = new_data_store(\"ccizarr\")\n</pre> zarr_store = new_data_store(\"ccizarr\") In\u00a0[15]: Copied! <pre>zarr_store.list_data_ids()\n</pre> zarr_store.list_data_ids() Out[15]: <pre>['ESACCI-BIOMASS-L4-AGB-MERGED-100m-2010-2018-fv2.0.zarr',\n 'ESACCI-BIOMASS-L4-AGB-MERGED-100m-2010-2020-fv4.0.zarr',\n 'ESACCI-GHG-L2-CH4-SCIAMACHY-WFMD-2002-2011-fv1.zarr',\n 'ESACCI-GHG-L2-CO2-OCO-2-FOCAL-2014-2021-v10.zarr',\n 'ESACCI-GHG-L2-CO2-SCIAMACHY-WFMD-2002-2012-fv1.zarr',\n 'ESACCI-ICESHEETS_Antarctica_GMB-2002-2016-v1.1.zarr',\n 'ESACCI-ICESHEETS_Greenland_GMB-2003-2016-v1.1.zarr',\n 'ESACCI-L3C_CLOUD-CLD_PRODUCTS-AVHRR_NOAA-1982-2016-fv3.0.zarr',\n 'ESACCI-L3C_SNOW-SWE-1979-2018-fv1.0.zarr',\n 'ESACCI-L3C_SNOW-SWE-1979-2020-fv2.0.zarr',\n 'ESACCI-L4_GHRSST-SST-GMPE-GLOB_CDR2.0-1981-2016-v02.0-fv01.0.zarr',\n 'ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992-2015-v2.0.7b.zarr',\n 'ESACCI-LST-L3C-LST-MODISA-0.01deg_1DAILY_DAY-2002-2018-fv3.00.zarr',\n 'ESACCI-LST-L3C-LST-MODISA-0.01deg_1DAILY_NIGHT-2002-2018-fv3.00.zarr',\n 'ESACCI-LST-L3S-LST-IRCDR_-0.01deg_1DAILY_DAY-1995-2020-fv3.00.zarr',\n 'ESACCI-LST-L3S-LST-IRCDR_-0.01deg_1DAILY_NIGHT-1995-2020-fv3.00.zarr',\n 'ESACCI-LST-L3S-LST-IRCDR_-0.01deg_1MONTHLY_DAY-1995-2020-fv3.00.zarr',\n 'ESACCI-LST-L3S-LST-IRCDR_-0.01deg_1MONTHLY_NIGHT-1995-2020-fv3.00.zarr',\n 'ESACCI-OC-L3S-IOP-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-1997-2020-fv5.0.zarr',\n 'ESACCI-OC-L3S-IOP-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-IOP-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-IOP-MERGED-1Y_YEARLY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-IOP-MERGED-5D_DAILY_4km_GEO_PML_OCx_QAA-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-IOP-MERGED-8D_DAILY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1Y_YEARLY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-OC_PRODUCTS-MERGED-5D_DAILY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-OC_PRODUCTS-MERGED-8D_DAILY_4km_GEO_PML_OCx_QAA-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-RRS-MERGED-1M_MONTHLY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-RRS-MERGED-1Y_YEARLY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-RRS-MERGED-1D_DAILY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-RRS-MERGED-5D_DAILY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-RRS-MERGED-8D_DAILY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr',\n 'ESACCI-PERMAFROST-L4-ALT-MODISLST-AREA4_PP-1997-2018-fv02.0.zarr',\n 'ESACCI-SEAICE-L3C-SITHICK-RA2_ENVISAT-NH25KMEASE2-2002-2012-fv2.0.zarr',\n 'ESACCI-SEAICE-L3C-SITHICK-SIRAL_CRYOSAT2-NH25KMEASE2-2010-2017-fv2.0.zarr',\n 'ESACCI-SEAICE-L4-SICONC-AMSR_50.0kmEASE2-NH-2002-2017-fv2.1.zarr',\n 'ESACCI-SEALEVEL-IND-MSLTR-MERGED-1993-2016-fv02.zarr',\n 'ESACCI-SEALEVEL-L4-MSLA-MERGED-1993-2015-fv02.zarr',\n 'ESACCI-SOILMOISTURE-L3S-SSMV-COMBINED-1978-2020-fv05.3.zarr',\n 'ESACCI-SOILMOISTURE-L3S-SSMV-COMBINED-1978-2021-fv07.1.zarr']</pre> <p>The data store ccizarr has no dedicated data store parameters. The dataset must be opened as a whole, temporal and spatial subsets can follow afterwards.</p> In\u00a0[16]: Copied! <pre>dataset = zarr_store.open_data(\n    \"ESACCI-OC-L3S-IOP-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-1997-2020-fv5.0.zarr\"\n)\n</pre> dataset = zarr_store.open_data(     \"ESACCI-OC-L3S-IOP-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-1997-2020-fv5.0.zarr\" ) In\u00a0[17]: Copied! <pre>dataset\n</pre> dataset Out[17]: <pre>&lt;xarray.Dataset&gt; Size: 2TB\nDimensions:           (time: 280, lat: 4320, lon: 8640)\nCoordinates:\n  * lat               (lat) float64 35kB 89.98 89.94 89.9 ... -89.94 -89.98\n  * lon               (lon) float64 69kB -180.0 -179.9 -179.9 ... 179.9 180.0\n  * time              (time) datetime64[ns] 2kB 1997-09-04 ... 2020-12-01\nData variables: (12/55)\n    MERIS_nobs_sum    (time, lat, lon) float32 42GB dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;\n    MODISA_nobs_sum   (time, lat, lon) float32 42GB dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;\n    OLCI_nobs_sum     (time, lat, lon) float32 42GB dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;\n    SeaWiFS_nobs_sum  (time, lat, lon) float32 42GB dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;\n    VIIRS_nobs_sum    (time, lat, lon) float32 42GB dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;\n    adg_412           (time, lat, lon) float32 42GB dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;\n    ...                ...\n    bbp_490           (time, lat, lon) float32 42GB dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;\n    bbp_510           (time, lat, lon) float32 42GB dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;\n    bbp_560           (time, lat, lon) float32 42GB dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;\n    bbp_665           (time, lat, lon) float32 42GB dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;\n    crs               (time) int32 1kB dask.array&lt;chunksize=(16,), meta=np.ndarray&gt;\n    total_nobs_sum    (time, lat, lon) float32 42GB dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;\nAttributes: (12/51)\n    Conventions:                       CF-1.7\n    Metadata_Conventions:              Unidata Dataset Discovery v1.0\n    catalogue_url:                     https://catalogue.ceda.ac.uk/uuid/88c2...\n    cdm_data_type:                     Grid\n    comment:                           See summary attribute\n    creation_date:                     Tue Feb  2 10:58:12 2021\n    ...                                ...\n    time_coverage_duration:            P1M\n    time_coverage_end:                 19700101T000000Z\n    time_coverage_resolution:          P1M\n    time_coverage_start:               19700101T000000Z\n    title:                             ESA CCI Ocean Colour Product\n    tracking_id:                       95e99081-7011-4044-8f2b-d7db84276a4e</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 280</li><li>lat: 4320</li><li>lon: 8640</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6489.98 89.94 89.9 ... -89.94 -89.98axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northvalid_max :89.97916666666667valid_min :-89.97916666666666<pre>array([ 89.979167,  89.9375  ,  89.895833, ..., -89.895833, -89.9375  ,\n       -89.979167])</pre></li><li>lon(lon)float64-180.0 -179.9 ... 179.9 180.0axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_max :179.97916666666663valid_min :-179.97916666666666<pre>array([-179.979167, -179.9375  , -179.895833, ...,  179.895833,  179.9375  ,\n        179.979167])</pre></li><li>time(time)datetime64[ns]1997-09-04 ... 2020-12-01axis :Tstandard_name :time<pre>array(['1997-09-04T00:00:00.000000000', '1997-10-01T00:00:00.000000000',\n       '1997-11-01T00:00:00.000000000', ..., '2020-10-01T00:00:00.000000000',\n       '2020-11-01T00:00:00.000000000', '2020-12-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (55)<ul><li>MERIS_nobs_sum(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;long_name :Count of the number of observations from the MERIS sensor contributing to this bin cellnumber_of_files_composited :19  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>MODISA_nobs_sum(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;long_name :Count of the number of observations from the MODIS (Aqua) sensor contributing to this bin cellnumber_of_files_composited :19  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>OLCI_nobs_sum(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;long_name :Count of the number of observations from the OLCI sensor contributing to this bin cellnumber_of_files_composited :19  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>SeaWiFS_nobs_sum(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;long_name :Count of the number of observations from the SeaWiFS (GAC and LAC) sensor contributing to this bin cellnumber_of_files_composited :19  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>VIIRS_nobs_sum(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;long_name :Count of the number of observations from the VIIRS sensor contributing to this bin cellnumber_of_files_composited :19  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_412(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;ancillary_variables :adg_412_rmsd adg_412_biasgrid_mapping :crslong_name :Absorption coefficient for dissolved and detrital material at 412 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0063standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :412  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_412_bias(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/adg/cci_iop_adg_bias.datgrid_mapping :crslong_name :Bias of absorption coefficient for dissolved and detrital material at 412 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :412  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_412_rmsd(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/adg/cci_iop_adg_rmsd.datgrid_mapping :crslong_name :Root-mean-square-difference of absorption coefficient for dissolved and detrital material at 412 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :412  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_443(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;ancillary_variables :adg_443_rmsd adg_443_biasgrid_mapping :crslong_name :Absorption coefficient for dissolved and detrital material at 443 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0063standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :443  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_443_bias(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/adg/cci_iop_adg_bias.datgrid_mapping :crslong_name :Bias of absorption coefficient for dissolved and detrital material at 443 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :443  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_443_rmsd(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/adg/cci_iop_adg_rmsd.datgrid_mapping :crslong_name :Root-mean-square-difference of absorption coefficient for dissolved and detrital material at 443 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :443  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_490(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;ancillary_variables :adg_490_rmsd adg_490_biasgrid_mapping :crslong_name :Absorption coefficient for dissolved and detrital material at 490 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0063standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :490  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_490_bias(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/adg/cci_iop_adg_bias.datgrid_mapping :crslong_name :Bias of absorption coefficient for dissolved and detrital material at 490 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :490  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_490_rmsd(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/adg/cci_iop_adg_rmsd.datgrid_mapping :crslong_name :Root-mean-square-difference of absorption coefficient for dissolved and detrital material at 490 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :490  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_510(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;ancillary_variables :adg_510_rmsd adg_510_biasgrid_mapping :crslong_name :Absorption coefficient for dissolved and detrital material at 510 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0063standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :510  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_510_bias(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/adg/cci_iop_adg_bias.datgrid_mapping :crslong_name :Bias of absorption coefficient for dissolved and detrital material at 510 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :510  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_510_rmsd(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/adg/cci_iop_adg_rmsd.datgrid_mapping :crslong_name :Root-mean-square-difference of absorption coefficient for dissolved and detrital material at 510 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :510  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_560(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;ancillary_variables :adg_560_rmsd adg_560_biasgrid_mapping :crslong_name :Absorption coefficient for dissolved and detrital material at 560 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0063standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :560  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_560_bias(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/adg/cci_iop_adg_bias.datgrid_mapping :crslong_name :Bias of absorption coefficient for dissolved and detrital material at 560 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :560  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_560_rmsd(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/adg/cci_iop_adg_rmsd.datgrid_mapping :crslong_name :Root-mean-square-difference of absorption coefficient for dissolved and detrital material at 560 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :560  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_665(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;ancillary_variables :adg_665_rmsd adg_665_biasgrid_mapping :crslong_name :Absorption coefficient for dissolved and detrital material at 665 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0063standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :665  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_665_bias(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/adg/cci_iop_adg_bias.datgrid_mapping :crslong_name :Bias of absorption coefficient for dissolved and detrital material at 665 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :665  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>adg_665_rmsd(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/adg/cci_iop_adg_rmsd.datgrid_mapping :crslong_name :Root-mean-square-difference of absorption coefficient for dissolved and detrital material at 665 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :665  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_412(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;ancillary_variables :aph_412_rmsd aph_412_biasgrid_mapping :crslong_name :Phytoplankton absorption coefficient at 412 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0063standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :412  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_412_bias(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/aph/cci_iop_aph_bias.datgrid_mapping :crslong_name :Bias of phytoplankton absorption coefficient at 412 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :412  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_412_rmsd(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/aph/cci_iop_aph_rmsd.datgrid_mapping :crslong_name :Root-mean-square-difference of phytoplankton absorption coefficient at 412 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :412  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_443(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;ancillary_variables :aph_443_rmsd aph_443_biasgrid_mapping :crslong_name :Phytoplankton absorption coefficient at 443 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0063standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :443  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_443_bias(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/aph/cci_iop_aph_bias.datgrid_mapping :crslong_name :Bias of phytoplankton absorption coefficient at 443 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :443  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_443_rmsd(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/aph/cci_iop_aph_rmsd.datgrid_mapping :crslong_name :Root-mean-square-difference of phytoplankton absorption coefficient at 443 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :443  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_490(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;ancillary_variables :aph_490_rmsd aph_490_biasgrid_mapping :crslong_name :Phytoplankton absorption coefficient at 490 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0063standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :490  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_490_bias(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/aph/cci_iop_aph_bias.datgrid_mapping :crslong_name :Bias of phytoplankton absorption coefficient at 490 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :490  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_490_rmsd(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/aph/cci_iop_aph_rmsd.datgrid_mapping :crslong_name :Root-mean-square-difference of phytoplankton absorption coefficient at 490 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :490  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_510(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;ancillary_variables :aph_510_rmsd aph_510_biasgrid_mapping :crslong_name :Phytoplankton absorption coefficient at 510 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0063standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :510  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_510_bias(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/aph/cci_iop_aph_bias.datgrid_mapping :crslong_name :Bias of phytoplankton absorption coefficient at 510 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :510  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_510_rmsd(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/aph/cci_iop_aph_rmsd.datgrid_mapping :crslong_name :Root-mean-square-difference of phytoplankton absorption coefficient at 510 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :510  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_560(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;ancillary_variables :aph_560_rmsd aph_560_biasgrid_mapping :crslong_name :Phytoplankton absorption coefficient at 560 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0063standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :560  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_560_bias(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/aph/cci_iop_aph_bias.datgrid_mapping :crslong_name :Bias of phytoplankton absorption coefficient at 560 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :560  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_560_rmsd(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/aph/cci_iop_aph_rmsd.datgrid_mapping :crslong_name :Root-mean-square-difference of phytoplankton absorption coefficient at 560 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :560  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_665(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;ancillary_variables :aph_665_rmsd aph_665_biasgrid_mapping :crslong_name :Phytoplankton absorption coefficient at 665 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0063standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :665  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_665_bias(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/aph/cci_iop_aph_bias.datgrid_mapping :crslong_name :Bias of phytoplankton absorption coefficient at 665 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :665  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>aph_665_rmsd(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;comment :Uncertainty lookups derived from file: /data/datasets/CCI/v5.0-production/stage09b-uncertainty_tables/aph/cci_iop_aph_rmsd.datgrid_mapping :crslong_name :Root-mean-square-difference of phytoplankton absorption coefficient at 665 nm.ref :http://www.uncertweb.orgrel :uncertaintyunits :m-1units_nonstandard :m^-1wavelength :665  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>atot_412(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;grid_mapping :crslong_name :Total absorption coefficient at 412 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0062standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :412  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>atot_443(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;grid_mapping :crslong_name :Total absorption coefficient at 443 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0062standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :443  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>atot_490(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;grid_mapping :crslong_name :Total absorption coefficient at 490 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0062standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :490  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>atot_510(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;grid_mapping :crslong_name :Total absorption coefficient at 510 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0062standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :510  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>atot_560(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;grid_mapping :crslong_name :Total absorption coefficient at 560 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0062standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :560  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>atot_665(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;grid_mapping :crslong_name :Total absorption coefficient at 665 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.ndg.nerc.ac.uk/term/P071/19/CFSN0062standard_name :volume_absorption_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :665  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>bbp_412(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;grid_mapping :crslong_name :Particulate backscattering coefficient for dissolved and detrital material at 412 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.nerc.ac.uk/collection/P04/current/standard_name :volume_backwards_scattering_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :412  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>bbp_443(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;grid_mapping :crslong_name :Particulate backscattering coefficient for dissolved and detrital material at 443 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.nerc.ac.uk/collection/P04/current/standard_name :volume_backwards_scattering_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :443  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>bbp_490(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;grid_mapping :crslong_name :Particulate backscattering coefficient for dissolved and detrital material at 490 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.nerc.ac.uk/collection/P04/current/standard_name :volume_backwards_scattering_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :490  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>bbp_510(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;grid_mapping :crslong_name :Particulate backscattering coefficient for dissolved and detrital material at 510 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.nerc.ac.uk/collection/P04/current/standard_name :volume_backwards_scattering_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :510  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>bbp_560(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;grid_mapping :crslong_name :Particulate backscattering coefficient for dissolved and detrital material at 560 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.nerc.ac.uk/collection/P04/current/standard_name :volume_backwards_scattering_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :560  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>bbp_665(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;grid_mapping :crslong_name :Particulate backscattering coefficient for dissolved and detrital material at 665 nm as derived using the QAA model, generated by SeaDAS for MERISparameter_vocab_uri :http://vocab.nerc.ac.uk/collection/P04/current/standard_name :volume_backwards_scattering_coefficient_of_radiative_flux_in_sea_waterunits :m-1units_nonstandard :m^-1wavelength :665  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li><li>crs(time)int32dask.array&lt;chunksize=(16,), meta=np.ndarray&gt;grid_mapping_name :latitude_longitude  Array   Chunk   Bytes   1.09 kiB   64 B   Shape   (280,)   (16,)   Dask graph   18 chunks in 2 graph layers   Data type   int32 numpy.ndarray  280 1 </li><li>total_nobs_sum(time, lat, lon)float32dask.array&lt;chunksize=(16, 1080, 1080), meta=np.ndarray&gt;long_name :Count of the total number of observations contributing to this bin cellnumber_of_files_composited :19  Array   Chunk   Bytes   38.93 GiB   71.19 MiB   Shape   (280, 4320, 8640)   (16, 1080, 1080)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  8640 4320 280 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.97916666666667,            89.9375,  89.89583333333333,\n        89.85416666666667,            89.8125,  89.77083333333333,\n        89.72916666666667,            89.6875,  89.64583333333333,\n        89.60416666666667,\n       ...\n       -89.60416666666666, -89.64583333333331,           -89.6875,\n       -89.72916666666666, -89.77083333333331,           -89.8125,\n       -89.85416666666666, -89.89583333333331,           -89.9375,\n       -89.97916666666666],\n      dtype='float64', name='lat', length=4320))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.97916666666666,           -179.9375, -179.89583333333334,\n       -179.85416666666666,           -179.8125, -179.77083333333334,\n       -179.72916666666666,           -179.6875, -179.64583333333334,\n       -179.60416666666666,\n       ...\n        179.60416666666663,  179.64583333333331,            179.6875,\n        179.72916666666663,  179.77083333333331,            179.8125,\n        179.85416666666663,  179.89583333333331,            179.9375,\n        179.97916666666663],\n      dtype='float64', name='lon', length=8640))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1997-09-04', '1997-10-01', '1997-11-01', '1997-12-01',\n               '1998-01-01', '1998-02-01', '1998-03-01', '1998-04-01',\n               '1998-05-01', '1998-06-01',\n               ...\n               '2020-03-01', '2020-04-01', '2020-05-01', '2020-06-01',\n               '2020-07-01', '2020-08-01', '2020-09-01', '2020-10-01',\n               '2020-11-01', '2020-12-01'],\n              dtype='datetime64[ns]', name='time', length=280, freq=None))</pre></li></ul></li><li>Attributes: (51)Conventions :CF-1.7Metadata_Conventions :Unidata Dataset Discovery v1.0catalogue_url :https://catalogue.ceda.ac.uk/uuid/88c2bc7af4f0402d8ceecad611c58cc5cdm_data_type :Gridcomment :See summary attributecreation_date :Tue Feb  2 10:58:12 2021creator_email :help@esa-oceancolour-cci.orgcreator_name :Plymouth Marine Laboratorycreator_url :http://esa-oceancolour-cci.orgdate_created :Tue Feb  2 10:58:12 2021geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :.04166666666666666666geospatial_lat_units :decimal degrees northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :.04166666666666666666geospatial_lon_units :decimal degrees eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0git_commit_hash :33d54b27cf9a6b4d9d9f8113854f910c95bc287dhistory :Source data were: ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201201-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201202-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201203-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201204-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201205-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201206-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201207-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201208-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201209-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201210-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201211-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201212-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201213-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201214-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201215-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201216-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201217-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201218-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201219-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201220-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201221-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201222-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201223-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201224-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201225-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201226-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201227-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201228-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201229-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201230-fv5.0.nc, ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-20201231-fv5.0.nc; netcdf_compositor_cci composites  Rrs_412, Rrs_443, Rrs_490, Rrs_510, Rrs_560, Rrs_665, water_class1, water_class2, water_class3, water_class4, water_class5, water_class6, water_class7, water_class8, water_class9, water_class10, water_class11, water_class12, water_class13, water_class14, atot_412, atot_443, atot_490, atot_510, atot_560, atot_665, aph_412, aph_443, aph_490, aph_510, aph_560, aph_665, adg_412, adg_443, adg_490, adg_510, adg_560, adg_665, bbp_412, bbp_443, bbp_490, bbp_510, bbp_560, bbp_665, chlor_a, kd_490, Rrs_412_bias, Rrs_443_bias, Rrs_490_bias, Rrs_510_bias, Rrs_560_bias, Rrs_665_bias, chlor_a_log10_bias, aph_412_bias, aph_443_bias, aph_490_bias, aph_510_bias, aph_560_bias, aph_665_bias, adg_412_bias, adg_443_bias, adg_490_bias, adg_510_bias, adg_560_bias, adg_665_bias, kd_490_bias with --mean,  Rrs_412_rmsd, Rrs_443_rmsd, Rrs_490_rmsd, Rrs_510_rmsd, Rrs_560_rmsd, Rrs_665_rmsd, chlor_a_log10_rmsd, aph_412_rmsd, aph_443_rmsd, aph_490_rmsd, aph_510_rmsd, aph_560_rmsd, aph_665_rmsd, adg_412_rmsd, adg_443_rmsd, adg_490_rmsd, adg_510_rmsd, adg_560_rmsd, adg_665_rmsd, kd_490_rmsd with --root-mean-square, and  MODISA_nobs, VIIRS_nobs, OLCI_nobs, MERIS_nobs, SeaWiFS_nobs, total_nobs - with --total 1612270525 Subsetted from standardised_geo/ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-202012-fv5.0.nc to only include variables MERIS_nobs_sum,MODISA_nobs_sum,OLCI_nobs_sum,SeaWiFS_nobs_sum,VIIRS_nobs_sum,adg_412,adg_412_bias,adg_412_rmsd,adg_443,adg_443_bias,adg_443_rmsd,adg_490,adg_490_bias,adg_490_rmsd,adg_510,adg_510_bias,adg_510_rmsd,adg_560,adg_560_bias,adg_560_rmsd,adg_665,adg_665_bias,adg_665_rmsd,aph_412,aph_412_bias,aph_412_rmsd,aph_443,aph_443_bias,aph_443_rmsd,aph_490,aph_490_bias,aph_490_rmsd,aph_510,aph_510_bias,aph_510_rmsd,aph_560,aph_560_bias,aph_560_rmsd,aph_665,aph_665_bias,aph_665_rmsd,atot_412,atot_443,atot_490,atot_510,atot_560,atot_665,bbp_412,bbp_443,bbp_490,bbp_510,bbp_560,bbp_665,crs,lat,lon,time,total_nobs_sumid :ESACCI-OC-L3S-IOP-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-202012-fv5.0.ncinstitution :Plymouth Marine Laboratorykeywords :satellite,observation,ocean,ocean colourkeywords_vocabulary :nonelicense :ESA CCI Data Policy: free and open access.  When referencing, please use: Ocean Colour Climate Change Initiative dataset, Version &lt;Version Number&gt;, European Space Agency, available online at http://www.esa-oceancolour-cci.org.  We would also appreciate being notified of publications so that we can list them on the project website at http://www.esa-oceancolour-cci.org/?q=publicationsnaming_authority :uk.ac.pmlnetcdf_file_type :NETCDF4_CLASSICnumber_of_bands_used_to_classify :4number_of_files_composited :31number_of_optical_water_types :14platform :Orbview-2,Aqua,Envisat,Suomi-NPP, Sentinel-3aprocessing_level :Level-3product_version :5.0project :Climate Change Initiative - European Space Agencyreferences :http://www.esa-oceancolour-cci.org/sensor :SeaWiFS,MODIS,MERIS,VIIRS,OLCIsensors_present : MODISA OLCIa VIIRSNsource :NASA SeaWiFS  L1A and L2 R2018.0 LAC and GAC, MODIS-Aqua L1A and L2 R2018.0, MERIS L1B 3rd reprocessing inc OCL corrections, NASA VIIRS L1A and L2 R2018.0, OLCI L1Bspatial_resolution :4km nominal at equatorstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventions Version 1.7start_date :01-DEC-2020 00:00:00.000000stop_date :31-DEC-2020 23:59:00.000000summary :Data products generated by the Ocean Colour component of the European Space Agency Climate Change Initiative project. These files are monthly composites of merged sensor (MERIS, MODIS Aqua, SeaWiFS LAC &amp; GAC, VIIRS, OLCI) products.  MODIS Aqua and SeaWiFS were band-shifted and bias-corrected to MERIS bands and values using a temporally and spatially varying scheme based on the overlap years of 2003-2007.  VIIRS was band-shifted and bias-corrected in a second stage against the MODIS Rrs that had already been corrected to MERIS levels, for the overlap period 2012-2013; and at the third stage OLCI was bias corrected against already corrected MODIS, for overlap period 2016-07-01 to 2019-06-30.  VIIRS, MODIS, SeaWiFS and MERIS Rrs were derived from a combination of NASA's l2gen (for basic sensor geometry corrections, etc) and HYGEOS Polymer v4.12 (for atmospheric correction). OLCI Rrs were sourced at L1b (already geometrically corrected) and processed with polymer.  The Rrs were binned to a sinusoidal 4km level-3 grid, and later to 4km geographic projection, by Brockmann Consult's SNAP.  Derived products were generally computed with the standard algorithmsthrough SeaDAS.  QAA IOPs were derived using the standard SeaDAS algorithm but with a modified backscattering table to match that used in the bandshifting.  The final chlorophyll is a combination of OCI, OCI2, OC2 and OCx, depending on the water class memberships.  Uncertainty estimates were added using the fuzzy water classifier and uncertainty estimation algorithm of Tim Moore as documented in Jackson et al (2017). and updated accorsing to Jackson et al. (in prep).time_coverage_duration :P1Mtime_coverage_end :19700101T000000Ztime_coverage_resolution :P1Mtime_coverage_start :19700101T000000Ztitle :ESA CCI Ocean Colour Producttracking_id :95e99081-7011-4044-8f2b-d7db84276a4e</li></ul> <p>Get an overview of the parameter scheme of the datasets in the zarr_store and create subsets accordingly</p> In\u00a0[18]: Copied! <pre>zarr_store.get_open_data_params_schema(\n    \"ESACCI-GHG-L2-CH4-SCIAMACHY-WFMD-2002-2011-fv1.zarr\"\n)\n</pre> zarr_store.get_open_data_params_schema(     \"ESACCI-GHG-L2-CH4-SCIAMACHY-WFMD-2002-2011-fv1.zarr\" ) Out[18]: <pre>&lt;xcube.util.jsonschema.JsonObjectSchema at 0x7f9dcf92d150&gt;</pre> In\u00a0[19]: Copied! <pre>def open_zarrstore(filename, time_range, variables):\n    ds = zarr_store.open_data(filename)\n    subset = ds.sel(time=slice(time_range[0], time_range[1]))\n    subset = subset[variables]\n\n    return subset\n\n\nzarr_dataset = open_zarrstore(\n    \"ESACCI-L4_GHRSST-SST-GMPE-GLOB_CDR2.0-1981-2016-v02.0-fv01.0.zarr\",\n    time_range=[datetime.datetime(1982, 1, 1), datetime.datetime(2016, 12, 31)],\n    variables=[\"analysed_sst\"],\n)\n</pre> def open_zarrstore(filename, time_range, variables):     ds = zarr_store.open_data(filename)     subset = ds.sel(time=slice(time_range[0], time_range[1]))     subset = subset[variables]      return subset   zarr_dataset = open_zarrstore(     \"ESACCI-L4_GHRSST-SST-GMPE-GLOB_CDR2.0-1981-2016-v02.0-fv01.0.zarr\",     time_range=[datetime.datetime(1982, 1, 1), datetime.datetime(2016, 12, 31)],     variables=[\"analysed_sst\"], ) In\u00a0[20]: Copied! <pre>zarr_dataset\n</pre> zarr_dataset Out[20]: <pre>&lt;xarray.Dataset&gt; Size: 106GB\nDimensions:       (time: 12783, lat: 720, lon: 1440)\nCoordinates:\n  * lat           (lat) float32 3kB -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n  * lon           (lon) float32 6kB -179.9 -179.6 -179.4 ... 179.4 179.6 179.9\n  * time          (time) datetime64[ns] 102kB 1982-01-01T12:00:00 ... 2016-12...\nData variables:\n    analysed_sst  (time, lat, lon) float64 106GB dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.4\n    acknowledgment:             Funded by ESA\n    cdm_data_type:              grid\n    comment:                    \n    creator_email:              science.leader@esa-sst-cci.org\n    creator_name:               SST_cci\n    ...                         ...\n    summary:                    An ensemble product with input from a number ...\n    time_coverage_end:          20170101T000000Z\n    time_coverage_start:        20161231T000000Z\n    title:                      Global SST Ensemble, L4 GMPE\n    uuid:                       dc0c5b25-93bf-4943-aba1-7f0de9109620\n    westernmost_longitude:      -180.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 12783</li><li>lat: 720</li><li>lon: 1440</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float32-89.88 -89.62 ... 89.62 89.88axis :Yreference_datum :geographical coordinates, WGS84 projectionstandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875],\n      dtype=float32)</pre></li><li>lon(lon)float32-179.9 -179.6 ... 179.6 179.9axis :Xreference_datum :geographical coordinates, WGS84 projectionstandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875],\n      dtype=float32)</pre></li><li>time(time)datetime64[ns]1982-01-01T12:00:00 ... 2016-12-...axis :Tlong_name :reference time of sst fieldstandard_name :time<pre>array(['1982-01-01T12:00:00.000000000', '1982-01-02T12:00:00.000000000',\n       '1982-01-03T12:00:00.000000000', ..., '2016-12-28T12:00:00.000000000',\n       '2016-12-29T12:00:00.000000000', '2016-12-30T12:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>analysed_sst(time, lat, lon)float64dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;long_name :median SST from GMPEsource :SST CCI V2, SST CCI V1.1, MyOcean, AVHRR_OI, CMC, HadISST.2.2.0.0_r0, HadISST.2.2.0.0_r1, HadISST.2.2.0.0_r2, HadISST.2.2.0.0_r3, HadISST.2.2.0.0_r4, HadISST.2.2.0.0_r5, HadISST.2.2.0.0_r6, HadISST.2.2.0.0_r7, HadISST.2.2.0.0_r8, HadISST.2.2.0.0_r9, MGDSSTstandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_max :4500valid_min :-300  Array   Chunk   Bytes   98.75 GiB   63.28 MiB   Shape   (12783, 720, 1440)   (16, 720, 720)   Dask graph   1598 chunks in 3 graph layers   Data type   float64 numpy.ndarray  1440 720 12783 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([-89.875, -89.625, -89.375, -89.125, -88.875, -88.625, -88.375, -88.125,\n       -87.875, -87.625,\n       ...\n        87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,  89.375,\n        89.625,  89.875],\n      dtype='float32', name='lat', length=720))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625, -178.375,\n       -178.125, -177.875, -177.625,\n       ...\n        177.625,  177.875,  178.125,  178.375,  178.625,  178.875,  179.125,\n        179.375,  179.625,  179.875],\n      dtype='float32', name='lon', length=1440))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1982-01-01 12:00:00', '1982-01-02 12:00:00',\n               '1982-01-03 12:00:00', '1982-01-04 12:00:00',\n               '1982-01-05 12:00:00', '1982-01-06 12:00:00',\n               '1982-01-07 12:00:00', '1982-01-08 12:00:00',\n               '1982-01-09 12:00:00', '1982-01-10 12:00:00',\n               ...\n               '2016-12-21 12:00:00', '2016-12-22 12:00:00',\n               '2016-12-23 12:00:00', '2016-12-24 12:00:00',\n               '2016-12-25 12:00:00', '2016-12-26 12:00:00',\n               '2016-12-27 12:00:00', '2016-12-28 12:00:00',\n               '2016-12-29 12:00:00', '2016-12-30 12:00:00'],\n              dtype='datetime64[ns]', name='time', length=12783, freq=None))</pre></li></ul></li><li>Attributes: (47)Conventions :CF-1.4acknowledgment :Funded by ESAcdm_data_type :gridcomment :creator_email :science.leader@esa-sst-cci.orgcreator_name :SST_ccicreator_url :http://www.esa-sst-cci.orgdate_created :20180711T172223Zeasternmost_longitude :180.0file_quality_level :3gds_version_id :2.0geospatial_lat_resolution :0.25 degreegeospatial_lat_units :degrees northgeospatial_lon_resolution :0.25 degreegeospatial_lon_units :degrees easthistory :NULLid :UKMO-L4LRens-GLOB-GMPEinstitution :UKMOkeywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Creative Commons Licence by attribution (https://creativecommons.org/licenses/by/4.0/metadata_conventions :Unidata Observation Dataset v1.0metadata_link :not availablenaming_authority :org.ghrsstnetcdf_version_id :3.6northernmost_latitude :90.0platform :processing_level :GMPEproduct_version :3.0project :Climate Change Initiative - European Space Agencypublisher_email :science.leader@esa-sst-cci.orgpublisher_name :ESACCIpublisher_url :http://www.esa-sst-cci.orgreferences :Martin et al., Deep Sea Research II, 2011sensor :source :SST CCI V2, SST CCI V1.1, MyOcean, AVHRR_OI, CMC, HadISST.2.2.0.0_r0, HadISST.2.2.0.0_r1, HadISST.2.2.0.0_r2, HadISST.2.2.0.0_r3, HadISST.2.2.0.0_r4, HadISST.2.2.0.0_r5, HadISST.2.2.0.0_r6, HadISST.2.2.0.0_r7, HadISST.2.2.0.0_r8, HadISST.2.2.0.0_r9, MGDSSTsouthernmost_latitude :-90.0spatial_resolution :0.25 degreestandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionstart_time :20161231T000000Zstop_time :20170101T000000Zsummary :An ensemble product with input from a number of L4 SST analysestime_coverage_end :20170101T000000Ztime_coverage_start :20161231T000000Ztitle :Global SST Ensemble, L4 GMPEuuid :dc0c5b25-93bf-4943-aba1-7f0de9109620westernmost_longitude :-180.0</li></ul> In\u00a0[21]: Copied! <pre>zarr_dataset.analysed_sst.isel(time=0).plot.imshow(cmap=\"plasma\")\n</pre> zarr_dataset.analysed_sst.isel(time=0).plot.imshow(cmap=\"plasma\") Out[21]: <pre>&lt;matplotlib.image.AxesImage at 0x7f9dde164d10&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/Generate_CCI_cubes/#generate-cci-data-cubes","title":"Generate CCI data cubes\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_CCI_cubes/#a-deepesdl-example-notebook","title":"A DeepESDL example notebook\u00b6","text":"<p>This notebook demonstrates how to access CCI data via two dedicated xcube stores, which provide dynamic data cube views into each gridded data set. More information on the data sets offered can be found in the Open Data Portal of the ESA Climate Change Initiative (CCI).</p> <p>The xcube data store <code>cciodp</code> provides direct access to the datasets of the Open Data Portal. On the other hand, the xcube data store <code>ccizarr</code> is a subset of the <code>cciodp</code> store and contains CCI data that is already available in cloud-ready format, resulting in better data access performance. The store is regularly updated with new data. Details on the functionality of the two data stores can be found in the xcube documentation.</p> <p>Please, also refer to the DeepESDL documentation and visit the platform's website for further information!</p> <p>Brockmann Consult, 2024</p> <p>This notebook runs with the python environment <code>deepesdl-xcube-1.7.1</code>, please checkout the documentation for help on changing the environment.</p>"},{"location":"guide/jupyterlab/notebooks/Generate_CCI_cubes/#1-xcube-data-store-cciodp","title":"1. xcube data store <code>cciodp</code>\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_CCI_cubes/#2-xcube-data-store-ccizarr","title":"2. xcube data store <code>ccizarr</code>\u00b6","text":"<p>Some of the CCI data is already available in a cloud-ready format, and new data is added on a regular basis. To access this data, select the <code>ccizarr</code> store instead of the <code>cciodp</code> store.</p>"},{"location":"guide/jupyterlab/notebooks/Generate_CMEMS_cubes/","title":"Generate CMEMS cubes","text":"<p>If you do not have cmems user yet, you can register for an account. For DeepESDL Basic Jupyter Profile default credentials are configured, but due to bandwith limitation by CMEMS they might not be performant when used by several people simultanously.</p> In\u00a0[1]: Copied! <pre>import os\n</pre> import os In\u00a0[2]: Copied! <pre>## replace with your user name and pwd\n# os.environ[\"CMEMS_USERNAME\"] = \"&lt;your-username&gt;\"\n# os.environ[\"CMEMS_PASSWORD\"] = \"&lt;your-pwd&gt;\"\n</pre> ## replace with your user name and pwd # os.environ[\"CMEMS_USERNAME\"] = \"\" # os.environ[\"CMEMS_PASSWORD\"] = \"\" In\u00a0[3]: Copied! <pre># mandatory imports\nfrom IPython.display import JSON\nfrom xcube.core.store import (\n    find_data_store_extensions,\n    get_data_store_params_schema,\n    new_data_store,\n)\n</pre> # mandatory imports from IPython.display import JSON from xcube.core.store import (     find_data_store_extensions,     get_data_store_params_schema,     new_data_store, ) <p>Check whether the <code>CMEMS</code> store is among the available stores, if not please follow the installation information from the top of this notebook.</p> In\u00a0[4]: Copied! <pre>JSON({e.name: e.metadata for e in find_data_store_extensions()})\n</pre> JSON({e.name: e.metadata for e in find_data_store_extensions()}) Out[4]: <pre>&lt;IPython.core.display.JSON object&gt;</pre> <p>Usually we need more information to get the actual data store object. Which data store parameters are available for <code>cmems</code>?</p> In\u00a0[5]: Copied! <pre>get_data_store_params_schema(\"cmems\")\n</pre> get_data_store_params_schema(\"cmems\") Out[5]: <pre>&lt;xcube.util.jsonschema.JsonObjectSchema at 0x7fdd96ec93d0&gt;</pre> <p>If you do not have cmems user yet, you can register for an account. For DeepESDL Basic Jupyter Profile default credentials are configured, but due to bandwith limitation by CMEMS they might not be performant when used by several people simultanously.</p> In\u00a0[6]: Copied! <pre># cmems_store = new_data_store('cmems', cmems_username=os.environ[\"CMEMS_USERNAME\"], cmems_password = os.environ[\"CMEMS_PASSWORD\"])\ncmems_store = new_data_store(\"cmems\")\n</pre> # cmems_store = new_data_store('cmems', cmems_username=os.environ[\"CMEMS_USERNAME\"], cmems_password = os.environ[\"CMEMS_PASSWORD\"]) cmems_store = new_data_store(\"cmems\") In\u00a0[7]: Copied! <pre>cmems_store.list_data_ids()\n</pre> cmems_store.list_data_ids() <pre>Fetching catalog: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:06&lt;00:00,  2.29s/it]\n</pre> Out[7]: <pre>['antarctic_omi_si_extent',\n 'antarctic_omi_si_extent_obs',\n 'cmems_mod_arc_bgc_anfc_ecosmo_P1D-m',\n 'cmems_mod_arc_bgc_anfc_ecosmo_P1M-m',\n 'cmems_mod_arc_phy_anfc_6km_detided_P1D-m',\n 'cmems_mod_arc_phy_anfc_6km_detided_P1M-m',\n 'cmems_mod_arc_phy_anfc_6km_detided_PT1H-i',\n 'cmems_mod_arc_phy_anfc_6km_detided_PT6H-m',\n 'cmems_mod_arc_phy_anfc_nextsim_P1M-m',\n 'cmems_mod_arc_phy_anfc_nextsim_hm',\n 'dataset-topaz6-arc-15min-3km-be',\n 'dataset-wam-arctic-1hr3km-be',\n 'cmems_mod_arc_bgc_my_ecosmo_P1D-m',\n 'cmems_mod_arc_bgc_my_ecosmo_P1M',\n 'cmems_mod_arc_bgc_my_ecosmo_P1Y',\n 'cmems_mod_arc_phy_my_hflux_P1D-m',\n 'cmems_mod_arc_phy_my_hflux_P1M-m',\n 'cmems_mod_arc_phy_my_mflux_P1D-m',\n 'cmems_mod_arc_phy_my_mflux_P1M-m',\n 'cmems_mod_arc_phy_my_topaz4_P1D-m',\n 'cmems_mod_arc_phy_my_topaz4_P1M',\n 'cmems_mod_arc_phy_my_topaz4_P1Y',\n 'cmems_mod_arc_phy_my_nextsim_P1D-m',\n 'cmems_mod_arc_phy_my_nextsim_P1M-m',\n 'cmems_mod_arc_wav_my_3km-climatology_P1M-m',\n 'cmems_mod_arc_wav_my_3km_PT1H-i',\n 'arctic_omi_si_transport_nordicseas',\n 'arctic_omi_si_extent',\n 'arctic_omi_si_extent_obs',\n 'arctic_omi_tempsal_fwc',\n 'cmems_mod_bal_bgc-pp_anfc_7-10days_P1D-i',\n 'cmems_mod_bal_bgc-pp_anfc_P1D-i',\n 'cmems_mod_bal_bgc_anfc_7-10days_P1D-m',\n 'cmems_mod_bal_bgc_anfc_P1D-m',\n 'cmems_mod_bal_bgc_anfc_P1M-m',\n 'cmems_mod_bal_bgc_anfc_static',\n 'cmems_mod_bal_phy-cur_anfc_detided-7-10days_P1D-m',\n 'cmems_mod_bal_phy-cur_anfc_detided_P1D-m',\n 'cmems_mod_bal_phy-ssh_anfc_detided-7-10days_P1D-m',\n 'cmems_mod_bal_phy-ssh_anfc_detided_P1D-m',\n 'cmems_mod_bal_phy_anfc_7-10days_P1D-m',\n 'cmems_mod_bal_phy_anfc_7-10days_PT15M-i',\n 'cmems_mod_bal_phy_anfc_7-10days_PT1H-i',\n 'cmems_mod_bal_phy_anfc_P1D-m',\n 'cmems_mod_bal_phy_anfc_P1M-m',\n 'cmems_mod_bal_phy_anfc_PT15M-i',\n 'cmems_mod_bal_phy_anfc_PT1H-i',\n 'cmems_mod_bal_phy_anfc_static',\n 'cmems_mod_bal_wav_anfc_7-10days_PT1H-i',\n 'cmems_mod_bal_wav_anfc_PT1H-i',\n 'cmems_mod_bal_wav_anfc_static',\n 'cmems_mod_bal_bgc_my_P1D-m',\n 'cmems_mod_bal_bgc_my_P1M-m',\n 'cmems_mod_bal_bgc_my_P1Y-m',\n 'cmems_mod_bal_bgc_my_static',\n 'cmems_mod_bal_phy_my_P1D-m',\n 'cmems_mod_bal_phy_my_P1M-m',\n 'cmems_mod_bal_phy_my_P1Y-m',\n 'cmems_mod_bal_phy_my_static',\n 'cmems_mod_bal_wav_my_2km-climatology_P1M-m',\n 'cmems_mod_bal_wav_my_PT1H-i',\n 'cmems_mod_bal_wav_my_aflux_PT1H-i',\n 'cmems_mod_bal_wav_my_static',\n 'baltic_omi_health_codt_volume',\n 'baltic_omi_ohc_area_averaged_anomalies',\n 'baltic_omi_si_extent',\n 'baltic_omi_si_extent_yday',\n 'baltic_omi_si_volume',\n 'baltic_omi_si_volume_yday',\n 'baltic_omi_tempsal_Stz_area_averaged_trend',\n 'baltic_omi_tempsal_Ttz_area_averaged_trend',\n 'baltic_omi_wmhe_mbi_bottom_salinity_arkona_bornholm',\n 'baltic_omi_wmhe_mbi_sto2tz_gotland',\n 'cmems_mod_blk_bgc-bio_anfc_3km_P1D-m',\n 'cmems_mod_blk_bgc-bio_anfc_3km_P1M-m',\n 'cmems_mod_blk_bgc-car_anfc_2.5km_P1D-m',\n 'cmems_mod_blk_bgc-car_anfc_2.5km_P1M-m',\n 'cmems_mod_blk_bgc-car_anfc_3km_P1D-m',\n 'cmems_mod_blk_bgc-car_anfc_3km_P1M-m',\n 'cmems_mod_blk_bgc-co2_anfc_2.5km_P1D-m',\n 'cmems_mod_blk_bgc-co2_anfc_2.5km_P1M-m',\n 'cmems_mod_blk_bgc-co2_anfc_2.5km_PT1H-m',\n 'cmems_mod_blk_bgc-co2_anfc_3km_P1D-m',\n 'cmems_mod_blk_bgc-co2_anfc_3km_P1M-m',\n 'cmems_mod_blk_bgc-co2_anfc_3km_PT1H-m',\n 'cmems_mod_blk_bgc-nut_anfc_2.5km_P1D-m',\n 'cmems_mod_blk_bgc-nut_anfc_2.5km_P1M-m',\n 'cmems_mod_blk_bgc-nut_anfc_3km_P1D-m',\n 'cmems_mod_blk_bgc-nut_anfc_3km_P1M-m',\n 'cmems_mod_blk_bgc-opt_anfc_3km_P1D-m',\n 'cmems_mod_blk_bgc-opt_anfc_3km_P1M-m',\n 'cmems_mod_blk_bgc-optics_anfc_2.5km_P1D-m',\n 'cmems_mod_blk_bgc-optics_anfc_2.5km_P1M-m',\n 'cmems_mod_blk_bgc-pft_anfc_2.5km_P1D-m',\n 'cmems_mod_blk_bgc-pft_anfc_2.5km_P1M-m',\n 'cmems_mod_blk_bgc-pft_anfc_3km_P1D-m',\n 'cmems_mod_blk_bgc-pft_anfc_3km_P1M-m',\n 'cmems_mod_blk_bgc-pp-o2_anfc_2.5km_P1D-m',\n 'cmems_mod_blk_bgc-pp-o2_anfc_2.5km_P1M-m',\n 'cmems_mod_blk_bgc_anfc_2.5km_static',\n 'cmems_mod_blk_bgc_anfc_3km_static',\n 'cmems_mod_blk_phy-cur_anfc_2.5km_P1D-m',\n 'cmems_mod_blk_phy-cur_anfc_2.5km_P1M-m',\n 'cmems_mod_blk_phy-cur_anfc_2.5km_PT15M-i',\n 'cmems_mod_blk_phy-cur_anfc_2.5km_PT1H-m',\n 'cmems_mod_blk_phy-cur_anfc_detided-2.5km_P1D-m',\n 'cmems_mod_blk_phy-cur_anfc_detided_2.5km_P1D-m',\n 'cmems_mod_blk_phy-cur_anfc_mrm-500m_P1D-m',\n 'cmems_mod_blk_phy-cur_anfc_mrm-500m_PT1H-i',\n 'cmems_mod_blk_phy-mld_anfc_2.5km_P1D-m',\n 'cmems_mod_blk_phy-mld_anfc_2.5km_P1M-m',\n 'cmems_mod_blk_phy-mld_anfc_2.5km_PT1H-m',\n 'cmems_mod_blk_phy-sal_anfc_2.5km_P1D-m',\n 'cmems_mod_blk_phy-sal_anfc_2.5km_P1M-m',\n 'cmems_mod_blk_phy-sal_anfc_2.5km_PT1H-m',\n 'cmems_mod_blk_phy-sal_anfc_mrm-500m_P1D-m',\n 'cmems_mod_blk_phy-sal_anfc_mrm-500m_PT1H-i',\n 'cmems_mod_blk_phy-ssh_anfc_2.5km_P1D-m',\n 'cmems_mod_blk_phy-ssh_anfc_2.5km_P1M-m',\n 'cmems_mod_blk_phy-ssh_anfc_2.5km_PT15M-i',\n 'cmems_mod_blk_phy-ssh_anfc_2.5km_PT1H-m',\n 'cmems_mod_blk_phy-ssh_anfc_detided-2.5km_P1D-m',\n 'cmems_mod_blk_phy-ssh_anfc_detided_2.5km_P1D-m',\n 'cmems_mod_blk_phy-ssh_anfc_mrm-500m_P1D-m',\n 'cmems_mod_blk_phy-ssh_anfc_mrm-500m_PT1H-i',\n 'cmems_mod_blk_phy-tem_anfc_2.5km_P1D-m',\n 'cmems_mod_blk_phy-tem_anfc_2.5km_P1M-m',\n 'cmems_mod_blk_phy-tem_anfc_2.5km_PT1H-m',\n 'cmems_mod_blk_phy-tem_anfc_mrm-500m_P1D-m',\n 'cmems_mod_blk_phy-tem_anfc_mrm-500m_PT1H-i',\n 'cmems_mod_blk_phy-temp_anfc_2.5km_P1D-m',\n 'cmems_mod_blk_phy-temp_anfc_2.5km_P1M-m',\n 'cmems_mod_blk_phy-temp_anfc_2.5km_PT1H-m',\n 'cmems_mod_blk_phy_anfc_2.5km_static',\n 'cmems_mod_blk_phy_anfc_mrm-500m_static',\n 'cmems_mod_blk_wav_anfc_2.5km_PT1H-i',\n 'cmems_mod_blk_wav_anfc_2.5km_static',\n 'cmems_mod_blk_bgc-bio_my_2.5km_P1D-m',\n 'cmems_mod_blk_bgc-bio_my_2.5km_P1M-m',\n 'cmems_mod_blk_bgc-bio_my_2.5km_P1Y-m',\n 'cmems_mod_blk_bgc-bio_my_2.5km_climatology_P1M-m',\n 'cmems_mod_blk_bgc-bio_myint_2.5km_P1M-m',\n 'cmems_mod_blk_bgc-car_my_2.5km_P1D-m',\n 'cmems_mod_blk_bgc-car_my_2.5km_P1M-m',\n 'cmems_mod_blk_bgc-car_my_2.5km_P1Y-m',\n 'cmems_mod_blk_bgc-car_my_2.5km_climatology_P1M-m',\n 'cmems_mod_blk_bgc-car_myint_2.5km_P1M-m',\n 'cmems_mod_blk_bgc-co2_my_2.5km_P1D-m',\n 'cmems_mod_blk_bgc-co2_my_2.5km_P1M-m',\n 'cmems_mod_blk_bgc-co2_my_2.5km_P1Y-m',\n 'cmems_mod_blk_bgc-co2_my_2.5km_climatology_P1M-m',\n 'cmems_mod_blk_bgc-co2_myint_2.5km_P1M-m',\n 'cmems_mod_blk_bgc-nut_my_2.5km_P1D-m',\n 'cmems_mod_blk_bgc-nut_my_2.5km_P1M-m',\n 'cmems_mod_blk_bgc-nut_my_2.5km_P1Y-m',\n 'cmems_mod_blk_bgc-nut_my_2.5km_climatology_P1M-m',\n 'cmems_mod_blk_bgc-nut_myint_2.5km_P1M-m',\n 'cmems_mod_blk_bgc-plankton_my_2.5km_P1D-m',\n 'cmems_mod_blk_bgc-plankton_my_2.5km_P1M-m',\n 'cmems_mod_blk_bgc-plankton_my_2.5km_P1Y-m',\n 'cmems_mod_blk_bgc-plankton_my_2.5km_climatology_P1M-m',\n 'cmems_mod_blk_bgc-plankton_myint_2.5km_P1M-m',\n 'cmems_mod_blk_bgc_my_2.5km_static',\n 'cmems_mod_blk_phy-cur_my_2.5km-climatology_P1M-m',\n 'cmems_mod_blk_phy-cur_my_2.5km_P1D-m',\n 'cmems_mod_blk_phy-cur_my_2.5km_P1M-m',\n 'cmems_mod_blk_phy-cur_my_2.5km_P1Y-m',\n 'cmems_mod_blk_phy-cur_myint_2.5km_P1M-m',\n 'cmems_mod_blk_phy-hflux_my_2.5km_P1D-m',\n 'cmems_mod_blk_phy-hflux_my_2.5km_P1M-m',\n 'cmems_mod_blk_phy-mflux_my_2.5km_P1D-m',\n 'cmems_mod_blk_phy-mflux_my_2.5km_P1M-m',\n 'cmems_mod_blk_phy-mld_my_2.5km-climatology_P1M-m',\n 'cmems_mod_blk_phy-mld_my_2.5km_P1D-m',\n 'cmems_mod_blk_phy-mld_my_2.5km_P1M-m',\n 'cmems_mod_blk_phy-mld_my_2.5km_P1Y-m',\n 'cmems_mod_blk_phy-mld_myint_2.5km_P1M-m',\n 'cmems_mod_blk_phy-sal_my_2.5km-climatology_P1M-m',\n 'cmems_mod_blk_phy-sal_my_2.5km_P1D-m',\n 'cmems_mod_blk_phy-sal_my_2.5km_P1M-m',\n 'cmems_mod_blk_phy-sal_my_2.5km_P1Y-m',\n 'cmems_mod_blk_phy-sal_myint_2.5km_P1M-m',\n 'cmems_mod_blk_phy-ssh_my_2.5km-climatology_P1M-m',\n 'cmems_mod_blk_phy-ssh_my_2.5km_P1D-m',\n 'cmems_mod_blk_phy-ssh_my_2.5km_P1M-m',\n 'cmems_mod_blk_phy-ssh_my_2.5km_P1Y-m',\n 'cmems_mod_blk_phy-ssh_myint_2.5km_P1M-m',\n 'cmems_mod_blk_phy-temp_my_2.5km-climatology_P1M-m',\n 'cmems_mod_blk_phy-temp_my_2.5km_P1D-m',\n 'cmems_mod_blk_phy-temp_my_2.5km_P1M-m',\n 'cmems_mod_blk_phy-temp_my_2.5km_P1Y-m',\n 'cmems_mod_blk_phy-temp_myint_2.5km_P1M-m',\n 'cmems_mod_blk_phy-wflux_my_2.5km_P1D-m',\n 'cmems_mod_blk_phy-wflux_my_2.5km_P1M-m',\n 'cmems_mod_blk_phy_my_2.5km_static',\n 'cmems_mod_blk_wav-aflux_my_2.5km_PT1H-i',\n 'cmems_mod_blk_wav_my_2.5km-climatology_PT1M-m',\n 'cmems_mod_blk_wav_my_2.5km_PT1H-i',\n 'cmems_mod_blk_wav_my_2.5km_static',\n 'cmems_mod_blk_wav_myint_2.5km_PT1H-i',\n 'blksea_omi_health_oxygen_trend_annual',\n 'blksea_omi_health_oxygen_trend_monthly',\n 'blksea_omi_seastate_extreme_var_swh_mean_and_anomaly',\n 'blksea_omi_tempsal_extreme_var_temp_mean_and_anomaly',\n 'blksea_omi_tempsal_sst_area_averaged_anomalies',\n 'blksea_omi_tempsal_sst_trend',\n 'cmems_mod_glo_bgc-bio_anfc_0.25deg_P1D-m',\n 'cmems_mod_glo_bgc-bio_anfc_0.25deg_P1M-m',\n 'cmems_mod_glo_bgc-car_anfc_0.25deg_P1D-m',\n 'cmems_mod_glo_bgc-car_anfc_0.25deg_P1M-m',\n 'cmems_mod_glo_bgc-co2_anfc_0.25deg_P1D-m',\n 'cmems_mod_glo_bgc-co2_anfc_0.25deg_P1M-m',\n 'cmems_mod_glo_bgc-nut_anfc_0.25deg_P1D-m',\n 'cmems_mod_glo_bgc-nut_anfc_0.25deg_P1M-m',\n 'cmems_mod_glo_bgc-optics_anfc_0.25deg_P1D-m',\n 'cmems_mod_glo_bgc-optics_anfc_0.25deg_P1M-m',\n 'cmems_mod_glo_bgc-pft_anfc_0.25deg_P1D-m',\n 'cmems_mod_glo_bgc-pft_anfc_0.25deg_P1M-m',\n 'cmems_mod_glo_bgc-plankton_anfc_0.25deg_P1D-m',\n 'cmems_mod_glo_bgc-plankton_anfc_0.25deg_P1M-m',\n 'cmems_mod_glo_bgc_anfc_0.25deg_static',\n 'cmems_mod_glo_phy-cur_anfc_0.083deg_P1D-m',\n 'cmems_mod_glo_phy-cur_anfc_0.083deg_P1M-m',\n 'cmems_mod_glo_phy-cur_anfc_0.083deg_PT6H-i',\n 'cmems_mod_glo_phy-so_anfc_0.083deg_P1D-m',\n 'cmems_mod_glo_phy-so_anfc_0.083deg_P1M-m',\n 'cmems_mod_glo_phy-so_anfc_0.083deg_PT6H-i',\n 'cmems_mod_glo_phy-thetao_anfc_0.083deg_P1D-m',\n 'cmems_mod_glo_phy-thetao_anfc_0.083deg_P1M-m',\n 'cmems_mod_glo_phy-thetao_anfc_0.083deg_PT6H-i',\n 'cmems_mod_glo_phy-wcur_anfc_0.083deg_P1D-m',\n 'cmems_mod_glo_phy-wcur_anfc_0.083deg_P1M-m',\n 'cmems_mod_glo_phy_anfc_0.083deg-climatology-uncertainty_P1M-m',\n 'cmems_mod_glo_phy_anfc_0.083deg-sst-anomaly_P1D-m',\n 'cmems_mod_glo_phy_anfc_0.083deg-sst-anomaly_P1M-m',\n 'cmems_mod_glo_phy_anfc_0.083deg_P1D-m',\n 'cmems_mod_glo_phy_anfc_0.083deg_P1M-m',\n 'cmems_mod_glo_phy_anfc_0.083deg_PT1H-m',\n 'cmems_mod_glo_phy_anfc_0.083deg_static',\n 'cmems_mod_glo_phy_anfc_merged-sl_PT1H-i',\n 'cmems_mod_glo_phy_anfc_merged-uv_PT1H-i',\n 'cmems_mod_glo_wav_anfc_0.083deg_PT3H-i',\n 'cmems_mod_wav_anfc_0.083deg_static',\n 'cmems_mod_glo_bgc_my_0.25deg_P1D-m',\n 'cmems_mod_glo_bgc_my_0.25deg_P1M-m',\n 'cmems_mod_glo_bgc_my_0.25deg_static',\n 'cmems_mod_glo_bgc_myint_0.25deg_P1D-m',\n 'cmems_mod_glo_bgc_myint_0.25deg_P1M-m',\n 'cmems_mod_glo_bgc_my_0.083deg-lmtl-Fphy_PT1D-i',\n 'cmems_mod_glo_bgc_my_0.083deg-lmtl_PT1D-i',\n 'cmems_mod_glo_phy_my_0.083deg-climatology_P1M-m',\n 'cmems_mod_glo_phy_my_0.083deg_P1D-m',\n 'cmems_mod_glo_phy_my_0.083deg_P1M-m',\n 'cmems_mod_glo_phy_my_0.083deg_static',\n 'cmems_mod_glo_phy_myint_0.083deg_P1D-m',\n 'cmems_mod_glo_phy_myint_0.083deg_P1M-m',\n 'cmems_mod_glo_phy-all_my_0.25deg_P1D-m',\n 'cmems_mod_glo_phy-all_my_0.25deg_P1M-m',\n 'cmems_mod_glo_phy-mnstd_my_0.25deg_P1D-m',\n 'cmems_mod_glo_phy-mnstd_my_0.25deg_P1M-m',\n 'cmems_mod_glo_wav_my_0.2deg-climatology_P1M-m',\n 'cmems_mod_glo_wav_my_0.2deg_PT3H-i',\n 'cmems_mod_glo_wav_my_0.2deg_static',\n 'cmems_mod_glo_wav_myint_0.2deg_PT3H-i',\n 'global_omi_climate-variability_nino34_tzt_anom',\n 'global_omi_climate-variability_nino34_sst_anom',\n 'global_omi_health_carbon_co2_flux_integrated',\n 'global_omi_health_carbon_ph_area_averaged',\n 'global_omi_health_carbon_ph_trend',\n 'global_omi_natlantic_amoc_26N_profile',\n 'global_omi_natlantic_amoc_max26N_timeseries',\n 'global_omi_ohc_area_averaged_anomalies_0_2000',\n 'global_omi_ohc_area_averaged_anomalies_0_300',\n 'global_omi_ohc_area_averaged_anomalies_0_700',\n 'global_omi_ohc_trend',\n 'global_omi_sl_thsl_area_averaged_anomalies_0_2000',\n 'global_omi_sl_thsl_area_averaged_anomalies_0_700',\n 'global_omi_sl_thsl_trend',\n 'global_omi_tempsal_tyz_trend',\n 'global_omi_tempsal_sst_area_averaged_anomalies',\n 'global_omi_tempsal_sst_trend',\n 'global_omi_wmhe_heattrp',\n 'global_omi_wmhe_northward_mht',\n 'global_omi_wmhe_voltrp',\n 'cmems_mod_ibi_bgc-optics_anfc_0.027deg_P1D-m',\n 'cmems_mod_ibi_bgc-optics_anfc_0.027deg_P1M-m',\n 'cmems_mod_ibi_bgc_anfc_0.027deg-3D_P1D-m',\n 'cmems_mod_ibi_bgc_anfc_0.027deg-3D_P1M-m',\n 'cmems_mod_ibi_bgc_anfc_0.027deg-3D_static',\n 'cmems_mod_ibi_phy-cur_anfc_detided-0.027deg_P1D-m',\n 'cmems_mod_ibi_phy-cur_anfc_detided-0.027deg_P1M-m',\n 'cmems_mod_ibi_phy-ssh_anfc_detided-0.027deg_P1D-m',\n 'cmems_mod_ibi_phy-ssh_anfc_detided-0.027deg_P1M-m',\n 'cmems_mod_ibi_phy-wcur_anfc_0.027deg_P1D-m',\n 'cmems_mod_ibi_phy-wcur_anfc_0.027deg_P1M-m',\n 'cmems_mod_ibi_phy_anfc_0.027deg-2D_PT15M-i',\n 'cmems_mod_ibi_phy_anfc_0.027deg-2D_PT1H-m',\n 'cmems_mod_ibi_phy_anfc_0.027deg-3D_P1D-m',\n 'cmems_mod_ibi_phy_anfc_0.027deg-3D_P1M-m',\n 'cmems_mod_ibi_phy_anfc_0.027deg-3D_PT1H-m',\n 'cmems_mod_ibi_phy_anfc_0.027deg-3D_static',\n 'cmems_mod_ibi_wav_anfc_0.027deg_PT1H-i',\n 'cmems_mod_ibi_wav_anfc_0.027deg_static',\n 'cmems_mod_ibi_wav_anfc_0.05deg_PT1H-i',\n 'cmems_mod_ibi_wav_anfc_0.05deg_static',\n 'cmems_mod_ibi_bgc-plankton_my_0.083deg_P1D-m',\n 'cmems_mod_ibi_bgc-plankton_my_0.083deg_P1M-m',\n 'cmems_mod_ibi_bgc-plankton_my_0.083deg_P1Y-m',\n 'cmems_mod_ibi_bgc-plankton_myint_0.083deg_P1D-m',\n 'cmems_mod_ibi_bgc-plankton_myint_0.083deg_P1M-m',\n 'cmems_mod_ibi_bgc_my_0.083deg-3D-climatology_P1M-m',\n 'cmems_mod_ibi_bgc_my_0.083deg-3D_P1D-m',\n 'cmems_mod_ibi_bgc_my_0.083deg-3D_P1M-m',\n 'cmems_mod_ibi_bgc_my_0.083deg-3D_P1Y-m',\n 'cmems_mod_ibi_bgc_my_0.083deg-3D_static',\n 'cmems_mod_ibi_bgc_myint_0.083deg_P1D-m',\n 'cmems_mod_ibi_bgc_myint_0.083deg_P1M-m',\n 'cmems_mod_ibi_phy_my-hflux_0.083deg_P1D-m',\n 'cmems_mod_ibi_phy_my-hflux_0.083deg_P1M-m',\n 'cmems_mod_ibi_phy_my-mflux_0.083deg_P1D-m',\n 'cmems_mod_ibi_phy_my-mflux_0.083deg_P1M-m',\n 'cmems_mod_ibi_phy_my-wcur_0.083deg_P1D-m',\n 'cmems_mod_ibi_phy_my-wcur_0.083deg_P1M-m',\n 'cmems_mod_ibi_phy_my-wcur_0.083deg_P1Y-m',\n 'cmems_mod_ibi_phy_my-wflux_0.083deg_P1D-m',\n 'cmems_mod_ibi_phy_my-wflux_0.083deg_P1M-m',\n 'cmems_mod_ibi_phy_my_0.083deg-2D_PT1H-m',\n 'cmems_mod_ibi_phy_my_0.083deg-3D-climatology_P1M-m',\n 'cmems_mod_ibi_phy_my_0.083deg-3D_P1D-m',\n 'cmems_mod_ibi_phy_my_0.083deg-3D_P1M-m',\n 'cmems_mod_ibi_phy_my_0.083deg-3D_P1Y-m',\n 'cmems_mod_ibi_phy_my_0.083deg-3D_static',\n 'cmems_mod_ibi_phy_myint-hflux_0.083deg_P1D-m',\n 'cmems_mod_ibi_phy_myint-hflux_0.083deg_P1M-m',\n 'cmems_mod_ibi_phy_myint-mflux_0.083deg_P1D-m',\n 'cmems_mod_ibi_phy_myint-mflux_0.083deg_P1M-m',\n 'cmems_mod_ibi_phy_myint-wcur_0.083deg_P1D-m',\n 'cmems_mod_ibi_phy_myint-wcur_0.083deg_P1M-m',\n 'cmems_mod_ibi_phy_myint-wflux_0.083deg_P1D-m',\n 'cmems_mod_ibi_phy_myint-wflux_0.083deg_P1M-m',\n 'cmems_mod_ibi_phy_myint_0.083deg_P1D-m',\n 'cmems_mod_ibi_phy_myint_0.083deg_P1M-m',\n 'cmems_mod_ibi_phy_myint_0.083deg_PT1H-m',\n 'cmems_mod_ibi_wav_my-aflux_0.027deg_P1H-i',\n 'cmems_mod_ibi_wav_my_0.027deg-climatology_P1M-m',\n 'cmems_mod_ibi_wav_my_0.027deg_PT1H-i',\n 'cmems_mod_ibi_wav_my_0.027deg_static',\n 'ibi_omi_currents_cui_index',\n 'ibi_omi_currents_cui_trend',\n 'ibi_omi_seastate_extreme_var_swh_mean_and_anomaly',\n 'ibi_omi_seastate_swi_anomaly',\n 'ibi_omi_seastate_swi_mean_percentile',\n 'ibi_omi_tempsal_extreme_var_temp_mean_and_anomaly',\n 'ibi_omi_wmhe_mow_anomalies_north',\n 'ibi_omi_wmhe_mow_anomalies_reservoir',\n 'ibi_omi_wmhe_mow_anomalies_south',\n 'ibi_omi_wmhe_mow_anomalies_west',\n 'cmems_obs-ins_arc_phybgcwav_mynrt_na_irr',\n 'cmems_obs-ins_bal_phybgcwav_mynrt_na_irr',\n 'cmems_obs-ins_blk_phybgcwav_mynrt_na_irr',\n 'cmems_obs-ins_glo_bgc-car_my_glodap-gridded_irr',\n 'cmems_obs-ins_glo_bgc-car_my_glodap-obs_irr',\n 'cmems_obs-ins_glo_bgc-car_my_socat-gridded_irr',\n 'cmems_obs-ins_glo_bgc-car_my_socat-obs_irr',\n 'cmems_obs-ins_glo_bgc-chl_my_na_irr',\n 'cmems_obs-ins_glo_bgc-nut_my_na_irr',\n 'cmems_obs-ins_glo_bgc-ox_my_na_irr',\n 'cmems_obs-ins_glo_phybgcwav_mynrt_na_irr',\n 'cmems_obs-ins_glo_phy-ssh_my_na_PT1H',\n 'cmems_obs-ins_glo_phy-ssh_my_na_irr',\n 'cmems_obs-ins_ibi_phy-ssh_my_tide-surge_PT1H',\n 'cmems_obs-ins_glo_phy-temp-sal_my_cora_irr',\n 'cmems_obs-ins_glo_phy-temp-sal_my_easycora_irr',\n 'cmems_obs-ins_glo_phy-temp-sal_my_cora-oa_P1M',\n 'cmems_obs-ins_glo_phy-temp-sal_nrt_oa_P1M',\n 'cmems_obs-ins_glo_phy-cur_my_adcp_irr',\n 'cmems_obs-ins_glo_phy-cur_my_argo_irr',\n 'cmems_obs-ins_glo_phy-cur_my_drifter_PT1H',\n 'cmems_obs-ins_glo_phy-cur_my_glider_irr',\n 'cmems_obs-ins_glo_phy-cur_my_radar-radial_irr',\n 'cmems_obs-ins_glo_phy-cur_my_radar-total_irr',\n 'cmems_obs-ins_glo_phy-cur_nrt_argo_irr',\n 'cmems_obs-ins_glo_phy-cur_nrt_drifter_irr',\n 'cmems_obs-ins_glo_phy-cur_nrt_radar-radial_irr',\n 'cmems_obs-ins_glo_phy-cur_nrt_radar-total_irr',\n 'cmems_obs-ins_glo_wav_my_na_PT1H',\n 'cmems_obs-ins_glo_wav_my_na_irr',\n 'cmems_obs-ins_ibi_phybgcwav_mynrt_na_irr',\n 'cmems_obs-ins_med_phybgcwav_mynrt_na_irr',\n 'cmems_obs-ins_nws_phybgcwav_mynrt_na_irr',\n 'cmems_mod_med_bgc-bio_anfc_4.2km_P1D-m',\n 'cmems_mod_med_bgc-bio_anfc_4.2km_P1M-m',\n 'cmems_mod_med_bgc-car_anfc_4.2km_P1D-m',\n 'cmems_mod_med_bgc-car_anfc_4.2km_P1M-m',\n 'cmems_mod_med_bgc-co2_anfc_4.2km_P1D-m',\n 'cmems_mod_med_bgc-co2_anfc_4.2km_P1M-m',\n 'cmems_mod_med_bgc-nut_anfc_4.2km_P1D-m',\n 'cmems_mod_med_bgc-nut_anfc_4.2km_P1M-m',\n 'cmems_mod_med_bgc-optics_anfc_4.2km_P1D-m',\n 'cmems_mod_med_bgc-optics_anfc_4.2km_P1M-m',\n 'cmems_mod_med_bgc-pft_anfc_4.2km_P1D-m',\n 'cmems_mod_med_bgc-pft_anfc_4.2km_P1M-m',\n 'cmems_mod_med_bgc_anfc_4.2km_static',\n 'cmems_mod_med_phy-cur_anfc_4.2km-2D_PT1H-m',\n 'cmems_mod_med_phy-cur_anfc_4.2km-3D_PT1H-m',\n 'cmems_mod_med_phy-cur_anfc_4.2km_P1D-m',\n 'cmems_mod_med_phy-cur_anfc_4.2km_P1M-m',\n 'cmems_mod_med_phy-cur_anfc_4.2km_PT15M-i',\n 'cmems_mod_med_phy-cur_anfc_detided_4.2km_P1D-m',\n 'cmems_mod_med_phy-mld_anfc_4.2km-2D_PT1H-m',\n 'cmems_mod_med_phy-mld_anfc_4.2km_P1D-m',\n 'cmems_mod_med_phy-mld_anfc_4.2km_P1M-m',\n 'cmems_mod_med_phy-sal_anfc_4.2km-2D_PT1H-m',\n 'cmems_mod_med_phy-sal_anfc_4.2km-3D_PT1H-m',\n 'cmems_mod_med_phy-sal_anfc_4.2km_P1D-m',\n 'cmems_mod_med_phy-sal_anfc_4.2km_P1M-m',\n 'cmems_mod_med_phy-ssh_anfc_4.2km-2D_PT1H-m',\n 'cmems_mod_med_phy-ssh_anfc_4.2km_P1D-m',\n 'cmems_mod_med_phy-ssh_anfc_4.2km_P1M-m',\n 'cmems_mod_med_phy-ssh_anfc_4.2km_PT15M-i',\n 'cmems_mod_med_phy-ssh_anfc_detided_4.2km_P1D-m',\n 'cmems_mod_med_phy-tem_anfc_4.2km-2D_PT1H-m',\n 'cmems_mod_med_phy-tem_anfc_4.2km-3D_PT1H-m',\n 'cmems_mod_med_phy-tem_anfc_4.2km_P1D-m',\n 'cmems_mod_med_phy-tem_anfc_4.2km_P1M-m',\n 'cmems_mod_med_phy-wcur_anfc_4.2km_P1D-m',\n 'cmems_mod_med_phy-wcur_anfc_4.2km_P1M-m',\n 'cmems_mod_med_phy_anfc_4.2km_static',\n 'cmems_mod_med_wav_anfc_4.2km_PT1H-i',\n 'cmems_mod_med_wav_anfc_4.2km_static',\n 'cmems_mod_med_bgc-bio_my_4.2km_P1Y-m',\n 'cmems_mod_med_bgc-bio_myint_4.2km_P1M-m',\n 'cmems_mod_med_bgc-car_my_4.2km_P1Y-m',\n 'cmems_mod_med_bgc-car_myint_4.2km_P1M-m',\n 'cmems_mod_med_bgc-co2_my_4.2km_P1Y-m',\n 'cmems_mod_med_bgc-co2_myint_4.2km_P1M-m',\n 'cmems_mod_med_bgc-nut_my_4.2km_P1Y-m',\n 'cmems_mod_med_bgc-nut_myint_4.2km_P1M-m',\n 'cmems_mod_med_bgc-pft_myint_4.2km_P1M-m',\n 'cmems_mod_med_bgc-plankton_my_4.2km_P1Y-m',\n 'cmems_mod_med_bgc_my_4.2km-climatology_P1M-m',\n 'cmems_mod_med_bgc_my_4.2km_static',\n 'med-ogs-bio-rean-d',\n 'med-ogs-bio-rean-m',\n 'med-ogs-car-rean-d',\n 'med-ogs-car-rean-m',\n 'med-ogs-co2-rean-d',\n 'med-ogs-co2-rean-m',\n 'med-ogs-nut-rean-d',\n 'med-ogs-nut-rean-m',\n 'med-ogs-pft-rean-d',\n 'med-ogs-pft-rean-m',\n 'cmems_mod_med_phy-cur_my_4.2km_P1Y-m',\n 'cmems_mod_med_phy-hflux_my_4.2km_P1D-m',\n 'cmems_mod_med_phy-hflux_my_4.2km_P1M-m',\n 'cmems_mod_med_phy-mflux_my_4.2km_P1D-m',\n 'cmems_mod_med_phy-mflux_my_4.2km_P1M-m',\n 'cmems_mod_med_phy-mld_my_4.2km_P1Y-m',\n 'cmems_mod_med_phy-sal_my_4.2km_P1Y-m',\n 'cmems_mod_med_phy-ssh_my_4.2km_P1Y-m',\n 'cmems_mod_med_phy-tem_my_4.2km_P1Y-m',\n 'cmems_mod_med_phy-wflux_my_4.2km_P1D-m',\n 'cmems_mod_med_phy-wflux_my_4.2km_P1M-m',\n 'cmems_mod_med_phy_my_4.2km-climatology_P1M-m',\n 'cmems_mod_med_phy_my_4.2km_static',\n 'med-cmcc-cur-int-m',\n 'med-cmcc-cur-rean-d',\n 'med-cmcc-cur-rean-h',\n 'med-cmcc-cur-rean-m',\n 'med-cmcc-mld-int-m',\n 'med-cmcc-mld-rean-d',\n 'med-cmcc-mld-rean-m',\n 'med-cmcc-sal-int-m',\n 'med-cmcc-sal-rean-d',\n 'med-cmcc-sal-rean-m',\n 'med-cmcc-ssh-int-m',\n 'med-cmcc-ssh-rean-d',\n 'med-cmcc-ssh-rean-h',\n 'med-cmcc-ssh-rean-m',\n 'med-cmcc-tem-int-m',\n 'med-cmcc-tem-rean-d',\n 'med-cmcc-tem-rean-m',\n 'cmems_mod_med_wav_my_4.2km-climatology_P1M-m',\n 'cmems_mod_med_wav_my_4.2km_static',\n 'cmems_mod_med_wav_myint_4.2km_PT1H-i',\n 'med-hcmr-wav-rean-h',\n 'medsea_omi_ohc_area_averaged_anomalies',\n 'medsea_omi_seastate_extreme_var_swh_mean_and_anomaly',\n 'medsea_omi_tempsal_extreme_var_temp_mean_and_anomaly',\n 'medsea_omi_tempsal_sst_area_averaged_anomalies',\n 'medsea_omi_tempsal_sst_trend',\n 'cmems_obs-mob_glo_bgc-nut-car_mynrt_irr_i',\n 'cmems_obs-mob_glo_bgc-chl-poc_my_0.25deg-climatology_P1M-m',\n 'cmems_obs-mob_glo_bgc-chl-poc_my_0.25deg_P7D-m',\n 'cmems_obs-mob_glo_bgc-car_my_irr-i',\n 'cmems_obs-mob_glo_bgc-car_nrt_irr-i',\n 'cmems_obs-mob_glo_phy-cur_my_0.25deg_P1D-m',\n 'cmems_obs-mob_glo_phy-cur_my_0.25deg_P1M-m',\n 'cmems_obs-mob_glo_phy-cur_my_0.25deg_PT1H-i',\n 'cmems_obs-mob_glo_phy-cur_nrt_0.25deg_P1D-m',\n 'cmems_obs-mob_glo_phy-cur_nrt_0.25deg_P1M-m',\n 'cmems_obs-mob_glo_phy-cur_nrt_0.25deg_PT1H-i',\n 'cmems_obs_mob_glo_phy-cur_my_0.25deg_P1D-m',\n 'cmems_obs_mob_glo_phy-cur_my_0.25deg_P1M-m',\n 'cmems_obs_mob_glo_phy-cur_my_0.25deg_PT1H-i',\n 'cmems_obs_mob_glo_phy-cur_nrt_0.25deg_P1D-m',\n 'cmems_obs_mob_glo_phy-cur_nrt_0.25deg_P1M-m',\n 'cmems_obs_mob_glo_phy-cur_nrt_0.25deg_PT1H-i',\n 'cmems_obs-mob_glo_phy-sss_mynrt_smos-asc_P1D',\n 'cmems_obs-mob_glo_phy-sss_mynrt_smos-des_P1D',\n 'cmems_obs-mob_glo_phy-sss_mynrt_smos_P1D',\n 'cmems_obs-mob_glo_phy-sss_my_multi-oi_P1W',\n 'cmems_obs-mob_glo_phy-sss_my_multi_P1D',\n 'cmems_obs-mob_glo_phy-sss_my_multi_P1M',\n 'cmems_obs-mob_glo_phy-sss_nrt_multi_P1D',\n 'cmems_obs-mob_glo_phy-sss_nrt_multi_P1M',\n 'dataset-armor-3d-nrt-monthly',\n 'dataset-armor-3d-nrt-weekly',\n 'dataset-armor-3d-rep-monthly',\n 'dataset-armor-3d-rep-weekly',\n 'cmems_obs-mob_glo_phy-cur_my_0.25deg_P7D-i',\n 'northwestshelf_omi_tempsal_extreme_var_temp_mean_and_anomaly',\n 'cmems_mod_nws_bgc-optics_anfc_0.027deg_P1D-m',\n 'cmems_mod_nws_bgc-optics_anfc_0.027deg_P1M-m',\n 'cmems_mod_nws_bgc_anfc_0.027deg-3D_P1D-m',\n 'cmems_mod_nws_bgc_anfc_0.027deg-3D_P1M-m',\n 'cmems_mod_nws_bgc_anfc_0.027deg-3D_static',\n 'cmems_mod_nws_phy-cur_anfc_detided-0.027deg_P1D-m',\n 'cmems_mod_nws_phy-cur_anfc_detided-0.027deg_P1M-m',\n 'cmems_mod_nws_phy-ssh_anfc_detided-0.027deg_P1D-m',\n 'cmems_mod_nws_phy-ssh_anfc_detided-0.027deg_P1M-m',\n 'cmems_mod_nws_phy-wcur_anfc_0.027deg_P1D-m',\n 'cmems_mod_nws_phy-wcur_anfc_0.027deg_P1M-m',\n 'cmems_mod_nws_phy_anfc_0.027deg-2D_PT15M-i',\n 'cmems_mod_nws_phy_anfc_0.027deg-2D_PT1H-m',\n 'cmems_mod_nws_phy_anfc_0.027deg-3D_P1D-m',\n 'cmems_mod_nws_phy_anfc_0.027deg-3D_P1M-m',\n 'cmems_mod_nws_phy_anfc_0.027deg-3D_PT1H-m',\n 'cmems_mod_nws_phy_anfc_0.027deg-3D_static',\n 'cmems_mod_nws_wav_anfc_0.027deg_PT1H-i',\n 'cmems_mod_nws_wav_anfc_0.027deg_static',\n 'cmems_mod_nws_wav_anfc_0.05deg_PT1H-i',\n 'cmems_mod_nws_wav_anfc_0.05deg_static',\n 'cmems_mod_nws_bgc-chl_my_7km-3D_P1D-m',\n 'cmems_mod_nws_bgc-chl_my_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-chl_myint_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-kd_my_7km-3D_P1D-m',\n 'cmems_mod_nws_bgc-kd_my_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-kd_myint_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-no3_my_7km-3D_P1D-m',\n 'cmems_mod_nws_bgc-no3_my_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-no3_myint_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-o2_my_7km-3D_P1D-m',\n 'cmems_mod_nws_bgc-o2_my_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-o2_myint_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-pft_my_7km-3D-diato_P1D-m',\n 'cmems_mod_nws_bgc-pft_my_7km-3D-diato_P1M-m',\n 'cmems_mod_nws_bgc-pft_my_7km-3D-dino_P1D-m',\n 'cmems_mod_nws_bgc-pft_my_7km-3D-dino_P1M-m',\n 'cmems_mod_nws_bgc-pft_my_7km-3D-nano_P1D-m',\n 'cmems_mod_nws_bgc-pft_my_7km-3D-nano_P1M-m',\n 'cmems_mod_nws_bgc-pft_my_7km-3D-pico_P1D-m',\n 'cmems_mod_nws_bgc-pft_my_7km-3D-pico_P1M-m',\n 'cmems_mod_nws_bgc-pft_myint_7km-3D-diato_P1M-m',\n 'cmems_mod_nws_bgc-pft_myint_7km-3D-dino_P1M-m',\n 'cmems_mod_nws_bgc-pft_myint_7km-3D-nano_P1M-m',\n 'cmems_mod_nws_bgc-pft_myint_7km-3D-pico_P1M-m',\n 'cmems_mod_nws_bgc-ph_my_7km-3D_P1D-m',\n 'cmems_mod_nws_bgc-ph_my_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-ph_myint_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-phyc_my_7km-3D_P1D-m',\n 'cmems_mod_nws_bgc-phyc_my_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-phyc_myint_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-po4_my_7km-3D_P1D-m',\n 'cmems_mod_nws_bgc-po4_my_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-po4_myint_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-pp_my_7km-3D_P1D-m',\n 'cmems_mod_nws_bgc-pp_my_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-pp_myint_7km-3D_P1M-m',\n 'cmems_mod_nws_bgc-spco2_my_7km-2D_P1D-m',\n 'cmems_mod_nws_bgc-spco2_my_7km-2D_P1M-m',\n 'cmems_mod_nws_bgc-spco2_myint_7km-2D_P1M-m',\n 'cmems_mod_nws_phy-bottomt_my_7km-2D_P1D-m',\n 'cmems_mod_nws_phy-bottomt_my_7km-2D_P1M-m',\n 'cmems_mod_nws_phy-bottomt_my_7km-2D_PT1H-i',\n 'cmems_mod_nws_phy-bottomt_myint_7km-2D_P1M-m',\n 'cmems_mod_nws_phy-mld_my_7km-2D_P1D-m',\n 'cmems_mod_nws_phy-mld_my_7km-2D_P1M-m',\n 'cmems_mod_nws_phy-mld_my_7km-2D_PT1H-i',\n 'cmems_mod_nws_phy-mld_myint_7km-2D_P1M-m',\n 'cmems_mod_nws_phy-s_my_7km-3D_P1D-m',\n 'cmems_mod_nws_phy-s_my_7km-3D_P1M-m',\n 'cmems_mod_nws_phy-s_myint_7km-3D_P1M-m',\n 'cmems_mod_nws_phy-ssh_my_7km-2D_P1D-m',\n 'cmems_mod_nws_phy-ssh_my_7km-2D_P1M-m',\n 'cmems_mod_nws_phy-ssh_my_7km-2D_PT1H-i',\n 'cmems_mod_nws_phy-ssh_myint_7km-2D_P1M-m',\n 'cmems_mod_nws_phy-sss_my_7km-2D_PT1H-i',\n 'cmems_mod_nws_phy-sst_my_7km-2D_PT1H-i',\n 'cmems_mod_nws_phy-t_my_7km-3D_P1D-m',\n 'cmems_mod_nws_phy-t_my_7km-3D_P1M-m',\n 'cmems_mod_nws_phy-t_myint_7km-3D_P1M-m',\n 'cmems_mod_nws_phy-uv_my_7km-2D_PT1H-i',\n 'cmems_mod_nws_phy-uv_my_7km-3D_P1D-m',\n 'cmems_mod_nws_phy-uv_my_7km-3D_P1M-m',\n 'cmems_mod_nws_phy-uv_myint_7km-3D_P1M-m',\n 'MetO-NWS-WAV-RAN',\n 'cmems_obs_oc_arc_bgc_geophy_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_arc_bgc_optics_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_arc_bgc_transp_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_arc_bgc_geophy_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_arc_bgc_geophy_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_arc_bgc_optics_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_arc_bgc_optics_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_arc_bgc_transp_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_arc_bgc_transp_nrt_l4-hr_P1M-m',\n 'cmems_obs-oc_arc_bgc-plankton_my_l3-multi-4km_P1D',\n 'cmems_obs-oc_arc_bgc-plankton_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_arc_bgc-reflectance_my_l3-multi-4km_P1D',\n 'cmems_obs-oc_arc_bgc-reflectance_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_arc_bgc-transp_my_l3-multi-4km_P1D',\n 'cmems_obs-oc_arc_bgc-transp_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_arc_bgc-plankton_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_arc_bgc-reflectance_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_arc_bgc-transp_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_arc_bgc-plankton_my_l4-multi-4km_P1M',\n 'cmems_obs-oc_arc_bgc-plankton_my_l4-multi-climatology-4km_P1D',\n 'cmems_obs-oc_arc_bgc-plankton_my_l4-olci-300m_P1M',\n 'cmems_obs-oc_arc_bgc-plankton_nrt_l4-olci-300m_P1M',\n 'cmems_obs-oc_arc_bgc-transp_nrt_l4-olci-300m_P1M',\n 'cmems_obs-oc_atl_bgc-optics_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_atl_bgc-plankton_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_atl_bgc-plankton_my_l3-olci-1km_P1D',\n 'cmems_obs-oc_atl_bgc-plankton_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_atl_bgc-reflectance_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_atl_bgc-reflectance_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_atl_bgc-transp_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_atl_bgc-optics_nrt_l3-multi-1km_P1D',\n 'cmems_obs-oc_atl_bgc-plankton_nrt_l3-multi-1km_P1D',\n 'cmems_obs-oc_atl_bgc-plankton_nrt_l3-olci-1km_P1D',\n 'cmems_obs-oc_atl_bgc-plankton_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_atl_bgc-reflectance_nrt_l3-multi-1km_P1D',\n 'cmems_obs-oc_atl_bgc-reflectance_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_atl_bgc-transp_nrt_l3-multi-1km_P1D',\n 'cmems_obs-oc_atl_bgc-plankton_my_l4-gapfree-multi-1km_P1D',\n 'cmems_obs-oc_atl_bgc-plankton_my_l4-multi-1km_P1M',\n 'cmems_obs-oc_atl_bgc-pp_my_l4-multi-1km_P1M',\n 'cmems_obs-oc_atl_bgc-plankton_nrt_l4-gapfree-multi-1km_P1D',\n 'cmems_obs-oc_atl_bgc-plankton_nrt_l4-multi-1km_P1M',\n 'cmems_obs-oc_atl_bgc-pp_nrt_l4-multi-1km_P1M',\n 'cmems_obs_oc_bal_bgc_geophy_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_bal_bgc_optics_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_bal_bgc_transp_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_bal_bgc_tur-spm-chl_nrt_l3-hr-mosaic_P1D-m',\n 'cmems_obs_oc_bal_bgc_geophy_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_bal_bgc_geophy_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_bal_bgc_optics_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_bal_bgc_optics_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_bal_bgc_transp_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_bal_bgc_transp_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_bal_bgc_tur-spm-chl_nrt_l4-hr-mosaic_P1D-m',\n 'cmems_obs-oc_bal_bgc-optics_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_bal_bgc-plankton_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_bal_bgc-plankton_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_bal_bgc-reflectance_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_bal_bgc-reflectance_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_bal_bgc-transp_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_bal_bgc-transp_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_bal_bgc-optics_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_bal_bgc-plankton_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_bal_bgc-reflectance_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_bal_bgc-transp_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_bal_bgc-plankton_my_l4-multi-1km_P1M',\n 'cmems_obs-oc_bal_bgc-plankton_my_l4-olci-300m_P1M',\n 'cmems_obs-oc_bal_bgc-pp_my_l4-multi-4km_P1D',\n 'cmems_obs-oc_bal_bgc-pp_my_l4-multi-4km_P1M',\n 'cmems_obs-oc_bal_bgc-plankton_nrt_l4-olci-300m_P1M',\n 'cmems_obs_oc_blk_bgc_geophy_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_blk_bgc_optics_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_blk_bgc_transp_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_blk_bgc_tur-spm-chl_nrt_l3-hr-mosaic_P1D-m',\n 'cmems_obs_oc_blk_bgc_geophy_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_blk_bgc_geophy_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_blk_bgc_optics_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_blk_bgc_optics_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_blk_bgc_transp_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_blk_bgc_transp_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_blk_bgc_tur-spm-chl_nrt_l4-hr-mosaic_P1D-m',\n 'cmems_obs-oc_blk_bgc-optics_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_blk_bgc-plankton_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_blk_bgc-plankton_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_blk_bgc-reflectance_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_blk_bgc-reflectance_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_blk_bgc-transp_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_blk_bgc-transp_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_blk_bgc-optics_nrt_l3-multi-1km_P1D',\n 'cmems_obs-oc_blk_bgc-plankton_nrt_l3-multi-1km_P1D',\n 'cmems_obs-oc_blk_bgc-plankton_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_blk_bgc-reflectance_nrt_l3-multi-1km_P1D',\n 'cmems_obs-oc_blk_bgc-reflectance_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_blk_bgc-transp_nrt_l3-multi-1km_P1D',\n 'cmems_obs-oc_blk_bgc-transp_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_blk_bgc-plankton_my_l4-gapfree-multi-1km_P1D',\n 'cmems_obs-oc_blk_bgc-plankton_my_l4-multi-1km_P1M',\n 'cmems_obs-oc_blk_bgc-plankton_my_l4-multi-climatology-1km_P1D',\n 'cmems_obs-oc_blk_bgc-plankton_my_l4-olci-300m_P1M',\n 'cmems_obs-oc_blk_bgc-pp_my_l4-multi-4km_P1D',\n 'cmems_obs-oc_blk_bgc-pp_my_l4-multi-4km_P1M',\n 'cmems_obs-oc_blk_bgc-plankton_nrt_l4-gapfree-multi-1km_P1D',\n 'cmems_obs-oc_blk_bgc-plankton_nrt_l4-multi-1km_P1M',\n 'cmems_obs-oc_blk_bgc-plankton_nrt_l4-olci-300m_P1M',\n 'cmems_obs-oc_blk_bgc-pp_nrt_l4-multi-4km_P1D',\n 'cmems_obs-oc_blk_bgc-pp_nrt_l4-multi-4km_P1M',\n 'cmems_obs-oc_blk_bgc-transp_nrt_l4-multi-1km_P1M',\n 'cmems_obs-oc_blk_bgc-transp_nrt_l4-olci-300m_P1M',\n 'cmems_obs-oc_glo_bgc-optics_my_l3-multi-4km_P1D',\n 'cmems_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D',\n 'cmems_obs-oc_glo_bgc-plankton_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_glo_bgc-plankton_my_l3-olci-4km_P1D',\n 'cmems_obs-oc_glo_bgc-reflectance_my_l3-multi-4km_P1D',\n 'cmems_obs-oc_glo_bgc-reflectance_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_glo_bgc-reflectance_my_l3-olci-4km_P1D',\n 'cmems_obs-oc_glo_bgc-transp_my_l3-multi-4km_P1D',\n 'cmems_obs-oc_glo_bgc-transp_my_l3-olci-4km_P1D',\n 'c3s_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D',\n 'c3s_obs-oc_glo_bgc-reflectance_my_l3-multi-4km_P1D',\n 'cmems_obs-oc_glo_bgc-optics_nrt_l3-multi-4km_P1D',\n 'cmems_obs-oc_glo_bgc-plankton_nrt_l3-multi-4km_P1D',\n 'cmems_obs-oc_glo_bgc-plankton_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_glo_bgc-plankton_nrt_l3-olci-4km_P1D',\n 'cmems_obs-oc_glo_bgc-reflectance_nrt_l3-multi-4km_P1D',\n 'cmems_obs-oc_glo_bgc-reflectance_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_glo_bgc-reflectance_nrt_l3-olci-4km_P1D',\n 'cmems_obs-oc_glo_bgc-transp_nrt_l3-multi-4km_P1D',\n 'cmems_obs-oc_glo_bgc-transp_nrt_l3-olci-4km_P1D',\n 'cmems_obs-oc_glo_bgc-optics_my_l4-multi-4km_P1M',\n 'cmems_obs-oc_glo_bgc-plankton_my_l4-gapfree-multi-4km_P1D',\n 'cmems_obs-oc_glo_bgc-plankton_my_l4-multi-4km_P1M',\n 'cmems_obs-oc_glo_bgc-plankton_my_l4-multi-climatology-4km_P1D',\n 'cmems_obs-oc_glo_bgc-plankton_my_l4-olci-300m_P1M',\n 'cmems_obs-oc_glo_bgc-plankton_my_l4-olci-4km_P1M',\n 'cmems_obs-oc_glo_bgc-pp_my_l4-multi-4km_P1M',\n 'cmems_obs-oc_glo_bgc-reflectance_my_l4-multi-4km_P1M',\n 'cmems_obs-oc_glo_bgc-reflectance_my_l4-olci-300m_P1M',\n 'cmems_obs-oc_glo_bgc-reflectance_my_l4-olci-4km_P1M',\n 'cmems_obs-oc_glo_bgc-transp_my_l4-gapfree-multi-4km_P1D',\n 'cmems_obs-oc_glo_bgc-transp_my_l4-multi-4km_P1M',\n 'cmems_obs-oc_glo_bgc-transp_my_l4-olci-4km_P1M',\n 'c3s_obs-oc_glo_bgc-plankton_my_l4-multi-4km_P1M',\n 'cmems_obs-oc_glo_bgc-optics_nrt_l4-multi-4km_P1M',\n 'cmems_obs-oc_glo_bgc-plankton_nrt_l4-gapfree-multi-4km_P1D',\n 'cmems_obs-oc_glo_bgc-plankton_nrt_l4-multi-4km_P1M',\n 'cmems_obs-oc_glo_bgc-plankton_nrt_l4-olci-300m_P1M',\n 'cmems_obs-oc_glo_bgc-plankton_nrt_l4-olci-4km_P1M',\n 'cmems_obs-oc_glo_bgc-pp_nrt_l4-multi-4km_P1M',\n 'cmems_obs-oc_glo_bgc-reflectance_nrt_l4-multi-4km_P1M',\n 'cmems_obs-oc_glo_bgc-reflectance_nrt_l4-olci-300m_P1M',\n 'cmems_obs-oc_glo_bgc-reflectance_nrt_l4-olci-4km_P1M',\n 'cmems_obs-oc_glo_bgc-transp_nrt_l4-gapfree-multi-4km_P1D',\n 'cmems_obs-oc_glo_bgc-transp_nrt_l4-multi-4km_P1M',\n 'cmems_obs-oc_glo_bgc-transp_nrt_l4-olci-4km_P1M',\n 'cmems_obs_oc_ibi_bgc_geophy_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_ibi_bgc_optics_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_ibi_bgc_transp_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_ibi_bgc_tur-spm-chl_nrt_l3-hr-mosaic_P1D-m',\n 'cmems_obs_oc_ibi_bgc_geophy_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_ibi_bgc_geophy_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_ibi_bgc_optics_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_ibi_bgc_optics_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_ibi_bgc_transp_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_ibi_bgc_transp_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_ibi_bgc_tur-spm-chl_nrt_l4-hr-mosaic_P1D-m',\n 'cmems_obs_oc_med_bgc_geophy_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_med_bgc_optics_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_med_bgc_transp_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_med_bgc_tur-spm-chl_nrt_l3-hr-mosaic_P1D-m',\n 'cmems_obs_oc_med_bgc_geophy_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_med_bgc_geophy_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_med_bgc_optics_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_med_bgc_optics_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_med_bgc_transp_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_med_bgc_transp_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_med_bgc_tur-spm-chl_nrt_l4-hr-mosaic_P1D-m',\n 'cmems_obs-oc_med_bgc-optics_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_med_bgc-plankton_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_med_bgc-plankton_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_med_bgc-reflectance_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_med_bgc-reflectance_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_med_bgc-transp_my_l3-multi-1km_P1D',\n 'cmems_obs-oc_med_bgc-transp_my_l3-olci-300m_P1D',\n 'cmems_obs-oc_med_bgc-optics_nrt_l3-multi-1km_P1D',\n 'cmems_obs-oc_med_bgc-plankton_nrt_l3-multi-1km_P1D',\n 'cmems_obs-oc_med_bgc-plankton_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_med_bgc-reflectance_nrt_l3-multi-1km_P1D',\n 'cmems_obs-oc_med_bgc-reflectance_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_med_bgc-transp_nrt_l3-multi-1km_P1D',\n 'cmems_obs-oc_med_bgc-transp_nrt_l3-olci-300m_P1D',\n 'cmems_obs-oc_med_bgc-plankton_my_l4-gapfree-multi-1km_P1D',\n 'cmems_obs-oc_med_bgc-plankton_my_l4-multi-1km_P1M',\n 'cmems_obs-oc_med_bgc-plankton_my_l4-multi-climatology-1km_P1D',\n 'cmems_obs-oc_med_bgc-plankton_my_l4-olci-300m_P1M',\n 'cmems_obs-oc_med_bgc-pp_my_l4-multi-4km_P1D',\n 'cmems_obs-oc_med_bgc-pp_my_l4-multi-4km_P1M',\n 'cmems_obs-oc_med_bgc-plankton_nrt_l4-gapfree-multi-1km_P1D',\n 'cmems_obs-oc_med_bgc-plankton_nrt_l4-multi-1km_P1M',\n 'cmems_obs-oc_med_bgc-plankton_nrt_l4-olci-300m_P1M',\n 'cmems_obs-oc_med_bgc-pp_nrt_l4-multi-4km_P1D',\n 'cmems_obs-oc_med_bgc-pp_nrt_l4-multi-4km_P1M',\n 'cmems_obs-oc_med_bgc-transp_nrt_l4-multi-1km_P1M',\n 'cmems_obs-oc_med_bgc-transp_nrt_l4-olci-300m_P1M',\n 'cmems_obs_oc_nws_bgc_geophy_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_nws_bgc_optics_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_nws_bgc_transp_nrt_l3-hr_P1D-m',\n 'cmems_obs_oc_nws_bgc_tur-spm-chl_nrt_l3-hr-mosaic_P1D-m',\n 'cmems_obs_oc_nws_bgc_geophy_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_nws_bgc_geophy_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_nws_bgc_optics_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_nws_bgc_optics_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_nws_bgc_transp_nrt_l4-hr_P1D-m',\n 'cmems_obs_oc_nws_bgc_transp_nrt_l4-hr_P1M-m',\n 'cmems_obs_oc_nws_bgc_tur-spm-chl_nrt_l4-hr-mosaic_P1D-m',\n 'blksea_omi_circulation_rim_current_index',\n 'omi_circulation_boundary_pacific_kuroshio_phase_area_averaged',\n 'omi_circulation_moc_blksea_area_averaged_mean',\n 'omi_circulation_moc_medsea_area_averaged_mean',\n 'omi_circulation_voltrans_arctic_averaged',\n 'OMI_CIRCULATION_VOLTRANS_IBI_section_integrated_anomalies_AC',\n 'OMI_CIRCULATION_VOLTRANS_IBI_section_integrated_anomalies_ALC',\n 'OMI_CIRCULATION_VOLTRANS_IBI_section_integrated_anomalies_CC',\n 'OMI_CIRCULATION_VOLTRANS_IBI_section_integrated_anomalies_N48',\n 'OMI_CIRCULATION_VOLTRANS_IBI_section_integrated_anomalies_RTE',\n 'omi_climate_ofc_baltic_area_averaged_anomalies',\n 'omi_climate_ohc_blksea_area_averaged_anomalies',\n 'omi_climate_ohc_ibi_area_averaged_anomalies',\n 'omi_climate_osc_medsea_volume_mean',\n 'omi_climate_sl_baltic_area_averaged_anomalies',\n 'omi_climate_sl_blksea_area_averaged_anomalies',\n 'omi_climate_sl_europe_area_averaged_anomalies',\n 'omi_climate_sl_global_area_averaged_anomalies',\n 'omi_climate_sl_global_regional_trends',\n 'omi_climate_sl_ibi_area_averaged_anomalies',\n 'omi_climate_sl_medsea_area_averaged_anomalies',\n 'omi_climate_sl_northwestshelf_area_averaged_anomalies',\n 'omi_climate_sst_bal_area_averaged_anomalies',\n 'omi_climate_sst_bal_trend',\n 'omi_climate_sst_ibi_area_averaged_anomalies',\n 'omi_climate_sst_ibi_trend',\n 'omi_climate_sst_ist_arctic_anomaly',\n 'omi_climate_sst_ist_arctic_area_averaged_anomalies',\n 'omi_climate_sst_ist_arctic_trend',\n 'omi_climate_sst_northwestshelf_area_averaged_anomalies',\n 'omi_climate_sst_northwestshelf_trend',\n 'omi_extreme_climvar_pacific_npgo_sla_eof_mode_projection',\n 'cmems_mod_arc_phy_temp_my_mhw_barentssea_P1D-m',\n 'omi_extreme_seastate_global_swh_mean_and_p95_obs',\n 'omi_extreme_sl_baltic_slev_mean_and_anomaly_obs',\n 'omi_extreme_sl_ibi_slev_mean_and_anomaly_obs',\n 'omi_extreme_sl_medsea_slev_mean_and_anomaly_obs',\n 'omi_extreme_sl_northwestshelf_slev_mean_and_anomaly_obs',\n 'omi_extreme_sst_baltic_sst_mean_and_anomaly_obs',\n 'omi_extreme_sst_ibi_sst_mean_and_anomaly_obs',\n 'omi_extreme_sst_medsea_sst_mean_and_anomaly_obs',\n 'omi_extreme_sst_northwestshelf_sst_mean_and_anomaly_obs',\n 'omi_extreme_wave_baltic_swh_mean_and_anomaly_obs',\n 'omi_extreme_wave_blksea_recent_changes',\n 'omi_extreme_wave_blksea_wave_power',\n 'omi_extreme_wave_ibi_swh_mean_and_anomaly_obs',\n 'omi_extreme_wave_medsea_swh_mean_and_anomaly_obs',\n 'omi_extreme_wave_northwestshelf_swh_mean_and_anomaly_obs',\n 'omi_health_chl_arctic_oceancolour_area_averaged_mean',\n 'omi_health_chl_atlantic_oceancolour_area_averaged_mean',\n 'omi_health_chl_baltic_oceancolour_area_averaged_mean',\n 'omi_health_chl_baltic_oceancolour_trend',\n 'omi_health_chl_blksea_oceancolour_area_averaged_mean',\n 'omi_health_chl_blksea_oceancolour_trend',\n 'omi_health_chl_global_oceancolour_oligo_nag_area_mean',\n 'omi_health_chl_global_oceancolour_oligo_npg_area_mean',\n 'omi_health_chl_global_oceancolour_oligo_sag_area_mean',\n 'omi_health_chl_global_oceancolour_oligo_spg_area_mean',\n 'omi_health_chl_global_oceancolour_trend',\n 'omi_health_chl_medsea_oceancolour_area_averaged_mean',\n 'omi_health_chl_medsea_oceancolour_trend',\n 'omi_var_extreme_wmf_medsea_area_averaged_mean',\n 'cmems_obs-si_ant_phy_nrt_l3-1km_P1D',\n 'cmems_obs-si_ant_phy_my_drift-cfosat-ssmi-merged_P30D',\n 'cmems_obs-si_ant_phy_my_drift-cfosat-ssmi-merged_P3D',\n 'cmems_obs-si_ant_phy_my_drift-cfosat_P2D',\n 'cmems_obs-si_ant_phy_my_drift-cfosat_P3D',\n 'cmems_obs-si_ant_physic_my_drift-amsr_P2D',\n 'cmems_obs-si_ant_physic_my_drift-amsr_P3D',\n 'cmems_obs-si_arc_phy_my_l3_P1D',\n 'cmems_obs-si_arc_phy_nrt_l3_P1D',\n 'cmems_obs-si_arc_phy_my_l4_P1D',\n 'cmems_obs-si_arc_phy_nrt_l4_P1D',\n 'cmems_obs-si_arc_phy-icetype_nrt_L4-auto_P1D',\n 'cmems_obs-si_arc_phy-siconc_nrt_L4-auto_P1D',\n 'cmems_obs-si_arc_phy_nrt_l3_P1D',\n 'cmems_obs-si_arc_phy_my_L3S-DMIOI_P1D-m',\n 'cmems_obs_si_arc_phy_my_L4-DMIOI_P1D-m',\n 'cmems_obs-si_arc_physic_nrt_L2-EW_PT1H-irr',\n 'esa_obs-si_arc_phy-sit_nrt_l4_multi_P1D-m',\n 'CERSAT-GLO-SEAICE_30DAYS_DRIFT_ASCAT_SSMI_MERGED_RAN-OBS_FULL_TIME_SERIE',\n 'CERSAT-GLO-SEAICE_30DAYS_DRIFT_QUICKSCAT_SSMI_MERGED_RAN-OBS_FULL_TIME_SERIE',\n 'CERSAT-GLO-SEAICE_3DAYS_DRIFT_ASCAT_RAN-OBS_FULL_TIME_SERIE',\n 'CERSAT-GLO-SEAICE_3DAYS_DRIFT_ASCAT_SSMI_MERGED_RAN-OBS_FULL_TIME_SERIE',\n 'CERSAT-GLO-SEAICE_3DAYS_DRIFT_QUICKSCAT_RAN-OBS_FULL_TIME_SERIE',\n 'CERSAT-GLO-SEAICE_3DAYS_DRIFT_QUICKSCAT_SSMI_MERGED_RAN-OBS_FULL_TIME_SERIE',\n 'CERSAT-GLO-SEAICE_6DAYS_DRIFT_ASCAT_RAN-OBS_FULL_TIME_SERIE',\n 'CERSAT-GLO-SEAICE_6DAYS_DRIFT_QUICKSCAT_RAN-OBS_FULL_TIME_SERIE',\n 'cmems_obs-si_arc_phy-drift_my_l3-ssmi_P30D',\n 'cmems_obs-si_arc_phy-drift_my_l3-ssmi_P3D',\n 'cmems_obs-si_arc_phy_my_drift-amsr_P2D',\n 'cmems_obs-si_arc_phy_my_drift-amsr_P3D',\n 'cmems_obs-si_arc_phy_my_drift-amsr_P6D',\n 'cmems_obs-si_arc_phy_my_drift-cfosat-ssmi-merged_P30D',\n 'cmems_obs-si_arc_phy_my_drift-cfosat-ssmi-merged_P3D',\n 'cmems_obs-si_arc_phy_my_drift-cfosat_P3D',\n 'cmems_obs-si_arc_phy_my_drift-cfosat_P6D',\n 'METNO-ARC-SEAICE_CONC-L4-NRT-OBS',\n 'cmems_obs-si_arc_phy_nrt_1km-svb_P1D-irr',\n 'cmems_obs-si_arc_physic_nrt_1km-grl_P1D-irr',\n 'cmems_obs-si_arc_physic_nrt_1km-grl_P1WT3D-m',\n 'DMI-ARC-SEAICE_BERG-L4-NRT-OBS',\n 'DMI-ARC-SEAICE_BERG_IW-L4-NRT-OBS',\n 'DMI-ARC-SEAICE_BERG_MOSAIC-L4-NRT-OBS',\n 'DMI-ARC-SEAICE_BERG_MOSAIC_IW-L4-NRT-OBS',\n 'cmems_sat-si_arc_berg-point_nrt_ew_d',\n 'cmems_sat-si_arc_berg-point_nrt_iw_d',\n 'DMI-ARC-SEAICE_TEMP-L4-NRT-OBS',\n 'cmems_obs-si_bal_phy-sit_my_l4-1km_P1D-m',\n 'cmems_obs-si_bal_seaice-conc_my_1km',\n 'FMI-BAL-SEAICE_CONC-L4-NRT-OBS',\n 'FMI-BAL-SEAICE_THICK-L4-NRT-OBS',\n 'cmems_obs-si_bal_phy-sie_nrt_l4_P1D-m',\n 'FMI-BAL-SEAICE_DRIFT-SAR-NRT-OBS',\n 'FMI-BAL-SEAICE_THICK-MOSAIC-SAR-NRT-OBS',\n 'FMI-BAL-SEAICE_THICK-SAR-NRT-OBS',\n 'cmems_obs-si_bal_phy-sie_nrt_500m_P1D-m',\n 'cmems_obs-si_bal_phy-sit_nrt_l4_P1D-m',\n 'cmems_obs-si_bal_phy-sit_nrt_x-500m-l4_P1D',\n 'cmems_sat-si_bal_conc_nrt_500m_d',\n 'cmems_sat-si_bal_thick_nrt_500m_hi',\n 'c3s_obs-si_glo_phy_my_nh-l3_P1M',\n 'cmems_obs-si_glo_phy-drift-north_my_l4_P1D-m',\n 'cmems_obs-si_glo_phy-drift-south_my_l4_P1D-m',\n 'esa_obs-si_ant_phy-sit_nrt_l4-multi_P1D-m',\n 'esa_obs-si_arc_phy-sit_nrt_l4-multi_P1D-m',\n 'METNO-GLO-SEAICE_DRIFT-NORTH-L4-NRT-OBS',\n 'METNO-GLO-SEAICE_DRIFT-SOUTH-L4-NRT-OBS',\n 'osisaf_obs-si_glo_phy-sic-north_nrt_amsr2_l4_P1D-m',\n 'osisaf_obs-si_glo_phy-sic-north_nrt_ssmis_l4_P1D-m',\n 'osisaf_obs-si_glo_phy-sic-south_nrt_amsr2_l4_P1D-m',\n 'osisaf_obs-si_glo_phy-sic-south_nrt_ssmis_l4_P1D-m',\n 'osisaf_obs-si_glo_phy-sidrift_nrt_nh_P1D-m',\n 'osisaf_obs-si_glo_phy-sidrift_nrt_sh_P1D-m',\n 'osisaf_obs-si_glo_phy-siedge_nrt_nh-P1D',\n 'osisaf_obs-si_glo_phy-siedge_nrt_sh-P1D',\n 'osisaf_obs-si_glo_phy-sitype_nrt_nh-P1D',\n 'osisaf_obs-si_glo_phy-sitype_nrt_sh-P1D',\n 'DTU-GLO-SEAICE_DRIFT-NORTH-L4-NRT-OBS',\n 'DTU-GLO-SEAICE_DRIFT-SOUTH-L4-NRT-OBS',\n 'cmems_sat-si_glo_drift_nrt_north_d',\n 'cmems_sat-si_glo_drift_nrt_south_d',\n 'OSISAF-GLO-SEAICE_CONC_CONT_TIMESERIES-NH-LA-OBS',\n 'OSISAF-GLO-SEAICE_CONC_CONT_TIMESERIES-SH-LA-OBS',\n 'OSISAF-GLO-SEAICE_CONC_TIMESERIES-NH-LA-OBS',\n 'OSISAF-GLO-SEAICE_CONC_TIMESERIES-SH-LA-OBS',\n 'cmems_obs-sl_blk_phy-mdt_my_l4-0.0625deg_P20Y',\n 'cmems_obs-sl_eur_phy-ssh_my_al-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_alg-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_c2-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_c2n-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_e1-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_e1g-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_e2-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_en-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_enn-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_g2-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_h2a-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_h2ag-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_h2b-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_j1-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_j1g-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_j1n-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_j2-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_j2g-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_j2n-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_j3-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_j3n-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_s3a-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_s3b-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_s6a-lr-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_swon-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_swonc-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_tp-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_my_tpn-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_nrt_al-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_nrt_c2n-l3-duacs_PT1S',\n 'cmems_obs-sl_eur_phy-ssh_nrt_h2b-l3-duacs_PT0.2S',\n 'cmems_obs-sl_eur_phy-ssh_nrt_h2b-l3-duacs_PT1S',\n ...]</pre> In\u00a0[8]: Copied! <pre>data_des = cmems_store.describe_data(\"cmems_mod_bal_wav_my_PT1H-i\")\n</pre> data_des = cmems_store.describe_data(\"cmems_mod_bal_wav_my_PT1H-i\") <pre>Fetching catalog: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:07&lt;00:00,  2.38s/it]\nINFO - 2024-12-10T14:29:50Z - Dataset version was not specified, the latest one was selected: \"202411\"\nINFO - 2024-12-10T14:29:50Z - Dataset part was not specified, the first one was selected: \"default\"\nINFO - 2024-12-10T14:29:52Z - Service was not specified, the default one was selected: \"arco-geo-series\"\n/home/conda/deepesdl/909ac6b14a2e5a6c9dce2c44580b179aab714a951a659cea07707d516b61742a-20241209-101855-030681-626-xcube-1.7.1/lib/python3.11/site-packages/xcube/core/store/descriptor.py:242: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n  self.dims = dict(dims) if dims else None\n</pre> In\u00a0[9]: Copied! <pre>data_des.time_range\n</pre> data_des.time_range Out[9]: <pre>('1980-01-01', '2024-09-01')</pre> In\u00a0[10]: Copied! <pre>data_des.bbox\n</pre> data_des.bbox Out[10]: <pre>(9, 53, 30.221628189086914, 65.91610717773438)</pre> In\u00a0[11]: Copied! <pre>ds = cmems_store.open_data(\"cmems_mod_bal_wav_my_PT1H-i\")\nds\n</pre> ds = cmems_store.open_data(\"cmems_mod_bal_wav_my_PT1H-i\") ds <pre>Fetching catalog: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:07&lt;00:00,  2.47s/it]\nINFO - 2024-12-10T14:30:01Z - Dataset version was not specified, the latest one was selected: \"202411\"\nINFO - 2024-12-10T14:30:01Z - Dataset part was not specified, the first one was selected: \"default\"\nINFO - 2024-12-10T14:30:03Z - Service was not specified, the default one was selected: \"arco-geo-series\"\n</pre> Out[11]: <pre>&lt;xarray.Dataset&gt; Size: 18TB\nDimensions:    (time: 391560, latitude: 775, longitude: 764)\nCoordinates:\n  * latitude   (latitude) float32 3kB 53.01 53.02 53.04 ... 65.87 65.89 65.91\n  * longitude  (longitude) float32 3kB 9.014 9.042 9.069 ... 30.15 30.18 30.21\n  * time       (time) datetime64[ns] 3MB 1980-01-01T01:00:00 ... 2024-09-01\nData variables: (12/19)\n    VCMX       (time, latitude, longitude) float32 927GB ...\n    VHM0       (time, latitude, longitude) float32 927GB ...\n    VHM0_SW1   (time, latitude, longitude) float32 927GB ...\n    VHM0_SW2   (time, latitude, longitude) float32 927GB ...\n    VHM0_WW    (time, latitude, longitude) float32 927GB ...\n    VMDR       (time, latitude, longitude) float32 927GB ...\n    ...         ...\n    VTM01_SW1  (time, latitude, longitude) float32 927GB ...\n    VTM01_SW2  (time, latitude, longitude) float32 927GB ...\n    VTM01_WW   (time, latitude, longitude) float32 927GB ...\n    VTM02      (time, latitude, longitude) float32 927GB ...\n    VTM10      (time, latitude, longitude) float32 927GB ...\n    VTPK       (time, latitude, longitude) float32 927GB ...\nAttributes:\n    Conventions:            CF-1.0\n    cmems_product_id:       BALTICSEA_MULTIYEAR_WAV_003_015\n    easternmost_longitude:  30.2080\n    grid_resolution:        1 nautical mile (ie. 0.0167 degrees northward; 0....\n    institution:            Baltic MFC, PU Finnish Meteorological Institute\n    northernmost_latitude:  65.9081\n    source:                 FMI-WAM_CMEMS\n    southernmost_latitude:  53.0083\n    title:                  2D - Hourly Instantaneous\n    westernmost_longitude:  9.0138</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 391560</li><li>latitude: 775</li><li>longitude: 764</li></ul></li><li>Coordinates: (3)<ul><li>latitude(latitude)float3253.01 53.02 53.04 ... 65.89 65.91axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northvalid_min :53.00829valid_max :65.907776<pre>array([53.00829 , 53.024956, 53.041622, ..., 65.87444 , 65.891106, 65.907776],\n      dtype=float32)</pre></li><li>longitude(longitude)float329.014 9.042 9.069 ... 30.18 30.21axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_min :9.013887valid_max :30.207739<pre>array([ 9.013887,  9.041664,  9.069442, ..., 30.152184, 30.17996 , 30.207739],\n      dtype=float32)</pre></li><li>time(time)datetime64[ns]1980-01-01T01:00:00 ... 2024-09-01valid_min :315536400valid_max :1725148800<pre>array(['1980-01-01T01:00:00.000000000', '1980-01-01T02:00:00.000000000',\n       '1980-01-01T03:00:00.000000000', ..., '2024-08-31T22:00:00.000000000',\n       '2024-08-31T23:00:00.000000000', '2024-09-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (19)<ul><li>VCMX(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Maximum crest trough wave height (Hc,max)standard_name :sea_surface_wave_maximum_heightunits :mvalid_max :50.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VHM0(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Spectral significant wave height (Hm0)standard_name :sea_surface_wave_significant_heightunits :mvalid_max :20.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VHM0_SW1(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Spectral significant primary swell wave heightstandard_name :sea_surface_primary_swell_wave_significant_heightunits :mvalid_max :20.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VHM0_SW2(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Spectral significant secondary swell wave heightstandard_name :sea_surface_secondary_swell_wave_significant_heightunits :mvalid_max :20.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VHM0_WW(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Spectral significant wind wave heightstandard_name :sea_surface_wind_wave_significant_heightunits :mvalid_max :20.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VMDR(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Mean wave direction from (Mdir)standard_name :sea_surface_wave_from_directionunits :degreevalid_max :360.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VMDR_SW1(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Mean primary swell wave direction fromstandard_name :sea_surface_primary_swell_wave_from_directionunits :degreevalid_max :360.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VMDR_SW2(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Mean secondary swell wave direction fromstandard_name :sea_surface_secondary_swell_wave_from_directionunits :degreevalid_max :360.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VMDR_WW(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Mean wind wave direction fromstandard_name :sea_surface_wind_wave_from_directionunits :degreevalid_max :360.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VMXL(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Height of the highest creststandard_name :sea_surface_wave_maximum_crest_heightunits :mvalid_max :30.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VPED(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Wave principal direction at spectral peakstandard_name :sea_surface_wave_from_direction_at_variance_spectral_density_maximumunits :degreevalid_max :360.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VSDX(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Stokes drift Ustandard_name :sea_surface_wave_stokes_drift_x_velocityunits :m s-1valid_max :999.0valid_min :-999.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VSDY(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Stokes drift Vstandard_name :sea_surface_wave_stokes_drift_y_velocityunits :m s-1valid_max :999.0valid_min :-999.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VTM01_SW1(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Spectral moments (0,1) primary swell wave periodstandard_name :sea_surface_primary_swell_wave_mean_periodunits :svalid_max :25.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VTM01_SW2(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Spectral moments (0,1) secondary swell wave periodstandard_name :sea_surface_secondary_swell_wave_mean_periodunits :svalid_max :25.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VTM01_WW(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Spectral moments (0,1) wind wave periodstandard_name :sea_surface_wind_wave_mean_periodunits :svalid_max :20.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VTM02(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Spectral moments (0,2) wave period (Tm02)standard_name :sea_surface_wave_mean_period_from_variance_spectral_density_second_frequency_momentunits :svalid_max :20.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VTM10(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Spectral moments (-1,0) wave period (Tm-10)standard_name :sea_surface_wave_mean_period_from_variance_spectral_density_inverse_frequency_momentunits :svalid_max :20.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li><li>VTPK(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Wave period at spectral peak / peak period (Tp)standard_name :sea_surface_wave_period_at_variance_spectral_density_maximumunits :svalid_max :30.0valid_min :0.0<pre>[231842676000 values with dtype=float32]</pre></li></ul></li><li>Indexes: (3)<ul><li>latitudePandasIndex<pre>PandasIndex(Index([  53.0082893371582,  53.02495574951172, 53.041622161865234,\n        53.05828857421875, 53.074954986572266, 53.091617584228516,\n        53.10828399658203,  53.12495040893555,  53.14161682128906,\n        53.15828323364258,\n       ...\n        65.75778198242188,  65.77444458007812,   65.7911148071289,\n        65.80777740478516,  65.82444763183594,  65.84111022949219,\n        65.85777282714844,  65.87444305419922,  65.89110565185547,\n        65.90777587890625],\n      dtype='float32', name='latitude', length=775))</pre></li><li>longitudePandasIndex<pre>PandasIndex(Index([ 9.013887405395508,  9.041664123535156,  9.069441795349121,\n         9.09721851348877,  9.124995231628418,  9.152771949768066,\n        9.180549621582031,   9.20832633972168,  9.236103057861328,\n        9.263880729675293,\n       ...\n       29.957744598388672,  29.98552131652832,  30.01329803466797,\n        30.04107666015625,   30.0688533782959, 30.096630096435547,\n       30.124406814575195, 30.152183532714844, 30.179960250854492,\n       30.207738876342773],\n      dtype='float32', name='longitude', length=764))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1980-01-01 01:00:00', '1980-01-01 02:00:00',\n               '1980-01-01 03:00:00', '1980-01-01 04:00:00',\n               '1980-01-01 05:00:00', '1980-01-01 06:00:00',\n               '1980-01-01 07:00:00', '1980-01-01 08:00:00',\n               '1980-01-01 09:00:00', '1980-01-01 10:00:00',\n               ...\n               '2024-08-31 15:00:00', '2024-08-31 16:00:00',\n               '2024-08-31 17:00:00', '2024-08-31 18:00:00',\n               '2024-08-31 19:00:00', '2024-08-31 20:00:00',\n               '2024-08-31 21:00:00', '2024-08-31 22:00:00',\n               '2024-08-31 23:00:00', '2024-09-01 00:00:00'],\n              dtype='datetime64[ns]', name='time', length=391560, freq=None))</pre></li></ul></li><li>Attributes: (10)Conventions :CF-1.0cmems_product_id :BALTICSEA_MULTIYEAR_WAV_003_015easternmost_longitude :30.2080grid_resolution :1 nautical mile (ie. 0.0167 degrees northward; 0.0278 degrees eastwardinstitution :Baltic MFC, PU Finnish Meteorological Institutenorthernmost_latitude :65.9081source :FMI-WAM_CMEMSsouthernmost_latitude :53.0083title :2D - Hourly Instantaneouswesternmost_longitude :9.0138</li></ul> In\u00a0[12]: Copied! <pre>ds.VHM0\n</pre> ds.VHM0 Out[12]: <pre>&lt;xarray.DataArray 'VHM0' (time: 391560, latitude: 775, longitude: 764)&gt; Size: 927GB\n[231842676000 values with dtype=float32]\nCoordinates:\n  * latitude   (latitude) float32 3kB 53.01 53.02 53.04 ... 65.87 65.89 65.91\n  * longitude  (longitude) float32 3kB 9.014 9.042 9.069 ... 30.15 30.18 30.21\n  * time       (time) datetime64[ns] 3MB 1980-01-01T01:00:00 ... 2024-09-01\nAttributes:\n    cell_methods:   time: mean\n    long_name:      Spectral significant wave height (Hm0)\n    standard_name:  sea_surface_wave_significant_height\n    units:          m\n    valid_max:      20.0\n    valid_min:      0.0</pre>xarray.DataArray'VHM0'<ul><li>time: 391560</li><li>latitude: 775</li><li>longitude: 764</li></ul><ul><li>...<pre>[231842676000 values with dtype=float32]</pre></li><li>Coordinates: (3)<ul><li>latitude(latitude)float3253.01 53.02 53.04 ... 65.89 65.91axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northvalid_min :53.00829valid_max :65.907776<pre>array([53.00829 , 53.024956, 53.041622, ..., 65.87444 , 65.891106, 65.907776],\n      dtype=float32)</pre></li><li>longitude(longitude)float329.014 9.042 9.069 ... 30.18 30.21axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_min :9.013887valid_max :30.207739<pre>array([ 9.013887,  9.041664,  9.069442, ..., 30.152184, 30.17996 , 30.207739],\n      dtype=float32)</pre></li><li>time(time)datetime64[ns]1980-01-01T01:00:00 ... 2024-09-01valid_min :315536400valid_max :1725148800<pre>array(['1980-01-01T01:00:00.000000000', '1980-01-01T02:00:00.000000000',\n       '1980-01-01T03:00:00.000000000', ..., '2024-08-31T22:00:00.000000000',\n       '2024-08-31T23:00:00.000000000', '2024-09-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (3)<ul><li>latitudePandasIndex<pre>PandasIndex(Index([  53.0082893371582,  53.02495574951172, 53.041622161865234,\n        53.05828857421875, 53.074954986572266, 53.091617584228516,\n        53.10828399658203,  53.12495040893555,  53.14161682128906,\n        53.15828323364258,\n       ...\n        65.75778198242188,  65.77444458007812,   65.7911148071289,\n        65.80777740478516,  65.82444763183594,  65.84111022949219,\n        65.85777282714844,  65.87444305419922,  65.89110565185547,\n        65.90777587890625],\n      dtype='float32', name='latitude', length=775))</pre></li><li>longitudePandasIndex<pre>PandasIndex(Index([ 9.013887405395508,  9.041664123535156,  9.069441795349121,\n         9.09721851348877,  9.124995231628418,  9.152771949768066,\n        9.180549621582031,   9.20832633972168,  9.236103057861328,\n        9.263880729675293,\n       ...\n       29.957744598388672,  29.98552131652832,  30.01329803466797,\n        30.04107666015625,   30.0688533782959, 30.096630096435547,\n       30.124406814575195, 30.152183532714844, 30.179960250854492,\n       30.207738876342773],\n      dtype='float32', name='longitude', length=764))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1980-01-01 01:00:00', '1980-01-01 02:00:00',\n               '1980-01-01 03:00:00', '1980-01-01 04:00:00',\n               '1980-01-01 05:00:00', '1980-01-01 06:00:00',\n               '1980-01-01 07:00:00', '1980-01-01 08:00:00',\n               '1980-01-01 09:00:00', '1980-01-01 10:00:00',\n               ...\n               '2024-08-31 15:00:00', '2024-08-31 16:00:00',\n               '2024-08-31 17:00:00', '2024-08-31 18:00:00',\n               '2024-08-31 19:00:00', '2024-08-31 20:00:00',\n               '2024-08-31 21:00:00', '2024-08-31 22:00:00',\n               '2024-08-31 23:00:00', '2024-09-01 00:00:00'],\n              dtype='datetime64[ns]', name='time', length=391560, freq=None))</pre></li></ul></li><li>Attributes: (6)cell_methods :time: meanlong_name :Spectral significant wave height (Hm0)standard_name :sea_surface_wave_significant_heightunits :mvalid_max :20.0valid_min :0.0</li></ul> In\u00a0[13]: Copied! <pre>ds.VHM0.isel(time=0).plot.imshow()\n</pre> ds.VHM0.isel(time=0).plot.imshow() Out[13]: <pre>&lt;matplotlib.image.AxesImage at 0x7fdd86794410&gt;</pre> In\u00a0[14]: Copied! <pre>cmems_store.get_open_data_params_schema(\n    \"cmems_mod_bal_wav_my_PT1H-i\"\n)\n</pre> cmems_store.get_open_data_params_schema(     \"cmems_mod_bal_wav_my_PT1H-i\" ) Out[14]: <pre>&lt;xcube.util.jsonschema.JsonObjectSchema at 0x7fdd8d94da50&gt;</pre> In\u00a0[15]: Copied! <pre>ds = cmems_store.open_data(\n    \"cmems_mod_bal_wav_my_PT1H-i\",\n    variable_names=[\"VHM0\"],\n    time_range=(\"2021-11-01\", \"2021-12-13\"),\n    bbox=[10, 54, 20, 62]\n)\nds\n</pre> ds = cmems_store.open_data(     \"cmems_mod_bal_wav_my_PT1H-i\",     variable_names=[\"VHM0\"],     time_range=(\"2021-11-01\", \"2021-12-13\"),     bbox=[10, 54, 20, 62] ) ds <pre>Fetching catalog: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:07&lt;00:00,  2.34s/it]\nINFO - 2024-12-10T14:30:13Z - Dataset version was not specified, the latest one was selected: \"202411\"\nINFO - 2024-12-10T14:30:13Z - Dataset part was not specified, the first one was selected: \"default\"\nINFO - 2024-12-10T14:30:15Z - Service was not specified, the default one was selected: \"arco-geo-series\"\n</pre> Out[15]: <pre>&lt;xarray.Dataset&gt; Size: 697MB\nDimensions:    (time: 1009, latitude: 480, longitude: 360)\nCoordinates:\n  * latitude   (latitude) float32 2kB 54.01 54.02 54.04 ... 61.96 61.97 61.99\n  * longitude  (longitude) float32 1kB 10.01 10.04 10.07 ... 19.93 19.96 19.99\n  * time       (time) datetime64[ns] 8kB 2021-11-01 ... 2021-12-13\nData variables:\n    VHM0       (time, latitude, longitude) float32 697MB ...\nAttributes:\n    Conventions:            CF-1.0\n    cmems_product_id:       BALTICSEA_MULTIYEAR_WAV_003_015\n    easternmost_longitude:  30.2080\n    grid_resolution:        1 nautical mile (ie. 0.0167 degrees northward; 0....\n    institution:            Baltic MFC, PU Finnish Meteorological Institute\n    northernmost_latitude:  65.9081\n    source:                 FMI-WAM_CMEMS\n    southernmost_latitude:  53.0083\n    title:                  2D - Hourly Instantaneous\n    westernmost_longitude:  9.0138</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 1009</li><li>latitude: 480</li><li>longitude: 360</li></ul></li><li>Coordinates: (3)<ul><li>latitude(latitude)float3254.01 54.02 54.04 ... 61.97 61.99axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northvalid_min :54.00825valid_max :61.991264<pre>array([54.00825 , 54.024914, 54.04158 , ..., 61.95793 , 61.974598, 61.991264],\n      dtype=float32)</pre></li><li>longitude(longitude)float3210.01 10.04 10.07 ... 19.96 19.99axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_min :10.01386valid_max :19.985802<pre>array([10.01386 , 10.041636, 10.069413, ..., 19.930248, 19.958025, 19.985802],\n      dtype=float32)</pre></li><li>time(time)datetime64[ns]2021-11-01 ... 2021-12-13valid_min :1635724800valid_max :1639353600<pre>array(['2021-11-01T00:00:00.000000000', '2021-11-01T01:00:00.000000000',\n       '2021-11-01T02:00:00.000000000', ..., '2021-12-12T22:00:00.000000000',\n       '2021-12-12T23:00:00.000000000', '2021-12-13T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>VHM0(time, latitude, longitude)float32...cell_methods :time: meanlong_name :Spectral significant wave height (Hm0)standard_name :sea_surface_wave_significant_heightunits :mvalid_max :20.0valid_min :0.0<pre>[174355200 values with dtype=float32]</pre></li></ul></li><li>Indexes: (3)<ul><li>latitudePandasIndex<pre>PandasIndex(Index([ 54.00825119018555,   54.0249137878418,  54.04158020019531,\n        54.05824661254883, 54.074913024902344,  54.09157943725586,\n       54.108245849609375,  54.12491226196289, 54.141578674316406,\n        54.15824508666992,\n       ...\n       61.841270446777344,  61.85793685913086, 61.874603271484375,\n       61.891265869140625,  61.90793228149414, 61.924598693847656,\n        61.94126510620117,  61.95793151855469,   61.9745979309082,\n        61.99126434326172],\n      dtype='float32', name='latitude', length=480))</pre></li><li>longitudePandasIndex<pre>PandasIndex(Index([10.013859748840332,  10.04163646697998, 10.069413185119629,\n       10.097190856933594, 10.124967575073242,  10.15274429321289,\n       10.180521011352539, 10.208298683166504, 10.236075401306152,\n         10.2638521194458,\n       ...\n       19.735809326171875, 19.763586044311523, 19.791362762451172,\n        19.81913948059082,   19.8469181060791,  19.87469482421875,\n         19.9024715423584, 19.930248260498047, 19.958024978637695,\n       19.985801696777344],\n      dtype='float32', name='longitude', length=360))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2021-11-01 00:00:00', '2021-11-01 01:00:00',\n               '2021-11-01 02:00:00', '2021-11-01 03:00:00',\n               '2021-11-01 04:00:00', '2021-11-01 05:00:00',\n               '2021-11-01 06:00:00', '2021-11-01 07:00:00',\n               '2021-11-01 08:00:00', '2021-11-01 09:00:00',\n               ...\n               '2021-12-12 15:00:00', '2021-12-12 16:00:00',\n               '2021-12-12 17:00:00', '2021-12-12 18:00:00',\n               '2021-12-12 19:00:00', '2021-12-12 20:00:00',\n               '2021-12-12 21:00:00', '2021-12-12 22:00:00',\n               '2021-12-12 23:00:00', '2021-12-13 00:00:00'],\n              dtype='datetime64[ns]', name='time', length=1009, freq=None))</pre></li></ul></li><li>Attributes: (10)Conventions :CF-1.0cmems_product_id :BALTICSEA_MULTIYEAR_WAV_003_015easternmost_longitude :30.2080grid_resolution :1 nautical mile (ie. 0.0167 degrees northward; 0.0278 degrees eastwardinstitution :Baltic MFC, PU Finnish Meteorological Institutenorthernmost_latitude :65.9081source :FMI-WAM_CMEMSsouthernmost_latitude :53.0083title :2D - Hourly Instantaneouswesternmost_longitude :9.0138</li></ul> In\u00a0[16]: Copied! <pre>ds.VHM0.sel(time=\"2021-12-13T12:12.00\", method=\"nearest\").plot.imshow()\n</pre> ds.VHM0.sel(time=\"2021-12-13T12:12.00\", method=\"nearest\").plot.imshow() Out[16]: <pre>&lt;matplotlib.image.AxesImage at 0x7fdd8fa52d50&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/Generate_CMEMS_cubes/#generate-cmems-data-cubes","title":"Generate CMEMS data cubes\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_CMEMS_cubes/#a-deepesdl-example-notebook","title":"A DeepESDL example notebook\u00b6","text":"<p>This notebook demonstrates how to access CMEMS data via the dedicated xcube store, which provides dynamic data cube views into each gridded data set. More information on the data sets offered can be found in the Copernicus Marine Data Store.</p> <p>Please, also refer to the DeepESDL documentation and visit the platform's website for further information!</p> <p>Brockmann Consult, 2024</p> <p>This notebook runs with the python environment <code>deepesdl-xcube-1.7.1</code>, please checkout the documentation for help on changing the environment.</p>"},{"location":"guide/jupyterlab/notebooks/Generate_CMEMS_cubes/#import-the-xcube-cmems-store-and-create-store-instance","title":"Import the xcube CMEMS store and create store instance\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_CMEMS_cubes/#get-all-dataset_ids-for-cmems-api-and-let-store-describe-data-set","title":"Get all dataset_ids for CMEMS API and let store describe data set\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_CMEMS_cubes/#open-a-dataset-without-cube-parameters","title":"Open a dataset without cube parameters\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_CMEMS_cubes/#open-a-dataset-with-cube-parameters","title":"Open a dataset with cube parameters\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_SentinelHub_cubes/","title":"Generate SentinelHub cubes","text":"<p>This notebook provides a walk-through demonstrating how to use xcube and the xcube Sentinel Hub (SH) plugin to read and explore data from the Sentinel Hub cloud API.</p> <p>Please note: In order to access data from Sentinel Hub, you need Sentinel Hub API credentials. They may be passed as store parameters (see further below) or exported from environment variables. In case you have not exported them already, you may also set them by uncommenting the cell below and adjusting the content to your access credentials. However, we do not recommend this method!</p> <p>Within DeepESDL there is the possiblity to apply for SentinelHub access - please contact the DeepESDL team :) Brockmann Consult, 2024</p> <p>This notebook runs with the python environment <code>deepesdl-xcube-1.7.0</code>, please checkout the documentation for help on changing the environment.</p> <p>This notebook  gives an introduction to the xcube-sentinelhub store and additionally shows how to access different data sets through the Sentinel Hub API via xcube:</p> <ol> <li>Access Sentinel-1 through xcube Sentinel hub Store</li> <li>Access Sentinel-2 through xcube Sentinel hub Store</li> <li>Access Sentinel-5P through xcube Sentinel hub Store</li> <li>Access Sentinel-3 SLSTR through xcube Sentinel hub Store</li> <li>Access Sentinel-3 OLCI through xcube Sentinel hub Store</li> </ol> In\u00a0[1]: Copied! <pre># import os\n# os.environ[\"SH_CLIENT_ID\"] =  your_sh_client_id\n# os.environ[\"SH_CLIENT_SECRET\"] = your_sh_client_secret\n</pre> # import os # os.environ[\"SH_CLIENT_ID\"] =  your_sh_client_id # os.environ[\"SH_CLIENT_SECRET\"] = your_sh_client_secret In\u00a0[2]: Copied! <pre># mandatory imports\nfrom xcube.core.store import find_data_store_extensions\nfrom xcube.core.store import get_data_store_params_schema\nfrom xcube.core.store import new_data_store\n\n# Utilities for notebook visualization\nimport shapely.geometry\nimport IPython.display\nfrom IPython.display import JSON\nimport matplotlib.pyplot as plt\n</pre> # mandatory imports from xcube.core.store import find_data_store_extensions from xcube.core.store import get_data_store_params_schema from xcube.core.store import new_data_store  # Utilities for notebook visualization import shapely.geometry import IPython.display from IPython.display import JSON import matplotlib.pyplot as plt <p>Configure matplotlib to display graphs inline directly in the notebook and set a sensible default figure size.</p> In\u00a0[3]: Copied! <pre>%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = 16,12\n</pre> %matplotlib inline plt.rcParams[\"figure.figsize\"] = 16,12 <p>Check whether the <code>sentinelhub</code> store is among the available stores, if not you need to choose an environment where xcube-sh is included.</p> In\u00a0[4]: Copied! <pre>JSON({e.name: e.metadata for e in find_data_store_extensions()})\n</pre> JSON({e.name: e.metadata for e in find_data_store_extensions()}) Out[4]: <pre>&lt;IPython.core.display.JSON object&gt;</pre> <p>Usually we need more information to get the actual data store object. Which data store parameters are available for the <code>sentinelhub</code> store?</p> In\u00a0[5]: Copied! <pre>get_data_store_params_schema('sentinelhub')\n</pre> get_data_store_params_schema('sentinelhub') Out[5]: <pre>&lt;xcube.util.jsonschema.JsonObjectSchema at 0x7efe5ef65810&gt;</pre> <p>Please note the <code>client_id</code> and <code>client_secret</code> parameters mentioned at the beginning.</p> <p>Provide mandatory parameters to instantiate the store class:</p> In\u00a0[6]: Copied! <pre>store = new_data_store('sentinelhub', num_retries=400)\n</pre> store = new_data_store('sentinelhub', num_retries=400) <p>Which datasets are provided? (the list may contain both gridded and vector datasets):</p> In\u00a0[7]: Copied! <pre>list(store.get_data_ids())\n</pre> list(store.get_data_ids()) Out[7]: <pre>['S2L1C', 'S1GRD', 'S2L2A', 'DEM']</pre> <p>Get more info about a specific dataset. This includes a description of the possible open formats:</p> In\u00a0[8]: Copied! <pre>store.describe_data('S2L2A')\n</pre> store.describe_data('S2L2A') Out[8]: <pre>&lt;xcube.core.store.descriptor.DatasetDescriptor at 0x7f2fff2b8590&gt;</pre> <p>Which parameters must be passsed or are available to open the dataset?</p> In\u00a0[9]: Copied! <pre>store.get_open_data_params_schema('S2L2A')\n</pre> store.get_open_data_params_schema('S2L2A') Out[9]: <pre>&lt;xcube.util.jsonschema.JsonObjectSchema at 0x7f2fff2e6c90&gt;</pre> <p>There are 3 required parameters, so we need to provide them to open a dataset, one of them being bbox. Let's set a region covering Hamburg:</p> In\u00a0[10]: Copied! <pre>bbox=[9.7, 53.4, 10.2, 53.7]\n</pre> bbox=[9.7, 53.4, 10.2, 53.7] <p>Take a look at the bbox in order to make sure the area is correctly set:</p> In\u00a0[11]: Copied! <pre>IPython.display.GeoJSON(shapely.geometry.box(*bbox).__geo_interface__)\n</pre> IPython.display.GeoJSON(shapely.geometry.box(*bbox).__geo_interface__) <pre>&lt;IPython.display.GeoJSON object&gt;</pre> <p>Now set the other parameters for opening the dataset from the store:</p> In\u00a0[12]: Copied! <pre>dataset = store.open_data('S2L2A', \n                          variable_names=['B04'], \n                          bbox=bbox, \n                          spatial_res=0.00018, \n                          time_range=('2020-08-10','2020-08-20'), \n                          time_period='1D',\n                          tile_size= [1024, 1024])\ndataset\n</pre> dataset = store.open_data('S2L2A',                            variable_names=['B04'],                            bbox=bbox,                            spatial_res=0.00018,                            time_range=('2020-08-10','2020-08-20'),                            time_period='1D',                           tile_size= [1024, 1024]) dataset Out[12]: <pre>&lt;xarray.Dataset&gt; Size: 277MB\nDimensions:    (time: 11, lat: 2048, lon: 3072, bnds: 2)\nCoordinates:\n  * lat        (lat) float64 16kB 53.77 53.77 53.77 53.77 ... 53.4 53.4 53.4\n  * lon        (lon) float64 25kB 9.7 9.7 9.7 9.701 ... 10.25 10.25 10.25 10.25\n  * time       (time) datetime64[ns] 88B 2020-08-10T12:00:00 ... 2020-08-20T1...\n    time_bnds  (time, bnds) datetime64[ns] 176B dask.array&lt;chunksize=(11, 2), meta=np.ndarray&gt;\nDimensions without coordinates: bnds\nData variables:\n    B04        (time, lat, lon) float32 277MB dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\nAttributes: (12/13)\n    Conventions:               CF-1.7\n    title:                     S2L2A Data Cube Subset\n    history:                   [{'program': 'xcube_sh.chunkstore.SentinelHubC...\n    date_created:              2024-09-11T09:04:35.595067\n    time_coverage_start:       2020-08-10T00:00:00+00:00\n    time_coverage_end:         2020-08-21T00:00:00+00:00\n    ...                        ...\n    time_coverage_resolution:  P1DT0H0M0S\n    geospatial_lon_min:        9.7\n    geospatial_lat_min:        53.4\n    geospatial_lon_max:        10.25296\n    geospatial_lat_max:        53.76864\n    processing_level:          L2A</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 11</li><li>lat: 2048</li><li>lon: 3072</li><li>bnds: 2</li></ul></li><li>Coordinates: (4)<ul><li>lat(lat)float6453.77 53.77 53.77 ... 53.4 53.4units :decimal_degreeslong_name :latitudestandard_name :latitude<pre>array([53.76855, 53.76837, 53.76819, ..., 53.40045, 53.40027, 53.40009])</pre></li><li>lon(lon)float649.7 9.7 9.7 ... 10.25 10.25 10.25units :decimal_degreeslong_name :longitudestandard_name :longitude<pre>array([ 9.70009,  9.70027,  9.70045, ..., 10.25251, 10.25269, 10.25287])</pre></li><li>time(time)datetime64[ns]2020-08-10T12:00:00 ... 2020-08-...standard_name :timebounds :time_bnds<pre>array(['2020-08-10T12:00:00.000000000', '2020-08-11T12:00:00.000000000',\n       '2020-08-12T12:00:00.000000000', '2020-08-13T12:00:00.000000000',\n       '2020-08-14T12:00:00.000000000', '2020-08-15T12:00:00.000000000',\n       '2020-08-16T12:00:00.000000000', '2020-08-17T12:00:00.000000000',\n       '2020-08-18T12:00:00.000000000', '2020-08-19T12:00:00.000000000',\n       '2020-08-20T12:00:00.000000000'], dtype='datetime64[ns]')</pre></li><li>time_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(11, 2), meta=np.ndarray&gt;standard_name :time  Array   Chunk   Bytes   176 B   176 B   Shape   (11, 2)   (11, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 11 </li></ul></li><li>Data variables: (1)<ul><li>B04(time, lat, lon)float32dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;sample_type :FLOAT32units :reflectancewavelength :664.75wavelength_a :664.6wavelength_b :664.9bandwidth :31.0bandwidth_a :31bandwidth_b :31resolution :10  Array   Chunk   Bytes   264.00 MiB   4.00 MiB   Shape   (11, 2048, 3072)   (1, 1024, 1024)   Dask graph   66 chunks in 2 graph layers   Data type   float32 numpy.ndarray  3072 2048 11 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([          53.76855,           53.76837,           53.76819,\n                 53.76801,           53.76783, 53.767649999999996,\n       53.767469999999996, 53.767289999999996, 53.767109999999995,\n       53.766929999999995,\n       ...\n                 53.40171,           53.40153,           53.40135,\n                 53.40117,           53.40099,           53.40081,\n                 53.40063,           53.40045,           53.40027,\n                 53.40009],\n      dtype='float64', name='lat', length=2048))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([           9.70009,            9.70027,            9.70045,\n                  9.70063,  9.700809999999999,            9.70099,\n                  9.70117,            9.70135,            9.70153,\n                  9.70171,\n       ...\n       10.251249999999999,           10.25143,           10.25161,\n                 10.25179,           10.25197,           10.25215,\n                 10.25233,           10.25251,           10.25269,\n                 10.25287],\n      dtype='float64', name='lon', length=3072))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2020-08-10 12:00:00', '2020-08-11 12:00:00',\n               '2020-08-12 12:00:00', '2020-08-13 12:00:00',\n               '2020-08-14 12:00:00', '2020-08-15 12:00:00',\n               '2020-08-16 12:00:00', '2020-08-17 12:00:00',\n               '2020-08-18 12:00:00', '2020-08-19 12:00:00',\n               '2020-08-20 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (13)Conventions :CF-1.7title :S2L2A Data Cube Subsethistory :[{'program': 'xcube_sh.chunkstore.SentinelHubChunkStore', 'cube_config': {'dataset_name': 'S2L2A', 'band_names': ['B04'], 'band_fill_values': None, 'band_sample_types': None, 'band_units': None, 'tile_size': [1024, 1024], 'bbox': [9.7, 53.4, 10.25296, 53.76864], 'spatial_res': 0.00018, 'crs': 'WGS84', 'upsampling': 'NEAREST', 'downsampling': 'NEAREST', 'mosaicking_order': 'mostRecent', 'time_range': ['2020-08-10T00:00:00+00:00', '2020-08-20T00:00:00+00:00'], 'time_period': '1 days 00:00:00', 'time_tolerance': None, 'collection_id': None, 'four_d': False}}]date_created :2024-09-11T09:04:35.595067time_coverage_start :2020-08-10T00:00:00+00:00time_coverage_end :2020-08-21T00:00:00+00:00time_coverage_duration :P11DT0H0M0Stime_coverage_resolution :P1DT0H0M0Sgeospatial_lon_min :9.7geospatial_lat_min :53.4geospatial_lon_max :10.25296geospatial_lat_max :53.76864processing_level :L2A</li></ul> <p>For further information, a description about the dataset and the bands may be found here: https://docs.sentinel-hub.com/api/latest/data/sentinel-2-l2a/ .</p> <p>Plot one time stamp of the dataset for our requested variable:</p> In\u00a0[13]: Copied! <pre>dataset.B04.isel(time=2).plot.imshow(vmin=0, vmax=0.2, cmap='Greys_r')\n</pre> dataset.B04.isel(time=2).plot.imshow(vmin=0, vmax=0.2, cmap='Greys_r') Out[13]: <pre>&lt;matplotlib.image.AxesImage at 0x7f2ffc379c50&gt;</pre> <p>In case you wonder why we chose Hamburg - well, it is a lovely city!</p> <p>Setting of AOI bounding box</p> In\u00a0[12]: Copied! <pre>x1 = 10.00  # degree\ny1 = 54.27  # degree\nx2 = 11.00  # degree\ny2 = 54.60  # degree\n\nbbox = x1, y1, x2, y2\n</pre> x1 = 10.00  # degree y1 = 54.27  # degree x2 = 11.00  # degree y2 = 54.60  # degree  bbox = x1, y1, x2, y2 In\u00a0[13]: Copied! <pre>spatial_res = 0.00018   # = 20.038 meters in degree\n</pre> spatial_res = 0.00018   # = 20.038 meters in degree <p>Sentinel Hub currently supported Sentinel-1 GRD (Ground Range Detected) products: here</p> In\u00a0[16]: Copied! <pre>store = new_data_store('sentinelhub', num_retries=400)\n</pre> store = new_data_store('sentinelhub', num_retries=400) In\u00a0[17]: Copied! <pre>list(store.get_data_ids())\n</pre> list(store.get_data_ids()) Out[17]: <pre>['S2L1C', 'S1GRD', 'S2L2A', 'DEM']</pre> In\u00a0[18]: Copied! <pre>store.describe_data('S1GRD')\n</pre> store.describe_data('S1GRD') Out[18]: <pre>&lt;xcube.core.store.descriptor.DatasetDescriptor at 0x7f2ffc46ac90&gt;</pre> In\u00a0[19]: Copied! <pre>cube = store.open_data(\n    'S1GRD',\n    variable_names=['VH'],\n    tile_size=[512, 512],\n    crs = \"EPSG:4326\",\n    spatial_res = spatial_res,\n    bbox=bbox,\n    time_range=['2019-06-14', '2019-07-31'],\n    time_period='2D'\n)\ncube\n</pre> cube = store.open_data(     'S1GRD',     variable_names=['VH'],     tile_size=[512, 512],     crs = \"EPSG:4326\",     spatial_res = spatial_res,     bbox=bbox,     time_range=['2019-06-14', '2019-07-31'],     time_period='2D' ) cube Out[19]: <pre>&lt;xarray.Dataset&gt; Size: 1GB\nDimensions:    (time: 24, lat: 2048, lon: 5632, bnds: 2)\nCoordinates:\n  * lat        (lat) float64 16kB 54.64 54.64 54.64 54.64 ... 54.27 54.27 54.27\n  * lon        (lon) float64 45kB 10.0 10.0 10.0 10.0 ... 11.01 11.01 11.01\n  * time       (time) datetime64[ns] 192B 2019-06-15 2019-06-17 ... 2019-07-31\n    time_bnds  (time, bnds) datetime64[ns] 384B dask.array&lt;chunksize=(24, 2), meta=np.ndarray&gt;\nDimensions without coordinates: bnds\nData variables:\n    VH         (time, lat, lon) float32 1GB dask.array&lt;chunksize=(1, 512, 512), meta=np.ndarray&gt;\nAttributes: (12/13)\n    Conventions:               CF-1.7\n    title:                     S1GRD Data Cube Subset\n    history:                   [{'program': 'xcube_sh.chunkstore.SentinelHubC...\n    date_created:              2024-09-11T09:04:38.814801\n    time_coverage_start:       2019-06-14T00:00:00+00:00\n    time_coverage_end:         2019-08-01T00:00:00+00:00\n    ...                        ...\n    time_coverage_resolution:  P2DT0H0M0S\n    geospatial_lon_min:        10.0\n    geospatial_lat_min:        54.27\n    geospatial_lon_max:        11.01376\n    geospatial_lat_max:        54.63864\n    processing_level:          L1B</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 24</li><li>lat: 2048</li><li>lon: 5632</li><li>bnds: 2</li></ul></li><li>Coordinates: (4)<ul><li>lat(lat)float6454.64 54.64 54.64 ... 54.27 54.27units :decimal_degreeslong_name :latitudestandard_name :latitude<pre>array([54.63855, 54.63837, 54.63819, ..., 54.27045, 54.27027, 54.27009])</pre></li><li>lon(lon)float6410.0 10.0 10.0 ... 11.01 11.01units :decimal_degreeslong_name :longitudestandard_name :longitude<pre>array([10.00009, 10.00027, 10.00045, ..., 11.01331, 11.01349, 11.01367])</pre></li><li>time(time)datetime64[ns]2019-06-15 ... 2019-07-31standard_name :timebounds :time_bnds<pre>array(['2019-06-15T00:00:00.000000000', '2019-06-17T00:00:00.000000000',\n       '2019-06-19T00:00:00.000000000', '2019-06-21T00:00:00.000000000',\n       '2019-06-23T00:00:00.000000000', '2019-06-25T00:00:00.000000000',\n       '2019-06-27T00:00:00.000000000', '2019-06-29T00:00:00.000000000',\n       '2019-07-01T00:00:00.000000000', '2019-07-03T00:00:00.000000000',\n       '2019-07-05T00:00:00.000000000', '2019-07-07T00:00:00.000000000',\n       '2019-07-09T00:00:00.000000000', '2019-07-11T00:00:00.000000000',\n       '2019-07-13T00:00:00.000000000', '2019-07-15T00:00:00.000000000',\n       '2019-07-17T00:00:00.000000000', '2019-07-19T00:00:00.000000000',\n       '2019-07-21T00:00:00.000000000', '2019-07-23T00:00:00.000000000',\n       '2019-07-25T00:00:00.000000000', '2019-07-27T00:00:00.000000000',\n       '2019-07-29T00:00:00.000000000', '2019-07-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>time_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(24, 2), meta=np.ndarray&gt;standard_name :time  Array   Chunk   Bytes   384 B   384 B   Shape   (24, 2)   (24, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 24 </li></ul></li><li>Data variables: (1)<ul><li>VH(time, lat, lon)float32dask.array&lt;chunksize=(1, 512, 512), meta=np.ndarray&gt;sample_type :FLOAT32units :Linear power in the chosen backscattering coefficient  Array   Chunk   Bytes   1.03 GiB   1.00 MiB   Shape   (24, 2048, 5632)   (1, 512, 512)   Dask graph   1056 chunks in 2 graph layers   Data type   float32 numpy.ndarray  5632 2048 24 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([          54.63855,           54.63837,           54.63819,\n                 54.63801,           54.63783,           54.63765,\n                 54.63747,           54.63729,           54.63711,\n                 54.63693,\n       ...\n       54.271710000000006, 54.271530000000006, 54.271350000000005,\n       54.271170000000005, 54.270990000000005, 54.270810000000004,\n       54.270630000000004, 54.270450000000004, 54.270270000000004,\n                 54.27009],\n      dtype='float64', name='lat', length=2048))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([          10.00009,           10.00027,           10.00045,\n       10.000630000000001,           10.00081,           10.00099,\n                 10.00117,           10.00135,           10.00153,\n       10.001710000000001,\n       ...\n       11.012049999999999, 11.012229999999999,           11.01241,\n                 11.01259,           11.01277,           11.01295,\n                 11.01313, 11.013309999999999,           11.01349,\n                 11.01367],\n      dtype='float64', name='lon', length=5632))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2019-06-15', '2019-06-17', '2019-06-19', '2019-06-21',\n               '2019-06-23', '2019-06-25', '2019-06-27', '2019-06-29',\n               '2019-07-01', '2019-07-03', '2019-07-05', '2019-07-07',\n               '2019-07-09', '2019-07-11', '2019-07-13', '2019-07-15',\n               '2019-07-17', '2019-07-19', '2019-07-21', '2019-07-23',\n               '2019-07-25', '2019-07-27', '2019-07-29', '2019-07-31'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (13)Conventions :CF-1.7title :S1GRD Data Cube Subsethistory :[{'program': 'xcube_sh.chunkstore.SentinelHubChunkStore', 'cube_config': {'dataset_name': 'S1GRD', 'band_names': ['VH'], 'band_fill_values': None, 'band_sample_types': None, 'band_units': None, 'tile_size': [512, 512], 'bbox': [10.0, 54.27, 11.01376, 54.63864], 'spatial_res': 0.00018, 'crs': 'EPSG:4326', 'upsampling': 'NEAREST', 'downsampling': 'NEAREST', 'mosaicking_order': 'mostRecent', 'time_range': ['2019-06-14T00:00:00+00:00', '2019-07-31T00:00:00+00:00'], 'time_period': '2 days 00:00:00', 'time_tolerance': None, 'collection_id': None, 'four_d': False}}]date_created :2024-09-11T09:04:38.814801time_coverage_start :2019-06-14T00:00:00+00:00time_coverage_end :2019-08-01T00:00:00+00:00time_coverage_duration :P48DT0H0M0Stime_coverage_resolution :P2DT0H0M0Sgeospatial_lon_min :10.0geospatial_lat_min :54.27geospatial_lon_max :11.01376geospatial_lat_max :54.63864processing_level :L1B</li></ul> In\u00a0[20]: Copied! <pre>cube.VH.isel(time=1).plot.imshow(cmap='Greys',vmax =0.08, figsize = [16,12])\n</pre> cube.VH.isel(time=1).plot.imshow(cmap='Greys',vmax =0.08, figsize = [16,12]) Out[20]: <pre>&lt;matplotlib.image.AxesImage at 0x7f2ffe7d5550&gt;</pre> <p>Sentinel Hub currently supported Sentinel-2 products: here</p> In\u00a0[21]: Copied! <pre>store.describe_data('S2L2A')\n</pre> store.describe_data('S2L2A') Out[21]: <pre>&lt;xcube.core.store.descriptor.DatasetDescriptor at 0x7f2ffe69af50&gt;</pre> In\u00a0[22]: Copied! <pre>cube = store.open_data(\n    'S2L2A',\n    variable_names=['SCL'],\n    tile_size=[512, 512],\n    spatial_res = spatial_res,\n    bbox=bbox,\n    time_range=['2018-06-14', '2018-07-31'],\n    time_tolerance='30m'\n)\ncube\n</pre> cube = store.open_data(     'S2L2A',     variable_names=['SCL'],     tile_size=[512, 512],     spatial_res = spatial_res,     bbox=bbox,     time_range=['2018-06-14', '2018-07-31'],     time_tolerance='30m' ) cube <pre>/home/conda/deepesdl/6190a858fed144ca6ad331592f377c151465248da37d97c12b14115c866dd7b3-20240902-150843-312777-573-xcube-1.7.0/lib/python3.11/site-packages/xcube_sh/sentinelhub.py:254: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n  dt = pd.to_datetime(dt, infer_datetime_format=True, utc=True)\n/home/conda/deepesdl/6190a858fed144ca6ad331592f377c151465248da37d97c12b14115c866dd7b3-20240902-150843-312777-573-xcube-1.7.0/lib/python3.11/site-packages/xcube_sh/sentinelhub.py:317: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n  pd.to_timedelta(max_timedelta)\n</pre> Out[22]: <pre>&lt;xarray.Dataset&gt; Size: 323MB\nDimensions:    (time: 28, lat: 2048, lon: 5632, bnds: 2)\nCoordinates:\n  * lat        (lat) float64 16kB 54.64 54.64 54.64 54.64 ... 54.27 54.27 54.27\n  * lon        (lon) float64 45kB 10.0 10.0 10.0 10.0 ... 11.01 11.01 11.01\n  * time       (time) datetime64[ns] 224B 2018-06-14T10:30:21 ... 2018-07-29T...\n    time_bnds  (time, bnds) datetime64[ns] 448B dask.array&lt;chunksize=(28, 2), meta=np.ndarray&gt;\nDimensions without coordinates: bnds\nData variables:\n    SCL        (time, lat, lon) uint8 323MB dask.array&lt;chunksize=(1, 512, 512), meta=np.ndarray&gt;\nAttributes:\n    Conventions:             CF-1.7\n    title:                   S2L2A Data Cube Subset\n    history:                 [{'program': 'xcube_sh.chunkstore.SentinelHubChu...\n    date_created:            2024-09-11T09:04:44.682746\n    time_coverage_start:     2018-06-14T10:30:21+00:00\n    time_coverage_end:       2018-07-29T10:30:19+00:00\n    time_coverage_duration:  P44DT23H59M58S\n    geospatial_lon_min:      10.0\n    geospatial_lat_min:      54.27\n    geospatial_lon_max:      11.01376\n    geospatial_lat_max:      54.63864\n    processing_level:        L2A</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 28</li><li>lat: 2048</li><li>lon: 5632</li><li>bnds: 2</li></ul></li><li>Coordinates: (4)<ul><li>lat(lat)float6454.64 54.64 54.64 ... 54.27 54.27units :decimal_degreeslong_name :latitudestandard_name :latitude<pre>array([54.63855, 54.63837, 54.63819, ..., 54.27045, 54.27027, 54.27009])</pre></li><li>lon(lon)float6410.0 10.0 10.0 ... 11.01 11.01units :decimal_degreeslong_name :longitudestandard_name :longitude<pre>array([10.00009, 10.00027, 10.00045, ..., 11.01331, 11.01349, 11.01367])</pre></li><li>time(time)datetime64[ns]2018-06-14T10:30:21 ... 2018-07-...standard_name :timebounds :time_bnds<pre>array(['2018-06-14T10:30:21.000000000', '2018-06-16T10:20:21.000000000',\n       '2018-06-17T10:43:21.000000000', '2018-06-19T10:30:20.000000000',\n       '2018-06-21T10:23:16.000000000', '2018-06-22T10:40:21.000000000',\n       '2018-06-24T10:35:03.000000000', '2018-06-26T10:20:22.000000000',\n       '2018-06-27T10:40:23.000000000', '2018-06-29T10:30:21.000000000',\n       '2018-07-01T10:24:04.000000000', '2018-07-02T10:40:21.000000000',\n       '2018-07-04T10:30:23.000000000', '2018-07-06T10:20:22.000000000',\n       '2018-07-07T10:45:15.000000000', '2018-07-09T10:34:18.000000000',\n       '2018-07-11T10:20:24.000000000', '2018-07-12T10:40:20.000000000',\n       '2018-07-14T10:30:23.000000000', '2018-07-16T10:20:21.000000000',\n       '2018-07-17T10:42:58.000000000', '2018-07-19T10:30:20.000000000',\n       '2018-07-21T10:20:24.000000000', '2018-07-22T10:40:20.000000000',\n       '2018-07-24T10:30:23.000000000', '2018-07-26T10:21:50.000000000',\n       '2018-07-27T10:40:23.000000000', '2018-07-29T10:30:19.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>time_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(28, 2), meta=np.ndarray&gt;standard_name :time  Array   Chunk   Bytes   448 B   448 B   Shape   (28, 2)   (28, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 28 </li></ul></li><li>Data variables: (1)<ul><li>SCL(time, lat, lon)uint8dask.array&lt;chunksize=(1, 512, 512), meta=np.ndarray&gt;sample_type :UINT8flag_values :0,1,2,3,4,5,6,7,8,9,10,11flag_meanings :no_data saturated_or_defective dark_area_pixels cloud_shadows vegetation bare_soils water clouds_low_probability_or_unclassified clouds_medium_probability clouds_high_probability cirrus snow_or_ice  Array   Chunk   Bytes   308.00 MiB   256.00 kiB   Shape   (28, 2048, 5632)   (1, 512, 512)   Dask graph   1232 chunks in 2 graph layers   Data type   uint8 numpy.ndarray  5632 2048 28 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([          54.63855,           54.63837,           54.63819,\n                 54.63801,           54.63783,           54.63765,\n                 54.63747,           54.63729,           54.63711,\n                 54.63693,\n       ...\n       54.271710000000006, 54.271530000000006, 54.271350000000005,\n       54.271170000000005, 54.270990000000005, 54.270810000000004,\n       54.270630000000004, 54.270450000000004, 54.270270000000004,\n                 54.27009],\n      dtype='float64', name='lat', length=2048))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([          10.00009,           10.00027,           10.00045,\n       10.000630000000001,           10.00081,           10.00099,\n                 10.00117,           10.00135,           10.00153,\n       10.001710000000001,\n       ...\n       11.012049999999999, 11.012229999999999,           11.01241,\n                 11.01259,           11.01277,           11.01295,\n                 11.01313, 11.013309999999999,           11.01349,\n                 11.01367],\n      dtype='float64', name='lon', length=5632))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2018-06-14 10:30:21', '2018-06-16 10:20:21',\n               '2018-06-17 10:43:21', '2018-06-19 10:30:20',\n               '2018-06-21 10:23:16', '2018-06-22 10:40:21',\n               '2018-06-24 10:35:03', '2018-06-26 10:20:22',\n               '2018-06-27 10:40:23', '2018-06-29 10:30:21',\n               '2018-07-01 10:24:04', '2018-07-02 10:40:21',\n               '2018-07-04 10:30:23', '2018-07-06 10:20:22',\n               '2018-07-07 10:45:15', '2018-07-09 10:34:18',\n               '2018-07-11 10:20:24', '2018-07-12 10:40:20',\n               '2018-07-14 10:30:23', '2018-07-16 10:20:21',\n               '2018-07-17 10:42:58', '2018-07-19 10:30:20',\n               '2018-07-21 10:20:24', '2018-07-22 10:40:20',\n               '2018-07-24 10:30:23', '2018-07-26 10:21:50',\n               '2018-07-27 10:40:23', '2018-07-29 10:30:19'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (12)Conventions :CF-1.7title :S2L2A Data Cube Subsethistory :[{'program': 'xcube_sh.chunkstore.SentinelHubChunkStore', 'cube_config': {'dataset_name': 'S2L2A', 'band_names': ['SCL'], 'band_fill_values': None, 'band_sample_types': None, 'band_units': None, 'tile_size': [512, 512], 'bbox': [10.0, 54.27, 11.01376, 54.63864], 'spatial_res': 0.00018, 'crs': 'WGS84', 'upsampling': 'NEAREST', 'downsampling': 'NEAREST', 'mosaicking_order': 'mostRecent', 'time_range': ['2018-06-14T00:00:00+00:00', '2018-07-31T00:00:00+00:00'], 'time_period': None, 'time_tolerance': '0 days 00:30:00', 'collection_id': None, 'four_d': False}}]date_created :2024-09-11T09:04:44.682746time_coverage_start :2018-06-14T10:30:21+00:00time_coverage_end :2018-07-29T10:30:19+00:00time_coverage_duration :P44DT23H59M58Sgeospatial_lon_min :10.0geospatial_lat_min :54.27geospatial_lon_max :11.01376geospatial_lat_max :54.63864processing_level :L2A</li></ul> In\u00a0[23]: Copied! <pre>cube.SCL.isel(time=8, lat=slice(0,2000),lon=slice(0,2000)).plot.imshow(cmap='tab20c')\n</pre> cube.SCL.isel(time=8, lat=slice(0,2000),lon=slice(0,2000)).plot.imshow(cmap='tab20c') Out[23]: <pre>&lt;matplotlib.image.AxesImage at 0x7f2ffe774910&gt;</pre> <p>Sentinel Hub currently supported Sentinel-5P products: here. To access Sentinel-5P or Sentinel-3 data, another api endpoint is required. You can learn about the endpoints here. <code>services.sentinel-hub.com</code> is the default.</p> In\u00a0[8]: Copied! <pre>store = new_data_store('sentinelhub', api_url='https://creodias.sentinel-hub.com', num_retries=400)\n</pre> store = new_data_store('sentinelhub', api_url='https://creodias.sentinel-hub.com', num_retries=400) In\u00a0[9]: Copied! <pre>list(store.get_data_ids())\n</pre> list(store.get_data_ids()) Out[9]: <pre>['S2L1C', 'S3OLCI', 'S3SLSTR', 'S1GRD', 'S2L2A', 'S5PL2']</pre> In\u00a0[10]: Copied! <pre>store.describe_data('S5PL2')\n</pre> store.describe_data('S5PL2') Out[10]: <pre>&lt;xcube.core.store.descriptor.DatasetDescriptor at 0x7efe5ef8b2d0&gt;</pre> In\u00a0[14]: Copied! <pre>cube = store.open_data(\n    'S5PL2',\n    variable_names=['NO2'],\n    tile_size=[512, 512],\n    bbox=bbox,\n    spatial_res=(bbox[2]-bbox[0])/512,\n    upsampling='BILINEAR',\n    time_range=['2018-06-14', '2018-07-31'],\n    time_period='3D'\n)\ncube\n</pre> cube = store.open_data(     'S5PL2',     variable_names=['NO2'],     tile_size=[512, 512],     bbox=bbox,     spatial_res=(bbox[2]-bbox[0])/512,     upsampling='BILINEAR',     time_range=['2018-06-14', '2018-07-31'],     time_period='3D' ) cube Out[14]: <pre>&lt;xarray.Dataset&gt; Size: 6MB\nDimensions:    (time: 16, lat: 169, lon: 512, bnds: 2)\nCoordinates:\n  * lat        (lat) float64 1kB 54.6 54.6 54.6 54.59 ... 54.27 54.27 54.27\n  * lon        (lon) float64 4kB 10.0 10.0 10.0 10.01 ... 10.99 11.0 11.0 11.0\n  * time       (time) datetime64[ns] 128B 2018-06-15T12:00:00 ... 2018-07-30T...\n    time_bnds  (time, bnds) datetime64[ns] 256B dask.array&lt;chunksize=(16, 2), meta=np.ndarray&gt;\nDimensions without coordinates: bnds\nData variables:\n    NO2        (time, lat, lon) float32 6MB dask.array&lt;chunksize=(1, 169, 512), meta=np.ndarray&gt;\nAttributes:\n    Conventions:               CF-1.7\n    title:                     S5PL2 Data Cube Subset\n    history:                   [{'program': 'xcube_sh.chunkstore.SentinelHubC...\n    date_created:              2024-09-18T08:58:19.348015\n    time_coverage_start:       2018-06-14T00:00:00+00:00\n    time_coverage_end:         2018-08-01T00:00:00+00:00\n    time_coverage_duration:    P48DT0H0M0S\n    time_coverage_resolution:  P3DT0H0M0S\n    geospatial_lon_min:        10.0\n    geospatial_lat_min:        54.27\n    geospatial_lon_max:        11.0\n    geospatial_lat_max:        54.600078125</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 16</li><li>lat: 169</li><li>lon: 512</li><li>bnds: 2</li></ul></li><li>Coordinates: (4)<ul><li>lat(lat)float6454.6 54.6 54.6 ... 54.27 54.27units :decimal_degreeslong_name :latitudestandard_name :latitude<pre>array([54.599102, 54.597148, 54.595195, 54.593242, 54.591289, 54.589336,\n       54.587383, 54.58543 , 54.583477, 54.581523, 54.57957 , 54.577617,\n       54.575664, 54.573711, 54.571758, 54.569805, 54.567852, 54.565898,\n       54.563945, 54.561992, 54.560039, 54.558086, 54.556133, 54.55418 ,\n       54.552227, 54.550273, 54.54832 , 54.546367, 54.544414, 54.542461,\n       54.540508, 54.538555, 54.536602, 54.534648, 54.532695, 54.530742,\n       54.528789, 54.526836, 54.524883, 54.52293 , 54.520977, 54.519023,\n       54.51707 , 54.515117, 54.513164, 54.511211, 54.509258, 54.507305,\n       54.505352, 54.503398, 54.501445, 54.499492, 54.497539, 54.495586,\n       54.493633, 54.49168 , 54.489727, 54.487773, 54.48582 , 54.483867,\n       54.481914, 54.479961, 54.478008, 54.476055, 54.474102, 54.472148,\n       54.470195, 54.468242, 54.466289, 54.464336, 54.462383, 54.46043 ,\n       54.458477, 54.456523, 54.45457 , 54.452617, 54.450664, 54.448711,\n       54.446758, 54.444805, 54.442852, 54.440898, 54.438945, 54.436992,\n       54.435039, 54.433086, 54.431133, 54.42918 , 54.427227, 54.425273,\n       54.42332 , 54.421367, 54.419414, 54.417461, 54.415508, 54.413555,\n       54.411602, 54.409648, 54.407695, 54.405742, 54.403789, 54.401836,\n       54.399883, 54.39793 , 54.395977, 54.394023, 54.39207 , 54.390117,\n       54.388164, 54.386211, 54.384258, 54.382305, 54.380352, 54.378398,\n       54.376445, 54.374492, 54.372539, 54.370586, 54.368633, 54.36668 ,\n       54.364727, 54.362773, 54.36082 , 54.358867, 54.356914, 54.354961,\n       54.353008, 54.351055, 54.349102, 54.347148, 54.345195, 54.343242,\n       54.341289, 54.339336, 54.337383, 54.33543 , 54.333477, 54.331523,\n       54.32957 , 54.327617, 54.325664, 54.323711, 54.321758, 54.319805,\n       54.317852, 54.315898, 54.313945, 54.311992, 54.310039, 54.308086,\n       54.306133, 54.30418 , 54.302227, 54.300273, 54.29832 , 54.296367,\n       54.294414, 54.292461, 54.290508, 54.288555, 54.286602, 54.284648,\n       54.282695, 54.280742, 54.278789, 54.276836, 54.274883, 54.27293 ,\n       54.270977])</pre></li><li>lon(lon)float6410.0 10.0 10.0 ... 11.0 11.0 11.0units :decimal_degreeslong_name :longitudestandard_name :longitude<pre>array([10.000977, 10.00293 , 10.004883, ..., 10.995117, 10.99707 , 10.999023])</pre></li><li>time(time)datetime64[ns]2018-06-15T12:00:00 ... 2018-07-...standard_name :timebounds :time_bnds<pre>array(['2018-06-15T12:00:00.000000000', '2018-06-18T12:00:00.000000000',\n       '2018-06-21T12:00:00.000000000', '2018-06-24T12:00:00.000000000',\n       '2018-06-27T12:00:00.000000000', '2018-06-30T12:00:00.000000000',\n       '2018-07-03T12:00:00.000000000', '2018-07-06T12:00:00.000000000',\n       '2018-07-09T12:00:00.000000000', '2018-07-12T12:00:00.000000000',\n       '2018-07-15T12:00:00.000000000', '2018-07-18T12:00:00.000000000',\n       '2018-07-21T12:00:00.000000000', '2018-07-24T12:00:00.000000000',\n       '2018-07-27T12:00:00.000000000', '2018-07-30T12:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>time_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(16, 2), meta=np.ndarray&gt;standard_name :time  Array   Chunk   Bytes   256 B   256 B   Shape   (16, 2)   (16, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 16 </li></ul></li><li>Data variables: (1)<ul><li>NO2(time, lat, lon)float32dask.array&lt;chunksize=(1, 169, 512), meta=np.ndarray&gt;sample_type :FLOAT32units :mol/m^2  Array   Chunk   Bytes   5.28 MiB   338.00 kiB   Shape   (16, 169, 512)   (1, 169, 512)   Dask graph   16 chunks in 2 graph layers   Data type   float32 numpy.ndarray  512 169 16 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([54.5991015625, 54.5971484375, 54.5951953125, 54.5932421875,\n       54.5912890625, 54.5893359375, 54.5873828125, 54.5854296875,\n       54.5834765625, 54.5815234375,\n       ...\n       54.2885546875, 54.2866015625, 54.2846484375, 54.2826953125,\n       54.2807421875, 54.2787890625, 54.2768359375, 54.2748828125,\n       54.2729296875, 54.2709765625],\n      dtype='float64', name='lat', length=169))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([10.0009765625, 10.0029296875, 10.0048828125, 10.0068359375,\n       10.0087890625, 10.0107421875, 10.0126953125, 10.0146484375,\n       10.0166015625, 10.0185546875,\n       ...\n       10.9814453125, 10.9833984375, 10.9853515625, 10.9873046875,\n       10.9892578125, 10.9912109375, 10.9931640625, 10.9951171875,\n       10.9970703125, 10.9990234375],\n      dtype='float64', name='lon', length=512))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2018-06-15 12:00:00', '2018-06-18 12:00:00',\n               '2018-06-21 12:00:00', '2018-06-24 12:00:00',\n               '2018-06-27 12:00:00', '2018-06-30 12:00:00',\n               '2018-07-03 12:00:00', '2018-07-06 12:00:00',\n               '2018-07-09 12:00:00', '2018-07-12 12:00:00',\n               '2018-07-15 12:00:00', '2018-07-18 12:00:00',\n               '2018-07-21 12:00:00', '2018-07-24 12:00:00',\n               '2018-07-27 12:00:00', '2018-07-30 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (12)Conventions :CF-1.7title :S5PL2 Data Cube Subsethistory :[{'program': 'xcube_sh.chunkstore.SentinelHubChunkStore', 'cube_config': {'dataset_name': 'S5PL2', 'band_names': ['NO2'], 'band_fill_values': None, 'band_sample_types': None, 'band_units': None, 'tile_size': [512, 169], 'bbox': [10.0, 54.27, 11.0, 54.600078125], 'spatial_res': 0.001953125, 'crs': 'WGS84', 'upsampling': 'BILINEAR', 'downsampling': 'NEAREST', 'mosaicking_order': 'mostRecent', 'time_range': ['2018-06-14T00:00:00+00:00', '2018-07-31T00:00:00+00:00'], 'time_period': '3 days 00:00:00', 'time_tolerance': None, 'collection_id': None, 'four_d': False}}]date_created :2024-09-18T08:58:19.348015time_coverage_start :2018-06-14T00:00:00+00:00time_coverage_end :2018-08-01T00:00:00+00:00time_coverage_duration :P48DT0H0M0Stime_coverage_resolution :P3DT0H0M0Sgeospatial_lon_min :10.0geospatial_lat_min :54.27geospatial_lon_max :11.0geospatial_lat_max :54.600078125</li></ul> In\u00a0[15]: Copied! <pre>cube.NO2.isel(time=5).plot.imshow()\n</pre> cube.NO2.isel(time=5).plot.imshow() Out[15]: <pre>&lt;matplotlib.image.AxesImage at 0x7efe5edac450&gt;</pre> <p>Sentinel Hub currently supported Sentinel-3 SLSTR products: here.</p> In\u00a0[16]: Copied! <pre>store.describe_data('S3SLSTR')\n</pre> store.describe_data('S3SLSTR') Out[16]: <pre>&lt;xcube.core.store.descriptor.DatasetDescriptor at 0x7efe5ef7c110&gt;</pre> In\u00a0[17]: Copied! <pre>cube = store.open_data(\n    'S3SLSTR',\n    variable_names=['F1'],\n    tile_size=[512, 512],\n    bbox=bbox,\n    spatial_res=(bbox[2]-bbox[0])/512,\n    time_range=['2018-06-14', '2018-07-31'],\n    time_period='3D'\n)\ncube\n</pre> cube = store.open_data(     'S3SLSTR',     variable_names=['F1'],     tile_size=[512, 512],     bbox=bbox,     spatial_res=(bbox[2]-bbox[0])/512,     time_range=['2018-06-14', '2018-07-31'],     time_period='3D' ) cube Out[17]: <pre>&lt;xarray.Dataset&gt; Size: 6MB\nDimensions:    (time: 16, lat: 169, lon: 512, bnds: 2)\nCoordinates:\n  * lat        (lat) float64 1kB 54.6 54.6 54.6 54.59 ... 54.27 54.27 54.27\n  * lon        (lon) float64 4kB 10.0 10.0 10.0 10.01 ... 10.99 11.0 11.0 11.0\n  * time       (time) datetime64[ns] 128B 2018-06-15T12:00:00 ... 2018-07-30T...\n    time_bnds  (time, bnds) datetime64[ns] 256B dask.array&lt;chunksize=(16, 2), meta=np.ndarray&gt;\nDimensions without coordinates: bnds\nData variables:\n    F1         (time, lat, lon) float32 6MB dask.array&lt;chunksize=(1, 169, 512), meta=np.ndarray&gt;\nAttributes: (12/13)\n    Conventions:               CF-1.7\n    title:                     S3SLSTR Data Cube Subset\n    history:                   [{'program': 'xcube_sh.chunkstore.SentinelHubC...\n    date_created:              2024-09-18T08:58:29.134201\n    time_coverage_start:       2018-06-14T00:00:00+00:00\n    time_coverage_end:         2018-08-01T00:00:00+00:00\n    ...                        ...\n    time_coverage_resolution:  P3DT0H0M0S\n    geospatial_lon_min:        10.0\n    geospatial_lat_min:        54.27\n    geospatial_lon_max:        11.0\n    geospatial_lat_max:        54.600078125\n    processing_level:          L1B</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 16</li><li>lat: 169</li><li>lon: 512</li><li>bnds: 2</li></ul></li><li>Coordinates: (4)<ul><li>lat(lat)float6454.6 54.6 54.6 ... 54.27 54.27units :decimal_degreeslong_name :latitudestandard_name :latitude<pre>array([54.599102, 54.597148, 54.595195, 54.593242, 54.591289, 54.589336,\n       54.587383, 54.58543 , 54.583477, 54.581523, 54.57957 , 54.577617,\n       54.575664, 54.573711, 54.571758, 54.569805, 54.567852, 54.565898,\n       54.563945, 54.561992, 54.560039, 54.558086, 54.556133, 54.55418 ,\n       54.552227, 54.550273, 54.54832 , 54.546367, 54.544414, 54.542461,\n       54.540508, 54.538555, 54.536602, 54.534648, 54.532695, 54.530742,\n       54.528789, 54.526836, 54.524883, 54.52293 , 54.520977, 54.519023,\n       54.51707 , 54.515117, 54.513164, 54.511211, 54.509258, 54.507305,\n       54.505352, 54.503398, 54.501445, 54.499492, 54.497539, 54.495586,\n       54.493633, 54.49168 , 54.489727, 54.487773, 54.48582 , 54.483867,\n       54.481914, 54.479961, 54.478008, 54.476055, 54.474102, 54.472148,\n       54.470195, 54.468242, 54.466289, 54.464336, 54.462383, 54.46043 ,\n       54.458477, 54.456523, 54.45457 , 54.452617, 54.450664, 54.448711,\n       54.446758, 54.444805, 54.442852, 54.440898, 54.438945, 54.436992,\n       54.435039, 54.433086, 54.431133, 54.42918 , 54.427227, 54.425273,\n       54.42332 , 54.421367, 54.419414, 54.417461, 54.415508, 54.413555,\n       54.411602, 54.409648, 54.407695, 54.405742, 54.403789, 54.401836,\n       54.399883, 54.39793 , 54.395977, 54.394023, 54.39207 , 54.390117,\n       54.388164, 54.386211, 54.384258, 54.382305, 54.380352, 54.378398,\n       54.376445, 54.374492, 54.372539, 54.370586, 54.368633, 54.36668 ,\n       54.364727, 54.362773, 54.36082 , 54.358867, 54.356914, 54.354961,\n       54.353008, 54.351055, 54.349102, 54.347148, 54.345195, 54.343242,\n       54.341289, 54.339336, 54.337383, 54.33543 , 54.333477, 54.331523,\n       54.32957 , 54.327617, 54.325664, 54.323711, 54.321758, 54.319805,\n       54.317852, 54.315898, 54.313945, 54.311992, 54.310039, 54.308086,\n       54.306133, 54.30418 , 54.302227, 54.300273, 54.29832 , 54.296367,\n       54.294414, 54.292461, 54.290508, 54.288555, 54.286602, 54.284648,\n       54.282695, 54.280742, 54.278789, 54.276836, 54.274883, 54.27293 ,\n       54.270977])</pre></li><li>lon(lon)float6410.0 10.0 10.0 ... 11.0 11.0 11.0units :decimal_degreeslong_name :longitudestandard_name :longitude<pre>array([10.000977, 10.00293 , 10.004883, ..., 10.995117, 10.99707 , 10.999023])</pre></li><li>time(time)datetime64[ns]2018-06-15T12:00:00 ... 2018-07-...standard_name :timebounds :time_bnds<pre>array(['2018-06-15T12:00:00.000000000', '2018-06-18T12:00:00.000000000',\n       '2018-06-21T12:00:00.000000000', '2018-06-24T12:00:00.000000000',\n       '2018-06-27T12:00:00.000000000', '2018-06-30T12:00:00.000000000',\n       '2018-07-03T12:00:00.000000000', '2018-07-06T12:00:00.000000000',\n       '2018-07-09T12:00:00.000000000', '2018-07-12T12:00:00.000000000',\n       '2018-07-15T12:00:00.000000000', '2018-07-18T12:00:00.000000000',\n       '2018-07-21T12:00:00.000000000', '2018-07-24T12:00:00.000000000',\n       '2018-07-27T12:00:00.000000000', '2018-07-30T12:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>time_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(16, 2), meta=np.ndarray&gt;standard_name :time  Array   Chunk   Bytes   256 B   256 B   Shape   (16, 2)   (16, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 16 </li></ul></li><li>Data variables: (1)<ul><li>F1(time, lat, lon)float32dask.array&lt;chunksize=(1, 169, 512), meta=np.ndarray&gt;sample_type :UINT16units :kelvinwavelength :3742bandwidth :398.0resolution :1000  Array   Chunk   Bytes   5.28 MiB   338.00 kiB   Shape   (16, 169, 512)   (1, 169, 512)   Dask graph   16 chunks in 2 graph layers   Data type   float32 numpy.ndarray  512 169 16 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([54.5991015625, 54.5971484375, 54.5951953125, 54.5932421875,\n       54.5912890625, 54.5893359375, 54.5873828125, 54.5854296875,\n       54.5834765625, 54.5815234375,\n       ...\n       54.2885546875, 54.2866015625, 54.2846484375, 54.2826953125,\n       54.2807421875, 54.2787890625, 54.2768359375, 54.2748828125,\n       54.2729296875, 54.2709765625],\n      dtype='float64', name='lat', length=169))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([10.0009765625, 10.0029296875, 10.0048828125, 10.0068359375,\n       10.0087890625, 10.0107421875, 10.0126953125, 10.0146484375,\n       10.0166015625, 10.0185546875,\n       ...\n       10.9814453125, 10.9833984375, 10.9853515625, 10.9873046875,\n       10.9892578125, 10.9912109375, 10.9931640625, 10.9951171875,\n       10.9970703125, 10.9990234375],\n      dtype='float64', name='lon', length=512))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2018-06-15 12:00:00', '2018-06-18 12:00:00',\n               '2018-06-21 12:00:00', '2018-06-24 12:00:00',\n               '2018-06-27 12:00:00', '2018-06-30 12:00:00',\n               '2018-07-03 12:00:00', '2018-07-06 12:00:00',\n               '2018-07-09 12:00:00', '2018-07-12 12:00:00',\n               '2018-07-15 12:00:00', '2018-07-18 12:00:00',\n               '2018-07-21 12:00:00', '2018-07-24 12:00:00',\n               '2018-07-27 12:00:00', '2018-07-30 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (13)Conventions :CF-1.7title :S3SLSTR Data Cube Subsethistory :[{'program': 'xcube_sh.chunkstore.SentinelHubChunkStore', 'cube_config': {'dataset_name': 'S3SLSTR', 'band_names': ['F1'], 'band_fill_values': None, 'band_sample_types': None, 'band_units': None, 'tile_size': [512, 169], 'bbox': [10.0, 54.27, 11.0, 54.600078125], 'spatial_res': 0.001953125, 'crs': 'WGS84', 'upsampling': 'NEAREST', 'downsampling': 'NEAREST', 'mosaicking_order': 'mostRecent', 'time_range': ['2018-06-14T00:00:00+00:00', '2018-07-31T00:00:00+00:00'], 'time_period': '3 days 00:00:00', 'time_tolerance': None, 'collection_id': None, 'four_d': False}}]date_created :2024-09-18T08:58:29.134201time_coverage_start :2018-06-14T00:00:00+00:00time_coverage_end :2018-08-01T00:00:00+00:00time_coverage_duration :P48DT0H0M0Stime_coverage_resolution :P3DT0H0M0Sgeospatial_lon_min :10.0geospatial_lat_min :54.27geospatial_lon_max :11.0geospatial_lat_max :54.600078125processing_level :L1B</li></ul> In\u00a0[18]: Copied! <pre>cube.F1.isel(time=5).plot.imshow()\n</pre> cube.F1.isel(time=5).plot.imshow() Out[18]: <pre>&lt;matplotlib.image.AxesImage at 0x7efe5da6c450&gt;</pre> <p>Sentinel Hub currently supported Sentinel-3 OLCI products: here</p> In\u00a0[31]: Copied! <pre>store.describe_data('S3OLCI')\n</pre> store.describe_data('S3OLCI') Out[31]: <pre>&lt;xcube.core.store.descriptor.DatasetDescriptor at 0x7f2fff875610&gt;</pre> In\u00a0[32]: Copied! <pre>cube = store.open_data(\n    'S3OLCI',\n    variable_names=['B04'],\n    tile_size=[512, 512],\n    bbox=bbox,\n    spatial_res=(bbox[2]-bbox[0])/512,\n    time_range=['2018-06-14', '2018-07-31'],\n    time_period='3D'\n)\ncube\n</pre> cube = store.open_data(     'S3OLCI',     variable_names=['B04'],     tile_size=[512, 512],     bbox=bbox,     spatial_res=(bbox[2]-bbox[0])/512,     time_range=['2018-06-14', '2018-07-31'],     time_period='3D' ) cube Out[32]: <pre>&lt;xarray.Dataset&gt; Size: 6MB\nDimensions:    (time: 16, lat: 169, lon: 512, bnds: 2)\nCoordinates:\n  * lat        (lat) float64 1kB 54.6 54.6 54.6 54.59 ... 54.27 54.27 54.27\n  * lon        (lon) float64 4kB 10.0 10.0 10.0 10.01 ... 10.99 11.0 11.0 11.0\n  * time       (time) datetime64[ns] 128B 2018-06-15T12:00:00 ... 2018-07-30T...\n    time_bnds  (time, bnds) datetime64[ns] 256B dask.array&lt;chunksize=(16, 2), meta=np.ndarray&gt;\nDimensions without coordinates: bnds\nData variables:\n    B04        (time, lat, lon) float32 6MB dask.array&lt;chunksize=(1, 169, 512), meta=np.ndarray&gt;\nAttributes: (12/13)\n    Conventions:               CF-1.7\n    title:                     S3OLCI Data Cube Subset\n    history:                   [{'program': 'xcube_sh.chunkstore.SentinelHubC...\n    date_created:              2024-09-11T09:30:59.250197\n    time_coverage_start:       2018-06-14T00:00:00+00:00\n    time_coverage_end:         2018-08-01T00:00:00+00:00\n    ...                        ...\n    time_coverage_resolution:  P3DT0H0M0S\n    geospatial_lon_min:        10.0\n    geospatial_lat_min:        54.27\n    geospatial_lon_max:        11.0\n    geospatial_lat_max:        54.600078125\n    processing_level:          L1B</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 16</li><li>lat: 169</li><li>lon: 512</li><li>bnds: 2</li></ul></li><li>Coordinates: (4)<ul><li>lat(lat)float6454.6 54.6 54.6 ... 54.27 54.27units :decimal_degreeslong_name :latitudestandard_name :latitude<pre>array([54.599102, 54.597148, 54.595195, 54.593242, 54.591289, 54.589336,\n       54.587383, 54.58543 , 54.583477, 54.581523, 54.57957 , 54.577617,\n       54.575664, 54.573711, 54.571758, 54.569805, 54.567852, 54.565898,\n       54.563945, 54.561992, 54.560039, 54.558086, 54.556133, 54.55418 ,\n       54.552227, 54.550273, 54.54832 , 54.546367, 54.544414, 54.542461,\n       54.540508, 54.538555, 54.536602, 54.534648, 54.532695, 54.530742,\n       54.528789, 54.526836, 54.524883, 54.52293 , 54.520977, 54.519023,\n       54.51707 , 54.515117, 54.513164, 54.511211, 54.509258, 54.507305,\n       54.505352, 54.503398, 54.501445, 54.499492, 54.497539, 54.495586,\n       54.493633, 54.49168 , 54.489727, 54.487773, 54.48582 , 54.483867,\n       54.481914, 54.479961, 54.478008, 54.476055, 54.474102, 54.472148,\n       54.470195, 54.468242, 54.466289, 54.464336, 54.462383, 54.46043 ,\n       54.458477, 54.456523, 54.45457 , 54.452617, 54.450664, 54.448711,\n       54.446758, 54.444805, 54.442852, 54.440898, 54.438945, 54.436992,\n       54.435039, 54.433086, 54.431133, 54.42918 , 54.427227, 54.425273,\n       54.42332 , 54.421367, 54.419414, 54.417461, 54.415508, 54.413555,\n       54.411602, 54.409648, 54.407695, 54.405742, 54.403789, 54.401836,\n       54.399883, 54.39793 , 54.395977, 54.394023, 54.39207 , 54.390117,\n       54.388164, 54.386211, 54.384258, 54.382305, 54.380352, 54.378398,\n       54.376445, 54.374492, 54.372539, 54.370586, 54.368633, 54.36668 ,\n       54.364727, 54.362773, 54.36082 , 54.358867, 54.356914, 54.354961,\n       54.353008, 54.351055, 54.349102, 54.347148, 54.345195, 54.343242,\n       54.341289, 54.339336, 54.337383, 54.33543 , 54.333477, 54.331523,\n       54.32957 , 54.327617, 54.325664, 54.323711, 54.321758, 54.319805,\n       54.317852, 54.315898, 54.313945, 54.311992, 54.310039, 54.308086,\n       54.306133, 54.30418 , 54.302227, 54.300273, 54.29832 , 54.296367,\n       54.294414, 54.292461, 54.290508, 54.288555, 54.286602, 54.284648,\n       54.282695, 54.280742, 54.278789, 54.276836, 54.274883, 54.27293 ,\n       54.270977])</pre></li><li>lon(lon)float6410.0 10.0 10.0 ... 11.0 11.0 11.0units :decimal_degreeslong_name :longitudestandard_name :longitude<pre>array([10.000977, 10.00293 , 10.004883, ..., 10.995117, 10.99707 , 10.999023])</pre></li><li>time(time)datetime64[ns]2018-06-15T12:00:00 ... 2018-07-...standard_name :timebounds :time_bnds<pre>array(['2018-06-15T12:00:00.000000000', '2018-06-18T12:00:00.000000000',\n       '2018-06-21T12:00:00.000000000', '2018-06-24T12:00:00.000000000',\n       '2018-06-27T12:00:00.000000000', '2018-06-30T12:00:00.000000000',\n       '2018-07-03T12:00:00.000000000', '2018-07-06T12:00:00.000000000',\n       '2018-07-09T12:00:00.000000000', '2018-07-12T12:00:00.000000000',\n       '2018-07-15T12:00:00.000000000', '2018-07-18T12:00:00.000000000',\n       '2018-07-21T12:00:00.000000000', '2018-07-24T12:00:00.000000000',\n       '2018-07-27T12:00:00.000000000', '2018-07-30T12:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>time_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(16, 2), meta=np.ndarray&gt;standard_name :time  Array   Chunk   Bytes   256 B   256 B   Shape   (16, 2)   (16, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 16 </li></ul></li><li>Data variables: (1)<ul><li>B04(time, lat, lon)float32dask.array&lt;chunksize=(1, 169, 512), meta=np.ndarray&gt;sample_type :FLOAT32units :reflectancewavelength :490bandwidth :10resolution :300  Array   Chunk   Bytes   5.28 MiB   338.00 kiB   Shape   (16, 169, 512)   (1, 169, 512)   Dask graph   16 chunks in 2 graph layers   Data type   float32 numpy.ndarray  512 169 16 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([54.5991015625, 54.5971484375, 54.5951953125, 54.5932421875,\n       54.5912890625, 54.5893359375, 54.5873828125, 54.5854296875,\n       54.5834765625, 54.5815234375,\n       ...\n       54.2885546875, 54.2866015625, 54.2846484375, 54.2826953125,\n       54.2807421875, 54.2787890625, 54.2768359375, 54.2748828125,\n       54.2729296875, 54.2709765625],\n      dtype='float64', name='lat', length=169))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([10.0009765625, 10.0029296875, 10.0048828125, 10.0068359375,\n       10.0087890625, 10.0107421875, 10.0126953125, 10.0146484375,\n       10.0166015625, 10.0185546875,\n       ...\n       10.9814453125, 10.9833984375, 10.9853515625, 10.9873046875,\n       10.9892578125, 10.9912109375, 10.9931640625, 10.9951171875,\n       10.9970703125, 10.9990234375],\n      dtype='float64', name='lon', length=512))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2018-06-15 12:00:00', '2018-06-18 12:00:00',\n               '2018-06-21 12:00:00', '2018-06-24 12:00:00',\n               '2018-06-27 12:00:00', '2018-06-30 12:00:00',\n               '2018-07-03 12:00:00', '2018-07-06 12:00:00',\n               '2018-07-09 12:00:00', '2018-07-12 12:00:00',\n               '2018-07-15 12:00:00', '2018-07-18 12:00:00',\n               '2018-07-21 12:00:00', '2018-07-24 12:00:00',\n               '2018-07-27 12:00:00', '2018-07-30 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (13)Conventions :CF-1.7title :S3OLCI Data Cube Subsethistory :[{'program': 'xcube_sh.chunkstore.SentinelHubChunkStore', 'cube_config': {'dataset_name': 'S3OLCI', 'band_names': ['B04'], 'band_fill_values': None, 'band_sample_types': None, 'band_units': None, 'tile_size': [512, 169], 'bbox': [10.0, 54.27, 11.0, 54.600078125], 'spatial_res': 0.001953125, 'crs': 'WGS84', 'upsampling': 'NEAREST', 'downsampling': 'NEAREST', 'mosaicking_order': 'mostRecent', 'time_range': ['2018-06-14T00:00:00+00:00', '2018-07-31T00:00:00+00:00'], 'time_period': '3 days 00:00:00', 'time_tolerance': None, 'collection_id': None, 'four_d': False}}]date_created :2024-09-11T09:30:59.250197time_coverage_start :2018-06-14T00:00:00+00:00time_coverage_end :2018-08-01T00:00:00+00:00time_coverage_duration :P48DT0H0M0Stime_coverage_resolution :P3DT0H0M0Sgeospatial_lon_min :10.0geospatial_lat_min :54.27geospatial_lon_max :11.0geospatial_lat_max :54.600078125processing_level :L1B</li></ul> In\u00a0[33]: Copied! <pre>cube.B04.isel(time=5).plot.imshow()\n</pre> cube.B04.isel(time=5).plot.imshow() Out[33]: <pre>&lt;matplotlib.image.AxesImage at 0x7f2ffc3add10&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/Generate_SentinelHub_cubes/#xcube-data-store-framework-sentinel-hub","title":"xcube Data Store Framework - Sentinel Hub\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_SentinelHub_cubes/#lets-have-a-look-at-the-different-products-which-are-available-via-xcube-sentinelhub","title":"Let's have a look at the different products, which are available via xcube-sentinelhub.\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_SentinelHub_cubes/#1-sentinel-1-grd","title":"1. Sentinel-1 GRD\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_SentinelHub_cubes/#2-sentinel-2-l2a-scl","title":"2. Sentinel-2 L2A (SCL)\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_SentinelHub_cubes/#3-sentinel-5p-l2","title":"3. Sentinel-5P L2\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_SentinelHub_cubes/#4-sentinel-3-slstr","title":"4. Sentinel-3 SLSTR\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Generate_SentinelHub_cubes/#5-sentinel-3-olci","title":"5. Sentinel-3 OLCI\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/How_to_make_use_of_Dask_clusters/","title":"How to make use of Dask clusters","text":"<p>This notebook shows how to use Dask clusters to parallelize workflows. Parallelization is useful when running workflows that require more computing resources than are available locally (or here in your DeepESDL workspace). The methods presented in this notebook take care of setting up the cluster with a desired number of workers, installing the environment on the worker nodes, and performing the parallel computation. When a task is finished and you no longer need the resources, you shut down the cluster with a single line of code, and all resources are shut down.</p> <p>If you are new to Dask, you can get an introduction here. Please also refer to the [DeepESDL documentation] (https://deepesdl.readthedocs.io/en/latest/guide/jupyterlab/) and visit the platform's website for further information!</p> <p>Brockmann Consult, 2024</p> <p>To run this notebook you need a python environment with:</p> <ul> <li>python=3.11</li> <li>xcube&gt;=1.1.2</li> <li>coiled</li> <li>dask</li> <li>ipykernel</li> </ul> <p>To create a custom environment, please refer to the documentation.</p> In order to work with the cluster service, the Dask processing cluster has to be added to your team from the NoR (Network of Resources) portal. The example notebook will only run if your team has already access to the service and a suitable environment is selected. <p>If your team has access, you have to execute the following line once in your terminal: <code>$ coiled login --account bc --token $COILED_TOKEN</code></p> <p>You just need to do this on the very first usage.</p> In\u00a0[1]: Copied! <pre>from xcube.util.dask import new_cluster\nfrom dask.distributed import Client\nimport os\n</pre> from xcube.util.dask import new_cluster from dask.distributed import Client import os In\u00a0[2]: Copied! <pre># mandatory: Dask cluster service mirrors your currently selected environment and does not take the default environment\ndel os.environ['JUPYTER_IMAGE']\n</pre> # mandatory: Dask cluster service mirrors your currently selected environment and does not take the default environment del os.environ['JUPYTER_IMAGE'] In\u00a0[3]: Copied! <pre># default instance type selected contains 4 cores\ncluster = new_cluster(name='team_computing_monthly_means', n_workers=2)\n</pre> # default instance type selected contains 4 cores cluster = new_cluster(name='team_computing_monthly_means', n_workers=2)  <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Coiled Cluster \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502              https://cloud.coiled.io/clusters/592165?account=bc              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Overview \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Configuration \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                      \u2502\u2502                                      \u2502\n\u2502 Name: team_computing_monthly_means   \u2502\u2502 Region: eu-central-1                 \u2502\n\u2502                                      \u2502\u2502                                      \u2502\n\u2502 Scheduler Status: started            \u2502\u2502 Scheduler: m6i.xlarge                \u2502\n\u2502                                      \u2502\u2502                                      \u2502\n\u2502 Dashboard:                           \u2502\u2502 Workers:   m6i.xlarge (2)            \u2502\n\u2502 https://cluster-zppgt.dask.host?toke \u2502\u2502                                      \u2502\n\u2502 n=gRqjkEMXnTB-t8k4                   \u2502\u2502 Workers Requested: 2                 \u2502\n\u2502                                      \u2502\u2502                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 (2024/09/18 08:49:33 UTC) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                              \u2502\n\u2502                              All workers ready.                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\n\n</pre> <pre></pre> <p>Connect your newly started cluster to the Dask client:</p> In\u00a0[4]: Copied! <pre># check cluster status\ncluster.status\n</pre> # check cluster status cluster.status Out[4]: <pre>&lt;Status.running: 'running'&gt;</pre> In\u00a0[5]: Copied! <pre>client = Client(cluster)\n</pre> client = Client(cluster) <p>To test the cluster, we will create monthly means for a variable from the Earth System Data Cube provided by DeepESDL. To get more inspiration, when Dask may be useful, check out the example section.</p> In\u00a0[6]: Copied! <pre>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nstore.list_data_ids()\n</pre> from xcube.core.store import new_data_store store = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True)) store.list_data_ids() Out[6]: <pre>['LC-1x2160x2160-1.0.0.levels',\n 'SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000.levels',\n 'SMOS-L2C-OS-20230101-20231231-1W-res0-53x120x120.zarr',\n 'SMOS-L2C-OS-20230101-20231231-1W-res0.zarr',\n 'SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000.levels',\n 'SMOS-L2C-SM-20230101-20231231-1W-res0-53x120x120.zarr',\n 'SMOS-L2C-SM-20230101-20231231-1W-res0.zarr',\n 'SMOS-freezethaw-1x720x720-1.0.1.zarr',\n 'SMOS-freezethaw-4267x10x10-1.0.1.zarr',\n 'SeasFireCube_v3.zarr',\n 'black-sea-1x1024x1024.levels',\n 'black-sea-256x128x128.zarr',\n 'esa-cci-permafrost-1x1151x1641-0.0.2.levels',\n 'esdc-8d-0.25deg-1x720x1440-3.0.1.zarr',\n 'esdc-8d-0.25deg-256x128x128-3.0.1.zarr',\n 'extrAIM-merged-cube-1x86x179.zarr',\n 'hydrology-1D-0.009deg-100x60x60-3.0.2.zarr',\n 'hydrology-1D-0.009deg-1418x70x76-2.0.0.zarr',\n 'hydrology-1D-0.009deg-1x1102x2415-2.0.0.levels',\n 'hydrology-1D-0.009deg-1x1102x966-3.0.2.levels',\n 'ocean-1M-9km-1x1080x1080-1.4.0.levels',\n 'ocean-1M-9km-64x256x256-1.4.0.zarr',\n 'polar-100m-1x2048x2048-1.0.1.zarr']</pre> <p>We select a cube, that contains 256 timestamps in one chunk.</p> In\u00a0[7]: Copied! <pre>dataset = store.open_data('esdc-8d-0.25deg-256x128x128-3.0.1.zarr')\ndataset\n</pre> dataset = store.open_data('esdc-8d-0.25deg-256x128x128-3.0.1.zarr') dataset Out[7]: <pre>&lt;xarray.Dataset&gt; Size: 353GB\nDimensions:                            (time: 1978, lat: 720, lon: 1440)\nCoordinates:\n  * lat                                (lat) float64 6kB -89.88 -89.62 ... 89.88\n  * lon                                (lon) float64 12kB -179.9 ... 179.9\n  * time                               (time) datetime64[ns] 16kB 1979-01-05 ...\nData variables: (12/42)\n    aerosol_optical_thickness_550      (time, lat, lon) float32 8GB dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;\n    air_temperature_2m                 (time, lat, lon) float32 8GB dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;\n    bare_soil_evaporation              (time, lat, lon) float32 8GB dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;\n    burnt_area                         (time, lat, lon) float64 16GB dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;\n    cot                                (time, lat, lon) float32 8GB dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;\n    cth                                (time, lat, lon) float32 8GB dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;\n    ...                                 ...\n    sif_rtsif                          (time, lat, lon) float32 8GB dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;\n    sm                                 (time, lat, lon) float32 8GB dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;\n    snow_sublimation                   (time, lat, lon) float32 8GB dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;\n    surface_moisture                   (time, lat, lon) float32 8GB dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;\n    terrestrial_ecosystem_respiration  (time, lat, lon) float32 8GB dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;\n    transpiration                      (time, lat, lon) float32 8GB dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;\nAttributes: (12/23)\n    Conventions:                CF-1.9\n    acknowledgment:             All ESDC data providers are acknowledged insi...\n    contributor_name:           ['University of Leipzig', 'Max Planck Institu...\n    contributor_url:            ['https://www.uni-leipzig.de/', 'https://www....\n    creator_name:               ['University of Leipzig', 'Brockmann Consult ...\n    creator_url:                ['https://www.uni-leipzig.de/', 'https://www....\n    ...                         ...\n    publisher_url:              https://www.earthsystemdatalab.net/\n    time_coverage_end:          2021-12-31T00:00:00.000000000\n    time_coverage_start:        1979-01-05T00:00:00.000000000\n    time_period:                8D\n    time_period_reported_day:   5.0\n    title:                      Earth System Data Cube (ESDC) v3.0.1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 1978</li><li>lat: 720</li><li>lon: 1440</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float64-89.88 -89.62 ... 89.62 89.88long_name :latitudestandard_name :latitudeunits :degrees_north<pre>array([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875])</pre></li><li>lon(lon)float64-179.9 -179.6 ... 179.6 179.9long_name :longitudestandard_name :longitudeunits :degrees_east<pre>array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])</pre></li><li>time(time)datetime64[ns]1979-01-05 ... 2021-12-31long_name :timestandard_name :time<pre>array(['1979-01-05T00:00:00.000000000', '1979-01-13T00:00:00.000000000',\n       '1979-01-21T00:00:00.000000000', ..., '2021-12-15T00:00:00.000000000',\n       '2021-12-23T00:00:00.000000000', '2021-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (42)<ul><li>aerosol_optical_thickness_550(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :ESA Aerosol Climate Change Initiative (Aerosol_cci)date_modified :2022-10-13 03:15:18.312024description :ESA Aerosol Climate Change Initiative (Aerosol_cci): Level 3 aerosol products from AATSR (ensemble product), Version 2.6geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Aerosol Optical Thickness at 550 nmoriginal_add_offset :0.0original_name :AOD550_meanoriginal_scale_factor :1.0processing_steps :['Loading data using the cciodp xcube data store', 'Resampling by 8-day mean', 'Upsampling to 0.25 degrees using nearest neighbor']project :DeepESDLreferences :['https://doi.org/10.5194/amt-6-1919-2013']reported_day :5.0source :['https://catalogue.ceda.ac.uk/uuid/c183044b88734442b6d37f5c4f6b0092']standard_name :atmosphere_optical_thickness_due_to_ambient_aerosoltemporal_resolution :8Dtime_coverage_end :2012-04-10T00:00:00.000000000time_coverage_start :2002-05-21T00:00:00.000000000time_period :8Dunits :1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>air_temperature_2m(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Mean Air Temperature at 2 moriginal_add_offset :0.0original_name :t2moriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily mean', 'Converting to \u00b0C from K', 'Resampling by 8-day mean', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :mean_air_temperature_2mtemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :\u00b0C  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>bare_soil_evaporation(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Bare Soil Evaporationoriginal_add_offset :0.0original_name :Eboriginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :bare_soil_evaporationtemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>burnt_area(time, lat, lon)float64dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://www.globalfiredata.org/date_modified :2022-10-13 14:55:35.002779description :Global Fire Emissions Database (GFED) 4 Monthly Burnt Areageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Monthly Burnt Areaoriginal_add_offset :0.0original_name :burnt_areaoriginal_scale_factor :0.01processing_steps :['Merging hdf files', 'Resampling by 8-day nearest neighbor']project :DeepESDLreferences :['https://doi.org/10.1002/jgrg.20042']reported_day :5.0source :['https://www.globalfiredata.org/']standard_name :burnt_areatemporal_resolution :8Dtime_coverage_end :2016-12-30T00:00:00.000000000time_coverage_start :1995-06-06T00:00:00.000000000time_period :8Dunits :hectares  Array   Chunk   Bytes   15.28 GiB   32.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float64 numpy.ndarray  1440 720 1978 </li><li>cot(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :ESA Cloud Climate Change Initiative (Cloud_cci)date_modified :2022-11-04 13:33:18.450458description :ESA Cloud Climate Change Initiative (Cloud_cci): MODIS-TERRA monthly gridded cloud properties, version 2.0geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Cloud Optical Thicknessoriginal_add_offset :0.0original_name :cotoriginal_scale_factor :1.0processing_steps :['Loading data using the cciodp xcube data store', 'Resampling by 8-day nearest neighbor', 'Upsampling to 0.25 degrees using nearest neighbor']project :DeepESDLreferences :['https://public.satproj.klima.dwd.de/data/ESA_Cloud_CCI/CLD_PRODUCTS/v2.0/DOIs/DOI_ESA_Cloud_cci_MODIS-Terra_v2.0_landingpage.html', 'https://dap.ceda.ac.uk/neodc/esacci/cloud/docs/DataSet_Desc_ESA_Cloud_cci_CC4CL_1.5.pdf']reported_day :5.0source :['https://catalogue.ceda.ac.uk/uuid/f1ab07b5292f4813bd3090b51d270aa8']standard_name :atmosphere_optical_thickness_due_to_cloudtemporal_resolution :8Dtime_coverage_end :2014-12-15T00:00:00.000000000time_coverage_start :2000-02-22T00:00:00.000000000time_period :8Dunits :1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>cth(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :ESA Cloud Climate Change Initiative (Cloud_cci)date_modified :2022-11-04 13:33:18.450458description :ESA Cloud Climate Change Initiative (Cloud_cci): MODIS-TERRA monthly gridded cloud properties, version 2.0geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Cloud Top Heightoriginal_add_offset :0.0original_name :cthoriginal_scale_factor :1.0processing_steps :['Loading data using the cciodp xcube data store', 'Resampling by 8-day nearest neighbor', 'Upsampling to 0.25 degrees using nearest neighbor']project :DeepESDLreferences :['https://public.satproj.klima.dwd.de/data/ESA_Cloud_CCI/CLD_PRODUCTS/v2.0/DOIs/DOI_ESA_Cloud_cci_MODIS-Terra_v2.0_landingpage.html', 'https://dap.ceda.ac.uk/neodc/esacci/cloud/docs/DataSet_Desc_ESA_Cloud_cci_CC4CL_1.5.pdf']reported_day :5.0source :['https://catalogue.ceda.ac.uk/uuid/f1ab07b5292f4813bd3090b51d270aa8']standard_name :cloud_top_altitudetemporal_resolution :8Dtime_coverage_end :2014-12-15T00:00:00.000000000time_coverage_start :2000-02-22T00:00:00.000000000time_period :8Dunits :km  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>ctt(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :ESA Cloud Climate Change Initiative (Cloud_cci)date_modified :2022-11-04 13:33:18.450458description :ESA Cloud Climate Change Initiative (Cloud_cci): MODIS-TERRA monthly gridded cloud properties, version 2.0geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Cloud Top Temperatureoriginal_add_offset :0.0original_name :cttoriginal_scale_factor :1.0processing_steps :['Loading data using the cciodp xcube data store', 'Resampling by 8-day nearest neighbor', 'Upsampling to 0.25 degrees using nearest neighbor']project :DeepESDLreferences :['https://public.satproj.klima.dwd.de/data/ESA_Cloud_CCI/CLD_PRODUCTS/v2.0/DOIs/DOI_ESA_Cloud_cci_MODIS-Terra_v2.0_landingpage.html', 'https://dap.ceda.ac.uk/neodc/esacci/cloud/docs/DataSet_Desc_ESA_Cloud_cci_CC4CL_1.5.pdf']reported_day :5.0source :['https://catalogue.ceda.ac.uk/uuid/f1ab07b5292f4813bd3090b51d270aa8']standard_name :air_temperature_at_cloud_toptemporal_resolution :8Dtime_coverage_end :2014-12-15T00:00:00.000000000time_coverage_start :2000-02-22T00:00:00.000000000time_period :8Dunits :K  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>evaporation(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Actual Evaporationoriginal_add_offset :0.0original_name :Eoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :actual_evaporationtemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>evaporation_era5(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Evaporationoriginal_add_offset :0.0original_name :eoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily sum', 'Converting to mm from m', 'Resampling by 8-day mean', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :lwe_thickness_of_water_evaporation_amounttemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>evaporative_stress(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Evaporative Stressoriginal_add_offset :0.0original_name :Soriginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :evaporative_stresstemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>gross_primary_productivity(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :FLUXCOMdate_modified :2022-10-17 22:14:30.401506description :FLUXCOMgeospatial_lat_max :89.87499928049999geospatial_lat_min :-89.8750000005geospatial_lat_resolution :0.25geospatial_lon_max :179.87499856049996geospatial_lon_min :-179.8750000005geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Gross Primary Productivityoriginal_add_offset :0.0original_name :GPPoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.5194/bg-13-4291-2016', 'https://doi.org/10.1038/s41597-019-0076-8']reported_day :5.0source :['https://www.fluxcom.org/']standard_name :gross_primary_productivity_of_carbontemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :2001-01-05T00:00:00.000000000time_period :8Dunits :g C m^-2 d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>interception_loss(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Interception Lossoriginal_add_offset :0.0original_name :Eioriginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :interception_losstemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>kndvi(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/date_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Kernel Normalized Difference Vegetation Indexoriginal_add_offset :0.0original_name :kNDVIoriginal_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.1126/sciadv.abc7447', 'https://github.com/awesome-spectral-indices/awesome-spectral-indices', 'https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://github.com/awesome-spectral-indices/spyndex', 'https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :kNDVItemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>latent_energy(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :FLUXCOMdate_modified :2022-10-17 22:14:30.401506description :FLUXCOMgeospatial_lat_max :89.87499928049999geospatial_lat_min :-89.8750000005geospatial_lat_resolution :0.25geospatial_lon_max :179.87499856049996geospatial_lon_min :-179.8750000005geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Latent Energyoriginal_add_offset :0.0original_name :LEoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.5194/bg-13-4291-2016', 'https://doi.org/10.1038/s41597-019-0076-8']reported_day :5.0source :['https://www.fluxcom.org/']standard_name :surface_upward_latent_heat_fluxtemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :2001-01-05T00:00:00.000000000time_period :8Dunits :MJ m^-2 d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>max_air_temperature_2m(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Maximum Air Temperature at 2 moriginal_add_offset :0.0original_name :t2m_maxoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily max', 'Converting to \u00b0C from K', 'Resampling by 8-day max', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :max_air_temperature_2mtemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :\u00b0C  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>min_air_temperature_2m(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Minimum Air Temperature at 2 moriginal_add_offset :0.0original_name :t2m_minoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily min', 'Converting to \u00b0C from K', 'Resampling by 8-day min', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :min_air_temperature_2mtemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :\u00b0C  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nbar_blue(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/bandwidth :20.0bandwidth_units :nmdate_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Nadir BRDF Adjusted Reflectance of Band 3 (blue)original_add_offset :0.0original_name :Nadir_Reflectance_Band3original_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :nadir_reflectance_band3temporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1wavelength :469.0wavelength_units :nm  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nbar_green(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/bandwidth :20.0bandwidth_units :nmdate_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Nadir BRDF Adjusted Reflectance of Band 4 (green)original_add_offset :0.0original_name :Nadir_Reflectance_Band4original_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :nadir_reflectance_band4temporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1wavelength :555.0wavelength_units :nm  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nbar_nir(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/bandwidth :35.0bandwidth_units :nmdate_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Nadir BRDF Adjusted Reflectance of Band 2 (NIR)original_add_offset :0.0original_name :Nadir_Reflectance_Band2original_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :nadir_reflectance_band2temporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1wavelength :858.5wavelength_units :nm  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nbar_red(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/bandwidth :50.0bandwidth_units :nmdate_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Nadir BRDF Adjusted Reflectance of Band 1 (red)original_add_offset :0.0original_name :Nadir_Reflectance_Band1original_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :nadir_reflectance_band1temporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1wavelength :645.0wavelength_units :nm  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nbar_swir1(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/bandwidth :20.0bandwidth_units :nmdate_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Nadir BRDF Adjusted Reflectance of Band 5 (SWIR1)original_add_offset :0.0original_name :Nadir_Reflectance_Band5original_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :nadir_reflectance_band5temporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1wavelength :1240.0wavelength_units :nm  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nbar_swir2(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/bandwidth :24.0bandwidth_units :nmdate_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Nadir BRDF Adjusted Reflectance of Band 6 (SWIR2)original_add_offset :0.0original_name :Nadir_Reflectance_Band6original_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :nadir_reflectance_band6temporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1wavelength :1640.0wavelength_units :nm  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nbar_swir3(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/bandwidth :50.0bandwidth_units :nmdate_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Nadir BRDF Adjusted Reflectance of Band 7 (SWIR3)original_add_offset :0.0original_name :Nadir_Reflectance_Band7original_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :nadir_reflectance_band7temporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1wavelength :2130.0wavelength_units :nm  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>ndvi(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/date_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Normalized Difference Vegetation Indexoriginal_add_offset :0.0original_name :NDVIoriginal_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://ntrs.nasa.gov/citations/19740022614', 'https://github.com/awesome-spectral-indices/awesome-spectral-indices', 'https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://github.com/awesome-spectral-indices/spyndex', 'https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :NDVItemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>net_ecosystem_exchange(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :FLUXCOMdate_modified :2022-10-17 22:14:30.401506description :FLUXCOMgeospatial_lat_max :89.87499928049999geospatial_lat_min :-89.8750000005geospatial_lat_resolution :0.25geospatial_lon_max :179.87499856049996geospatial_lon_min :-179.8750000005geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Net Ecosystem Exchangeoriginal_add_offset :0.0original_name :NEEoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.5194/bg-13-4291-2016', 'https://doi.org/10.1038/s41597-019-0076-8']reported_day :5.0source :['https://www.fluxcom.org/']standard_name :net_primary_productivity_of_carbontemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :2001-01-05T00:00:00.000000000time_period :8Dunits :g C m^-2 d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>net_radiation(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :FLUXCOMdate_modified :2022-10-17 22:14:30.401506description :FLUXCOMgeospatial_lat_max :89.87499928049999geospatial_lat_min :-89.8750000005geospatial_lat_resolution :0.25geospatial_lon_max :179.87499856049996geospatial_lon_min :-179.8750000005geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Net Radiationoriginal_add_offset :0.0original_name :Rnoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.5194/bg-13-4291-2016', 'https://doi.org/10.1038/s41597-019-0076-8']reported_day :5.0source :['https://www.fluxcom.org/']standard_name :surface_net_radiation_fluxtemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :2001-01-05T00:00:00.000000000time_period :8Dunits :MJ m^-2 d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>nirv(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://lpdaac.usgs.gov/products/mcd43c4v061/date_modified :2022-10-11 23:51:00.603768description :MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indicesgeospatial_lat_max :89.875geospatial_lat_min :-89.87499999998977geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000008183geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Near Infrared Reflectance of Vegetationoriginal_add_offset :0.0original_name :NIRvoriginal_scale_factor :1.0processing_steps :['Merging hdf files', 'Computing NDVI, NIRv, and kNDVI', 'resampling by 8-day mean', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation', 'Masking water using GLEAM as reference']project :DeepESDLreferences :['https://doi.org/10.1126/sciadv.1602244', 'https://github.com/awesome-spectral-indices/awesome-spectral-indices', 'https://doi.org/10.5067/MODIS/MCD43C4.061', 'https://www.umb.edu/spectralmass/terra_aqua_modis/v006']reported_day :5.0source :['https://github.com/awesome-spectral-indices/spyndex', 'https://lpdaac.usgs.gov/products/mcd43c4v061/']standard_name :NIRvtemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>open_water_evaporation(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Open-water Evaporationoriginal_add_offset :0.0original_name :Eworiginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :open_water_evaporationtemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>potential_evaporation(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Potential Evaporationoriginal_add_offset :0.0original_name :Eporiginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :potential_evaporationtemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>precipitation_era5(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Total Precipitationoriginal_add_offset :0.0original_name :tporiginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily sum', 'Converting to mm from m', 'Resampling by 8-day mean', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :total_precipitationtemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>radiation_era5(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Surface Net Solar Radiationoriginal_add_offset :0.0original_name :ssroriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily mean', 'Resampling by 8-day mean', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :surface_net_downward_shortwave_fluxtemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :J m^-2  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>root_moisture(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Root-zone Soil Moistureoriginal_add_offset :0.0original_name :SMrootoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :root_zone_soil_moisturetemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>sensible_heat(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :FLUXCOMdate_modified :2022-10-17 22:14:30.401506description :FLUXCOMgeospatial_lat_max :89.87499928049999geospatial_lat_min :-89.8750000005geospatial_lat_resolution :0.25geospatial_lon_max :179.87499856049996geospatial_lon_min :-179.8750000005geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Sensible Heatoriginal_add_offset :0.0original_name :Horiginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.5194/bg-13-4291-2016', 'https://doi.org/10.1038/s41597-019-0076-8']reported_day :5.0source :['https://www.fluxcom.org/']standard_name :surface_upward_sensible_heat_fluxtemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :2001-01-05T00:00:00.000000000time_period :8Dunits :MJ m^-2 d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>sif_gome2_jj(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://doi.org/10.5194/essd-12-1101-2020date_modified :2022-10-11 22:36:53.583022description :Spatially Downscaled Sun-Induced Fluorescence (JJ Method)geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000000003geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Downscaled Daily Corrected Sun-Induced Chlorophyll Fluorescence at 740 nmoriginal_add_offset :0.0original_name :SIForiginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation']project :DeepESDLreferences :['https://doi.org/10.5194/essd-12-1101-2020']reported_day :9.0source :['https://data.jrc.ec.europa.eu/dataset/21935ffc-b797-4bee-94da-8fec85b3f9e1']standard_name :siftemporal_resolution :16Dtime_coverage_end :2018-10-04T00:00:00.000000000time_coverage_start :2007-01-21T00:00:00.000000000time_period :8Dunits :m W m^-2 sr^-1 nm^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>sif_gome2_pk(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://doi.org/10.5194/essd-12-1101-2020date_modified :2022-10-11 22:43:08.258033description :Spatially Downscaled Sun-Induced Fluorescence (PK Method)geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000000003geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Downscaled Daily Corrected Sun-Induced Chlorophyll Fluorescence at 740 nmoriginal_add_offset :0.0original_name :SIForiginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean', 'Interpolating NA with linear interpolation']project :DeepESDLreferences :['https://doi.org/10.5194/essd-12-1101-2020']reported_day :9.0source :['https://data.jrc.ec.europa.eu/dataset/21935ffc-b797-4bee-94da-8fec85b3f9e1']standard_name :siftemporal_resolution :16Dtime_coverage_end :2018-12-31T00:00:00.000000000time_coverage_start :2007-01-21T00:00:00.000000000time_period :8Dunits :m W m^-2 sr^-1 nm^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>sif_gosif(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://doi.org/10.3390/rs11050517date_modified :2022-10-11 22:20:05.841847description :GOSIF Solar-Induced Chlorophyll Fluorescence Derived from OCO-2, MODIS, and Reanalysis Datageospatial_lat_max :89.87499999999999geospatial_lat_min :-89.87500000000001geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000000003geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Sun-Induced Chlorophyll Fluorescence at 757 nmoriginal_add_offset :0.0original_name :siforiginal_scale_factor :0.0001processing_steps :['Merging tif files', 'Converting water bodies and snow covered areas to NaN', 'Applying original scale factor', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.3390/rs11050517']reported_day :5.0source :['https://globalecology.unh.edu/data.html']standard_name :siftemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :2000-03-01T00:00:00.000000000time_period :8Dunits :W m^-2 sr^-1 um^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>sif_rtsif(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://doi.org/10.1038/s41597-022-01520-1date_modified :2022-10-12 14:26:02.972963description :Long-term Reconstructed TROPOMI Solar-Induced Fluorescence (RTSIF)geospatial_lat_max :89.87499999999999geospatial_lat_min :-89.87500000000001geospatial_lat_resolution :0.25geospatial_lon_max :179.87500000000003geospatial_lon_min :-179.87499999999997geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Sun-Induced Chlorophyll Fluorescence at 740 nmoriginal_add_offset :0.0original_name :siforiginal_scale_factor :1.0processing_steps :['Merging tif files', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.1038/s41597-022-01520-1']reported_day :5.0source :['https://figshare.com/articles/dataset/RTSIF_dataset/19336346/2']standard_name :siftemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :2001-01-05T00:00:00.000000000time_period :8Dunits :m W m^-2 sr^-1 um^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>sm(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :ESA Soil Moisture Climate Change Initiative (Soil_Moisture_cci)date_modified :2022-10-13 20:42:41.277132description :ESA Soil Moisture Climate Change Initiative (Soil_Moisture_cci): COMBINED product, Version 06.1geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Volumetric Soil Moistureoriginal_add_offset :0.0original_name :smoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by 8-day mean']project :DeepESDLreferences :['https://data.cci.ceda.ac.uk/thredds/fileServer/esacci/soil_moisture/docs/v06.1/ESA_CCI_SM_RD_D2.1_v2_ATBD_v06.1_issue_1.1.pdf', 'https://doi.org/10.5194/essd-11-717-2019', 'https://doi.org/10.1016/j.rse.2017.07.001']reported_day :5.0source :['https://catalogue.ceda.ac.uk/uuid/43d73291472444e6b9c2d2420dbad7d6']standard_name :volumetric_soil_moisturetemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :1979-01-05T00:00:00.000000000time_period :8Dunits :m^3 m^-3  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>snow_sublimation(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Snow Sublimationoriginal_add_offset :0.0original_name :Esoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :snow_sublimationtemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>surface_moisture(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Surface Soil Moistureoriginal_add_offset :0.0original_name :SMsurforiginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :surface_soil_moisturetemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>terrestrial_ecosystem_respiration(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :FLUXCOMdate_modified :2022-10-17 22:14:30.401506description :FLUXCOMgeospatial_lat_max :89.87499928049999geospatial_lat_min :-89.8750000005geospatial_lat_resolution :0.25geospatial_lon_max :179.87499856049996geospatial_lon_min :-179.8750000005geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Terrestrial Ecosystem Respirationoriginal_add_offset :0.0original_name :TERoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Downsampling to 0.25 deg with mean']project :DeepESDLreferences :['https://doi.org/10.5194/bg-13-4291-2016', 'https://doi.org/10.1038/s41597-019-0076-8']reported_day :5.0source :['https://www.fluxcom.org/']standard_name :ecosystem_respiration_carbon_fluxtemporal_resolution :8Dtime_coverage_end :2020-12-30T00:00:00.000000000time_coverage_start :2001-01-05T00:00:00.000000000time_period :8Dunits :g C m^-2 d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li><li>transpiration(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :https://www.gleam.eu/date_modified :2022-10-11 16:47:36.234042description :Global Land Evaporation Amsterdam Model (GLEAM) v3.6ageospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :-0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Transpirationoriginal_add_offset :0.0original_name :Etoriginal_scale_factor :1.0processing_steps :['Merging nc files', 'resampling by 8-day mean']project :DeepESDLreferences :['https://doi.org/10.5194/gmd-10-1903-2017', 'https://doi.org/10.5194/hess-15-453-2011']reported_day :5.0source :['GLEAM v3.6a', 'https://www.gleam.eu/']standard_name :transpirationtemporal_resolution :8Dtime_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1980-01-05T00:00:00.000000000time_period :8Dunits :mm d^-1  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([-89.875, -89.625, -89.375, -89.125, -88.875, -88.625, -88.375, -88.125,\n       -87.875, -87.625,\n       ...\n        87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,  89.375,\n        89.625,  89.875],\n      dtype='float64', name='lat', length=720))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625, -178.375,\n       -178.125, -177.875, -177.625,\n       ...\n        177.625,  177.875,  178.125,  178.375,  178.625,  178.875,  179.125,\n        179.375,  179.625,  179.875],\n      dtype='float64', name='lon', length=1440))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1979-01-05', '1979-01-13', '1979-01-21', '1979-01-29',\n               '1979-02-06', '1979-02-14', '1979-02-22', '1979-03-02',\n               '1979-03-10', '1979-03-18',\n               ...\n               '2021-10-20', '2021-10-28', '2021-11-05', '2021-11-13',\n               '2021-11-21', '2021-11-29', '2021-12-07', '2021-12-15',\n               '2021-12-23', '2021-12-31'],\n              dtype='datetime64[ns]', name='time', length=1978, freq=None))</pre></li></ul></li><li>Attributes: (23)Conventions :CF-1.9acknowledgment :All ESDC data providers are acknowledged inside each variablecontributor_name :['University of Leipzig', 'Max Planck Institute', 'Brockmann Consult GmbH']contributor_url :['https://www.uni-leipzig.de/', 'https://www.mpg.de/en', 'https://www.brockmann-consult.de/']creator_name :['University of Leipzig', 'Brockmann Consult GmbH']creator_url :['https://www.uni-leipzig.de/', 'https://www.brockmann-consult.de/']date_modified :2022-11-25 23:13:03.350030geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25id :esdc-8d-0.25deg-256x128x128-3.0.1license :Terms and conditions of the DeepESDL data distributionproject :DeepESDLpublisher_name :DeepESDL Teampublisher_url :https://www.earthsystemdatalab.net/time_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1979-01-05T00:00:00.000000000time_period :8Dtime_period_reported_day :5.0title :Earth System Data Cube (ESDC) v3.0.1</li></ul> <p>Select a variable, for which you would like to calculate monthly means. In this example, we select air_temperature_2m.</p> In\u00a0[8]: Copied! <pre>air_dataset = dataset[['air_temperature_2m']]\nair_dataset\n</pre> air_dataset = dataset[['air_temperature_2m']] air_dataset Out[8]: <pre>&lt;xarray.Dataset&gt; Size: 8GB\nDimensions:             (time: 1978, lat: 720, lon: 1440)\nCoordinates:\n  * lat                 (lat) float64 6kB -89.88 -89.62 -89.38 ... 89.62 89.88\n  * lon                 (lon) float64 12kB -179.9 -179.6 -179.4 ... 179.6 179.9\n  * time                (time) datetime64[ns] 16kB 1979-01-05 ... 2021-12-31\nData variables:\n    air_temperature_2m  (time, lat, lon) float32 8GB dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;\nAttributes: (12/23)\n    Conventions:                CF-1.9\n    acknowledgment:             All ESDC data providers are acknowledged insi...\n    contributor_name:           ['University of Leipzig', 'Max Planck Institu...\n    contributor_url:            ['https://www.uni-leipzig.de/', 'https://www....\n    creator_name:               ['University of Leipzig', 'Brockmann Consult ...\n    creator_url:                ['https://www.uni-leipzig.de/', 'https://www....\n    ...                         ...\n    publisher_url:              https://www.earthsystemdatalab.net/\n    time_coverage_end:          2021-12-31T00:00:00.000000000\n    time_coverage_start:        1979-01-05T00:00:00.000000000\n    time_period:                8D\n    time_period_reported_day:   5.0\n    title:                      Earth System Data Cube (ESDC) v3.0.1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 1978</li><li>lat: 720</li><li>lon: 1440</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float64-89.88 -89.62 ... 89.62 89.88long_name :latitudestandard_name :latitudeunits :degrees_north<pre>array([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875])</pre></li><li>lon(lon)float64-179.9 -179.6 ... 179.6 179.9long_name :longitudestandard_name :longitudeunits :degrees_east<pre>array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])</pre></li><li>time(time)datetime64[ns]1979-01-05 ... 2021-12-31long_name :timestandard_name :time<pre>array(['1979-01-05T00:00:00.000000000', '1979-01-13T00:00:00.000000000',\n       '1979-01-21T00:00:00.000000000', ..., '2021-12-15T00:00:00.000000000',\n       '2021-12-23T00:00:00.000000000', '2021-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air_temperature_2m(time, lat, lon)float32dask.array&lt;chunksize=(256, 128, 128), meta=np.ndarray&gt;acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Mean Air Temperature at 2 moriginal_add_offset :0.0original_name :t2moriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily mean', 'Converting to \u00b0C from K', 'Resampling by 8-day mean', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :mean_air_temperature_2mtemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :\u00b0C  Array   Chunk   Bytes   7.64 GiB   16.00 MiB   Shape   (1978, 720, 1440)   (256, 128, 128)   Dask graph   576 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 1978 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([-89.875, -89.625, -89.375, -89.125, -88.875, -88.625, -88.375, -88.125,\n       -87.875, -87.625,\n       ...\n        87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,  89.375,\n        89.625,  89.875],\n      dtype='float64', name='lat', length=720))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625, -178.375,\n       -178.125, -177.875, -177.625,\n       ...\n        177.625,  177.875,  178.125,  178.375,  178.625,  178.875,  179.125,\n        179.375,  179.625,  179.875],\n      dtype='float64', name='lon', length=1440))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1979-01-05', '1979-01-13', '1979-01-21', '1979-01-29',\n               '1979-02-06', '1979-02-14', '1979-02-22', '1979-03-02',\n               '1979-03-10', '1979-03-18',\n               ...\n               '2021-10-20', '2021-10-28', '2021-11-05', '2021-11-13',\n               '2021-11-21', '2021-11-29', '2021-12-07', '2021-12-15',\n               '2021-12-23', '2021-12-31'],\n              dtype='datetime64[ns]', name='time', length=1978, freq=None))</pre></li></ul></li><li>Attributes: (23)Conventions :CF-1.9acknowledgment :All ESDC data providers are acknowledged inside each variablecontributor_name :['University of Leipzig', 'Max Planck Institute', 'Brockmann Consult GmbH']contributor_url :['https://www.uni-leipzig.de/', 'https://www.mpg.de/en', 'https://www.brockmann-consult.de/']creator_name :['University of Leipzig', 'Brockmann Consult GmbH']creator_url :['https://www.uni-leipzig.de/', 'https://www.brockmann-consult.de/']date_modified :2022-11-25 23:13:03.350030geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25id :esdc-8d-0.25deg-256x128x128-3.0.1license :Terms and conditions of the DeepESDL data distributionproject :DeepESDLpublisher_name :DeepESDL Teampublisher_url :https://www.earthsystemdatalab.net/time_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1979-01-05T00:00:00.000000000time_period :8Dtime_period_reported_day :5.0title :Earth System Data Cube (ESDC) v3.0.1</li></ul> <p>Next, we will specify how to resample the dataset. It is important to note that this is a lazy operation. After executing the next cell, the <code>air_dataset_monthly</code> should only contain one value for each month in its metadata and structure, even though the data has not yet been resampled.</p> In\u00a0[9]: Copied! <pre>air_dataset_monthly = air_dataset.resample({'time':'1ME'}).mean()\nair_dataset_monthly\n</pre> air_dataset_monthly = air_dataset.resample({'time':'1ME'}).mean() air_dataset_monthly Out[9]: <pre>&lt;xarray.Dataset&gt; Size: 2GB\nDimensions:             (time: 516, lat: 720, lon: 1440)\nCoordinates:\n  * lat                 (lat) float64 6kB -89.88 -89.62 -89.38 ... 89.62 89.88\n  * lon                 (lon) float64 12kB -179.9 -179.6 -179.4 ... 179.6 179.9\n  * time                (time) datetime64[ns] 4kB 1979-01-31 ... 2021-12-31\nData variables:\n    air_temperature_2m  (time, lat, lon) float32 2GB dask.array&lt;chunksize=(1, 128, 128), meta=np.ndarray&gt;\nAttributes: (12/23)\n    Conventions:                CF-1.9\n    acknowledgment:             All ESDC data providers are acknowledged insi...\n    contributor_name:           ['University of Leipzig', 'Max Planck Institu...\n    contributor_url:            ['https://www.uni-leipzig.de/', 'https://www....\n    creator_name:               ['University of Leipzig', 'Brockmann Consult ...\n    creator_url:                ['https://www.uni-leipzig.de/', 'https://www....\n    ...                         ...\n    publisher_url:              https://www.earthsystemdatalab.net/\n    time_coverage_end:          2021-12-31T00:00:00.000000000\n    time_coverage_start:        1979-01-05T00:00:00.000000000\n    time_period:                8D\n    time_period_reported_day:   5.0\n    title:                      Earth System Data Cube (ESDC) v3.0.1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 516</li><li>lat: 720</li><li>lon: 1440</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float64-89.88 -89.62 ... 89.62 89.88long_name :latitudestandard_name :latitudeunits :degrees_north<pre>array([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875])</pre></li><li>lon(lon)float64-179.9 -179.6 ... 179.6 179.9long_name :longitudestandard_name :longitudeunits :degrees_east<pre>array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])</pre></li><li>time(time)datetime64[ns]1979-01-31 ... 2021-12-31long_name :timestandard_name :time<pre>array(['1979-01-31T00:00:00.000000000', '1979-02-28T00:00:00.000000000',\n       '1979-03-31T00:00:00.000000000', ..., '2021-10-31T00:00:00.000000000',\n       '2021-11-30T00:00:00.000000000', '2021-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air_temperature_2m(time, lat, lon)float32dask.array&lt;chunksize=(1, 128, 128), meta=np.ndarray&gt;acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Mean Air Temperature at 2 moriginal_add_offset :0.0original_name :t2moriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily mean', 'Converting to \u00b0C from K', 'Resampling by 8-day mean', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :mean_air_temperature_2mtemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :\u00b0C  Array   Chunk   Bytes   1.99 GiB   64.00 kiB   Shape   (516, 720, 1440)   (1, 128, 128)   Dask graph   37152 chunks in 2067 graph layers   Data type   float32 numpy.ndarray  1440 720 516 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([-89.875, -89.625, -89.375, -89.125, -88.875, -88.625, -88.375, -88.125,\n       -87.875, -87.625,\n       ...\n        87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,  89.375,\n        89.625,  89.875],\n      dtype='float64', name='lat', length=720))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625, -178.375,\n       -178.125, -177.875, -177.625,\n       ...\n        177.625,  177.875,  178.125,  178.375,  178.625,  178.875,  179.125,\n        179.375,  179.625,  179.875],\n      dtype='float64', name='lon', length=1440))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1979-01-31', '1979-02-28', '1979-03-31', '1979-04-30',\n               '1979-05-31', '1979-06-30', '1979-07-31', '1979-08-31',\n               '1979-09-30', '1979-10-31',\n               ...\n               '2021-03-31', '2021-04-30', '2021-05-31', '2021-06-30',\n               '2021-07-31', '2021-08-31', '2021-09-30', '2021-10-31',\n               '2021-11-30', '2021-12-31'],\n              dtype='datetime64[ns]', name='time', length=516, freq='ME'))</pre></li></ul></li><li>Attributes: (23)Conventions :CF-1.9acknowledgment :All ESDC data providers are acknowledged inside each variablecontributor_name :['University of Leipzig', 'Max Planck Institute', 'Brockmann Consult GmbH']contributor_url :['https://www.uni-leipzig.de/', 'https://www.mpg.de/en', 'https://www.brockmann-consult.de/']creator_name :['University of Leipzig', 'Brockmann Consult GmbH']creator_url :['https://www.uni-leipzig.de/', 'https://www.brockmann-consult.de/']date_modified :2022-11-25 23:13:03.350030geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25id :esdc-8d-0.25deg-256x128x128-3.0.1license :Terms and conditions of the DeepESDL data distributionproject :DeepESDLpublisher_name :DeepESDL Teampublisher_url :https://www.earthsystemdatalab.net/time_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1979-01-05T00:00:00.000000000time_period :8Dtime_period_reported_day :5.0title :Earth System Data Cube (ESDC) v3.0.1</li></ul> <p>Next, in order to use the data for further analysis, the monthly resampling needs to be run. The following command will take some time.</p> In\u00a0[10]: Copied! <pre>computed_air_dataset_monthly = air_dataset_monthly.compute()\n</pre> computed_air_dataset_monthly = air_dataset_monthly.compute()  In\u00a0[11]: Copied! <pre>computed_air_dataset_monthly\n</pre> computed_air_dataset_monthly Out[11]: <pre>&lt;xarray.Dataset&gt; Size: 2GB\nDimensions:             (time: 516, lat: 720, lon: 1440)\nCoordinates:\n  * lat                 (lat) float64 6kB -89.88 -89.62 -89.38 ... 89.62 89.88\n  * lon                 (lon) float64 12kB -179.9 -179.6 -179.4 ... 179.6 179.9\n  * time                (time) datetime64[ns] 4kB 1979-01-31 ... 2021-12-31\nData variables:\n    air_temperature_2m  (time, lat, lon) float32 2GB -30.6 -30.6 ... -23.28\nAttributes: (12/23)\n    Conventions:                CF-1.9\n    acknowledgment:             All ESDC data providers are acknowledged insi...\n    contributor_name:           ['University of Leipzig', 'Max Planck Institu...\n    contributor_url:            ['https://www.uni-leipzig.de/', 'https://www....\n    creator_name:               ['University of Leipzig', 'Brockmann Consult ...\n    creator_url:                ['https://www.uni-leipzig.de/', 'https://www....\n    ...                         ...\n    publisher_url:              https://www.earthsystemdatalab.net/\n    time_coverage_end:          2021-12-31T00:00:00.000000000\n    time_coverage_start:        1979-01-05T00:00:00.000000000\n    time_period:                8D\n    time_period_reported_day:   5.0\n    title:                      Earth System Data Cube (ESDC) v3.0.1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 516</li><li>lat: 720</li><li>lon: 1440</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float64-89.88 -89.62 ... 89.62 89.88long_name :latitudestandard_name :latitudeunits :degrees_north<pre>array([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875])</pre></li><li>lon(lon)float64-179.9 -179.6 ... 179.6 179.9long_name :longitudestandard_name :longitudeunits :degrees_east<pre>array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])</pre></li><li>time(time)datetime64[ns]1979-01-31 ... 2021-12-31long_name :timestandard_name :time<pre>array(['1979-01-31T00:00:00.000000000', '1979-02-28T00:00:00.000000000',\n       '1979-03-31T00:00:00.000000000', ..., '2021-10-31T00:00:00.000000000',\n       '2021-11-30T00:00:00.000000000', '2021-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>air_temperature_2m(time, lat, lon)float32-30.6 -30.6 -30.6 ... -23.28 -23.28acknowledgment :ERA5 hourly data on single levels from 1959 to presentdate_modified :2022-11-04 15:41:36.233472description :ERA5 Reanalysis Productsgeospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25license :Terms and conditions of the DeepESDL data distributionlong_name :Mean Air Temperature at 2 moriginal_add_offset :0.0original_name :t2moriginal_scale_factor :1.0processing_steps :['Merging nc files', 'Resampling by daily mean', 'Converting to \u00b0C from K', 'Resampling by 8-day mean', 'Resampling to 0.25 degrees using bilinear interpolation']project :DeepESDLreferences :['https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation']reported_day :5.0source :['https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels']standard_name :mean_air_temperature_2mtemporal_resolution :8Dtime_coverage_end :2021-12-27T00:00:00.000000000time_coverage_start :1979-01-01T00:00:00.000000000time_period :8Dunits :\u00b0C<pre>array([[[-30.604319 , -30.603416 , -30.602505 , ..., -30.607002 ,\n         -30.606113 , -30.605225 ],\n        [-30.997593 , -30.994892 , -30.992188 , ..., -31.00566  ,\n         -31.00301  , -31.000326 ],\n        [-31.401316 , -31.396795 , -31.392254 , ..., -31.415653 ,\n         -31.410519 , -31.405838 ],\n        ...,\n        [-28.86119  , -28.862188 , -28.863615 , ..., -28.858183 ,\n         -28.859447 , -28.86039  ],\n        [-28.825958 , -28.826452 , -28.826956 , ..., -28.824509 ,\n         -28.825    , -28.825487 ],\n        [-28.634472 , -28.634634 , -28.63481  , ..., -28.633997 ,\n         -28.634163 , -28.634321 ]],\n\n       [[-38.47369  , -38.472347 , -38.471    , ..., -38.477657 ,\n         -38.476353 , -38.475033 ],\n        [-38.85001  , -38.845985 , -38.841953 , ..., -38.861927 ,\n         -38.85798  , -38.854004 ],\n        [-39.241528 , -39.234924 , -39.22851  , ..., -39.262314 ,\n         -39.255016 , -39.24818  ],\n...\n        [-20.72065  , -20.718517 , -20.716475 , ..., -20.72755  ,\n         -20.72508  , -20.722828 ],\n        [-20.490475 , -20.489168 , -20.487873 , ..., -20.494385 ,\n         -20.493073 , -20.491772 ],\n        [-20.113615 , -20.113174 , -20.112738 , ..., -20.114912 ,\n         -20.11448  , -20.114038 ]],\n\n       [[-25.563875 , -25.562918 , -25.561962 , ..., -25.566689 ,\n         -25.565746 , -25.564808 ],\n        [-25.990168 , -25.987343 , -25.984467 , ..., -25.998623 ,\n         -25.995815 , -25.992983 ],\n        [-26.477713 , -26.473064 , -26.468483 , ..., -26.49173  ,\n         -26.48712  , -26.482414 ],\n        ...,\n        [-23.76227  , -23.762304 , -23.762413 , ..., -23.76266  ,\n         -23.76236  , -23.762276 ],\n        [-23.610622 , -23.61064  , -23.610657 , ..., -23.610603 ,\n         -23.61062  , -23.610626 ],\n        [-23.282463 , -23.282469 , -23.282478 , ..., -23.282448 ,\n         -23.282454 , -23.28246  ]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([-89.875, -89.625, -89.375, -89.125, -88.875, -88.625, -88.375, -88.125,\n       -87.875, -87.625,\n       ...\n        87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,  89.375,\n        89.625,  89.875],\n      dtype='float64', name='lat', length=720))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625, -178.375,\n       -178.125, -177.875, -177.625,\n       ...\n        177.625,  177.875,  178.125,  178.375,  178.625,  178.875,  179.125,\n        179.375,  179.625,  179.875],\n      dtype='float64', name='lon', length=1440))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1979-01-31', '1979-02-28', '1979-03-31', '1979-04-30',\n               '1979-05-31', '1979-06-30', '1979-07-31', '1979-08-31',\n               '1979-09-30', '1979-10-31',\n               ...\n               '2021-03-31', '2021-04-30', '2021-05-31', '2021-06-30',\n               '2021-07-31', '2021-08-31', '2021-09-30', '2021-10-31',\n               '2021-11-30', '2021-12-31'],\n              dtype='datetime64[ns]', name='time', length=516, freq='ME'))</pre></li></ul></li><li>Attributes: (23)Conventions :CF-1.9acknowledgment :All ESDC data providers are acknowledged inside each variablecontributor_name :['University of Leipzig', 'Max Planck Institute', 'Brockmann Consult GmbH']contributor_url :['https://www.uni-leipzig.de/', 'https://www.mpg.de/en', 'https://www.brockmann-consult.de/']creator_name :['University of Leipzig', 'Brockmann Consult GmbH']creator_url :['https://www.uni-leipzig.de/', 'https://www.brockmann-consult.de/']date_modified :2022-11-25 23:13:03.350030geospatial_lat_max :89.875geospatial_lat_min :-89.875geospatial_lat_resolution :0.25geospatial_lon_max :179.875geospatial_lon_min :-179.875geospatial_lon_resolution :0.25id :esdc-8d-0.25deg-256x128x128-3.0.1license :Terms and conditions of the DeepESDL data distributionproject :DeepESDLpublisher_name :DeepESDL Teampublisher_url :https://www.earthsystemdatalab.net/time_coverage_end :2021-12-31T00:00:00.000000000time_coverage_start :1979-01-05T00:00:00.000000000time_period :8Dtime_period_reported_day :5.0title :Earth System Data Cube (ESDC) v3.0.1</li></ul> In\u00a0[12]: Copied! <pre>cluster.shutdown()\n</pre> cluster.shutdown() <p>Note, resampling by month, will always put the date for the month on the last day of the month per default.</p> In\u00a0[13]: Copied! <pre>computed_air_dataset_monthly.air_temperature_2m.isel(time=1).plot.imshow()\n</pre> computed_air_dataset_monthly.air_temperature_2m.isel(time=1).plot.imshow() Out[13]: <pre>&lt;matplotlib.image.AxesImage at 0x7f4d2265aed0&gt;</pre> <p>If you wish to persist the dataset to your team storage, please checkout the example notebook 05 SAVE CUBE TO TEAM STORAGE.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/How_to_make_use_of_Dask_clusters/#how-to-make-use-of-dask-clusters-for-parallel-computing","title":"How to make use of Dask clusters for parallel computing\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/How_to_make_use_of_Dask_clusters/#a-deepesdl-example-notebook","title":"A DeepESDL example notebook\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/How_to_make_use_of_Dask_clusters/#set-up-dask-cluster","title":"Set up Dask cluster\u00b6","text":"<p>Before starting a cluster, think about a good cluster name as well as how many workers you expect to need for your task. The cluster name is helpful in case your Jupyter notebook crashes or you wish to connect to a running cluster from another notebook. It will then recognize that a cluster with the indicated name is already running and connect to it.\u00a0</p> <p>The following cell creates a cluster with two worker nodes using the default instance type (spot instances). For additional settings, please refer to the xcube documentation.</p> <p>Whenever you start a cluster, please make sure to shut it down with <code>cluster.shutdown()</code> once finished! Otherwise, your processing units will go unused.</p>"},{"location":"guide/jupyterlab/notebooks/How_to_make_use_of_Dask_clusters/#access-data-and-create-monthly-means","title":"Access Data and create monthly means\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/How_to_make_use_of_Dask_clusters/#shut-down-dask-cluster","title":"Shut down Dask cluster\u00b6","text":"<p>Once you are finished with your computations, make sure to shutdown the cluster to minimize costs.</p>"},{"location":"guide/jupyterlab/notebooks/Save_cube_to_team_storage/","title":"Save cube to team storage","text":"In\u00a0[1]: Copied! <pre># mandatory imports\nimport datetime\nimport os\n\nimport matplotlib.pyplot as plt\nimport shapely.geometry\nfrom xcube.core.store import new_data_store\n</pre> # mandatory imports import datetime import os  import matplotlib.pyplot as plt import shapely.geometry from xcube.core.store import new_data_store <p>Configure matplotlib to display graphs inline directly in the notebook and set a sensible default figure size.</p> In\u00a0[2]: Copied! <pre>%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = 16, 8\n</pre> %matplotlib inline plt.rcParams[\"figure.figsize\"] = 16, 8 <p>Provide mandatory parameters to instantiate the store class:</p> In\u00a0[3]: Copied! <pre>store = new_data_store(\"ccizarr\")\n</pre> store = new_data_store(\"ccizarr\") <p>Let's open an example dataset:</p> In\u00a0[4]: Copied! <pre>def open_zarrstore(filename, time_range, variables, bbox):\n    min_lat, min_lon, max_lat, max_lon = bbox\n    ds = store.open_data(filename)\n\n    subset = ds.sel(\n        lat=slice(min_lat, max_lat),\n        lon=slice(min_lon, max_lon),\n        time=slice(time_range[0], time_range[1]),\n    )\n    subset = subset[variables]\n\n    return subset\n\n\ndataset = open_zarrstore(\n    \"ESACCI-LST-L3C-LST-MODISA-0.01deg_1DAILY_DAY-2002-2018-fv3.00.zarr\",\n    time_range=[datetime.datetime(2016, 10, 1), datetime.datetime(2016, 10, 2)],\n    variables=[\"lst\"],\n    bbox=[35, -10, 70, 30],\n)\n</pre> def open_zarrstore(filename, time_range, variables, bbox):     min_lat, min_lon, max_lat, max_lon = bbox     ds = store.open_data(filename)      subset = ds.sel(         lat=slice(min_lat, max_lat),         lon=slice(min_lon, max_lon),         time=slice(time_range[0], time_range[1]),     )     subset = subset[variables]      return subset   dataset = open_zarrstore(     \"ESACCI-LST-L3C-LST-MODISA-0.01deg_1DAILY_DAY-2002-2018-fv3.00.zarr\",     time_range=[datetime.datetime(2016, 10, 1), datetime.datetime(2016, 10, 2)],     variables=[\"lst\"],     bbox=[35, -10, 70, 30], ) In\u00a0[5]: Copied! <pre>dataset\n</pre> dataset Out[5]: <pre>&lt;xarray.Dataset&gt; Size: 224MB\nDimensions:  (time: 2, lat: 3500, lon: 4000)\nCoordinates:\n  * lat      (lat) float32 14kB 35.01 35.01 35.02 35.03 ... 69.97 69.98 69.99\n  * lon      (lon) float32 16kB -9.995 -9.985 -9.975 ... 29.98 29.98 29.99\n  * time     (time) datetime64[ns] 16B 2016-10-01 2016-10-02\nData variables:\n    lst      (time, lat, lon) float64 224MB dask.array&lt;chunksize=(1, 1500, 1000), meta=np.ndarray&gt;\nAttributes: (12/42)\n    Conventions:                CF-1.8\n    catalogue_url:              https://catalogue.ceda.ac.uk/uuid/6babb8d9a8d...\n    cdm_data_type:              grid\n    comment:                    These data were produced as part of the ESA L...\n    creator_email:              djg20@le.ac.uk\n    creator_name:               University of Leicester Surface Temperature G...\n    ...                         ...\n    summary:                    This file contains level L3C global land surf...\n    time_coverage_duration:     PT04M59S\n    time_coverage_end:          19700101T000001Z\n    time_coverage_resolution:   P1D\n    time_coverage_start:        19700101T000001Z\n    title:                      ESA LST CCI land surface temperature data at ...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 2</li><li>lat: 3500</li><li>lon: 4000</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3235.01 35.01 35.02 ... 69.98 69.99long_name :latitude_coordinatesreference_datum :geographical coordinates, WGS84 projectionstandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([35.005   , 35.014996, 35.024998, ..., 69.975   , 69.98499 , 69.99499 ],\n      dtype=float32)</pre></li><li>lon(lon)float32-9.995 -9.985 ... 29.98 29.99long_name :longitude_coordinatesreference_datum :geographical coordinates, WGS84 projectionstandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-9.995   , -9.985005, -9.975011, ..., 29.975   , 29.984995, 29.99499 ],\n      dtype=float32)</pre></li><li>time(time)datetime64[ns]2016-10-01 2016-10-02long_name :reference time of filestandard_name :time<pre>array(['2016-10-01T00:00:00.000000000', '2016-10-02T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>lst(time, lat, lon)float64dask.array&lt;chunksize=(1, 1500, 1000), meta=np.ndarray&gt;long_name :land surface temperatureunits :kelvinvalid_max :7685valid_min :-8315  Array   Chunk   Bytes   213.62 MiB   30.52 MiB   Shape   (2, 3500, 4000)   (1, 2000, 2000)   Dask graph   12 chunks in 3 graph layers   Data type   float64 numpy.ndarray  4000 3500 2 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([35.005001068115234,  35.01499557495117,  35.02499771118164,\n        35.03499984741211,  35.04500198364258, 35.054996490478516,\n       35.064998626708984,  35.07500076293945,  35.08499526977539,\n        35.09499740600586,\n       ...\n        69.90499114990234,  69.91500091552734,  69.92499542236328,\n        69.93498992919922,  69.94499969482422,  69.95499420166016,\n         69.9649887084961,   69.9749984741211,  69.98499298095703,\n        69.99498748779297],\n      dtype='float32', name='lat', length=3500))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-9.994999885559082, -9.985005378723145, -9.975010871887207,\n       -9.965001106262207,  -9.95500659942627,  -9.94499683380127,\n       -9.935002326965332, -9.925007820129395, -9.914998054504395,\n       -9.905003547668457,\n       ...\n       29.904993057250977, 29.914987564086914, 29.924997329711914,\n        29.93499183654785,  29.94500160217285,  29.95499610900879,\n       29.964990615844727, 29.975000381469727, 29.984994888305664,\n         29.9949893951416],\n      dtype='float32', name='lon', length=4000))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2016-10-01', '2016-10-02'], dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (42)Conventions :CF-1.8catalogue_url :https://catalogue.ceda.ac.uk/uuid/6babb8d9a8d247bcb3da6aed42f4b59acdm_data_type :gridcomment :These data were produced as part of the ESA LST CCI project.creator_email :djg20@le.ac.ukcreator_name :University of Leicester Surface Temperature Groupcreator_url :https://climate.esa.int/en/projects/land-surface-temperaturedate_created :20211204T115246Zdoi :10.5285/6babb8d9a8d247bcb3da6aed42f4b59aformat_version :CCI Data Standards v2.2geospatial_lat_max :-68.13521575927734geospatial_lat_min :-89.99625396728516geospatial_lat_resolution :0.009999999776482582geospatial_lat_units :degrees_northgeospatial_lon_max :179.9976348876953geospatial_lon_min :-179.99952697753906geospatial_lon_resolution :0.009999999776482582geospatial_lon_units :degrees_eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0history :Created using software developed at University of Leicesterid :ESACCI-LST-L3C-LST-MODISA-0.01deg_1DAILY_DAY-20181231000000-fv3.00.ncinstitution :University of Leicesterkey_variables :land_surface_temperaturekeywords :Earth Science; Land Surface; Land Temperature; Land Surface Temperaturekeywords_vocabulary :NASA Global change Master Directory (GCMD) Science Keywordslicense :ESA CCI Data Policy: free and open accessnaming_authority :le.ac.ukplatform :Aquaproduct_version :3.00project :Climate Change Initiative - European Space Agencyreferences :https://climate.esa.int/en/projects/land-surface-temperaturesensor :MODISsource :ESA LST CCI MODISA L3U V3.00spatial_resolution :0.01 degreestandard_name_vocabulary :CF Standard Name Table v71summary :This file contains level L3C global land surface temperatures from MODIS. L3C data are derived from multiple orbits of single sensor L3U data combined onto a space and/or time grid.time_coverage_duration :PT04M59Stime_coverage_end :19700101T000001Ztime_coverage_resolution :P1Dtime_coverage_start :19700101T000001Ztitle :ESA LST CCI land surface temperature data at product level L3C from Moderate Resolution Imaging Spectroradiometer.</li></ul> <p>Plot one time stamp of the dataset for a analysed_sst in order to take a brief look at the dataset:</p> In\u00a0[6]: Copied! <pre>dataset.lst.isel(time=0).plot.imshow(cmap=\"plasma\")\n</pre> dataset.lst.isel(time=0).plot.imshow(cmap=\"plasma\") Out[6]: <pre>&lt;matplotlib.image.AxesImage at 0x7f7ebf4a8950&gt;</pre> <p>To store the cube in your teams user space, please first retrieve the details from your environment variables as the following:</p> In\u00a0[7]: Copied! <pre>S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"]\nS3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"]\nS3_USER_STORAGE_BUCKET = os.environ[\"S3_USER_STORAGE_BUCKET\"]\n</pre> S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"] S3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"] S3_USER_STORAGE_BUCKET = os.environ[\"S3_USER_STORAGE_BUCKET\"] <p>You need to instantiate a s3 datastore pointing to the team bucket:</p> In\u00a0[8]: Copied! <pre>team_store = new_data_store(\n    \"s3\",\n    root=S3_USER_STORAGE_BUCKET,\n    storage_options=dict(\n        anon=False, key=S3_USER_STORAGE_KEY, secret=S3_USER_STORAGE_SECRET\n    ),\n)\n</pre> team_store = new_data_store(     \"s3\",     root=S3_USER_STORAGE_BUCKET,     storage_options=dict(         anon=False, key=S3_USER_STORAGE_KEY, secret=S3_USER_STORAGE_SECRET     ), ) <p>If you have stored no data to your user space, the returned list will be empty:</p> In\u00a0[9]: Copied! <pre>list(team_store.get_data_ids())\n</pre> list(team_store.get_data_ids()) Out[9]: <pre>['SST.levels',\n 'amazonas_v8.zarr',\n 'amazonas_v9.zarr',\n 'noise_trajectory.zarr',\n 'reanalysis-era5-single-levels-monthly-means-subset-2001-2010_TMH.zarr']</pre> <p>Save in levels format to generate data pyramid for efficient visualisation</p> In\u00a0[10]: Copied! <pre>team_store.write_data(\n    dataset, \"analysed_lst.levels\", replace=True, use_saved_levels=True\n)\n</pre> team_store.write_data(     dataset, \"analysed_lst.levels\", replace=True, use_saved_levels=True ) Out[10]: <pre>'analysed_lst.levels'</pre> <p>If you list the content of you datastore again, you will now see the newly written dataset in the list:</p> In\u00a0[11]: Copied! <pre>list(team_store.get_data_ids())\n</pre> list(team_store.get_data_ids()) Out[11]: <pre>['SST.levels',\n 'amazonas_v8.zarr',\n 'amazonas_v9.zarr',\n 'analysed_lst.levels',\n 'noise_trajectory.zarr',\n 'reanalysis-era5-single-levels-monthly-means-subset-2001-2010_TMH.zarr']</pre> <p>Open data from your team storage:</p> In\u00a0[12]: Copied! <pre>ml_dataset = team_store.open_data(\"analysed_lst.levels\")\nml_dataset\n</pre> ml_dataset = team_store.open_data(\"analysed_lst.levels\") ml_dataset Out[12]: <pre>&lt;xcube.core.mldataset.fs.FsMultiLevelDataset at 0x7f7ebe387290&gt;</pre> <p>Check, how many levels were written:</p> In\u00a0[13]: Copied! <pre>ml_dataset.num_levels\n</pre> ml_dataset.num_levels Out[13]: <pre>2</pre> <p>Print metadata details for each multilevel dataset</p> In\u00a0[14]: Copied! <pre>for level in range(ml_dataset.num_levels):\n    dataset_i = ml_dataset.get_dataset(level)\n    display(dataset_i)\n</pre> for level in range(ml_dataset.num_levels):     dataset_i = ml_dataset.get_dataset(level)     display(dataset_i) <pre>&lt;xarray.Dataset&gt; Size: 224MB\nDimensions:  (lat: 3500, lon: 4000, time: 2)\nCoordinates:\n  * lat      (lat) float32 14kB 35.01 35.01 35.02 35.03 ... 69.97 69.98 69.99\n  * lon      (lon) float32 16kB -9.995 -9.985 -9.975 ... 29.98 29.98 29.99\n  * time     (time) datetime64[ns] 16B 2016-10-01 2016-10-02\nData variables:\n    lst      (time, lat, lon) float64 224MB dask.array&lt;chunksize=(1, 2000, 2000), meta=np.ndarray&gt;\nAttributes: (12/42)\n    Conventions:                CF-1.8\n    catalogue_url:              https://catalogue.ceda.ac.uk/uuid/6babb8d9a8d...\n    cdm_data_type:              grid\n    comment:                    These data were produced as part of the ESA L...\n    creator_email:              djg20@le.ac.uk\n    creator_name:               University of Leicester Surface Temperature G...\n    ...                         ...\n    summary:                    This file contains level L3C global land surf...\n    time_coverage_duration:     PT04M59S\n    time_coverage_end:          19700101T000001Z\n    time_coverage_resolution:   P1D\n    time_coverage_start:        19700101T000001Z\n    title:                      ESA LST CCI land surface temperature data at ...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 3500</li><li>lon: 4000</li><li>time: 2</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3235.01 35.01 35.02 ... 69.98 69.99long_name :latitude_coordinatesreference_datum :geographical coordinates, WGS84 projectionstandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([35.005   , 35.014996, 35.024998, ..., 69.975   , 69.98499 , 69.99499 ],\n      dtype=float32)</pre></li><li>lon(lon)float32-9.995 -9.985 ... 29.98 29.99long_name :longitude_coordinatesreference_datum :geographical coordinates, WGS84 projectionstandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-9.995   , -9.985005, -9.975011, ..., 29.975   , 29.984995, 29.99499 ],\n      dtype=float32)</pre></li><li>time(time)datetime64[ns]2016-10-01 2016-10-02long_name :reference time of filestandard_name :time<pre>array(['2016-10-01T00:00:00.000000000', '2016-10-02T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>lst(time, lat, lon)float64dask.array&lt;chunksize=(1, 2000, 2000), meta=np.ndarray&gt;long_name :land surface temperatureunits :kelvinvalid_max :7685valid_min :-8315  Array   Chunk   Bytes   213.62 MiB   30.52 MiB   Shape   (2, 3500, 4000)   (1, 2000, 2000)   Dask graph   8 chunks in 2 graph layers   Data type   float64 numpy.ndarray  4000 3500 2 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([35.005001068115234,  35.01499557495117,  35.02499771118164,\n        35.03499984741211,  35.04500198364258, 35.054996490478516,\n       35.064998626708984,  35.07500076293945,  35.08499526977539,\n        35.09499740600586,\n       ...\n        69.90499114990234,  69.91500091552734,  69.92499542236328,\n        69.93498992919922,  69.94499969482422,  69.95499420166016,\n         69.9649887084961,   69.9749984741211,  69.98499298095703,\n        69.99498748779297],\n      dtype='float32', name='lat', length=3500))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-9.994999885559082, -9.985005378723145, -9.975010871887207,\n       -9.965001106262207,  -9.95500659942627,  -9.94499683380127,\n       -9.935002326965332, -9.925007820129395, -9.914998054504395,\n       -9.905003547668457,\n       ...\n       29.904993057250977, 29.914987564086914, 29.924997329711914,\n        29.93499183654785,  29.94500160217285,  29.95499610900879,\n       29.964990615844727, 29.975000381469727, 29.984994888305664,\n         29.9949893951416],\n      dtype='float32', name='lon', length=4000))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2016-10-01', '2016-10-02'], dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (42)Conventions :CF-1.8catalogue_url :https://catalogue.ceda.ac.uk/uuid/6babb8d9a8d247bcb3da6aed42f4b59acdm_data_type :gridcomment :These data were produced as part of the ESA LST CCI project.creator_email :djg20@le.ac.ukcreator_name :University of Leicester Surface Temperature Groupcreator_url :https://climate.esa.int/en/projects/land-surface-temperaturedate_created :20211204T115246Zdoi :10.5285/6babb8d9a8d247bcb3da6aed42f4b59aformat_version :CCI Data Standards v2.2geospatial_lat_max :-68.13521575927734geospatial_lat_min :-89.99625396728516geospatial_lat_resolution :0.009999999776482582geospatial_lat_units :degrees_northgeospatial_lon_max :179.9976348876953geospatial_lon_min :-179.99952697753906geospatial_lon_resolution :0.009999999776482582geospatial_lon_units :degrees_eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0history :Created using software developed at University of Leicesterid :ESACCI-LST-L3C-LST-MODISA-0.01deg_1DAILY_DAY-20181231000000-fv3.00.ncinstitution :University of Leicesterkey_variables :land_surface_temperaturekeywords :Earth Science; Land Surface; Land Temperature; Land Surface Temperaturekeywords_vocabulary :NASA Global change Master Directory (GCMD) Science Keywordslicense :ESA CCI Data Policy: free and open accessnaming_authority :le.ac.ukplatform :Aquaproduct_version :3.00project :Climate Change Initiative - European Space Agencyreferences :https://climate.esa.int/en/projects/land-surface-temperaturesensor :MODISsource :ESA LST CCI MODISA L3U V3.00spatial_resolution :0.01 degreestandard_name_vocabulary :CF Standard Name Table v71summary :This file contains level L3C global land surface temperatures from MODIS. L3C data are derived from multiple orbits of single sensor L3U data combined onto a space and/or time grid.time_coverage_duration :PT04M59Stime_coverage_end :19700101T000001Ztime_coverage_resolution :P1Dtime_coverage_start :19700101T000001Ztitle :ESA LST CCI land surface temperature data at product level L3C from Moderate Resolution Imaging Spectroradiometer.</li></ul> <pre>&lt;xarray.Dataset&gt; Size: 56MB\nDimensions:  (lat: 1750, lon: 2000, time: 2)\nCoordinates:\n  * lat      (lat) float32 7kB 35.01 35.02 35.05 35.06 ... 69.94 69.96 69.98\n  * lon      (lon) float32 8kB -9.995 -9.975 -9.955 -9.935 ... 29.95 29.96 29.98\n  * time     (time) datetime64[ns] 16B 2016-10-01 2016-10-02\nData variables:\n    lst      (time, lat, lon) float64 56MB dask.array&lt;chunksize=(1, 1750, 2000), meta=np.ndarray&gt;\nAttributes: (12/42)\n    Conventions:                CF-1.8\n    catalogue_url:              https://catalogue.ceda.ac.uk/uuid/6babb8d9a8d...\n    cdm_data_type:              grid\n    comment:                    These data were produced as part of the ESA L...\n    creator_email:              djg20@le.ac.uk\n    creator_name:               University of Leicester Surface Temperature G...\n    ...                         ...\n    summary:                    This file contains level L3C global land surf...\n    time_coverage_duration:     PT04M59S\n    time_coverage_end:          19700101T000001Z\n    time_coverage_resolution:   P1D\n    time_coverage_start:        19700101T000001Z\n    title:                      ESA LST CCI land surface temperature data at ...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 1750</li><li>lon: 2000</li><li>time: 2</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3235.01 35.02 35.05 ... 69.96 69.98long_name :latitude_coordinatesreference_datum :geographical coordinates, WGS84 projectionstandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([35.005   , 35.024998, 35.045002, ..., 69.945   , 69.96499 , 69.98499 ],\n      dtype=float32)</pre></li><li>lon(lon)float32-9.995 -9.975 ... 29.96 29.98long_name :longitude_coordinatesreference_datum :geographical coordinates, WGS84 projectionstandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-9.995   , -9.975011, -9.955007, ..., 29.945002, 29.96499 , 29.984995],\n      dtype=float32)</pre></li><li>time(time)datetime64[ns]2016-10-01 2016-10-02long_name :reference time of filestandard_name :time<pre>array(['2016-10-01T00:00:00.000000000', '2016-10-02T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>lst(time, lat, lon)float64dask.array&lt;chunksize=(1, 1750, 2000), meta=np.ndarray&gt;long_name :land surface temperatureunits :kelvinvalid_max :7685valid_min :-8315  Array   Chunk   Bytes   53.41 MiB   26.70 MiB   Shape   (2, 1750, 2000)   (1, 1750, 2000)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2000 1750 2 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([35.005001068115234,  35.02499771118164,  35.04500198364258,\n       35.064998626708984,  35.08499526977539,  35.10499954223633,\n       35.124996185302734,  35.14500045776367,  35.16499710083008,\n       35.185001373291016,\n       ...\n        69.80500030517578,  69.82498931884766,   69.8449935913086,\n        69.86499786376953,   69.8849868774414,  69.90499114990234,\n        69.92499542236328,  69.94499969482422,   69.9649887084961,\n        69.98499298095703],\n      dtype='float32', name='lat', length=1750))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-9.994999885559082, -9.975010871887207,  -9.95500659942627,\n       -9.935002326965332, -9.914998054504395,  -9.89500904083252,\n       -9.875004768371582, -9.855000495910645, -9.834996223449707,\n       -9.815007209777832,\n       ...\n        29.80498695373535,  29.82499122619629, 29.844995498657227,\n       29.864999771118164,  29.88498878479004, 29.904993057250977,\n       29.924997329711914,  29.94500160217285, 29.964990615844727,\n       29.984994888305664],\n      dtype='float32', name='lon', length=2000))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2016-10-01', '2016-10-02'], dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (42)Conventions :CF-1.8catalogue_url :https://catalogue.ceda.ac.uk/uuid/6babb8d9a8d247bcb3da6aed42f4b59acdm_data_type :gridcomment :These data were produced as part of the ESA LST CCI project.creator_email :djg20@le.ac.ukcreator_name :University of Leicester Surface Temperature Groupcreator_url :https://climate.esa.int/en/projects/land-surface-temperaturedate_created :20211204T115246Zdoi :10.5285/6babb8d9a8d247bcb3da6aed42f4b59aformat_version :CCI Data Standards v2.2geospatial_lat_max :-68.13521575927734geospatial_lat_min :-89.99625396728516geospatial_lat_resolution :0.009999999776482582geospatial_lat_units :degrees_northgeospatial_lon_max :179.9976348876953geospatial_lon_min :-179.99952697753906geospatial_lon_resolution :0.009999999776482582geospatial_lon_units :degrees_eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0history :Created using software developed at University of Leicesterid :ESACCI-LST-L3C-LST-MODISA-0.01deg_1DAILY_DAY-20181231000000-fv3.00.ncinstitution :University of Leicesterkey_variables :land_surface_temperaturekeywords :Earth Science; Land Surface; Land Temperature; Land Surface Temperaturekeywords_vocabulary :NASA Global change Master Directory (GCMD) Science Keywordslicense :ESA CCI Data Policy: free and open accessnaming_authority :le.ac.ukplatform :Aquaproduct_version :3.00project :Climate Change Initiative - European Space Agencyreferences :https://climate.esa.int/en/projects/land-surface-temperaturesensor :MODISsource :ESA LST CCI MODISA L3U V3.00spatial_resolution :0.01 degreestandard_name_vocabulary :CF Standard Name Table v71summary :This file contains level L3C global land surface temperatures from MODIS. L3C data are derived from multiple orbits of single sensor L3U data combined onto a space and/or time grid.time_coverage_duration :PT04M59Stime_coverage_end :19700101T000001Ztime_coverage_resolution :P1Dtime_coverage_start :19700101T000001Ztitle :ESA LST CCI land surface temperature data at product level L3C from Moderate Resolution Imaging Spectroradiometer.</li></ul> <p>To work with a certain level:</p> In\u00a0[15]: Copied! <pre>dataset_1 = ml_dataset.get_dataset(1)\n</pre> dataset_1 = ml_dataset.get_dataset(1) In\u00a0[16]: Copied! <pre>dataset_1\n</pre> dataset_1 Out[16]: <pre>&lt;xarray.Dataset&gt; Size: 56MB\nDimensions:  (lat: 1750, lon: 2000, time: 2)\nCoordinates:\n  * lat      (lat) float32 7kB 35.01 35.02 35.05 35.06 ... 69.94 69.96 69.98\n  * lon      (lon) float32 8kB -9.995 -9.975 -9.955 -9.935 ... 29.95 29.96 29.98\n  * time     (time) datetime64[ns] 16B 2016-10-01 2016-10-02\nData variables:\n    lst      (time, lat, lon) float64 56MB dask.array&lt;chunksize=(1, 1750, 2000), meta=np.ndarray&gt;\nAttributes: (12/42)\n    Conventions:                CF-1.8\n    catalogue_url:              https://catalogue.ceda.ac.uk/uuid/6babb8d9a8d...\n    cdm_data_type:              grid\n    comment:                    These data were produced as part of the ESA L...\n    creator_email:              djg20@le.ac.uk\n    creator_name:               University of Leicester Surface Temperature G...\n    ...                         ...\n    summary:                    This file contains level L3C global land surf...\n    time_coverage_duration:     PT04M59S\n    time_coverage_end:          19700101T000001Z\n    time_coverage_resolution:   P1D\n    time_coverage_start:        19700101T000001Z\n    title:                      ESA LST CCI land surface temperature data at ...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 1750</li><li>lon: 2000</li><li>time: 2</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float3235.01 35.02 35.05 ... 69.96 69.98long_name :latitude_coordinatesreference_datum :geographical coordinates, WGS84 projectionstandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([35.005   , 35.024998, 35.045002, ..., 69.945   , 69.96499 , 69.98499 ],\n      dtype=float32)</pre></li><li>lon(lon)float32-9.995 -9.975 ... 29.96 29.98long_name :longitude_coordinatesreference_datum :geographical coordinates, WGS84 projectionstandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-9.995   , -9.975011, -9.955007, ..., 29.945002, 29.96499 , 29.984995],\n      dtype=float32)</pre></li><li>time(time)datetime64[ns]2016-10-01 2016-10-02long_name :reference time of filestandard_name :time<pre>array(['2016-10-01T00:00:00.000000000', '2016-10-02T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>lst(time, lat, lon)float64dask.array&lt;chunksize=(1, 1750, 2000), meta=np.ndarray&gt;long_name :land surface temperatureunits :kelvinvalid_max :7685valid_min :-8315  Array   Chunk   Bytes   53.41 MiB   26.70 MiB   Shape   (2, 1750, 2000)   (1, 1750, 2000)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2000 1750 2 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([35.005001068115234,  35.02499771118164,  35.04500198364258,\n       35.064998626708984,  35.08499526977539,  35.10499954223633,\n       35.124996185302734,  35.14500045776367,  35.16499710083008,\n       35.185001373291016,\n       ...\n        69.80500030517578,  69.82498931884766,   69.8449935913086,\n        69.86499786376953,   69.8849868774414,  69.90499114990234,\n        69.92499542236328,  69.94499969482422,   69.9649887084961,\n        69.98499298095703],\n      dtype='float32', name='lat', length=1750))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-9.994999885559082, -9.975010871887207,  -9.95500659942627,\n       -9.935002326965332, -9.914998054504395,  -9.89500904083252,\n       -9.875004768371582, -9.855000495910645, -9.834996223449707,\n       -9.815007209777832,\n       ...\n        29.80498695373535,  29.82499122619629, 29.844995498657227,\n       29.864999771118164,  29.88498878479004, 29.904993057250977,\n       29.924997329711914,  29.94500160217285, 29.964990615844727,\n       29.984994888305664],\n      dtype='float32', name='lon', length=2000))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2016-10-01', '2016-10-02'], dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (42)Conventions :CF-1.8catalogue_url :https://catalogue.ceda.ac.uk/uuid/6babb8d9a8d247bcb3da6aed42f4b59acdm_data_type :gridcomment :These data were produced as part of the ESA LST CCI project.creator_email :djg20@le.ac.ukcreator_name :University of Leicester Surface Temperature Groupcreator_url :https://climate.esa.int/en/projects/land-surface-temperaturedate_created :20211204T115246Zdoi :10.5285/6babb8d9a8d247bcb3da6aed42f4b59aformat_version :CCI Data Standards v2.2geospatial_lat_max :-68.13521575927734geospatial_lat_min :-89.99625396728516geospatial_lat_resolution :0.009999999776482582geospatial_lat_units :degrees_northgeospatial_lon_max :179.9976348876953geospatial_lon_min :-179.99952697753906geospatial_lon_resolution :0.009999999776482582geospatial_lon_units :degrees_eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0history :Created using software developed at University of Leicesterid :ESACCI-LST-L3C-LST-MODISA-0.01deg_1DAILY_DAY-20181231000000-fv3.00.ncinstitution :University of Leicesterkey_variables :land_surface_temperaturekeywords :Earth Science; Land Surface; Land Temperature; Land Surface Temperaturekeywords_vocabulary :NASA Global change Master Directory (GCMD) Science Keywordslicense :ESA CCI Data Policy: free and open accessnaming_authority :le.ac.ukplatform :Aquaproduct_version :3.00project :Climate Change Initiative - European Space Agencyreferences :https://climate.esa.int/en/projects/land-surface-temperaturesensor :MODISsource :ESA LST CCI MODISA L3U V3.00spatial_resolution :0.01 degreestandard_name_vocabulary :CF Standard Name Table v71summary :This file contains level L3C global land surface temperatures from MODIS. L3C data are derived from multiple orbits of single sensor L3U data combined onto a space and/or time grid.time_coverage_duration :PT04M59Stime_coverage_end :19700101T000001Ztime_coverage_resolution :P1Dtime_coverage_start :19700101T000001Ztitle :ESA LST CCI land surface temperature data at product level L3C from Moderate Resolution Imaging Spectroradiometer.</li></ul> <p>To delete a dataset from your team user space:</p> In\u00a0[17]: Copied! <pre>team_store.delete_data(\"analysed_lst.levels\")\n</pre> team_store.delete_data(\"analysed_lst.levels\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/Save_cube_to_team_storage/#save-a-datacube-to-your-team-storage","title":"Save a datacube to your team storage\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Save_cube_to_team_storage/#a-deepesdl-example-notebook","title":"A DeepESDL example notebook\u00b6","text":"<p>This notebook demonstrates how save a generated datacube to your team storage. In the example the CCI data store is used, for details concerning the CCI store are given in example notebook 03_Generate_CCI_cubes.ipynb</p> <p>Please, also refer to the DeepESDL documentation and visit the platform's website for further information!</p> <p>Brockmann Consult, 2024</p> <p>This notebook runs with the python environment <code>deepesdl-xcube-1.7.0</code>, please checkout the documentation for help on changing the environment.</p>"},{"location":"guide/jupyterlab/notebooks/Spatial_Regridding_and_Masking/","title":"Spatial Regridding and Masking","text":"In\u00a0[1]: Copied! <pre>import datetime\n\nimport numpy as np\nimport pandas as pd\nimport shapely.geometry\nimport xarray as xr\nfrom IPython.display import JSON\nfrom xcube.core.gridmapping import GridMapping\nfrom xcube.core.maskset import MaskSet\nfrom xcube.core.resampling import resample_in_space\nfrom xcube.core.store import new_data_store\n</pre> import datetime  import numpy as np import pandas as pd import shapely.geometry import xarray as xr from IPython.display import JSON from xcube.core.gridmapping import GridMapping from xcube.core.maskset import MaskSet from xcube.core.resampling import resample_in_space from xcube.core.store import new_data_store <p>Open CCI store</p> In\u00a0[2]: Copied! <pre>cci_store = new_data_store(\"ccizarr\")\n</pre> cci_store = new_data_store(\"ccizarr\") <p>Open s3 store and list all datasets</p> In\u00a0[3]: Copied! <pre>root = \"deep-esdl-public\"\n</pre> root = \"deep-esdl-public\" In\u00a0[4]: Copied! <pre>s3_store = new_data_store(\"s3\", root=root)\n</pre> s3_store = new_data_store(\"s3\", root=root) In\u00a0[5]: Copied! <pre>list(s3_store.get_data_ids())\n</pre> list(s3_store.get_data_ids()) Out[5]: <pre>['LC-1x2160x2160-1.0.0.levels',\n 'SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000.levels',\n 'SMOS-L2C-OS-20230101-20231231-1W-res0-53x120x120.zarr',\n 'SMOS-L2C-OS-20230101-20231231-1W-res0.zarr',\n 'SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000.levels',\n 'SMOS-L2C-SM-20230101-20231231-1W-res0-53x120x120.zarr',\n 'SMOS-L2C-SM-20230101-20231231-1W-res0.zarr',\n 'SMOS-freezethaw-1x720x720-1.0.1.zarr',\n 'SMOS-freezethaw-4267x10x10-1.0.1.zarr',\n 'SeasFireCube_v3.zarr',\n 'black-sea-1x1024x1024.levels',\n 'black-sea-256x128x128.zarr',\n 'esa-cci-permafrost-1x1151x1641-0.0.2.levels',\n 'esdc-8d-0.25deg-1x720x1440-3.0.1.zarr',\n 'esdc-8d-0.25deg-256x128x128-3.0.1.zarr',\n 'hydrology-1D-0.009deg-100x60x60-3.0.2.zarr',\n 'hydrology-1D-0.009deg-1418x70x76-2.0.0.zarr',\n 'hydrology-1D-0.009deg-1x1102x2415-2.0.0.levels',\n 'hydrology-1D-0.009deg-1x1102x966-3.0.2.levels',\n 'ocean-1M-9km-1x1080x1080-1.4.0.levels',\n 'ocean-1M-9km-64x256x256-1.4.0.zarr',\n 'polar-100m-1x2048x2048-1.0.1.zarr']</pre> <p>Open Land Cover (LC) dataset from s3 bucket, which is saved as a multilevel dataset:</p> In\u00a0[6]: Copied! <pre>ml_LC = s3_store.open_data(\"LC-1x2160x2160-1.0.0.levels\", decode_cf=True)\n</pre> ml_LC = s3_store.open_data(\"LC-1x2160x2160-1.0.0.levels\", decode_cf=True) <p>Lets open level 0, which is the base level and therefore has the highest resolution:</p> In\u00a0[7]: Copied! <pre>LC = ml_LC.get_dataset(0)\n</pre> LC = ml_LC.get_dataset(0) <p>Now let's search for soil datasets provided via the xcube cci store:</p> In\u00a0[8]: Copied! <pre>cci_store.list_data_ids()\n</pre> cci_store.list_data_ids() Out[8]: <pre>['ESACCI-BIOMASS-L4-AGB-MERGED-100m-2010-2018-fv2.0.zarr',\n 'ESACCI-BIOMASS-L4-AGB-MERGED-100m-2010-2020-fv4.0.zarr',\n 'ESACCI-GHG-L2-CH4-SCIAMACHY-WFMD-2002-2011-fv1.zarr',\n 'ESACCI-GHG-L2-CO2-OCO-2-FOCAL-2014-2021-v10.zarr',\n 'ESACCI-GHG-L2-CO2-SCIAMACHY-WFMD-2002-2012-fv1.zarr',\n 'ESACCI-ICESHEETS_Antarctica_GMB-2002-2016-v1.1.zarr',\n 'ESACCI-ICESHEETS_Greenland_GMB-2003-2016-v1.1.zarr',\n 'ESACCI-L3C_CLOUD-CLD_PRODUCTS-AVHRR_NOAA-1982-2016-fv3.0.zarr',\n 'ESACCI-L3C_SNOW-SWE-1979-2018-fv1.0.zarr',\n 'ESACCI-L3C_SNOW-SWE-1979-2020-fv2.0.zarr',\n 'ESACCI-L4_GHRSST-SST-GMPE-GLOB_CDR2.0-1981-2016-v02.0-fv01.0.zarr',\n 'ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992-2015-v2.0.7b.zarr',\n 'ESACCI-LST-L3C-LST-MODISA-0.01deg_1DAILY_DAY-2002-2018-fv3.00.zarr',\n 'ESACCI-LST-L3C-LST-MODISA-0.01deg_1DAILY_NIGHT-2002-2018-fv3.00.zarr',\n 'ESACCI-LST-L3S-LST-IRCDR_-0.01deg_1DAILY_DAY-1995-2020-fv3.00.zarr',\n 'ESACCI-LST-L3S-LST-IRCDR_-0.01deg_1DAILY_NIGHT-1995-2020-fv3.00.zarr',\n 'ESACCI-LST-L3S-LST-IRCDR_-0.01deg_1MONTHLY_DAY-1995-2020-fv3.00.zarr',\n 'ESACCI-LST-L3S-LST-IRCDR_-0.01deg_1MONTHLY_NIGHT-1995-2020-fv3.00.zarr',\n 'ESACCI-OC-L3S-IOP-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-1997-2020-fv5.0.zarr',\n 'ESACCI-OC-L3S-IOP-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-IOP-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-IOP-MERGED-1Y_YEARLY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-IOP-MERGED-5D_DAILY_4km_GEO_PML_OCx_QAA-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-IOP-MERGED-8D_DAILY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1D_DAILY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1M_MONTHLY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-OC_PRODUCTS-MERGED-1Y_YEARLY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-OC_PRODUCTS-MERGED-5D_DAILY_4km_GEO_PML_OCx_QAA-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-OC_PRODUCTS-MERGED-8D_DAILY_4km_GEO_PML_OCx_QAA-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-RRS-MERGED-1M_MONTHLY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-RRS-MERGED-1Y_YEARLY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-RRS-MERGED-1D_DAILY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-RRS-MERGED-5D_DAILY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr',\n 'ESACCI-OC-L3S-RRS-MERGED-8D_DAILY_4km_GEO_PML_RRS-1997-2022-fv6.0.zarr',\n 'ESACCI-PERMAFROST-L4-ALT-MODISLST-AREA4_PP-1997-2018-fv02.0.zarr',\n 'ESACCI-SEAICE-L3C-SITHICK-RA2_ENVISAT-NH25KMEASE2-2002-2012-fv2.0.zarr',\n 'ESACCI-SEAICE-L3C-SITHICK-SIRAL_CRYOSAT2-NH25KMEASE2-2010-2017-fv2.0.zarr',\n 'ESACCI-SEAICE-L4-SICONC-AMSR_50.0kmEASE2-NH-2002-2017-fv2.1.zarr',\n 'ESACCI-SEALEVEL-IND-MSLTR-MERGED-1993-2016-fv02.zarr',\n 'ESACCI-SEALEVEL-L4-MSLA-MERGED-1993-2015-fv02.zarr',\n 'ESACCI-SOILMOISTURE-L3S-SSMV-COMBINED-1978-2020-fv05.3.zarr',\n 'ESACCI-SOILMOISTURE-L3S-SSMV-COMBINED-1978-2021-fv07.1.zarr']</pre> <p>Open Soil Moisture (SM) dataset from xcube cci store:</p> In\u00a0[9]: Copied! <pre>SM = cci_store.open_data(\"ESACCI-SOILMOISTURE-L3S-SSMV-COMBINED-1978-2021-fv07.1.zarr\")\n</pre> SM = cci_store.open_data(\"ESACCI-SOILMOISTURE-L3S-SSMV-COMBINED-1978-2021-fv07.1.zarr\") In\u00a0[10]: Copied! <pre>SM\n</pre> SM Out[10]: <pre>&lt;xarray.Dataset&gt; Size: 654GB\nDimensions:         (time: 15767, lat: 720, lon: 1440)\nCoordinates:\n  * lat             (lat) float64 6kB 89.88 89.62 89.38 ... -89.38 -89.62 -89.88\n  * lon             (lon) float64 12kB -179.9 -179.6 -179.4 ... 179.6 179.9\n  * time            (time) datetime64[ns] 126kB 1978-11-01 ... 2021-12-31\nData variables:\n    dnflag          (time, lat, lon) float32 65GB dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;\n    flag            (time, lat, lon) float32 65GB dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;\n    freqbandID      (time, lat, lon) float32 65GB dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;\n    mode            (time, lat, lon) float32 65GB dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;\n    sensor          (time, lat, lon) float64 131GB dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;\n    sm              (time, lat, lon) float32 65GB dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;\n    sm_uncertainty  (time, lat, lon) float32 65GB dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;\n    t0              (time, lat, lon) float64 131GB dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;\nAttributes: (12/44)\n    Conventions:                  CF-1.9\n    cdm_data_type:                Grid\n    comment:                      This dataset was produced with funding of t...\n    contact:                      cci_sm_contact@eodc.eu\n    creator_email:                cci_sm_developer@eodc.eu\n    creator_name:                 Department of Geodesy and Geoinformation, V...\n    ...                           ...\n    time_coverage_end_product:    20211231T235959Z\n    time_coverage_resolution:     P1D\n    time_coverage_start:          1978-11-01 00:00:00\n    time_coverage_start_product:  19781101T000000Z\n    title:                        ESA CCI Surface Soil Moisture COMBINED acti...\n    tracking_id:                  ad35798e-58e0-488f-b5b9-593874a47700</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 15767</li><li>lat: 720</li><li>lon: 1440</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6489.88 89.62 89.38 ... -89.62 -89.88_CoordinateAxisType :Latstandard_name :latitudeunits :degrees_northvalid_range :[-90.0, 90.0]<pre>array([ 89.875,  89.625,  89.375, ..., -89.375, -89.625, -89.875])</pre></li><li>lon(lon)float64-179.9 -179.6 ... 179.6 179.9_CoordinateAxisType :Lonstandard_name :longitudeunits :degrees_eastvalid_range :[-180.0, 180.0]<pre>array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])</pre></li><li>time(time)datetime64[ns]1978-11-01 ... 2021-12-31<pre>array(['1978-11-01T00:00:00.000000000', '1978-11-02T00:00:00.000000000',\n       '1978-11-03T00:00:00.000000000', ..., '2021-12-29T00:00:00.000000000',\n       '2021-12-30T00:00:00.000000000', '2021-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (8)<ul><li>dnflag(time, lat, lon)float32dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['NaN', 'day', 'night']bits :['0b0', '0b1', '0b10']dtype :int8flag_meanings :['NaN', 'day', 'night', 'day_night_combination']flag_values :[0, 1, 2, 3]long_name :Day / Night Flagvalid_range :[0, 3]  Array   Chunk   Bytes   60.90 GiB   31.64 MiB   Shape   (15767, 720, 1440)   (16, 720, 720)   Dask graph   1972 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 15767 </li><li>flag(time, lat, lon)float32dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['no_data_inconsistency_detected', 'snow_coverage_or_temperature_below_zero', 'dense_vegetation', 'others_no_convergence_in_the_model_thus_no_valid_sm_estimates', 'soil_moisture_value_exceeds_physical_boundary', 'weight_of_measurement_below_threshold', 'all_datasets_deemed_unreliable', 'barren_ground_advisory_flag', 'NaN']bits :['0b0', '0b1', '0b10', '0b100', '0b1000', '0b10000', '0b100000', '0b1000000', '0b10000000']dtype :int16flag_meanings :['no_data_inconsistency_detected', 'snow_coverage_or_temperature_below_zero', 'dense_vegetation', 'combination_of_flag_values_1_and_2', 'soil_moisture_value_exceeds_physical_boundary', 'combination_of_flag_values_1_and_8', 'combination_of_flag_values_2_and_8', 'barren_ground_advisory_flag', 'combination_of_flag_values_1_and_64', 'combination_of_flag_values_2_and_64', 'combination_of_flag_values_1_and_2_and_64', 'combination_of_flag_values_8_and_64', 'combination_of_flag_values_1_and_8_and_64', 'combination_of_flag_values_2_and_8_and_64', 'combination_of_flag_values_1_and_2_and_4_and_8_and_16_and_32_and_64']flag_values :[0, 1, 2, 3, 8, 9, 10, 64, 65, 66, 67, 72, 73, 74, 127]long_name :Flagstandard_name :soil_moisture_content status_flagvalid_range :[0, 255]  Array   Chunk   Bytes   60.90 GiB   31.64 MiB   Shape   (15767, 720, 1440)   (16, 720, 720)   Dask graph   1972 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 15767 </li><li>freqbandID(time, lat, lon)float32dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['NaN', 'L14', 'C53', 'C66', 'C68', 'C69', 'C73', 'X107', 'K194', 'MODEL']bits :['0b0', '0b1', '0b10', '0b100', '0b1000', '0b10000', '0b100000', '0b1000000', '0b10000000', '0b100000000']dtype :int16flag_meanings :['NaN', 'C66', 'X107', 'C66+X107']flag_values :[0, 4, 64, 68]long_name :Frequency Band Identificationvalid_range :[0, 511]  Array   Chunk   Bytes   60.90 GiB   31.64 MiB   Shape   (15767, 720, 1440)   (16, 720, 720)   Dask graph   1972 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 15767 </li><li>mode(time, lat, lon)float32dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['NaN', 'ascending', 'descending']bits :['0b0', '0b1', '0b10']dtype :int8flag_meanings :['NaN', 'ascending', 'descending', 'ascending_descending_combination']flag_values :[0, 1, 2, 3]long_name :Satellite Modevalid_range :[0, 3]  Array   Chunk   Bytes   60.90 GiB   31.64 MiB   Shape   (15767, 720, 1440)   (16, 720, 720)   Dask graph   1972 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 15767 </li><li>sensor(time, lat, lon)float64dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['NaN', 'SMMR', 'SSMI', 'TMI', 'AMSRE', 'WindSat', 'AMSR2', 'SMOS', 'AMIWS', 'ASCATA', 'ASCATB', 'SMAP', 'MODEL', 'GPM', 'FY3B', 'FY3D', 'ASCATC', 'FY3C']bits :['0b0', '0b1', '0b10', '0b100', '0b1000', '0b10000', '0b100000', '0b1000000', '0b10000000', '0b100000000', '0b1000000000', '0b10000000000', '0b100000000000', '0b1000000000000', '0b10000000000000', '0b100000000000000', '0b1000000000000000', '0b10000000000000000']dtype :int32flag_meanings :['NaN', 'SMMR']flag_values :[0, 1]long_name :Sensorvalid_range :[0, 131071]  Array   Chunk   Bytes   121.80 GiB   63.28 MiB   Shape   (15767, 720, 1440)   (16, 720, 720)   Dask graph   1972 chunks in 2 graph layers   Data type   float64 numpy.ndarray  1440 720 15767 </li><li>sm(time, lat, lon)float32dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;_CoordinateAxes :time lat lonancillary_variables :sm_uncertainty flag t0dtype :float32long_name :Volumetric Soil Moisturestandard_name :soil_moisture_contentunits :m3 m-3valid_range :[0.0, 1.0]  Array   Chunk   Bytes   60.90 GiB   31.64 MiB   Shape   (15767, 720, 1440)   (16, 720, 720)   Dask graph   1972 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 15767 </li><li>sm_uncertainty(time, lat, lon)float32dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;_CoordinateAxes :time lat londtype :float32long_name :Volumetric Soil Moisture Uncertaintystandard_name :soil_moisture_content standard_errorunits :m3 m-3valid_range :[0.0, 1.0]  Array   Chunk   Bytes   60.90 GiB   31.64 MiB   Shape   (15767, 720, 1440)   (16, 720, 720)   Dask graph   1972 chunks in 2 graph layers   Data type   float32 numpy.ndarray  1440 720 15767 </li><li>t0(time, lat, lon)float64dask.array&lt;chunksize=(16, 720, 720), meta=np.ndarray&gt;_CoordinateAxes :time lat londtype :float64long_name :Observation Timestampunits :days after 1970-01-01 00:00:00 UTCvalid_range :[3225.0, 3227.0]  Array   Chunk   Bytes   121.80 GiB   63.28 MiB   Shape   (15767, 720, 1440)   (16, 720, 720)   Dask graph   1972 chunks in 2 graph layers   Data type   float64 numpy.ndarray  1440 720 15767 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.875,  89.625,  89.375,  89.125,  88.875,  88.625,  88.375,  88.125,\n        87.875,  87.625,\n       ...\n       -87.625, -87.875, -88.125, -88.375, -88.625, -88.875, -89.125, -89.375,\n       -89.625, -89.875],\n      dtype='float64', name='lat', length=720))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625, -178.375,\n       -178.125, -177.875, -177.625,\n       ...\n        177.625,  177.875,  178.125,  178.375,  178.625,  178.875,  179.125,\n        179.375,  179.625,  179.875],\n      dtype='float64', name='lon', length=1440))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1978-11-01', '1978-11-02', '1978-11-03', '1978-11-04',\n               '1978-11-05', '1978-11-06', '1978-11-07', '1978-11-08',\n               '1978-11-09', '1978-11-10',\n               ...\n               '2021-12-22', '2021-12-23', '2021-12-24', '2021-12-25',\n               '2021-12-26', '2021-12-27', '2021-12-28', '2021-12-29',\n               '2021-12-30', '2021-12-31'],\n              dtype='datetime64[ns]', name='time', length=15767, freq=None))</pre></li></ul></li><li>Attributes: (44)Conventions :CF-1.9cdm_data_type :Gridcomment :This dataset was produced with funding of the ESA CCI+ Soil Moisture project; ESRIN Contract No: 4000126684/19/I-NBcontact :cci_sm_contact@eodc.eucreator_email :cci_sm_developer@eodc.eucreator_name :Department of Geodesy and Geoinformation, Vienna University of Technologycreator_url :https://climers.geo.tuwien.ac.at/date_created :File created: 2022-04-06 12:31:39.526359geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.25 degreegeospatial_lat_units :degrees_northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :0.25 degreegeospatial_lon_units :degrees_eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0history :2022-04-06 12:29:35 - product produced 2023-08-09 17:33:51 - converted by nc2zarr, version 1.1.2.dev0id :ESACCI-SOILMOISTURE-L3S-SSMV-COMBINED-20211231000000-fv07.1.ncinstitution :TU Wien (AUT); VanderSat B.V. (NL)key_variables :smkeywords :soil moisture, soil water content, microwave remote sensing, active, passive, combined, climate data recordkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Data use is free and open for all registered users.naming_authority :TU Wienplatform :Nimbus 7, DMSP, TRMM, AQUA, Coriolis, GCOM-W1, MIRAS, SMAP, GPM, FengYun-3B, FengYun-3C, FengYun-3D; ERS-1, ERS-2, METOP-A, METOP-Bprocessing_level :Quality-controlled, super-collocated (L3S) Satellite Soil Moisture (SM) data from multiple sensorsproduct_version :v07.1project :Climate Change Initiative - European Space Agencyreferences :http://www.esa-soilmoisture-cci.org; Dorigo, W.A., Wagner, W., Albergel, C., Albrecht, F.,  Balsamo, G., Brocca, L., Chung, D., Ertl, M., Forkel, M., Gruber, A., Haas, E., Hamer, D. P. Hirschi, M., Ikonen, J., De Jeu, R. Kidd, R. Lahoz, W., Liu, Y.Y., Miralles, D., Lecomte, P. (2017) ESA CCI Soil Moisture for improved Earth system understanding: State-of-the art and future directions. In Remote Sensing of Environment, 2017, ISSN 0034-4257, https://doi.org/10.1016/j.rse.2017.07.001; Gruber, A., Scanlon, T., van der Schalie, R., Wagner, W., Dorigo, W. (2019) Evolution of the ESA CCI Soil Moisture Climate Data Records and their underlying merging methodology. Earth System Science Data 11, 717-739, https://doi.org/10.5194/essd-11-717-2019; Gruber, A., Dorigo, W. A., Crow, W., Wagner W. (2017). Triple Collocation-Based Merging of Satellite Soil Moisture Retrievals. IEEE Transactions on Geoscience and Remote Sensing. PP. 1-13. https://doi.org/10.1109/TGRS.2017.2734070sensor :SMMR, SSM/I, TMI, AMSR-E, WindSat, AMSR2, SMOS, SMAP_radiometer, GMI, VIRR-3B, VIRR-3C, VIRR-3D; AMI-WS, ASCAT-A, ASCAT-B, ASCAT-Csource :WARP 5.5R1.1/AMI-WS/ERS12 Level 2 Soil Moisture; WARP 5.4R1.0/AMI-WS/ERS2 Level 2 Soil Moisture; H115: Metop ASCAT Surface Soil Moisture Climate Data Record v5 12.5 km sampling, DOI: 10.15770/EUM_SAF_H_0006; H116: Metop ASCAT Surface Soil Moisture Climate Data Record v5 Extension 12.5 km sampling; H115: Metop ASCAT Surface Soil Moisture Climate Data Record v5 12.5 km sampling, DOI: 10.15770/EUM_SAF_H_0006; H116: Metop ASCAT Surface Soil Moisture Climate Data Record v5 Extension 12.5 km sampling; H115: Metop ASCAT Surface Soil Moisture Climate Data Record v5 12.5 km sampling, DOI: 10.15770/EUM_SAF_H_0006; H116: Metop ASCAT Surface Soil Moisture Climate Data Record v5 Extension 12.5 km sampling;; LPRMv7/SMMR/Nimbus 7 L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/SSMI/F08, F11, F13 DMSP L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/TMI/TRMM L2 Surface Soil Moisture, Ancillary Params, and QC; LPRMv7/AMSR-E/Aqua L2B Surface Soil Moisture, Ancillary Params, and QC; LPRMv7/WINDSAT/CORIOLIS L2 Surface Soil Moisture, Ancillary Params, and QC; LPRMv7/AMSR2/GCOM-W1 L3 Surface Soil Moisture, Ancillary Params; LPRMv7/SMOS/MIRAS L3 Surface Soil Moisture, CATDS Level 3 Brightness Temperatures (L3TB) version 300 RE03 &amp; RE04; LPRMv7/SMAP_radiometer/SMAP L2 Surface Soil Moisture, Ancillary Params, and QC; LPRMv7/GMI/GPM L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/VIRR/FengYun-3B L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/VIRR/FengYun-3C L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/VIRR/FengYun-3D L3 Surface Soil Moisture, Ancillary Params, and quality flags;;spatial_resolution :25kmstandard_name_vocabulary :CF Standard Name Table v77summary :This dataset was produced with funding of the ESA CCI+ Soil Moisture project; ESRIN Contract No: 4000126684/19/I-NBtime_coverage_duration :P43Ytime_coverage_end :2021-12-31 00:00:00time_coverage_end_product :20211231T235959Ztime_coverage_resolution :P1Dtime_coverage_start :1978-11-01 00:00:00time_coverage_start_product :19781101T000000Ztitle :ESA CCI Surface Soil Moisture COMBINED active+passive Producttracking_id :ad35798e-58e0-488f-b5b9-593874a47700</li></ul> <p>Subsetting time and space for the sake of an efficient example</p> <p>Temporal subset, entire 2020:</p> In\u00a0[11]: Copied! <pre>start_date = datetime.datetime(2020, 1, 1)\nstop_date = datetime.datetime(2020, 12, 31)\n</pre> start_date = datetime.datetime(2020, 1, 1) stop_date = datetime.datetime(2020, 12, 31) <p>Spatial subset:</p> In\u00a0[12]: Copied! <pre>min_lat = 70.0\nmax_lat = 60.0\nmin_lon = 15\nmax_lon = 25.0\n</pre> min_lat = 70.0 max_lat = 60.0 min_lon = 15 max_lon = 25.0 <p>Subsetting the Land Cover dataset, results in only one time slice per year for LC:</p> In\u00a0[13]: Copied! <pre>LC = LC.sel(\n    lat=slice(min_lat, max_lat),\n    lon=slice(min_lon, max_lon),\n    time=slice(start_date, stop_date),\n)\n</pre> LC = LC.sel(     lat=slice(min_lat, max_lat),     lon=slice(min_lon, max_lon),     time=slice(start_date, stop_date), ) In\u00a0[14]: Copied! <pre>LC\n</pre> LC Out[14]: <pre>&lt;xarray.Dataset&gt; Size: 156MB\nDimensions:              (time: 1, lat: 3600, lon: 3600, bounds: 2)\nCoordinates:\n  * lat                  (lat) float64 29kB 70.0 70.0 69.99 ... 60.01 60.0 60.0\n  * lon                  (lon) float64 29kB 15.0 15.0 15.01 ... 24.99 25.0 25.0\n  * time                 (time) datetime64[ns] 8B 2020-01-01\nDimensions without coordinates: bounds\nData variables:\n    change_count         (time, lat, lon) uint8 13MB dask.array&lt;chunksize=(1, 1440, 1080), meta=np.ndarray&gt;\n    crs                  (time) int32 4B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n    current_pixel_state  (time, lat, lon) float32 52MB dask.array&lt;chunksize=(1, 1440, 1080), meta=np.ndarray&gt;\n    lat_bounds           (time, lat, bounds) float64 58kB dask.array&lt;chunksize=(1, 1440, 2), meta=np.ndarray&gt;\n    lccs_class           (time, lat, lon) uint8 13MB dask.array&lt;chunksize=(1, 1440, 1080), meta=np.ndarray&gt;\n    lon_bounds           (time, lon, bounds) float64 58kB dask.array&lt;chunksize=(1, 1080, 2), meta=np.ndarray&gt;\n    observation_count    (time, lat, lon) uint16 26MB dask.array&lt;chunksize=(1, 1440, 1080), meta=np.ndarray&gt;\n    processed_flag       (time, lat, lon) float32 52MB dask.array&lt;chunksize=(1, 1440, 1080), meta=np.ndarray&gt;\n    time_bounds          (time, bounds) datetime64[ns] 16B dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\nAttributes: (12/38)\n    Conventions:                CF-1.6\n    TileSize:                   2025:2025\n    cdm_data_type:              grid\n    comment:                    \n    contact:                    https://www.ecmwf.int/en/about/contact-us/get...\n    creation_date:              20181130T095431Z\n    ...                         ...\n    time_coverage_end:          20101231\n    time_coverage_resolution:   P1Y\n    time_coverage_start:        20100101\n    title:                      Land Cover Map of ESA CCI brokered by CDS\n    tracking_id:                96ac9aca-1ca7-45c6-b4a5-ab448c692646\n    type:                       ESACCI-LC-L4-LCCS-Map-300m-P1Y</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 1</li><li>lat: 3600</li><li>lon: 3600</li><li>bounds: 2</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6470.0 70.0 69.99 ... 60.01 60.0 60.0axis :Ybounds :lat_boundslong_name :latitudestandard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([69.998611, 69.995833, 69.993056, ..., 60.006944, 60.004167, 60.001389])</pre></li><li>lon(lon)float6415.0 15.0 15.01 ... 24.99 25.0 25.0axis :Xbounds :lon_boundslong_name :longitudestandard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([15.001389, 15.004167, 15.006944, ..., 24.993056, 24.995833, 24.998611])</pre></li><li>time(time)datetime64[ns]2020-01-01axis :Tbounds :time_boundslong_name :timestandard_name :time<pre>array(['2020-01-01T00:00:00.000000000'], dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (9)<ul><li>change_count(time, lat, lon)uint8dask.array&lt;chunksize=(1, 1440, 1080), meta=np.ndarray&gt;long_name :number of class changesvalid_max :100valid_min :0  Array   Chunk   Bytes   12.36 MiB   4.45 MiB   Shape   (1, 3600, 3600)   (1, 2160, 2160)   Dask graph   6 chunks in 3 graph layers   Data type   uint8 numpy.ndarray  3600 3600 1 </li><li>crs(time)int32dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;i2m :0.002777777777778,0.0,0.0,-0.002777777777778,-180.0,90.0wkt :GEOGCS[\"WGS 84\",    DATUM[\"World Geodetic System 1984\",      SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]],      AUTHORITY[\"EPSG\",\"6326\"]],    PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]],    UNIT[\"degree\", 0.017453292519943295],    AXIS[\"Geodetic longitude\", EAST],    AXIS[\"Geodetic latitude\", NORTH],    AUTHORITY[\"EPSG\",\"4326\"]]  Array   Chunk   Bytes   4 B   4 B   Shape   (1,)   (1,)   Dask graph   1 chunks in 3 graph layers   Data type   int32 numpy.ndarray  1 1 </li><li>current_pixel_state(time, lat, lon)float32dask.array&lt;chunksize=(1, 1440, 1080), meta=np.ndarray&gt;flag_meanings :invalid clear_land clear_water clear_snow_ice cloud cloud_shadowflag_values :[0, 1, 2, 3, 4, 5]long_name :LC pixel type maskstandard_name :land_cover_lccs status_flagvalid_max :5valid_min :0  Array   Chunk   Bytes   49.44 MiB   17.80 MiB   Shape   (1, 3600, 3600)   (1, 2160, 2160)   Dask graph   6 chunks in 3 graph layers   Data type   float32 numpy.ndarray  3600 3600 1 </li><li>lat_bounds(time, lat, bounds)float64dask.array&lt;chunksize=(1, 1440, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   56.25 kiB   33.75 kiB   Shape   (1, 3600, 2)   (1, 2160, 2)   Dask graph   2 chunks in 3 graph layers   Data type   float64 numpy.ndarray  2 3600 1 </li><li>lccs_class(time, lat, lon)uint8dask.array&lt;chunksize=(1, 1440, 1080), meta=np.ndarray&gt;ancillary_variables :processed_flag current_pixel_state observation_count change_countflag_colors :#ffff64 #ffff64 #ffff00 #aaf0f0 #dcf064 #c8c864 #006400 #00a000 #00a000 #aac800 #003c00 #003c00 #005000 #285000 #285000 #286400 #788200 #8ca000 #be9600 #966400 #966400 #966400 #ffb432 #ffdcd2 #ffebaf #ffc864 #ffd278 #ffebaf #00785a #009678 #00dc82 #c31400 #fff5d7 #dcdcdc #fff5d7 #0046c8 #ffffffflag_meanings :no_data cropland_rainfed cropland_rainfed_herbaceous_cover cropland_rainfed_tree_or_shrub_cover cropland_irrigated mosaic_cropland mosaic_natural_vegetation tree_broadleaved_evergreen_closed_to_open tree_broadleaved_deciduous_closed_to_open tree_broadleaved_deciduous_closed tree_broadleaved_deciduous_open tree_needleleaved_evergreen_closed_to_open tree_needleleaved_evergreen_closed tree_needleleaved_evergreen_open tree_needleleaved_deciduous_closed_to_open tree_needleleaved_deciduous_closed tree_needleleaved_deciduous_open tree_mixed mosaic_tree_and_shrub mosaic_herbaceous shrubland shrubland_evergreen shrubland_deciduous grassland lichens_and_mosses sparse_vegetation sparse_tree sparse_shrub sparse_herbaceous tree_cover_flooded_fresh_or_brakish_water tree_cover_flooded_saline_water shrub_or_herbaceous_cover_flooded urban bare_areas bare_areas_consolidated bare_areas_unconsolidated water snow_and_iceflag_values :[0, 10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100, 110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180, 190, 200, 201, 202, 210, 220]long_name :Land cover class defined in LCCSstandard_name :land_cover_lccsvalid_max :220valid_min :1  Array   Chunk   Bytes   12.36 MiB   4.45 MiB   Shape   (1, 3600, 3600)   (1, 2160, 2160)   Dask graph   6 chunks in 3 graph layers   Data type   uint8 numpy.ndarray  3600 3600 1 </li><li>lon_bounds(time, lon, bounds)float64dask.array&lt;chunksize=(1, 1080, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   56.25 kiB   33.75 kiB   Shape   (1, 3600, 2)   (1, 2160, 2)   Dask graph   3 chunks in 3 graph layers   Data type   float64 numpy.ndarray  2 3600 1 </li><li>observation_count(time, lat, lon)uint16dask.array&lt;chunksize=(1, 1440, 1080), meta=np.ndarray&gt;long_name :number of valid observationsstandard_name :land_cover_lccs number_of_observationsvalid_max :32767valid_min :0  Array   Chunk   Bytes   24.72 MiB   8.90 MiB   Shape   (1, 3600, 3600)   (1, 2160, 2160)   Dask graph   6 chunks in 3 graph layers   Data type   uint16 numpy.ndarray  3600 3600 1 </li><li>processed_flag(time, lat, lon)float32dask.array&lt;chunksize=(1, 1440, 1080), meta=np.ndarray&gt;flag_meanings :not_processed processedflag_values :[0, 1]long_name :LC map processed area flagstandard_name :land_cover_lccs status_flagvalid_max :1valid_min :0  Array   Chunk   Bytes   49.44 MiB   17.80 MiB   Shape   (1, 3600, 3600)   (1, 2160, 2160)   Dask graph   6 chunks in 3 graph layers   Data type   float32 numpy.ndarray  3600 3600 1 </li><li>time_bounds(time, bounds)datetime64[ns]dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   16 B   16 B   Shape   (1, 2)   (1, 2)   Dask graph   1 chunks in 3 graph layers   Data type   datetime64[ns] numpy.ndarray  2 1 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([69.99861111111113, 69.99583333333334, 69.99305555555557,\n       69.99027777777778, 69.98750000000001, 69.98472222222222,\n       69.98194444444445, 69.97916666666669, 69.97638888888889,\n       69.97361111111113,\n       ...\n        60.0263888888889, 60.02361111111111, 60.02083333333334,\n       60.01805555555555, 60.01527777777778, 60.01250000000002,\n       60.00972222222222, 60.00694444444446, 60.00416666666666,\n        60.0013888888889],\n      dtype='float64', name='lat', length=3600))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([15.001388888888897, 15.004166666666663, 15.006944444444457,\n       15.009722222222223, 15.012500000000017, 15.015277777777783,\n       15.018055555555577, 15.020833333333343, 15.023611111111109,\n       15.026388888888903,\n       ...\n       24.973611111111126,  24.97638888888889, 24.979166666666686,\n        24.98194444444445, 24.984722222222217,  24.98750000000001,\n       24.990277777777777,  24.99305555555557, 24.995833333333337,\n        24.99861111111113],\n      dtype='float64', name='lon', length=3600))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2020-01-01'], dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (38)Conventions :CF-1.6TileSize :2025:2025cdm_data_type :gridcomment :contact :https://www.ecmwf.int/en/about/contact-us/get-supportcreation_date :20181130T095431Zcreator_email :landcover-cci@uclouvain.becreator_name :UCLouvaincreator_url :http://www.uclouvain.be/geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.002778geospatial_lat_units :degrees_northgeospatial_lon_max :180geospatial_lon_min :-180geospatial_lon_resolution :0.002778geospatial_lon_units :degrees_easthistory :amorgos-4,0, lc-sdr-1.0, lc-sr-1.0, lc-classification-1.0,lc-user-tools-3.13,lc-user-tools-4.3id :ESACCI-LC-L4-LCCS-Map-300m-P1Y-2010-v2.0.7cdsinstitution :UCLouvainkeywords :land cover classification,satellite,observationkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :ESA CCI Data Policy: free and open accessnaming_authority :org.esa-cciproduct_version :2.0.7cdsproject :Climate Change Initiative - European Space Agencyreferences :http://www.esa-landcover-cci.org/source :MERIS FR L1B version 5.05, MERIS RR L1B version 8.0, SPOT VGT Pspatial_resolution :300mstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Standard Names version 21summary :This dataset characterizes the land cover of a particular year (see time_coverage). The land cover was derived from the analysis of satellite data time series of the full period.time_coverage_duration :P1Ytime_coverage_end :20101231time_coverage_resolution :P1Ytime_coverage_start :20100101title :Land Cover Map of ESA CCI brokered by CDStracking_id :96ac9aca-1ca7-45c6-b4a5-ab448c692646type :ESACCI-LC-L4-LCCS-Map-300m-P1Y</li></ul> <p>Subsetting the Soil Moisture dataset, choosing only one month here:</p> In\u00a0[15]: Copied! <pre>start_date = datetime.datetime(2020, 8, 1)\nstop_date = datetime.datetime(2020, 8, 31)\n</pre> start_date = datetime.datetime(2020, 8, 1) stop_date = datetime.datetime(2020, 8, 31) In\u00a0[16]: Copied! <pre>SM = SM.sel(\n    lat=slice(min_lat, max_lat),\n    lon=slice(min_lon, max_lon),\n    time=slice(start_date, stop_date),\n)\n</pre> SM = SM.sel(     lat=slice(min_lat, max_lat),     lon=slice(min_lon, max_lon),     time=slice(start_date, stop_date), ) In\u00a0[17]: Copied! <pre>SM\n</pre> SM Out[17]: <pre>&lt;xarray.Dataset&gt; Size: 2MB\nDimensions:         (time: 31, lat: 40, lon: 40)\nCoordinates:\n  * lat             (lat) float64 320B 69.88 69.62 69.38 ... 60.62 60.38 60.12\n  * lon             (lon) float64 320B 15.12 15.38 15.62 ... 24.38 24.62 24.88\n  * time            (time) datetime64[ns] 248B 2020-08-01 ... 2020-08-31\nData variables:\n    dnflag          (time, lat, lon) float32 198kB dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;\n    flag            (time, lat, lon) float32 198kB dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;\n    freqbandID      (time, lat, lon) float32 198kB dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;\n    mode            (time, lat, lon) float32 198kB dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;\n    sensor          (time, lat, lon) float64 397kB dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;\n    sm              (time, lat, lon) float32 198kB dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;\n    sm_uncertainty  (time, lat, lon) float32 198kB dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;\n    t0              (time, lat, lon) float64 397kB dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;\nAttributes: (12/44)\n    Conventions:                  CF-1.9\n    cdm_data_type:                Grid\n    comment:                      This dataset was produced with funding of t...\n    contact:                      cci_sm_contact@eodc.eu\n    creator_email:                cci_sm_developer@eodc.eu\n    creator_name:                 Department of Geodesy and Geoinformation, V...\n    ...                           ...\n    time_coverage_end_product:    20211231T235959Z\n    time_coverage_resolution:     P1D\n    time_coverage_start:          1978-11-01 00:00:00\n    time_coverage_start_product:  19781101T000000Z\n    title:                        ESA CCI Surface Soil Moisture COMBINED acti...\n    tracking_id:                  ad35798e-58e0-488f-b5b9-593874a47700</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 31</li><li>lat: 40</li><li>lon: 40</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6469.88 69.62 69.38 ... 60.38 60.12_CoordinateAxisType :Latstandard_name :latitudeunits :degrees_northvalid_range :[-90.0, 90.0]<pre>array([69.875, 69.625, 69.375, 69.125, 68.875, 68.625, 68.375, 68.125, 67.875,\n       67.625, 67.375, 67.125, 66.875, 66.625, 66.375, 66.125, 65.875, 65.625,\n       65.375, 65.125, 64.875, 64.625, 64.375, 64.125, 63.875, 63.625, 63.375,\n       63.125, 62.875, 62.625, 62.375, 62.125, 61.875, 61.625, 61.375, 61.125,\n       60.875, 60.625, 60.375, 60.125])</pre></li><li>lon(lon)float6415.12 15.38 15.62 ... 24.62 24.88_CoordinateAxisType :Lonstandard_name :longitudeunits :degrees_eastvalid_range :[-180.0, 180.0]<pre>array([15.125, 15.375, 15.625, 15.875, 16.125, 16.375, 16.625, 16.875, 17.125,\n       17.375, 17.625, 17.875, 18.125, 18.375, 18.625, 18.875, 19.125, 19.375,\n       19.625, 19.875, 20.125, 20.375, 20.625, 20.875, 21.125, 21.375, 21.625,\n       21.875, 22.125, 22.375, 22.625, 22.875, 23.125, 23.375, 23.625, 23.875,\n       24.125, 24.375, 24.625, 24.875])</pre></li><li>time(time)datetime64[ns]2020-08-01 ... 2020-08-31<pre>array(['2020-08-01T00:00:00.000000000', '2020-08-02T00:00:00.000000000',\n       '2020-08-03T00:00:00.000000000', '2020-08-04T00:00:00.000000000',\n       '2020-08-05T00:00:00.000000000', '2020-08-06T00:00:00.000000000',\n       '2020-08-07T00:00:00.000000000', '2020-08-08T00:00:00.000000000',\n       '2020-08-09T00:00:00.000000000', '2020-08-10T00:00:00.000000000',\n       '2020-08-11T00:00:00.000000000', '2020-08-12T00:00:00.000000000',\n       '2020-08-13T00:00:00.000000000', '2020-08-14T00:00:00.000000000',\n       '2020-08-15T00:00:00.000000000', '2020-08-16T00:00:00.000000000',\n       '2020-08-17T00:00:00.000000000', '2020-08-18T00:00:00.000000000',\n       '2020-08-19T00:00:00.000000000', '2020-08-20T00:00:00.000000000',\n       '2020-08-21T00:00:00.000000000', '2020-08-22T00:00:00.000000000',\n       '2020-08-23T00:00:00.000000000', '2020-08-24T00:00:00.000000000',\n       '2020-08-25T00:00:00.000000000', '2020-08-26T00:00:00.000000000',\n       '2020-08-27T00:00:00.000000000', '2020-08-28T00:00:00.000000000',\n       '2020-08-29T00:00:00.000000000', '2020-08-30T00:00:00.000000000',\n       '2020-08-31T00:00:00.000000000'], dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (8)<ul><li>dnflag(time, lat, lon)float32dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['NaN', 'day', 'night']bits :['0b0', '0b1', '0b10']dtype :int8flag_meanings :['NaN', 'day', 'night', 'day_night_combination']flag_values :[0, 1, 2, 3]long_name :Day / Night Flagvalid_range :[0, 3]  Array   Chunk   Bytes   193.75 kiB   100.00 kiB   Shape   (31, 40, 40)   (16, 40, 40)   Dask graph   2 chunks in 3 graph layers   Data type   float32 numpy.ndarray  40 40 31 </li><li>flag(time, lat, lon)float32dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['no_data_inconsistency_detected', 'snow_coverage_or_temperature_below_zero', 'dense_vegetation', 'others_no_convergence_in_the_model_thus_no_valid_sm_estimates', 'soil_moisture_value_exceeds_physical_boundary', 'weight_of_measurement_below_threshold', 'all_datasets_deemed_unreliable', 'barren_ground_advisory_flag', 'NaN']bits :['0b0', '0b1', '0b10', '0b100', '0b1000', '0b10000', '0b100000', '0b1000000', '0b10000000']dtype :int16flag_meanings :['no_data_inconsistency_detected', 'snow_coverage_or_temperature_below_zero', 'dense_vegetation', 'combination_of_flag_values_1_and_2', 'soil_moisture_value_exceeds_physical_boundary', 'combination_of_flag_values_1_and_8', 'combination_of_flag_values_2_and_8', 'barren_ground_advisory_flag', 'combination_of_flag_values_1_and_64', 'combination_of_flag_values_2_and_64', 'combination_of_flag_values_1_and_2_and_64', 'combination_of_flag_values_8_and_64', 'combination_of_flag_values_1_and_8_and_64', 'combination_of_flag_values_2_and_8_and_64', 'combination_of_flag_values_1_and_2_and_4_and_8_and_16_and_32_and_64']flag_values :[0, 1, 2, 3, 8, 9, 10, 64, 65, 66, 67, 72, 73, 74, 127]long_name :Flagstandard_name :soil_moisture_content status_flagvalid_range :[0, 255]  Array   Chunk   Bytes   193.75 kiB   100.00 kiB   Shape   (31, 40, 40)   (16, 40, 40)   Dask graph   2 chunks in 3 graph layers   Data type   float32 numpy.ndarray  40 40 31 </li><li>freqbandID(time, lat, lon)float32dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['NaN', 'L14', 'C53', 'C66', 'C68', 'C69', 'C73', 'X107', 'K194', 'MODEL']bits :['0b0', '0b1', '0b10', '0b100', '0b1000', '0b10000', '0b100000', '0b1000000', '0b10000000', '0b100000000']dtype :int16flag_meanings :['NaN', 'C66', 'X107', 'C66+X107']flag_values :[0, 4, 64, 68]long_name :Frequency Band Identificationvalid_range :[0, 511]  Array   Chunk   Bytes   193.75 kiB   100.00 kiB   Shape   (31, 40, 40)   (16, 40, 40)   Dask graph   2 chunks in 3 graph layers   Data type   float32 numpy.ndarray  40 40 31 </li><li>mode(time, lat, lon)float32dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['NaN', 'ascending', 'descending']bits :['0b0', '0b1', '0b10']dtype :int8flag_meanings :['NaN', 'ascending', 'descending', 'ascending_descending_combination']flag_values :[0, 1, 2, 3]long_name :Satellite Modevalid_range :[0, 3]  Array   Chunk   Bytes   193.75 kiB   100.00 kiB   Shape   (31, 40, 40)   (16, 40, 40)   Dask graph   2 chunks in 3 graph layers   Data type   float32 numpy.ndarray  40 40 31 </li><li>sensor(time, lat, lon)float64dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['NaN', 'SMMR', 'SSMI', 'TMI', 'AMSRE', 'WindSat', 'AMSR2', 'SMOS', 'AMIWS', 'ASCATA', 'ASCATB', 'SMAP', 'MODEL', 'GPM', 'FY3B', 'FY3D', 'ASCATC', 'FY3C']bits :['0b0', '0b1', '0b10', '0b100', '0b1000', '0b10000', '0b100000', '0b1000000', '0b10000000', '0b100000000', '0b1000000000', '0b10000000000', '0b100000000000', '0b1000000000000', '0b10000000000000', '0b100000000000000', '0b1000000000000000', '0b10000000000000000']dtype :int32flag_meanings :['NaN', 'SMMR']flag_values :[0, 1]long_name :Sensorvalid_range :[0, 131071]  Array   Chunk   Bytes   387.50 kiB   200.00 kiB   Shape   (31, 40, 40)   (16, 40, 40)   Dask graph   2 chunks in 3 graph layers   Data type   float64 numpy.ndarray  40 40 31 </li><li>sm(time, lat, lon)float32dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;_CoordinateAxes :time lat lonancillary_variables :sm_uncertainty flag t0dtype :float32long_name :Volumetric Soil Moisturestandard_name :soil_moisture_contentunits :m3 m-3valid_range :[0.0, 1.0]  Array   Chunk   Bytes   193.75 kiB   100.00 kiB   Shape   (31, 40, 40)   (16, 40, 40)   Dask graph   2 chunks in 3 graph layers   Data type   float32 numpy.ndarray  40 40 31 </li><li>sm_uncertainty(time, lat, lon)float32dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;_CoordinateAxes :time lat londtype :float32long_name :Volumetric Soil Moisture Uncertaintystandard_name :soil_moisture_content standard_errorunits :m3 m-3valid_range :[0.0, 1.0]  Array   Chunk   Bytes   193.75 kiB   100.00 kiB   Shape   (31, 40, 40)   (16, 40, 40)   Dask graph   2 chunks in 3 graph layers   Data type   float32 numpy.ndarray  40 40 31 </li><li>t0(time, lat, lon)float64dask.array&lt;chunksize=(15, 40, 40), meta=np.ndarray&gt;_CoordinateAxes :time lat londtype :float64long_name :Observation Timestampunits :days after 1970-01-01 00:00:00 UTCvalid_range :[3225.0, 3227.0]  Array   Chunk   Bytes   387.50 kiB   200.00 kiB   Shape   (31, 40, 40)   (16, 40, 40)   Dask graph   2 chunks in 3 graph layers   Data type   float64 numpy.ndarray  40 40 31 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([69.875, 69.625, 69.375, 69.125, 68.875, 68.625, 68.375, 68.125, 67.875,\n       67.625, 67.375, 67.125, 66.875, 66.625, 66.375, 66.125, 65.875, 65.625,\n       65.375, 65.125, 64.875, 64.625, 64.375, 64.125, 63.875, 63.625, 63.375,\n       63.125, 62.875, 62.625, 62.375, 62.125, 61.875, 61.625, 61.375, 61.125,\n       60.875, 60.625, 60.375, 60.125],\n      dtype='float64', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([15.125, 15.375, 15.625, 15.875, 16.125, 16.375, 16.625, 16.875, 17.125,\n       17.375, 17.625, 17.875, 18.125, 18.375, 18.625, 18.875, 19.125, 19.375,\n       19.625, 19.875, 20.125, 20.375, 20.625, 20.875, 21.125, 21.375, 21.625,\n       21.875, 22.125, 22.375, 22.625, 22.875, 23.125, 23.375, 23.625, 23.875,\n       24.125, 24.375, 24.625, 24.875],\n      dtype='float64', name='lon'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2020-08-01', '2020-08-02', '2020-08-03', '2020-08-04',\n               '2020-08-05', '2020-08-06', '2020-08-07', '2020-08-08',\n               '2020-08-09', '2020-08-10', '2020-08-11', '2020-08-12',\n               '2020-08-13', '2020-08-14', '2020-08-15', '2020-08-16',\n               '2020-08-17', '2020-08-18', '2020-08-19', '2020-08-20',\n               '2020-08-21', '2020-08-22', '2020-08-23', '2020-08-24',\n               '2020-08-25', '2020-08-26', '2020-08-27', '2020-08-28',\n               '2020-08-29', '2020-08-30', '2020-08-31'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (44)Conventions :CF-1.9cdm_data_type :Gridcomment :This dataset was produced with funding of the ESA CCI+ Soil Moisture project; ESRIN Contract No: 4000126684/19/I-NBcontact :cci_sm_contact@eodc.eucreator_email :cci_sm_developer@eodc.eucreator_name :Department of Geodesy and Geoinformation, Vienna University of Technologycreator_url :https://climers.geo.tuwien.ac.at/date_created :File created: 2022-04-06 12:31:39.526359geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.25 degreegeospatial_lat_units :degrees_northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :0.25 degreegeospatial_lon_units :degrees_eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0history :2022-04-06 12:29:35 - product produced 2023-08-09 17:33:51 - converted by nc2zarr, version 1.1.2.dev0id :ESACCI-SOILMOISTURE-L3S-SSMV-COMBINED-20211231000000-fv07.1.ncinstitution :TU Wien (AUT); VanderSat B.V. (NL)key_variables :smkeywords :soil moisture, soil water content, microwave remote sensing, active, passive, combined, climate data recordkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Data use is free and open for all registered users.naming_authority :TU Wienplatform :Nimbus 7, DMSP, TRMM, AQUA, Coriolis, GCOM-W1, MIRAS, SMAP, GPM, FengYun-3B, FengYun-3C, FengYun-3D; ERS-1, ERS-2, METOP-A, METOP-Bprocessing_level :Quality-controlled, super-collocated (L3S) Satellite Soil Moisture (SM) data from multiple sensorsproduct_version :v07.1project :Climate Change Initiative - European Space Agencyreferences :http://www.esa-soilmoisture-cci.org; Dorigo, W.A., Wagner, W., Albergel, C., Albrecht, F.,  Balsamo, G., Brocca, L., Chung, D., Ertl, M., Forkel, M., Gruber, A., Haas, E., Hamer, D. P. Hirschi, M., Ikonen, J., De Jeu, R. Kidd, R. Lahoz, W., Liu, Y.Y., Miralles, D., Lecomte, P. (2017) ESA CCI Soil Moisture for improved Earth system understanding: State-of-the art and future directions. In Remote Sensing of Environment, 2017, ISSN 0034-4257, https://doi.org/10.1016/j.rse.2017.07.001; Gruber, A., Scanlon, T., van der Schalie, R., Wagner, W., Dorigo, W. (2019) Evolution of the ESA CCI Soil Moisture Climate Data Records and their underlying merging methodology. Earth System Science Data 11, 717-739, https://doi.org/10.5194/essd-11-717-2019; Gruber, A., Dorigo, W. A., Crow, W., Wagner W. (2017). Triple Collocation-Based Merging of Satellite Soil Moisture Retrievals. IEEE Transactions on Geoscience and Remote Sensing. PP. 1-13. https://doi.org/10.1109/TGRS.2017.2734070sensor :SMMR, SSM/I, TMI, AMSR-E, WindSat, AMSR2, SMOS, SMAP_radiometer, GMI, VIRR-3B, VIRR-3C, VIRR-3D; AMI-WS, ASCAT-A, ASCAT-B, ASCAT-Csource :WARP 5.5R1.1/AMI-WS/ERS12 Level 2 Soil Moisture; WARP 5.4R1.0/AMI-WS/ERS2 Level 2 Soil Moisture; H115: Metop ASCAT Surface Soil Moisture Climate Data Record v5 12.5 km sampling, DOI: 10.15770/EUM_SAF_H_0006; H116: Metop ASCAT Surface Soil Moisture Climate Data Record v5 Extension 12.5 km sampling; H115: Metop ASCAT Surface Soil Moisture Climate Data Record v5 12.5 km sampling, DOI: 10.15770/EUM_SAF_H_0006; H116: Metop ASCAT Surface Soil Moisture Climate Data Record v5 Extension 12.5 km sampling; H115: Metop ASCAT Surface Soil Moisture Climate Data Record v5 12.5 km sampling, DOI: 10.15770/EUM_SAF_H_0006; H116: Metop ASCAT Surface Soil Moisture Climate Data Record v5 Extension 12.5 km sampling;; LPRMv7/SMMR/Nimbus 7 L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/SSMI/F08, F11, F13 DMSP L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/TMI/TRMM L2 Surface Soil Moisture, Ancillary Params, and QC; LPRMv7/AMSR-E/Aqua L2B Surface Soil Moisture, Ancillary Params, and QC; LPRMv7/WINDSAT/CORIOLIS L2 Surface Soil Moisture, Ancillary Params, and QC; LPRMv7/AMSR2/GCOM-W1 L3 Surface Soil Moisture, Ancillary Params; LPRMv7/SMOS/MIRAS L3 Surface Soil Moisture, CATDS Level 3 Brightness Temperatures (L3TB) version 300 RE03 &amp; RE04; LPRMv7/SMAP_radiometer/SMAP L2 Surface Soil Moisture, Ancillary Params, and QC; LPRMv7/GMI/GPM L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/VIRR/FengYun-3B L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/VIRR/FengYun-3C L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/VIRR/FengYun-3D L3 Surface Soil Moisture, Ancillary Params, and quality flags;;spatial_resolution :25kmstandard_name_vocabulary :CF Standard Name Table v77summary :This dataset was produced with funding of the ESA CCI+ Soil Moisture project; ESRIN Contract No: 4000126684/19/I-NBtime_coverage_duration :P43Ytime_coverage_end :2021-12-31 00:00:00time_coverage_end_product :20211231T235959Ztime_coverage_resolution :P1Dtime_coverage_start :1978-11-01 00:00:00time_coverage_start_product :19781101T000000Ztitle :ESA CCI Surface Soil Moisture COMBINED active+passive Producttracking_id :ad35798e-58e0-488f-b5b9-593874a47700</li></ul> <p>Resample to the same Grid</p> <p>Here we use xcube's GridMapping method to extract the specification of both grids. Soil Moisture is the grid to be transformed.</p> In\u00a0[18]: Copied! <pre>source_gm = GridMapping.from_dataset(SM)\n</pre> source_gm = GridMapping.from_dataset(SM) In\u00a0[19]: Copied! <pre>source_gm\n</pre> source_gm Out[19]: <p>class: RegularGridMapping</p> <ul> <li>is_regular: True</li> <li>is_j_axis_up: False</li> <li>is_lon_360: False</li> <li>crs: EPSG:4326</li> <li>xy_res: (0.25, 0.25)</li> <li>xy_bbox: (15, 60, 25, 70)</li> <li>ij_bbox: (0, 0, 40, 40)</li> <li>xy_dim_names: ('lon', 'lat')</li> <li>xy_var_names: ('lon', 'lat')</li> <li>size: (40, 40)</li> <li>tile_size: (40, 40)</li> </ul> <p>The target grid mapping is the one from the Land Cover dataset:</p> In\u00a0[20]: Copied! <pre>target_gm = GridMapping.from_dataset(LC)\n</pre> target_gm = GridMapping.from_dataset(LC) In\u00a0[21]: Copied! <pre>target_gm\n</pre> target_gm Out[21]: <p>class: RegularGridMapping</p> <ul> <li>is_regular: True</li> <li>is_j_axis_up: False</li> <li>is_lon_360: False</li> <li>crs: EPSG:4326</li> <li>xy_res: (0.002777775, 0.002777775)</li> <li>xy_bbox: (15, 60, 25, 70)</li> <li>ij_bbox: (0, 0, 3600, 3600)</li> <li>xy_dim_names: ('lon', 'lat')</li> <li>xy_var_names: ('lon', 'lat')</li> <li>size: (3600, 3600)</li> <li>tile_size: (2160, 2160)</li> </ul> <p>Now we resample Soil Moisture to the grid provided by Land Cover:</p> In\u00a0[22]: Copied! <pre>resampled_SM = resample_in_space(SM, source_gm=source_gm, target_gm=target_gm)\n</pre> resampled_SM = resample_in_space(SM, source_gm=source_gm, target_gm=target_gm) <p>Lets compare the different grid mappings, to see whether the resampled Soil Moisture datasets has the desired grid mapping now:</p> <p>The grid mapping of Land Cover which was the target grid mapping:</p> In\u00a0[23]: Copied! <pre>target_gm\n</pre> target_gm Out[23]: <p>class: RegularGridMapping</p> <ul> <li>is_regular: True</li> <li>is_j_axis_up: False</li> <li>is_lon_360: False</li> <li>crs: EPSG:4326</li> <li>xy_res: (0.002777775, 0.002777775)</li> <li>xy_bbox: (15, 60, 25, 70)</li> <li>ij_bbox: (0, 0, 3600, 3600)</li> <li>xy_dim_names: ('lon', 'lat')</li> <li>xy_var_names: ('lon', 'lat')</li> <li>size: (3600, 3600)</li> <li>tile_size: (2160, 2160)</li> </ul> <p>The grid mapping of our resampled Soil Moisture, which is as expeced the same as the target grid mapping:</p> In\u00a0[24]: Copied! <pre>GridMapping.from_dataset(resampled_SM)\n</pre> GridMapping.from_dataset(resampled_SM) Out[24]: <p>class: RegularGridMapping</p> <ul> <li>is_regular: True</li> <li>is_j_axis_up: False</li> <li>is_lon_360: False</li> <li>crs: EPSG:4326</li> <li>xy_res: (0.002777775, 0.002777775)</li> <li>xy_bbox: (15, 60, 25, 70)</li> <li>ij_bbox: (0, 0, 3600, 3600)</li> <li>xy_dim_names: ('lon', 'lat')</li> <li>xy_var_names: ('lon', 'lat')</li> <li>size: (3600, 3600)</li> <li>tile_size: (2160, 2160)</li> </ul> <p>Create a mask from LC classes</p> <p>Now that we have our two datasets with the same grid mapping we can use the land cover classes to mask the Soil Moisture datasets.</p> <p>Converting as they need to be an numpy array to be used with MaskSet:</p> In\u00a0[25]: Copied! <pre>LC.lccs_class.attrs[\"flag_values\"] = np.array(LC.lccs_class.attrs[\"flag_values\"])\n</pre> LC.lccs_class.attrs[\"flag_values\"] = np.array(LC.lccs_class.attrs[\"flag_values\"]) <p>Creating a Land Cover mask from the classes using xcube's MaskSet:</p> In\u00a0[26]: Copied! <pre>LC_mask = MaskSet(LC.lccs_class)\n</pre> LC_mask = MaskSet(LC.lccs_class) In\u00a0[27]: Copied! <pre>LC_mask\n</pre> LC_mask Out[27]: Flag nameMaskValue no_dataNone0 cropland_rainfedNone10 cropland_rainfed_herbaceous_coverNone11 cropland_rainfed_tree_or_shrub_coverNone12 cropland_irrigatedNone20 mosaic_croplandNone30 mosaic_natural_vegetationNone40 tree_broadleaved_evergreen_closed_to_openNone50 tree_broadleaved_deciduous_closed_to_openNone60 tree_broadleaved_deciduous_closedNone61 tree_broadleaved_deciduous_openNone62 tree_needleleaved_evergreen_closed_to_openNone70 tree_needleleaved_evergreen_closedNone71 tree_needleleaved_evergreen_openNone72 tree_needleleaved_deciduous_closed_to_openNone80 tree_needleleaved_deciduous_closedNone81 tree_needleleaved_deciduous_openNone82 tree_mixedNone90 mosaic_tree_and_shrubNone100 mosaic_herbaceousNone110 shrublandNone120 shrubland_evergreenNone121 shrubland_deciduousNone122 grasslandNone130 lichens_and_mossesNone140 sparse_vegetationNone150 sparse_treeNone151 sparse_shrubNone152 sparse_herbaceousNone153 tree_cover_flooded_fresh_or_brakish_waterNone160 tree_cover_flooded_saline_waterNone170 shrub_or_herbaceous_cover_floodedNone180 urbanNone190 bare_areasNone200 bare_areas_consolidatedNone201 bare_areas_unconsolidatedNone202 waterNone210 snow_and_iceNone220 <p>For the masking to work, both data arrays must have identical coordinates, we thus enforce this condition by assigning coordinates and merging into one dataset</p> <p>For the masking example, one specific class, rainfed cropland, is choosen here:</p> In\u00a0[28]: Copied! <pre>LC_cropmask = LC_mask.cropland_rainfed.to_dataset()\n</pre> LC_cropmask = LC_mask.cropland_rainfed.to_dataset() <p>Although the coordinates of the LC dataset and the resampled_SM dataset are almost identical, tiny numerical differences would prevent the masking from working. Thus, the coordinates from the resampled_SM dataset are assigned as the coordinates to the LC_cropmask</p> In\u00a0[29]: Copied! <pre>LC_cropmask = LC_cropmask.assign_coords(lat=resampled_SM.lat, lon=resampled_SM.lon)\n</pre> LC_cropmask = LC_cropmask.assign_coords(lat=resampled_SM.lat, lon=resampled_SM.lon) <p>The resulting dataset is again a data cube with only one time slices</p> In\u00a0[30]: Copied! <pre>LC_cropmask\n</pre> LC_cropmask Out[30]: <pre>&lt;xarray.Dataset&gt; Size: 13MB\nDimensions:           (time: 1, lat: 3600, lon: 3600)\nCoordinates:\n  * time              (time) datetime64[ns] 8B 2020-01-01\n  * lat               (lat) float64 29kB 70.0 70.0 69.99 ... 60.01 60.0 60.0\n  * lon               (lon) float64 29kB 15.0 15.0 15.01 ... 24.99 25.0 25.0\nData variables:\n    cropland_rainfed  (time, lat, lon) uint8 13MB dask.array&lt;chunksize=(1, 1440, 1080), meta=np.ndarray&gt;</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 1</li><li>lat: 3600</li><li>lon: 3600</li></ul></li><li>Coordinates: (3)<ul><li>time(time)datetime64[ns]2020-01-01axis :Tbounds :time_boundslong_name :timestandard_name :time<pre>array(['2020-01-01T00:00:00.000000000'], dtype='datetime64[ns]')</pre></li><li>lat(lat)float6470.0 70.0 69.99 ... 60.01 60.0 60.0long_name :latitude coordinatestandard_name :latitudeunits :degrees_north<pre>array([69.998611, 69.995833, 69.993056, ..., 60.006944, 60.004167, 60.001389])</pre></li><li>lon(lon)float6415.0 15.0 15.01 ... 24.99 25.0 25.0long_name :longitude coordinatestandard_name :longitudeunits :degrees_east<pre>array([15.001389, 15.004167, 15.006944, ..., 24.993056, 24.995833, 24.998611])</pre></li></ul></li><li>Data variables: (1)<ul><li>cropland_rainfed(time, lat, lon)uint8dask.array&lt;chunksize=(1, 1440, 1080), meta=np.ndarray&gt;  Array   Chunk   Bytes   12.36 MiB   4.45 MiB   Shape   (1, 3600, 3600)   (1, 2160, 2160)   Dask graph   6 chunks in 7 graph layers   Data type   uint8 numpy.ndarray  3600 3600 1 </li></ul></li><li>Indexes: (3)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2020-01-01'], dtype='datetime64[ns]', name='time', freq=None))</pre></li><li>latPandasIndex<pre>PandasIndex(Index([     69.9986111125,  69.99583333472145,   69.9930555569429,\n        69.99027777916436,   69.9875000013858,  69.98472222360725,\n         69.9819444458287,  69.97916666805015,  69.97638889027161,\n        69.97361111249306,\n       ...\n        60.02638888750695,  60.02361110972839, 60.020833331949845,\n         60.0180555541713,  60.01527777639275, 60.012499998614196,\n        60.00972222083565, 60.006944443057094,  60.00416666527855,\n            60.0013888875],\n      dtype='float64', name='lat', length=3600))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([     15.0013888875, 15.004166665278548,   15.0069444430571,\n       15.009722220835648, 15.012499998614198, 15.015277776392747,\n       15.018055554171298, 15.020833331949847, 15.023611109728396,\n       15.026388887506945,\n       ...\n       24.973611112493053, 24.976388890271604, 24.979166668050155,\n       24.981944445828702, 24.984722223607253, 24.987500001385804,\n        24.99027777916435, 24.993055556942902, 24.995833334721453,\n            24.9986111125],\n      dtype='float64', name='lon', length=3600))</pre></li></ul></li><li>Attributes: (0)</li></ul> <p>Because both datasets share the same spatial grid, we can now insert the cropland_rainfed mask as an additional data variable into the resampled_SM datacube. By selecting the only time slice, we remove the time information here. The new data variable crop_mask in the resulting dataset thus only depending on lat and lon.</p> In\u00a0[31]: Copied! <pre>resampled_SM[\"crop_mask\"] = LC_cropmask.cropland_rainfed.isel(time=0)\n</pre> resampled_SM[\"crop_mask\"] = LC_cropmask.cropland_rainfed.isel(time=0) In\u00a0[32]: Copied! <pre>resampled_SM\n</pre> resampled_SM Out[32]: <pre>&lt;xarray.Dataset&gt; Size: 16GB\nDimensions:         (time: 31, lat: 3600, lon: 3600)\nCoordinates:\n  * time            (time) datetime64[ns] 248B 2020-08-01 ... 2020-08-31\n  * lon             (lon) float64 29kB 15.0 15.0 15.01 15.01 ... 24.99 25.0 25.0\n  * lat             (lat) float64 29kB 70.0 70.0 69.99 69.99 ... 60.01 60.0 60.0\nData variables:\n    dnflag          (time, lat, lon) float32 2GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    flag            (time, lat, lon) float32 2GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    freqbandID      (time, lat, lon) float32 2GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    mode            (time, lat, lon) float32 2GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    sensor          (time, lat, lon) float64 3GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    sm              (time, lat, lon) float32 2GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    sm_uncertainty  (time, lat, lon) float32 2GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    t0              (time, lat, lon) float64 3GB dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;\n    crop_mask       (lat, lon) uint8 13MB dask.array&lt;chunksize=(1440, 1080), meta=np.ndarray&gt;\nAttributes: (12/44)\n    Conventions:                  CF-1.9\n    cdm_data_type:                Grid\n    comment:                      This dataset was produced with funding of t...\n    contact:                      cci_sm_contact@eodc.eu\n    creator_email:                cci_sm_developer@eodc.eu\n    creator_name:                 Department of Geodesy and Geoinformation, V...\n    ...                           ...\n    time_coverage_end_product:    20211231T235959Z\n    time_coverage_resolution:     P1D\n    time_coverage_start:          1978-11-01 00:00:00\n    time_coverage_start_product:  19781101T000000Z\n    title:                        ESA CCI Surface Soil Moisture COMBINED acti...\n    tracking_id:                  ad35798e-58e0-488f-b5b9-593874a47700</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 31</li><li>lat: 3600</li><li>lon: 3600</li></ul></li><li>Coordinates: (3)<ul><li>time(time)datetime64[ns]2020-08-01 ... 2020-08-31<pre>array(['2020-08-01T00:00:00.000000000', '2020-08-02T00:00:00.000000000',\n       '2020-08-03T00:00:00.000000000', '2020-08-04T00:00:00.000000000',\n       '2020-08-05T00:00:00.000000000', '2020-08-06T00:00:00.000000000',\n       '2020-08-07T00:00:00.000000000', '2020-08-08T00:00:00.000000000',\n       '2020-08-09T00:00:00.000000000', '2020-08-10T00:00:00.000000000',\n       '2020-08-11T00:00:00.000000000', '2020-08-12T00:00:00.000000000',\n       '2020-08-13T00:00:00.000000000', '2020-08-14T00:00:00.000000000',\n       '2020-08-15T00:00:00.000000000', '2020-08-16T00:00:00.000000000',\n       '2020-08-17T00:00:00.000000000', '2020-08-18T00:00:00.000000000',\n       '2020-08-19T00:00:00.000000000', '2020-08-20T00:00:00.000000000',\n       '2020-08-21T00:00:00.000000000', '2020-08-22T00:00:00.000000000',\n       '2020-08-23T00:00:00.000000000', '2020-08-24T00:00:00.000000000',\n       '2020-08-25T00:00:00.000000000', '2020-08-26T00:00:00.000000000',\n       '2020-08-27T00:00:00.000000000', '2020-08-28T00:00:00.000000000',\n       '2020-08-29T00:00:00.000000000', '2020-08-30T00:00:00.000000000',\n       '2020-08-31T00:00:00.000000000'], dtype='datetime64[ns]')</pre></li><li>lon(lon)float6415.0 15.0 15.01 ... 24.99 25.0 25.0long_name :longitude coordinatestandard_name :longitudeunits :degrees_east<pre>array([15.001389, 15.004167, 15.006944, ..., 24.993056, 24.995833, 24.998611])</pre></li><li>lat(lat)float6470.0 70.0 69.99 ... 60.01 60.0 60.0long_name :latitude coordinatestandard_name :latitudeunits :degrees_north<pre>array([69.998611, 69.995833, 69.993056, ..., 60.006944, 60.004167, 60.001389])</pre></li></ul></li><li>Data variables: (9)<ul><li>dnflag(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['NaN', 'day', 'night']bits :['0b0', '0b1', '0b10']dtype :int8flag_meanings :['NaN', 'day', 'night', 'day_night_combination']flag_values :[0, 1, 2, 3]long_name :Day / Night Flagvalid_range :[0, 3]  Array   Chunk   Bytes   1.50 GiB   17.80 MiB   Shape   (31, 3600, 3600)   (1, 2160, 2160)   Dask graph   124 chunks in 128 graph layers   Data type   float32 numpy.ndarray  3600 3600 31 </li><li>flag(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['no_data_inconsistency_detected', 'snow_coverage_or_temperature_below_zero', 'dense_vegetation', 'others_no_convergence_in_the_model_thus_no_valid_sm_estimates', 'soil_moisture_value_exceeds_physical_boundary', 'weight_of_measurement_below_threshold', 'all_datasets_deemed_unreliable', 'barren_ground_advisory_flag', 'NaN']bits :['0b0', '0b1', '0b10', '0b100', '0b1000', '0b10000', '0b100000', '0b1000000', '0b10000000']dtype :int16flag_meanings :['no_data_inconsistency_detected', 'snow_coverage_or_temperature_below_zero', 'dense_vegetation', 'combination_of_flag_values_1_and_2', 'soil_moisture_value_exceeds_physical_boundary', 'combination_of_flag_values_1_and_8', 'combination_of_flag_values_2_and_8', 'barren_ground_advisory_flag', 'combination_of_flag_values_1_and_64', 'combination_of_flag_values_2_and_64', 'combination_of_flag_values_1_and_2_and_64', 'combination_of_flag_values_8_and_64', 'combination_of_flag_values_1_and_8_and_64', 'combination_of_flag_values_2_and_8_and_64', 'combination_of_flag_values_1_and_2_and_4_and_8_and_16_and_32_and_64']flag_values :[0, 1, 2, 3, 8, 9, 10, 64, 65, 66, 67, 72, 73, 74, 127]long_name :Flagstandard_name :soil_moisture_content status_flagvalid_range :[0, 255]  Array   Chunk   Bytes   1.50 GiB   17.80 MiB   Shape   (31, 3600, 3600)   (1, 2160, 2160)   Dask graph   124 chunks in 128 graph layers   Data type   float32 numpy.ndarray  3600 3600 31 </li><li>freqbandID(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['NaN', 'L14', 'C53', 'C66', 'C68', 'C69', 'C73', 'X107', 'K194', 'MODEL']bits :['0b0', '0b1', '0b10', '0b100', '0b1000', '0b10000', '0b100000', '0b1000000', '0b10000000', '0b100000000']dtype :int16flag_meanings :['NaN', 'C66', 'X107', 'C66+X107']flag_values :[0, 4, 64, 68]long_name :Frequency Band Identificationvalid_range :[0, 511]  Array   Chunk   Bytes   1.50 GiB   17.80 MiB   Shape   (31, 3600, 3600)   (1, 2160, 2160)   Dask graph   124 chunks in 128 graph layers   Data type   float32 numpy.ndarray  3600 3600 31 </li><li>mode(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['NaN', 'ascending', 'descending']bits :['0b0', '0b1', '0b10']dtype :int8flag_meanings :['NaN', 'ascending', 'descending', 'ascending_descending_combination']flag_values :[0, 1, 2, 3]long_name :Satellite Modevalid_range :[0, 3]  Array   Chunk   Bytes   1.50 GiB   17.80 MiB   Shape   (31, 3600, 3600)   (1, 2160, 2160)   Dask graph   124 chunks in 128 graph layers   Data type   float32 numpy.ndarray  3600 3600 31 </li><li>sensor(time, lat, lon)float64dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;_CoordinateAxes :time lat lonbit_meanings :['NaN', 'SMMR', 'SSMI', 'TMI', 'AMSRE', 'WindSat', 'AMSR2', 'SMOS', 'AMIWS', 'ASCATA', 'ASCATB', 'SMAP', 'MODEL', 'GPM', 'FY3B', 'FY3D', 'ASCATC', 'FY3C']bits :['0b0', '0b1', '0b10', '0b100', '0b1000', '0b10000', '0b100000', '0b1000000', '0b10000000', '0b100000000', '0b1000000000', '0b10000000000', '0b100000000000', '0b1000000000000', '0b10000000000000', '0b100000000000000', '0b1000000000000000', '0b10000000000000000']dtype :int32flag_meanings :['NaN', 'SMMR']flag_values :[0, 1]long_name :Sensorvalid_range :[0, 131071]  Array   Chunk   Bytes   2.99 GiB   35.60 MiB   Shape   (31, 3600, 3600)   (1, 2160, 2160)   Dask graph   124 chunks in 128 graph layers   Data type   float64 numpy.ndarray  3600 3600 31 </li><li>sm(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;_CoordinateAxes :time lat lonancillary_variables :sm_uncertainty flag t0dtype :float32long_name :Volumetric Soil Moisturestandard_name :soil_moisture_contentunits :m3 m-3valid_range :[0.0, 1.0]  Array   Chunk   Bytes   1.50 GiB   17.80 MiB   Shape   (31, 3600, 3600)   (1, 2160, 2160)   Dask graph   124 chunks in 128 graph layers   Data type   float32 numpy.ndarray  3600 3600 31 </li><li>sm_uncertainty(time, lat, lon)float32dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;_CoordinateAxes :time lat londtype :float32long_name :Volumetric Soil Moisture Uncertaintystandard_name :soil_moisture_content standard_errorunits :m3 m-3valid_range :[0.0, 1.0]  Array   Chunk   Bytes   1.50 GiB   17.80 MiB   Shape   (31, 3600, 3600)   (1, 2160, 2160)   Dask graph   124 chunks in 128 graph layers   Data type   float32 numpy.ndarray  3600 3600 31 </li><li>t0(time, lat, lon)float64dask.array&lt;chunksize=(1, 2160, 2160), meta=np.ndarray&gt;_CoordinateAxes :time lat londtype :float64long_name :Observation Timestampunits :days after 1970-01-01 00:00:00 UTCvalid_range :[3225.0, 3227.0]  Array   Chunk   Bytes   2.99 GiB   35.60 MiB   Shape   (31, 3600, 3600)   (1, 2160, 2160)   Dask graph   124 chunks in 128 graph layers   Data type   float64 numpy.ndarray  3600 3600 31 </li><li>crop_mask(lat, lon)uint8dask.array&lt;chunksize=(1440, 1080), meta=np.ndarray&gt;  Array   Chunk   Bytes   12.36 MiB   4.45 MiB   Shape   (3600, 3600)   (2160, 2160)   Dask graph   6 chunks in 8 graph layers   Data type   uint8 numpy.ndarray  3600 3600 </li></ul></li><li>Indexes: (3)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2020-08-01', '2020-08-02', '2020-08-03', '2020-08-04',\n               '2020-08-05', '2020-08-06', '2020-08-07', '2020-08-08',\n               '2020-08-09', '2020-08-10', '2020-08-11', '2020-08-12',\n               '2020-08-13', '2020-08-14', '2020-08-15', '2020-08-16',\n               '2020-08-17', '2020-08-18', '2020-08-19', '2020-08-20',\n               '2020-08-21', '2020-08-22', '2020-08-23', '2020-08-24',\n               '2020-08-25', '2020-08-26', '2020-08-27', '2020-08-28',\n               '2020-08-29', '2020-08-30', '2020-08-31'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([     15.0013888875, 15.004166665278548,   15.0069444430571,\n       15.009722220835648, 15.012499998614198, 15.015277776392747,\n       15.018055554171298, 15.020833331949847, 15.023611109728396,\n       15.026388887506945,\n       ...\n       24.973611112493053, 24.976388890271604, 24.979166668050155,\n       24.981944445828702, 24.984722223607253, 24.987500001385804,\n        24.99027777916435, 24.993055556942902, 24.995833334721453,\n            24.9986111125],\n      dtype='float64', name='lon', length=3600))</pre></li><li>latPandasIndex<pre>PandasIndex(Index([     69.9986111125,  69.99583333472145,   69.9930555569429,\n        69.99027777916436,   69.9875000013858,  69.98472222360725,\n         69.9819444458287,  69.97916666805015,  69.97638889027161,\n        69.97361111249306,\n       ...\n        60.02638888750695,  60.02361110972839, 60.020833331949845,\n         60.0180555541713,  60.01527777639275, 60.012499998614196,\n        60.00972222083565, 60.006944443057094,  60.00416666527855,\n            60.0013888875],\n      dtype='float64', name='lat', length=3600))</pre></li></ul></li><li>Attributes: (44)Conventions :CF-1.9cdm_data_type :Gridcomment :This dataset was produced with funding of the ESA CCI+ Soil Moisture project; ESRIN Contract No: 4000126684/19/I-NBcontact :cci_sm_contact@eodc.eucreator_email :cci_sm_developer@eodc.eucreator_name :Department of Geodesy and Geoinformation, Vienna University of Technologycreator_url :https://climers.geo.tuwien.ac.at/date_created :File created: 2022-04-06 12:31:39.526359geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.25 degreegeospatial_lat_units :degrees_northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :0.25 degreegeospatial_lon_units :degrees_eastgeospatial_vertical_max :0.0geospatial_vertical_min :0.0history :2022-04-06 12:29:35 - product produced 2023-08-09 17:33:51 - converted by nc2zarr, version 1.1.2.dev0id :ESACCI-SOILMOISTURE-L3S-SSMV-COMBINED-20211231000000-fv07.1.ncinstitution :TU Wien (AUT); VanderSat B.V. (NL)key_variables :smkeywords :soil moisture, soil water content, microwave remote sensing, active, passive, combined, climate data recordkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Data use is free and open for all registered users.naming_authority :TU Wienplatform :Nimbus 7, DMSP, TRMM, AQUA, Coriolis, GCOM-W1, MIRAS, SMAP, GPM, FengYun-3B, FengYun-3C, FengYun-3D; ERS-1, ERS-2, METOP-A, METOP-Bprocessing_level :Quality-controlled, super-collocated (L3S) Satellite Soil Moisture (SM) data from multiple sensorsproduct_version :v07.1project :Climate Change Initiative - European Space Agencyreferences :http://www.esa-soilmoisture-cci.org; Dorigo, W.A., Wagner, W., Albergel, C., Albrecht, F.,  Balsamo, G., Brocca, L., Chung, D., Ertl, M., Forkel, M., Gruber, A., Haas, E., Hamer, D. P. Hirschi, M., Ikonen, J., De Jeu, R. Kidd, R. Lahoz, W., Liu, Y.Y., Miralles, D., Lecomte, P. (2017) ESA CCI Soil Moisture for improved Earth system understanding: State-of-the art and future directions. In Remote Sensing of Environment, 2017, ISSN 0034-4257, https://doi.org/10.1016/j.rse.2017.07.001; Gruber, A., Scanlon, T., van der Schalie, R., Wagner, W., Dorigo, W. (2019) Evolution of the ESA CCI Soil Moisture Climate Data Records and their underlying merging methodology. Earth System Science Data 11, 717-739, https://doi.org/10.5194/essd-11-717-2019; Gruber, A., Dorigo, W. A., Crow, W., Wagner W. (2017). Triple Collocation-Based Merging of Satellite Soil Moisture Retrievals. IEEE Transactions on Geoscience and Remote Sensing. PP. 1-13. https://doi.org/10.1109/TGRS.2017.2734070sensor :SMMR, SSM/I, TMI, AMSR-E, WindSat, AMSR2, SMOS, SMAP_radiometer, GMI, VIRR-3B, VIRR-3C, VIRR-3D; AMI-WS, ASCAT-A, ASCAT-B, ASCAT-Csource :WARP 5.5R1.1/AMI-WS/ERS12 Level 2 Soil Moisture; WARP 5.4R1.0/AMI-WS/ERS2 Level 2 Soil Moisture; H115: Metop ASCAT Surface Soil Moisture Climate Data Record v5 12.5 km sampling, DOI: 10.15770/EUM_SAF_H_0006; H116: Metop ASCAT Surface Soil Moisture Climate Data Record v5 Extension 12.5 km sampling; H115: Metop ASCAT Surface Soil Moisture Climate Data Record v5 12.5 km sampling, DOI: 10.15770/EUM_SAF_H_0006; H116: Metop ASCAT Surface Soil Moisture Climate Data Record v5 Extension 12.5 km sampling; H115: Metop ASCAT Surface Soil Moisture Climate Data Record v5 12.5 km sampling, DOI: 10.15770/EUM_SAF_H_0006; H116: Metop ASCAT Surface Soil Moisture Climate Data Record v5 Extension 12.5 km sampling;; LPRMv7/SMMR/Nimbus 7 L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/SSMI/F08, F11, F13 DMSP L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/TMI/TRMM L2 Surface Soil Moisture, Ancillary Params, and QC; LPRMv7/AMSR-E/Aqua L2B Surface Soil Moisture, Ancillary Params, and QC; LPRMv7/WINDSAT/CORIOLIS L2 Surface Soil Moisture, Ancillary Params, and QC; LPRMv7/AMSR2/GCOM-W1 L3 Surface Soil Moisture, Ancillary Params; LPRMv7/SMOS/MIRAS L3 Surface Soil Moisture, CATDS Level 3 Brightness Temperatures (L3TB) version 300 RE03 &amp; RE04; LPRMv7/SMAP_radiometer/SMAP L2 Surface Soil Moisture, Ancillary Params, and QC; LPRMv7/GMI/GPM L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/VIRR/FengYun-3B L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/VIRR/FengYun-3C L3 Surface Soil Moisture, Ancillary Params, and quality flags; LPRMv7/VIRR/FengYun-3D L3 Surface Soil Moisture, Ancillary Params, and quality flags;;spatial_resolution :25kmstandard_name_vocabulary :CF Standard Name Table v77summary :This dataset was produced with funding of the ESA CCI+ Soil Moisture project; ESRIN Contract No: 4000126684/19/I-NBtime_coverage_duration :P43Ytime_coverage_end :2021-12-31 00:00:00time_coverage_end_product :20211231T235959Ztime_coverage_resolution :P1Dtime_coverage_start :1978-11-01 00:00:00time_coverage_start_product :19781101T000000Ztitle :ESA CCI Surface Soil Moisture COMBINED active+passive Producttracking_id :ad35798e-58e0-488f-b5b9-593874a47700</li></ul> <p>Plot the mask of rainfed croplands (1 True, 0 False).</p> In\u00a0[33]: Copied! <pre>resampled_SM.crop_mask.plot()\n</pre> resampled_SM.crop_mask.plot() Out[33]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7fbca8296850&gt;</pre> <p>Mask one data array with a condition from another, inserted into one dataset</p> <p>Select Soil Moisture (sm) only for rainfed croplands. The result is again a datacube of the same dimensions as the inputs, because we removed the time information from the cropmask before.</p> In\u00a0[34]: Copied! <pre>SM_crop = resampled_SM.sm.where(\n    resampled_SM.crop_mask == 1\n)\n\nSM_crop\n</pre> SM_crop = resampled_SM.sm.where(     resampled_SM.crop_mask == 1 )  SM_crop Out[34]: <pre>&lt;xarray.DataArray 'sm' (time: 31, lat: 3600, lon: 3600)&gt; Size: 2GB\ndask.array&lt;where, shape=(31, 3600, 3600), dtype=float32, chunksize=(1, 1440, 1080), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * time     (time) datetime64[ns] 248B 2020-08-01 2020-08-02 ... 2020-08-31\n  * lon      (lon) float64 29kB 15.0 15.0 15.01 15.01 ... 24.99 24.99 25.0 25.0\n  * lat      (lat) float64 29kB 70.0 70.0 69.99 69.99 ... 60.01 60.01 60.0 60.0\nAttributes:\n    _CoordinateAxes:      time lat lon\n    ancillary_variables:  sm_uncertainty flag t0\n    dtype:                float32\n    long_name:            Volumetric Soil Moisture\n    standard_name:        soil_moisture_content\n    units:                m3 m-3\n    valid_range:          [0.0, 1.0]</pre>xarray.DataArray'sm'<ul><li>time: 31</li><li>lat: 3600</li><li>lon: 3600</li></ul><ul><li>dask.array&lt;chunksize=(1, 1440, 1080), meta=np.ndarray&gt;  Array   Chunk   Bytes   1.50 GiB   5.93 MiB   Shape   (31, 3600, 3600)   (1, 1440, 1080)   Dask graph   372 chunks in 140 graph layers   Data type   float32 numpy.ndarray  3600 3600 31 </li><li>Coordinates: (3)<ul><li>time(time)datetime64[ns]2020-08-01 ... 2020-08-31<pre>array(['2020-08-01T00:00:00.000000000', '2020-08-02T00:00:00.000000000',\n       '2020-08-03T00:00:00.000000000', '2020-08-04T00:00:00.000000000',\n       '2020-08-05T00:00:00.000000000', '2020-08-06T00:00:00.000000000',\n       '2020-08-07T00:00:00.000000000', '2020-08-08T00:00:00.000000000',\n       '2020-08-09T00:00:00.000000000', '2020-08-10T00:00:00.000000000',\n       '2020-08-11T00:00:00.000000000', '2020-08-12T00:00:00.000000000',\n       '2020-08-13T00:00:00.000000000', '2020-08-14T00:00:00.000000000',\n       '2020-08-15T00:00:00.000000000', '2020-08-16T00:00:00.000000000',\n       '2020-08-17T00:00:00.000000000', '2020-08-18T00:00:00.000000000',\n       '2020-08-19T00:00:00.000000000', '2020-08-20T00:00:00.000000000',\n       '2020-08-21T00:00:00.000000000', '2020-08-22T00:00:00.000000000',\n       '2020-08-23T00:00:00.000000000', '2020-08-24T00:00:00.000000000',\n       '2020-08-25T00:00:00.000000000', '2020-08-26T00:00:00.000000000',\n       '2020-08-27T00:00:00.000000000', '2020-08-28T00:00:00.000000000',\n       '2020-08-29T00:00:00.000000000', '2020-08-30T00:00:00.000000000',\n       '2020-08-31T00:00:00.000000000'], dtype='datetime64[ns]')</pre></li><li>lon(lon)float6415.0 15.0 15.01 ... 24.99 25.0 25.0long_name :longitude coordinatestandard_name :longitudeunits :degrees_east<pre>array([15.001389, 15.004167, 15.006944, ..., 24.993056, 24.995833, 24.998611])</pre></li><li>lat(lat)float6470.0 70.0 69.99 ... 60.01 60.0 60.0long_name :latitude coordinatestandard_name :latitudeunits :degrees_north<pre>array([69.998611, 69.995833, 69.993056, ..., 60.006944, 60.004167, 60.001389])</pre></li></ul></li><li>Indexes: (3)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2020-08-01', '2020-08-02', '2020-08-03', '2020-08-04',\n               '2020-08-05', '2020-08-06', '2020-08-07', '2020-08-08',\n               '2020-08-09', '2020-08-10', '2020-08-11', '2020-08-12',\n               '2020-08-13', '2020-08-14', '2020-08-15', '2020-08-16',\n               '2020-08-17', '2020-08-18', '2020-08-19', '2020-08-20',\n               '2020-08-21', '2020-08-22', '2020-08-23', '2020-08-24',\n               '2020-08-25', '2020-08-26', '2020-08-27', '2020-08-28',\n               '2020-08-29', '2020-08-30', '2020-08-31'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([     15.0013888875, 15.004166665278548,   15.0069444430571,\n       15.009722220835648, 15.012499998614198, 15.015277776392747,\n       15.018055554171298, 15.020833331949847, 15.023611109728396,\n       15.026388887506945,\n       ...\n       24.973611112493053, 24.976388890271604, 24.979166668050155,\n       24.981944445828702, 24.984722223607253, 24.987500001385804,\n        24.99027777916435, 24.993055556942902, 24.995833334721453,\n            24.9986111125],\n      dtype='float64', name='lon', length=3600))</pre></li><li>latPandasIndex<pre>PandasIndex(Index([     69.9986111125,  69.99583333472145,   69.9930555569429,\n        69.99027777916436,   69.9875000013858,  69.98472222360725,\n         69.9819444458287,  69.97916666805015,  69.97638889027161,\n        69.97361111249306,\n       ...\n        60.02638888750695,  60.02361110972839, 60.020833331949845,\n         60.0180555541713,  60.01527777639275, 60.012499998614196,\n        60.00972222083565, 60.006944443057094,  60.00416666527855,\n            60.0013888875],\n      dtype='float64', name='lat', length=3600))</pre></li></ul></li><li>Attributes: (7)_CoordinateAxes :time lat lonancillary_variables :sm_uncertainty flag t0dtype :float32long_name :Volumetric Soil Moisturestandard_name :soil_moisture_contentunits :m3 m-3valid_range :[0.0, 1.0]</li></ul> <p>Visualise result: sm only on rainfed cropland</p> In\u00a0[35]: Copied! <pre>SM_crop.isel(time=0).plot()\n</pre> SM_crop.isel(time=0).plot() Out[35]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7fbca830dc90&gt;</pre> <p>The histogram of the sm variable on rainfed croplands:</p> In\u00a0[36]: Copied! <pre>SM_crop.isel(time=0).plot.hist(bins=100)\n</pre> SM_crop.isel(time=0).plot.hist(bins=100) Out[36]: <pre>(array([1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        2.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0000e+00, 0.0000e+00,\n        1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 6.0000e+00,\n        4.0000e+00, 0.0000e+00, 1.0000e+01, 1.7000e+01, 2.5000e+01,\n        3.0000e+01, 2.1000e+01, 8.2000e+01, 2.0000e+02, 2.7700e+02,\n        4.1500e+02, 5.9700e+02, 5.1100e+02, 9.4800e+02, 1.2230e+03,\n        1.2290e+03, 1.3000e+03, 1.2380e+03, 1.9540e+03, 4.3730e+03,\n        3.7250e+03, 5.9280e+03, 6.6450e+03, 7.2750e+03, 8.0110e+03,\n        8.6910e+03, 9.9490e+03, 1.0385e+04, 1.0353e+04, 1.0969e+04,\n        1.0769e+04, 8.9880e+03, 8.6950e+03, 9.2770e+03, 8.6980e+03,\n        7.1390e+03, 4.7680e+03, 2.8230e+03, 1.0370e+03, 9.5100e+02,\n        9.8600e+02, 9.1500e+02, 7.7800e+02, 6.8000e+02, 7.4100e+02,\n        7.4200e+02, 7.6600e+02, 7.8800e+02, 8.9100e+02, 9.5500e+02,\n        9.9200e+02, 1.1020e+03, 1.2780e+03, 1.1710e+03, 2.1290e+03,\n        3.6200e+03, 2.6400e+03, 3.0710e+03, 3.4170e+03, 8.3040e+03,\n        2.4350e+03, 2.2000e+03, 2.1750e+03, 2.2330e+03, 2.1700e+03,\n        2.0010e+03, 1.7240e+03, 1.2250e+03, 1.3030e+03, 1.2080e+03,\n        1.1760e+03, 7.2000e+02, 5.9800e+02, 1.9600e+02, 2.9000e+01]),\n array([0.17464125, 0.17603008, 0.1774189 , 0.17880774, 0.18019657,\n        0.18158539, 0.18297422, 0.18436304, 0.18575187, 0.1871407 ,\n        0.18852952, 0.18991835, 0.19130719, 0.19269601, 0.19408484,\n        0.19547367, 0.19686249, 0.19825132, 0.19964015, 0.20102897,\n        0.20241781, 0.20380664, 0.20519546, 0.20658429, 0.20797312,\n        0.20936194, 0.21075077, 0.21213959, 0.21352842, 0.21491724,\n        0.21630608, 0.21769491, 0.21908373, 0.22047256, 0.22186139,\n        0.22325021, 0.22463904, 0.22602788, 0.22741669, 0.22880553,\n        0.23019436, 0.23158318, 0.23297201, 0.23436084, 0.23574966,\n        0.23713849, 0.23852733, 0.23991615, 0.24130498, 0.24269381,\n        0.24408263, 0.24547145, 0.2468603 , 0.24824911, 0.24963793,\n        0.25102675, 0.2524156 , 0.25380442, 0.25519323, 0.25658208,\n        0.2579709 , 0.25935972, 0.26074857, 0.26213738, 0.2635262 ,\n        0.26491505, 0.26630387, 0.26769269, 0.26908153, 0.27047035,\n        0.27185917, 0.27324802, 0.27463683, 0.27602565, 0.2774145 ,\n        0.27880332, 0.28019214, 0.28158098, 0.2829698 , 0.28435862,\n        0.28574747, 0.28713629, 0.2885251 , 0.28991395, 0.29130277,\n        0.29269159, 0.29408044, 0.29546925, 0.29685807, 0.29824692,\n        0.29963574, 0.30102456, 0.3024134 , 0.30380222, 0.30519104,\n        0.30657989, 0.30796871, 0.30935752, 0.31074637, 0.31213516,\n        0.31352401]),\n &lt;BarContainer object of 100 artists&gt;)</pre> <p>For comparison, we also visualise the full soil moisture product, without masking condition.</p> In\u00a0[37]: Copied! <pre>SM.sm.isel(time=0).plot()\n</pre> SM.sm.isel(time=0).plot() Out[37]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7fbca810dd50&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/Spatial_Regridding_and_Masking/#spatial-regridding-and-masking-data-cubes","title":"Spatial Regridding and masking data cubes\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Spatial_Regridding_and_Masking/#a-deepesdl-example-notebook","title":"A DeepESDL example notebook\u00b6","text":"<p>This notebook demonstrates how to access two different data sets, transform one of them so that both share the same spatial grid, and mask one dataset by applying a condition from the other dataset.</p> <p>Please, also refer to the DeepESDL documentation and visit the platform's website for further information!</p> <p>Brockmann Consult, 2024</p> <p>This notebook runs with the python environment <code>deepesdl-xcube-1.7.0</code>, please checkout the documentation for help on changing the environment.</p>"},{"location":"guide/jupyterlab/notebooks/Upload_files_to_shared_team_s3_storage/","title":"Upload files to shared team s3 storage","text":"In\u00a0[1]: Copied! <pre># needed for uploading files to s3 storage\nimport os\nimport boto3\nfrom botocore.exceptions import NoCredentialsError\n\n# needed for access of uploaded files\nfrom xcube.core.store import new_data_store\n</pre> # needed for uploading files to s3 storage import os import boto3 from botocore.exceptions import NoCredentialsError  # needed for access of uploaded files from xcube.core.store import new_data_store <p>Get the environment variables, which are necessary for later specifications</p> In\u00a0[2]: Copied! <pre>S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"]\nS3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"]\nS3_USER_STORAGE_BUCKET = os.environ[\"endpoint\"]\n</pre> S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"] S3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"] S3_USER_STORAGE_BUCKET = os.environ[\"endpoint\"] <p>Connect to your team storage in S3</p> In\u00a0[3]: Copied! <pre># Note:If you use a prefix when uploading the data so you need the parameter max_depth=2\nstore = new_data_store(\"s3\",\n                       root=S3_USER_STORAGE_BUCKET,\n                       storage_options=dict(anon=False,\n                                            key=S3_USER_STORAGE_KEY,\n                                            secret=S3_USER_STORAGE_SECRET))\n</pre> # Note:If you use a prefix when uploading the data so you need the parameter max_depth=2 store = new_data_store(\"s3\",                        root=S3_USER_STORAGE_BUCKET,                        storage_options=dict(anon=False,                                             key=S3_USER_STORAGE_KEY,                                             secret=S3_USER_STORAGE_SECRET)) <p>You can check which dataformats are supported in xcube s3 store. This way you can find out which files you could easily store and then access by xcube from the s3 team storage space.</p> In\u00a0[4]: Copied! <pre>store.get_data_opener_ids()\n</pre> store.get_data_opener_ids() Out[4]: <pre>('dataset:netcdf:s3',\n 'dataset:zarr:s3',\n 'dataset:levels:s3',\n 'mldataset:levels:s3',\n 'dataset:geotiff:s3',\n 'mldataset:geotiff:s3',\n 'geodataframe:shapefile:s3',\n 'geodataframe:geojson:s3')</pre> <p>To upload files from your workspace to the s3 team shared storage, you must specify where your input files are:</p> In\u00a0[5]: Copied! <pre>input_datasets_dir = os.path.expanduser(\"~/&lt;path-to-your-files&gt;\")\n</pre> input_datasets_dir = os.path.expanduser(\"~/\") In\u00a0[6]: Copied! <pre>local_store = new_data_store(\"file\",\n                             root=input_datasets_dir)\n</pre> local_store = new_data_store(\"file\",                              root=input_datasets_dir) In\u00a0[7]: Copied! <pre>list(local_store.get_data_ids())\n</pre> list(local_store.get_data_ids()) Out[7]: <pre>[]</pre> In\u00a0[8]: Copied! <pre># function used to upload data to s3 storage\ndef upload_to_team_s3_bucket(local_file, bucket, s3_file):\n    s3 = boto3.client('s3', \n                      aws_access_key_id=S3_USER_STORAGE_KEY,\n                      aws_secret_access_key=S3_USER_STORAGE_SECRET)\n\n    try:\n        s3.upload_file(local_file, bucket, s3_file)\n        print(f\"Upload Successful of file {local_file}\")\n        return True\n    except NoCredentialsError:\n        print(\"Credentials not available\")\n        return False\n</pre> # function used to upload data to s3 storage def upload_to_team_s3_bucket(local_file, bucket, s3_file):     s3 = boto3.client('s3',                        aws_access_key_id=S3_USER_STORAGE_KEY,                       aws_secret_access_key=S3_USER_STORAGE_SECRET)      try:         s3.upload_file(local_file, bucket, s3_file)         print(f\"Upload Successful of file {local_file}\")         return True     except NoCredentialsError:         print(\"Credentials not available\")         return False In\u00a0[\u00a0]: Copied! <pre># filter only for files in the directory\ndata_files = [file for file in os.listdir(input_datasets_dir) if os.path.isfile(os.path.join(input_datasets_dir, file))]\n</pre> # filter only for files in the directory data_files = [file for file in os.listdir(input_datasets_dir) if os.path.isfile(os.path.join(input_datasets_dir, file))] In\u00a0[\u00a0]: Copied! <pre>data_files\n</pre> data_files In\u00a0[\u00a0]: Copied! <pre>prefix = \"input-datasets\" # giving a prefix, so a direcory like structure is created in s3 \n</pre> prefix = \"input-datasets\" # giving a prefix, so a direcory like structure is created in s3  In\u00a0[\u00a0]: Copied! <pre># looping through datasets and uploading them to s3 \nfor data_file in data_files:\n    path = os.path.join(input_datasets_dir, data_file)\n    target_path = f\"{prefix}/{data_file}\"\n    upload_to_team_s3_bucket(path, S3_USER_STORAGE_BUCKET, target_path)\n</pre> # looping through datasets and uploading them to s3  for data_file in data_files:     path = os.path.join(input_datasets_dir, data_file)     target_path = f\"{prefix}/{data_file}\"     upload_to_team_s3_bucket(path, S3_USER_STORAGE_BUCKET, target_path) <p>Now lets check for the data: You need to instantiate a s3 datastore pointing to the deep-esdl-output bucket:</p> In\u00a0[\u00a0]: Copied! <pre># Note: If you use a prefix when uploading the data so you need the parameter max_depth=2\nstore = new_data_store(\"s3\",\n                       max_depth=2,\n                       root=S3_USER_STORAGE_BUCKET,\n                       storage_options=dict(anon=False,\n                                            key=S3_USER_STORAGE_KEY,\n                                            secret=S3_USER_STORAGE_SECRET))\n</pre> # Note: If you use a prefix when uploading the data so you need the parameter max_depth=2 store = new_data_store(\"s3\",                        max_depth=2,                        root=S3_USER_STORAGE_BUCKET,                        storage_options=dict(anon=False,                                             key=S3_USER_STORAGE_KEY,                                             secret=S3_USER_STORAGE_SECRET)) In\u00a0[\u00a0]: Copied! <pre>store.describe_data('input-datasets/sample01-geotiff.tif')\n</pre> store.describe_data('input-datasets/sample01-geotiff.tif') <p>In case you wish to delete data:</p> In\u00a0[\u00a0]: Copied! <pre>store.delete_data('input-datasets/sample01-geotiff.tif')\n</pre> store.delete_data('input-datasets/sample01-geotiff.tif') In\u00a0[\u00a0]: Copied! <pre>store.delete_data('input-datasets/sample02-geotiff.tif')\n</pre> store.delete_data('input-datasets/sample02-geotiff.tif') In\u00a0[\u00a0]: Copied! <pre>list(store.get_data_ids())\n</pre> list(store.get_data_ids()) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/Upload_files_to_shared_team_s3_storage/#how-to-upload-files-to-shared-team-s3-storage","title":"How to upload files to shared team S3 storage\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Upload_files_to_shared_team_s3_storage/#a-deepesdl-example-notebook","title":"A DeepESDL example notebook\u00b6","text":"<p>This notebook demonstrates how to upload files to shared team s3 storage and how to access them using xcube.</p> <p>Please, also refer to the DeepESDL documentation and visit the platform's website for further information!</p> <p>Brockmann Consult, 2024</p> <p>This notebook runs with the python environment <code>deepesdl-xcube-1.7.0</code>, please checkout the documentation for help on changing the environment.</p>"},{"location":"guide/jupyterlab/notebooks/Visualise_data_with_xcube_viewer/","title":"Visualise data with xcube viewer","text":"<p>First, lets create a small cube, which we can visualise with xcube viewer. We will use ESA CCI data for this. Please head over to example notebook in xcube-datastores: Generate CCI data cubes to get more details about the xcube-cci data store :)</p> In\u00a0[1]: Copied! <pre>from xcube.core.store import new_data_store\nimport os\n</pre> from xcube.core.store import new_data_store import os In\u00a0[2]: Copied! <pre>store = new_data_store('cciodp')\nstore\n</pre> store = new_data_store('cciodp') store Out[2]: <pre>&lt;xcube_cci.dataaccess.CciOdpDataStore at 0x7f9af651fa90&gt;</pre> <p>We request a dataset from the datasore:</p> In\u00a0[3]: Copied! <pre>dataset = store.open_data('esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.sst', \n                          variable_names=['analysed_sst'],\n                          time_range=['1981-09-01','1981-09-07'])\n\ndataset\n</pre> dataset = store.open_data('esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.sst',                            variable_names=['analysed_sst'],                           time_range=['1981-09-01','1981-09-07'])  dataset Out[3]: <pre>&lt;xarray.Dataset&gt; Size: 1GB\nDimensions:       (time: 7, lat: 3600, lon: 7200, bnds: 2)\nCoordinates:\n  * lat           (lat) float32 14kB -89.97 -89.93 -89.88 ... 89.88 89.93 89.97\n    lat_bnds      (lat, bnds) float32 29kB dask.array&lt;chunksize=(3600, 2), meta=np.ndarray&gt;\n  * lon           (lon) float32 29kB -180.0 -179.9 -179.9 ... 179.9 179.9 180.0\n    lon_bnds      (lon, bnds) float32 58kB dask.array&lt;chunksize=(7200, 2), meta=np.ndarray&gt;\n  * time          (time) datetime64[ns] 56B 1981-09-01T12:00:00 ... 1981-09-0...\n    time_bnds     (time, bnds) datetime64[ns] 112B dask.array&lt;chunksize=(7, 2), meta=np.ndarray&gt;\nDimensions without coordinates: bnds\nData variables:\n    analysed_sst  (time, lat, lon) float64 1GB dask.array&lt;chunksize=(1, 1200, 2400), meta=np.ndarray&gt;\nAttributes:\n    Conventions:             CF-1.7\n    title:                   esacci.SST.day.L4.SSTdepth.multi-sensor.multi-pl...\n    date_created:            2024-09-18T08:07:18.447033\n    processing_level:        L4\n    time_coverage_start:     1981-09-01T00:00:00\n    time_coverage_end:       1981-09-08T00:00:00\n    time_coverage_duration:  P7DT0H0M0S\n    history:                 [{'program': 'xcube_cci.chunkstore.CciChunkStore...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 7</li><li>lat: 3600</li><li>lon: 7200</li><li>bnds: 2</li></ul></li><li>Coordinates: (6)<ul><li>lat(lat)float32-89.97 -89.93 ... 89.93 89.97standard_name :latitudeunits :degrees_northvalid_min :-90.0valid_max :90.0axis :Ycomment : Latitude geographical coordinates,WGS84 projectionlong_name :Latitudereference_datum :geographical coordinates, WGS84 projectionbounds :lat_bndsorig_data_type :float32fill_value :nansize :3600shape :[3600]chunk_sizes :3600file_chunk_sizes :3600data_type :float32dimensions :['lat']file_dimensions :['lat']<pre>array([-89.975, -89.925, -89.875, ...,  89.875,  89.925,  89.975],\n      dtype=float32)</pre></li><li>lat_bnds(lat, bnds)float32dask.array&lt;chunksize=(3600, 2), meta=np.ndarray&gt;units :degrees_northlong_name :Latitude cell boundariesvalid_min :-90.0valid_max :90.0comment :Contains the northern and southern boundaries of the grid cells.reference_datum :geographical coordinates, WGS84 projectionorig_data_type :float32fill_value :nansize :7200shape :[3600, 2]chunk_sizes :[3600, 2]file_chunk_sizes :[3600, 2]data_type :float32dimensions :['lat', 'bnds']file_dimensions :['lat', 'bnds']  Array   Chunk   Bytes   28.12 kiB   28.12 kiB   Shape   (3600, 2)   (3600, 2)   Dask graph   1 chunks in 2 graph layers   Data type   float32 numpy.ndarray  2 3600 </li><li>lon(lon)float32-180.0 -179.9 ... 179.9 180.0standard_name :longitudeunits :degrees_eastvalid_min :-180.0valid_max :180.0axis :Xcomment : Longitude geographical coordinates,WGS84 projectionlong_name :Longitudereference_datum :geographical coordinates, WGS84 projectionbounds :lon_bndsorig_data_type :float32fill_value :nansize :7200shape :[7200]chunk_sizes :7200file_chunk_sizes :7200data_type :float32dimensions :['lon']file_dimensions :['lon']<pre>array([-179.975, -179.925, -179.875, ...,  179.875,  179.925,  179.975],\n      dtype=float32)</pre></li><li>lon_bnds(lon, bnds)float32dask.array&lt;chunksize=(7200, 2), meta=np.ndarray&gt;valid_max :180.0comment :Contains the eastern and western boundaries of the grid cells.reference_datum :geographical coordinates, WGS84 projectionvalid_min :-180.0units :degrees_eastlong_name :Longitude cell boundariesorig_data_type :float32fill_value :nansize :14400shape :[7200, 2]chunk_sizes :[7200, 2]file_chunk_sizes :[7200, 2]data_type :float32dimensions :['lon', 'bnds']file_dimensions :['lon', 'bnds']  Array   Chunk   Bytes   56.25 kiB   56.25 kiB   Shape   (7200, 2)   (7200, 2)   Dask graph   1 chunks in 2 graph layers   Data type   float32 numpy.ndarray  2 7200 </li><li>time(time)datetime64[ns]1981-09-01T12:00:00 ... 1981-09-...standard_name :timebounds :time_bnds<pre>array(['1981-09-01T12:00:00.000000000', '1981-09-02T12:00:00.000000000',\n       '1981-09-03T12:00:00.000000000', '1981-09-04T12:00:00.000000000',\n       '1981-09-05T12:00:00.000000000', '1981-09-06T12:00:00.000000000',\n       '1981-09-07T12:00:00.000000000'], dtype='datetime64[ns]')</pre></li><li>time_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(7, 2), meta=np.ndarray&gt;standard_name :time_bnds  Array   Chunk   Bytes   112 B   112 B   Shape   (7, 2)   (7, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 7 </li></ul></li><li>Data variables: (1)<ul><li>analysed_sst(time, lat, lon)float64dask.array&lt;chunksize=(1, 1200, 2400), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_water_temperatureunits :kelvinvalid_min :-300valid_max :4500source :ATSR&lt;1,2&gt;-ESACCI-L3U-v2.0, AATSR-ESACCI-L3U-v2.0, AVHRR&lt;07,09,11,12,14,15,16,17,18,19&gt;_G-ESACCI-L3U-v2.0, AVHRRMTA_G-ESACCI-L3U-v2.0depth :20 cmorig_data_type :int16fill_value :-32768size :181440000shape :[7, 3600, 7200]chunk_sizes :[1, 1200, 2400]file_chunk_sizes :[1, 1200, 2400]data_type :int16dimensions :['time', 'lat', 'lon']file_dimensions :['time', 'lat', 'lon']  Array   Chunk   Bytes   1.35 GiB   21.97 MiB   Shape   (7, 3600, 7200)   (1, 1200, 2400)   Dask graph   63 chunks in 2 graph layers   Data type   float64 numpy.ndarray  7200 3600 7 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ -89.9749984741211, -89.92500305175781,            -89.875,\n       -89.82499694824219,  -89.7750015258789,  -89.7249984741211,\n       -89.67500305175781,            -89.625, -89.57499694824219,\n        -89.5250015258789,\n       ...\n         89.5250015258789,  89.57499694824219,             89.625,\n        89.67500305175781,   89.7249984741211,   89.7750015258789,\n        89.82499694824219,             89.875,  89.92500305175781,\n         89.9749984741211],\n      dtype='float32', name='lat', length=3600))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.97500610351562,  -179.9250030517578,            -179.875,\n        -179.8249969482422, -179.77499389648438, -179.72500610351562,\n        -179.6750030517578,            -179.625,  -179.5749969482422,\n       -179.52499389648438,\n       ...\n        179.52499389648438,   179.5749969482422,             179.625,\n         179.6750030517578,  179.72500610351562,  179.77499389648438,\n         179.8249969482422,             179.875,   179.9250030517578,\n        179.97500610351562],\n      dtype='float32', name='lon', length=7200))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1981-09-01 12:00:00', '1981-09-02 12:00:00',\n               '1981-09-03 12:00:00', '1981-09-04 12:00:00',\n               '1981-09-05 12:00:00', '1981-09-06 12:00:00',\n               '1981-09-07 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (8)Conventions :CF-1.7title :esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.sstdate_created :2024-09-18T08:07:18.447033processing_level :L4time_coverage_start :1981-09-01T00:00:00time_coverage_end :1981-09-08T00:00:00time_coverage_duration :P7DT0H0M0Shistory :[{'program': 'xcube_cci.chunkstore.CciChunkStore', 'cube_params': {'variable_names': ['analysed_sst'], 'time_range': ['1981-09-01T00:00:00', '1981-09-07T00:00:00']}}]</li></ul> <p>Next, save it to the team s3 storage:</p> <p>To store the cube in your teams user space, please first retrieve the details from your environment variables as the following:</p> In\u00a0[4]: Copied! <pre>S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"]\nS3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"]\nS3_USER_STORAGE_BUCKET = os.environ[\"S3_USER_STORAGE_BUCKET\"]\n</pre> S3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"] S3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"] S3_USER_STORAGE_BUCKET = os.environ[\"S3_USER_STORAGE_BUCKET\"] <p>You need to instantiate a s3 datastore pointing to the team bucket:</p> In\u00a0[5]: Copied! <pre>team_store = new_data_store(\"s3\", \n                       root=S3_USER_STORAGE_BUCKET, \n                       storage_options=dict(anon=False, \n                                            key=S3_USER_STORAGE_KEY, \n                                            secret=S3_USER_STORAGE_SECRET))\n</pre> team_store = new_data_store(\"s3\",                         root=S3_USER_STORAGE_BUCKET,                         storage_options=dict(anon=False,                                              key=S3_USER_STORAGE_KEY,                                              secret=S3_USER_STORAGE_SECRET))  <p>If you have stored no data to your user space, the returned list will be empty:</p> In\u00a0[6]: Copied! <pre>list(team_store.get_data_ids())\n</pre> list(team_store.get_data_ids()) Out[6]: <pre>['SST.levels',\n 'amazonas_v8.zarr',\n 'amazonas_v9.zarr',\n 'analysed_sst.zarr',\n 'noise_trajectory.zarr']</pre> <p>The writing will take a few moments, as the data is global and will be persisted into the team s3 storage.</p> In\u00a0[7]: Copied! <pre>output_id = 'analysed_sst.zarr'\n</pre> output_id = 'analysed_sst.zarr' In\u00a0[10]: Copied! <pre>team_store.write_data(dataset, output_id)\n</pre> team_store.write_data(dataset, output_id) Out[10]: <pre>'analysed_sst.zarr'</pre> <p>If you list the content of you datastore again, you will now see the newly written dataset in the list:</p> In\u00a0[11]: Copied! <pre>list(team_store.get_data_ids())\n</pre> list(team_store.get_data_ids()) Out[11]: <pre>['SST.levels',\n 'amazonas_v8.zarr',\n 'amazonas_v9.zarr',\n 'analysed_sst.zarr',\n 'noise_trajectory.zarr']</pre> <p>Once the cube is stored in our team s3 storage, we can use xcube viewer jupyterlab extention to visualise it.</p> In\u00a0[12]: Copied! <pre>from xcube.webapi.viewer import Viewer\n</pre> from xcube.webapi.viewer import Viewer <p>We use the xcube datastore framework here to open the dataset, but it could also be opened by other means, e.g., <code>xr.open_dataset()</code>, provided it has variables with dimensions [\"time\", \"y\", \"x\"] or [\"y\", \"x\"].</p> In\u00a0[13]: Copied! <pre>dataset = team_store.open_data(output_id)\n</pre> dataset = team_store.open_data(output_id) In\u00a0[14]: Copied! <pre>dataset\n</pre> dataset Out[14]: <pre>&lt;xarray.Dataset&gt; Size: 1GB\nDimensions:       (time: 7, lat: 3600, lon: 7200, bnds: 2)\nCoordinates:\n  * lat           (lat) float32 14kB -89.97 -89.93 -89.88 ... 89.88 89.93 89.97\n    lat_bnds      (lat, bnds) float32 29kB dask.array&lt;chunksize=(3600, 2), meta=np.ndarray&gt;\n  * lon           (lon) float32 29kB -180.0 -179.9 -179.9 ... 179.9 179.9 180.0\n    lon_bnds      (lon, bnds) float32 58kB dask.array&lt;chunksize=(7200, 2), meta=np.ndarray&gt;\n  * time          (time) datetime64[ns] 56B 1981-09-01T12:00:00 ... 1981-09-0...\n    time_bnds     (time, bnds) datetime64[ns] 112B dask.array&lt;chunksize=(7, 2), meta=np.ndarray&gt;\nDimensions without coordinates: bnds\nData variables:\n    analysed_sst  (time, lat, lon) float64 1GB dask.array&lt;chunksize=(1, 1200, 2400), meta=np.ndarray&gt;\nAttributes:\n    Conventions:             CF-1.7\n    date_created:            2024-09-18T08:07:18.447033\n    history:                 [{'cube_params': {'time_range': ['1981-09-01T00:...\n    processing_level:        L4\n    time_coverage_duration:  P7DT0H0M0S\n    time_coverage_end:       1981-09-08T00:00:00\n    time_coverage_start:     1981-09-01T00:00:00\n    title:                   esacci.SST.day.L4.SSTdepth.multi-sensor.multi-pl...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 7</li><li>lat: 3600</li><li>lon: 7200</li><li>bnds: 2</li></ul></li><li>Coordinates: (6)<ul><li>lat(lat)float32-89.97 -89.93 ... 89.93 89.97axis :Ybounds :lat_bndschunk_sizes :3600comment : Latitude geographical coordinates,WGS84 projectiondata_type :float32dimensions :['lat']file_chunk_sizes :3600file_dimensions :['lat']fill_value :nanlong_name :Latitudeorig_data_type :float32reference_datum :geographical coordinates, WGS84 projectionshape :[3600]size :3600standard_name :latitudeunits :degrees_northvalid_max :90.0valid_min :-90.0<pre>array([-89.975, -89.925, -89.875, ...,  89.875,  89.925,  89.975],\n      dtype=float32)</pre></li><li>lat_bnds(lat, bnds)float32dask.array&lt;chunksize=(3600, 2), meta=np.ndarray&gt;chunk_sizes :[3600, 2]comment :Contains the northern and southern boundaries of the grid cells.data_type :float32dimensions :['lat', 'bnds']file_chunk_sizes :[3600, 2]file_dimensions :['lat', 'bnds']fill_value :nanlong_name :Latitude cell boundariesorig_data_type :float32reference_datum :geographical coordinates, WGS84 projectionshape :[3600, 2]size :7200units :degrees_northvalid_max :90.0valid_min :-90.0  Array   Chunk   Bytes   28.12 kiB   28.12 kiB   Shape   (3600, 2)   (3600, 2)   Dask graph   1 chunks in 2 graph layers   Data type   float32 numpy.ndarray  2 3600 </li><li>lon(lon)float32-180.0 -179.9 ... 179.9 180.0axis :Xbounds :lon_bndschunk_sizes :7200comment : Longitude geographical coordinates,WGS84 projectiondata_type :float32dimensions :['lon']file_chunk_sizes :7200file_dimensions :['lon']fill_value :nanlong_name :Longitudeorig_data_type :float32reference_datum :geographical coordinates, WGS84 projectionshape :[7200]size :7200standard_name :longitudeunits :degrees_eastvalid_max :180.0valid_min :-180.0<pre>array([-179.975, -179.925, -179.875, ...,  179.875,  179.925,  179.975],\n      dtype=float32)</pre></li><li>lon_bnds(lon, bnds)float32dask.array&lt;chunksize=(7200, 2), meta=np.ndarray&gt;chunk_sizes :[7200, 2]comment :Contains the eastern and western boundaries of the grid cells.data_type :float32dimensions :['lon', 'bnds']file_chunk_sizes :[7200, 2]file_dimensions :['lon', 'bnds']fill_value :nanlong_name :Longitude cell boundariesorig_data_type :float32reference_datum :geographical coordinates, WGS84 projectionshape :[7200, 2]size :14400units :degrees_eastvalid_max :180.0valid_min :-180.0  Array   Chunk   Bytes   56.25 kiB   56.25 kiB   Shape   (7200, 2)   (7200, 2)   Dask graph   1 chunks in 2 graph layers   Data type   float32 numpy.ndarray  2 7200 </li><li>time(time)datetime64[ns]1981-09-01T12:00:00 ... 1981-09-...bounds :time_bndsstandard_name :time<pre>array(['1981-09-01T12:00:00.000000000', '1981-09-02T12:00:00.000000000',\n       '1981-09-03T12:00:00.000000000', '1981-09-04T12:00:00.000000000',\n       '1981-09-05T12:00:00.000000000', '1981-09-06T12:00:00.000000000',\n       '1981-09-07T12:00:00.000000000'], dtype='datetime64[ns]')</pre></li><li>time_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(7, 2), meta=np.ndarray&gt;standard_name :time_bnds  Array   Chunk   Bytes   112 B   112 B   Shape   (7, 2)   (7, 2)   Dask graph   1 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 7 </li></ul></li><li>Data variables: (1)<ul><li>analysed_sst(time, lat, lon)float64dask.array&lt;chunksize=(1, 1200, 2400), meta=np.ndarray&gt;chunk_sizes :[1, 1200, 2400]data_type :int16depth :20 cmdimensions :['time', 'lat', 'lon']file_chunk_sizes :[1, 1200, 2400]file_dimensions :['time', 'lat', 'lon']fill_value :-32768long_name :analysed sea surface temperatureorig_data_type :int16shape :[7, 3600, 7200]size :181440000source :ATSR&lt;1,2&gt;-ESACCI-L3U-v2.0, AATSR-ESACCI-L3U-v2.0, AVHRR&lt;07,09,11,12,14,15,16,17,18,19&gt;_G-ESACCI-L3U-v2.0, AVHRRMTA_G-ESACCI-L3U-v2.0standard_name :sea_water_temperatureunits :kelvinvalid_max :4500valid_min :-300  Array   Chunk   Bytes   1.35 GiB   21.97 MiB   Shape   (7, 3600, 7200)   (1, 1200, 2400)   Dask graph   63 chunks in 2 graph layers   Data type   float64 numpy.ndarray  7200 3600 7 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ -89.9749984741211, -89.92500305175781,            -89.875,\n       -89.82499694824219,  -89.7750015258789,  -89.7249984741211,\n       -89.67500305175781,            -89.625, -89.57499694824219,\n        -89.5250015258789,\n       ...\n         89.5250015258789,  89.57499694824219,             89.625,\n        89.67500305175781,   89.7249984741211,   89.7750015258789,\n        89.82499694824219,             89.875,  89.92500305175781,\n         89.9749984741211],\n      dtype='float32', name='lat', length=3600))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-179.97500610351562,  -179.9250030517578,            -179.875,\n        -179.8249969482422, -179.77499389648438, -179.72500610351562,\n        -179.6750030517578,            -179.625,  -179.5749969482422,\n       -179.52499389648438,\n       ...\n        179.52499389648438,   179.5749969482422,             179.625,\n         179.6750030517578,  179.72500610351562,  179.77499389648438,\n         179.8249969482422,             179.875,   179.9250030517578,\n        179.97500610351562],\n      dtype='float32', name='lon', length=7200))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1981-09-01 12:00:00', '1981-09-02 12:00:00',\n               '1981-09-03 12:00:00', '1981-09-04 12:00:00',\n               '1981-09-05 12:00:00', '1981-09-06 12:00:00',\n               '1981-09-07 12:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (8)Conventions :CF-1.7date_created :2024-09-18T08:07:18.447033history :[{'cube_params': {'time_range': ['1981-09-01T00:00:00', '1981-09-07T00:00:00'], 'variable_names': ['analysed_sst']}, 'program': 'xcube_cci.chunkstore.CciChunkStore'}]processing_level :L4time_coverage_duration :P7DT0H0M0Stime_coverage_end :1981-09-08T00:00:00time_coverage_start :1981-09-01T00:00:00title :esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.sst</li></ul> <p>Scenario 1: Open xcube Viewer for a dataset instances persisted from a certain source (saved datasets).</p> <p>For the functionalities of xcube viewer please head over to the documentation: https://xcube.readthedocs.io/en/latest/viewer.html#functionality</p> In\u00a0[15]: Copied! <pre>viewer = Viewer()\n</pre> viewer = Viewer() <pre>WARNING:tornado.general:404 GET /viewer/config/config.json (127.0.0.1): xcube viewer has not been been configured\nWARNING:tornado.general:404 GET /viewer/config/config.json (127.0.0.1): xcube viewer has not been been configured\nWARNING:tornado.access:404 GET /viewer/config/config.json (127.0.0.1) 4.77ms\nWARNING:tornado.access:404 GET /viewer/config/config.json (127.0.0.1) 4.77ms\n</pre> In\u00a0[16]: Copied! <pre>viewer.add_dataset(dataset)\n</pre> viewer.add_dataset(dataset) Out[16]: <pre>'11098dfc-54a0-4462-8de3-cc3abfd36d5f'</pre> <p>You can click on the viewer link to open xcube Viewer in a new browser tab:</p> In\u00a0[17]: Copied! <pre>viewer.info()\n</pre> viewer.info() <pre>Server: https://deep.earthsystemdatalab.net/user/alicebalfanz/proxy/8001\nViewer: https://deep.earthsystemdatalab.net/user/alicebalfanz/proxy/8001/viewer/?serverUrl=https://deep.earthsystemdatalab.net/user/alicebalfanz/proxy/8001\n</pre> <p>You can also open xcube Viewer inlined here:</p> In\u00a0[18]: Copied! <pre>viewer.show()\n</pre> viewer.show() Out[18]: <p>To stop the server and viewer:</p> In\u00a0[19]: Copied! <pre>viewer.stop_server()\n</pre> viewer.stop_server() <p>Scenario 2: Open xcube Viewer for a dataset instances opened or otherwise created in this Notebook (in-memory datasets).</p> <p>Below, let's fetch a dataset from CDS on the fly, without persisting it - for more details about xcube CDS datastore and how to get CDS credentials please checkout the example notebook in xcube-datastores GENERATE C3S CDS CUBES. Be aware of performance loss, so in case you plan to use a dataset a lot for analysis or visualisation, please persist it into the team s3 storage. The public cubes provided within DeepESDL are already persisted in S3, so you should not duplicate them in your team storage.</p> In\u00a0[20]: Copied! <pre>import os\nos.environ['CDSAPI_URL'] = 'https://cds-beta.climate.copernicus.eu/api'\nos.environ['CDSAPI_KEY'] = '[PERSONAL-ACCESS-TOKEN]'\n</pre> import os os.environ['CDSAPI_URL'] = 'https://cds-beta.climate.copernicus.eu/api' os.environ['CDSAPI_KEY'] = '[PERSONAL-ACCESS-TOKEN]' In\u00a0[22]: Copied! <pre>cds_store = new_data_store('cds')\n</pre> cds_store = new_data_store('cds') In\u00a0[44]: Copied! <pre>bbox=[-5, 45, 35, 65]\n</pre> bbox=[-5, 45, 35, 65] In\u00a0[47]: Copied! <pre>cds_dataset = cds_store.open_data('reanalysis-era5-single-levels-monthly-means:monthly_averaged_reanalysis', \n                          variable_names=['2m_temperature'], \n                          bbox=bbox, \n                          spatial_res=0.25, \n                          time_range=['2010-01-01', '2010-12-31'])\ncds_dataset\n</pre> cds_dataset = cds_store.open_data('reanalysis-era5-single-levels-monthly-means:monthly_averaged_reanalysis',                            variable_names=['2m_temperature'],                            bbox=bbox,                            spatial_res=0.25,                            time_range=['2010-01-01', '2010-12-31']) cds_dataset <pre>xcube-cds version 0.9.3\n2024-09-18 08:40:41,936 INFO Request ID is 6a55691a-15bb-4b7d-81dc-fedb6febd390\nINFO:cads_api_client.processing:Request ID is 6a55691a-15bb-4b7d-81dc-fedb6febd390\n2024-09-18 08:40:41,973 INFO status has been updated to accepted\nINFO:cads_api_client.processing:status has been updated to accepted\n2024-09-18 08:40:43,517 INFO status has been updated to running\nINFO:cads_api_client.processing:status has been updated to running\n2024-09-18 08:40:45,804 INFO status has been updated to successful\nINFO:cads_api_client.processing:status has been updated to successful\n</pre> <pre>9830c36a333f3eab454e3817a76610df.nc:   0%|          | 0.00/322k [00:00&lt;?, ?B/s]</pre> Out[47]: <pre>&lt;xarray.Dataset&gt; Size: 617kB\nDimensions:  (lat: 80, lon: 160, time: 12)\nCoordinates:\n    number   int64 8B ...\n  * lat      (lat) float64 640B 64.88 64.62 64.38 64.12 ... 45.62 45.38 45.12\n  * lon      (lon) float64 1kB -4.875 -4.625 -4.375 -4.125 ... 34.38 34.62 34.88\n    expver   (time) &lt;U4 192B ...\n  * time     (time) datetime64[ns] 96B 2010-01-01 2010-02-01 ... 2010-12-01\nData variables:\n    t2m      (time, lat, lon) float32 614kB ...\nAttributes:\n    GRIB_centre:             ecmf\n    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n    GRIB_subCentre:          0\n    Conventions:             CF-1.7\n    institution:             European Centre for Medium-Range Weather Forecasts\n    history:                 2024-09-18T08:40 GRIB to CDM+CF via cfgrib-0.9.1...</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 80</li><li>lon: 160</li><li>time: 12</li></ul></li><li>Coordinates: (5)<ul><li>number()int64...long_name :ensemble member numerical idunits :1standard_name :realization<pre>[1 values with dtype=int64]</pre></li><li>lat(lat)float6464.88 64.62 64.38 ... 45.38 45.12units :degrees_northstandard_name :latitudelong_name :latitudestored_direction :decreasing<pre>array([64.875, 64.625, 64.375, 64.125, 63.875, 63.625, 63.375, 63.125, 62.875,\n       62.625, 62.375, 62.125, 61.875, 61.625, 61.375, 61.125, 60.875, 60.625,\n       60.375, 60.125, 59.875, 59.625, 59.375, 59.125, 58.875, 58.625, 58.375,\n       58.125, 57.875, 57.625, 57.375, 57.125, 56.875, 56.625, 56.375, 56.125,\n       55.875, 55.625, 55.375, 55.125, 54.875, 54.625, 54.375, 54.125, 53.875,\n       53.625, 53.375, 53.125, 52.875, 52.625, 52.375, 52.125, 51.875, 51.625,\n       51.375, 51.125, 50.875, 50.625, 50.375, 50.125, 49.875, 49.625, 49.375,\n       49.125, 48.875, 48.625, 48.375, 48.125, 47.875, 47.625, 47.375, 47.125,\n       46.875, 46.625, 46.375, 46.125, 45.875, 45.625, 45.375, 45.125])</pre></li><li>lon(lon)float64-4.875 -4.625 ... 34.62 34.88units :degrees_eaststandard_name :longitudelong_name :longitude<pre>array([-4.875, -4.625, -4.375, -4.125, -3.875, -3.625, -3.375, -3.125, -2.875,\n       -2.625, -2.375, -2.125, -1.875, -1.625, -1.375, -1.125, -0.875, -0.625,\n       -0.375, -0.125,  0.125,  0.375,  0.625,  0.875,  1.125,  1.375,  1.625,\n        1.875,  2.125,  2.375,  2.625,  2.875,  3.125,  3.375,  3.625,  3.875,\n        4.125,  4.375,  4.625,  4.875,  5.125,  5.375,  5.625,  5.875,  6.125,\n        6.375,  6.625,  6.875,  7.125,  7.375,  7.625,  7.875,  8.125,  8.375,\n        8.625,  8.875,  9.125,  9.375,  9.625,  9.875, 10.125, 10.375, 10.625,\n       10.875, 11.125, 11.375, 11.625, 11.875, 12.125, 12.375, 12.625, 12.875,\n       13.125, 13.375, 13.625, 13.875, 14.125, 14.375, 14.625, 14.875, 15.125,\n       15.375, 15.625, 15.875, 16.125, 16.375, 16.625, 16.875, 17.125, 17.375,\n       17.625, 17.875, 18.125, 18.375, 18.625, 18.875, 19.125, 19.375, 19.625,\n       19.875, 20.125, 20.375, 20.625, 20.875, 21.125, 21.375, 21.625, 21.875,\n       22.125, 22.375, 22.625, 22.875, 23.125, 23.375, 23.625, 23.875, 24.125,\n       24.375, 24.625, 24.875, 25.125, 25.375, 25.625, 25.875, 26.125, 26.375,\n       26.625, 26.875, 27.125, 27.375, 27.625, 27.875, 28.125, 28.375, 28.625,\n       28.875, 29.125, 29.375, 29.625, 29.875, 30.125, 30.375, 30.625, 30.875,\n       31.125, 31.375, 31.625, 31.875, 32.125, 32.375, 32.625, 32.875, 33.125,\n       33.375, 33.625, 33.875, 34.125, 34.375, 34.625, 34.875])</pre></li><li>expver(time)&lt;U4...<pre>[12 values with dtype=&lt;U4]</pre></li><li>time(time)datetime64[ns]2010-01-01 ... 2010-12-01standard_name :time<pre>array(['2010-01-01T00:00:00.000000000', '2010-02-01T00:00:00.000000000',\n       '2010-03-01T00:00:00.000000000', '2010-04-01T00:00:00.000000000',\n       '2010-05-01T00:00:00.000000000', '2010-06-01T00:00:00.000000000',\n       '2010-07-01T00:00:00.000000000', '2010-08-01T00:00:00.000000000',\n       '2010-09-01T00:00:00.000000000', '2010-10-01T00:00:00.000000000',\n       '2010-11-01T00:00:00.000000000', '2010-12-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (1)<ul><li>t2m(time, lat, lon)float32...GRIB_paramId :167GRIB_dataType :anGRIB_numberOfPoints :12800GRIB_typeOfLevel :surfaceGRIB_stepUnits :1GRIB_stepType :avguaGRIB_gridType :regular_llGRIB_uvRelativeToGrid :0GRIB_NV :0GRIB_Nx :160GRIB_Ny :80GRIB_cfName :unknownGRIB_cfVarName :t2mGRIB_gridDefinitionDescription :Latitude/Longitude GridGRIB_iDirectionIncrementInDegrees :0.25GRIB_iScansNegatively :0GRIB_jDirectionIncrementInDegrees :0.25GRIB_jPointsAreConsecutive :0GRIB_jScansPositively :0GRIB_latitudeOfFirstGridPointInDegrees :64.875GRIB_latitudeOfLastGridPointInDegrees :45.125GRIB_longitudeOfFirstGridPointInDegrees :-4.875GRIB_longitudeOfLastGridPointInDegrees :34.875GRIB_missingValue :3.4028234663852886e+38GRIB_name :2 metre temperatureGRIB_shortName :2tGRIB_totalNumber :0GRIB_units :Klong_name :2 metre temperatureunits :Kstandard_name :unknownGRIB_surface :0.0<pre>[153600 values with dtype=float32]</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([64.875, 64.625, 64.375, 64.125, 63.875, 63.625, 63.375, 63.125, 62.875,\n       62.625, 62.375, 62.125, 61.875, 61.625, 61.375, 61.125, 60.875, 60.625,\n       60.375, 60.125, 59.875, 59.625, 59.375, 59.125, 58.875, 58.625, 58.375,\n       58.125, 57.875, 57.625, 57.375, 57.125, 56.875, 56.625, 56.375, 56.125,\n       55.875, 55.625, 55.375, 55.125, 54.875, 54.625, 54.375, 54.125, 53.875,\n       53.625, 53.375, 53.125, 52.875, 52.625, 52.375, 52.125, 51.875, 51.625,\n       51.375, 51.125, 50.875, 50.625, 50.375, 50.125, 49.875, 49.625, 49.375,\n       49.125, 48.875, 48.625, 48.375, 48.125, 47.875, 47.625, 47.375, 47.125,\n       46.875, 46.625, 46.375, 46.125, 45.875, 45.625, 45.375, 45.125],\n      dtype='float64', name='lat'))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([-4.875, -4.625, -4.375, -4.125, -3.875, -3.625, -3.375, -3.125, -2.875,\n       -2.625,\n       ...\n       32.625, 32.875, 33.125, 33.375, 33.625, 33.875, 34.125, 34.375, 34.625,\n       34.875],\n      dtype='float64', name='lon', length=160))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2010-01-01', '2010-02-01', '2010-03-01', '2010-04-01',\n               '2010-05-01', '2010-06-01', '2010-07-01', '2010-08-01',\n               '2010-09-01', '2010-10-01', '2010-11-01', '2010-12-01'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (6)GRIB_centre :ecmfGRIB_centreDescription :European Centre for Medium-Range Weather ForecastsGRIB_subCentre :0Conventions :CF-1.7institution :European Centre for Medium-Range Weather Forecastshistory :2024-09-18T08:40 GRIB to CDM+CF via cfgrib-0.9.14.0/ecCodes-2.36.0 with {\"source\": \"data.grib\", \"filter_by_keys\": {}, \"encode_cf\": [\"parameter\", \"time\", \"geography\", \"vertical\"]}</li></ul> In\u00a0[55]: Copied! <pre>cds_dataset.attrs[\"title\"] = \"ERA5 2m Temperature\"\n</pre> cds_dataset.attrs[\"title\"] = \"ERA5 2m Temperature\"  <p>Let's set some attributes of the variable, so the colormapping does not fall back on the default</p> In\u00a0[56]: Copied! <pre>cds_dataset.t2m.attrs[\"color_value_min\"] = 270\ncds_dataset.t2m.attrs[\"color_value_max\"] = 310\ncds_dataset.t2m.attrs[\"color_bar_name\"] = \"plasma\"\n</pre> cds_dataset.t2m.attrs[\"color_value_min\"] = 270 cds_dataset.t2m.attrs[\"color_value_max\"] = 310 cds_dataset.t2m.attrs[\"color_bar_name\"] = \"plasma\" In\u00a0[\u00a0]: Copied! <pre>viewer = Viewer()\n</pre> viewer = Viewer() In\u00a0[58]: Copied! <pre>viewer.add_dataset(dataset)\nviewer.add_dataset(cds_dataset)\n</pre> viewer.add_dataset(dataset) viewer.add_dataset(cds_dataset) Out[58]: <pre>'f6015205-89c6-49d5-a0c4-d7c90694d5b4'</pre> <p>You can click on the viewer link to open xcube Viewer in a new browser tab:</p> In\u00a0[59]: Copied! <pre>viewer.info()\n</pre> viewer.info() <pre>Server: https://deep.earthsystemdatalab.net/user/alicebalfanz/proxy/8006\nViewer: https://deep.earthsystemdatalab.net/user/alicebalfanz/proxy/8006/viewer/?serverUrl=https://deep.earthsystemdatalab.net/user/alicebalfanz/proxy/8006\n</pre> <p>You can also open xcube Viewer inlined here:</p> In\u00a0[60]: Copied! <pre>viewer.show()\n</pre> viewer.show() Out[60]: <p>To stop the server and viewer:</p> In\u00a0[54]: Copied! <pre>viewer.stop_server()\n</pre> viewer.stop_server() <p>Scenario 3: Use custom server configuration to start server and pass it to the viewer constructor. In this case, we have created a local file with the configuration and load it as a dictionary and pass it to the viewer.</p> <p>The custom configuration allows you to predefine your value ranges, the colormaps that should be used as well as which bands should be used to create an RGB image, then the RGB switch in the viewer will display the RGB image.</p> <p>If you do not have a server-config.yaml file in your directory, please create one with the following content:</p> <pre>DataStores:\n  - Identifier: deep-esdl-cci-sst\n    StoreId: s3\n    StoreParams:\n      root: $S3_USER_STORAGE_BUCKET\n      storage_options:\n          anon: false\n          key: $S3_USER_STORAGE_KEY\n          secret: $S3_USER_STORAGE_SECRET\n    Datasets:\n      - Path: \"*.zarr\"\n        Style: default\n\n        # ChunkCacheSize: 1G\n\n\nStyles:\n  - Identifier: default\n    ColorMappings:\n      analysed_sst:\n        ColorBar: plasma\n        ValueRange: [270, 310]\n## if you have bands that can create an RGB image, you can specify them as below. \n#      rgb:\n#        Red:\n#          Variable: B04\n#          ValueRange: [0., 0.25]\n#        Green:\n#          Variable: B03\n#          ValueRange: [0., 0.25]\n#        Blue:\n#          Variable: B02\n#          ValueRange: [0., 0.25]\n</pre> <p>For all possible settings within the server configuration file, please checkout the documentation: https://xcube.readthedocs.io/en/latest/cli/xcube_serve.html#configuration-file</p> In\u00a0[32]: Copied! <pre>from xcube.util.config import load_configs\n</pre> from xcube.util.config import load_configs In\u00a0[\u00a0]: Copied! <pre>viewer = Viewer(server_config=load_configs(\"server-config.yaml\"))\n</pre> viewer = Viewer(server_config=load_configs(\"server-config.yaml\")) In\u00a0[36]: Copied! <pre>viewer.show()\n</pre> viewer.show() Out[36]: <p>To stop the server and viewer:</p> In\u00a0[37]: Copied! <pre>viewer.stop_server()\n</pre> viewer.stop_server() <p>Afer all our testing and exploring, let's clean up the example cube :)</p> In\u00a0[38]: Copied! <pre>team_store.delete_data(output_id)\n</pre> team_store.delete_data(output_id) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"guide/jupyterlab/notebooks/Visualise_data_with_xcube_viewer/#how-to-visualise-data-with-xcube-viewer-in-your-workspace","title":"How to visualise data with xcube viewer in your workspace\u00b6","text":""},{"location":"guide/jupyterlab/notebooks/Visualise_data_with_xcube_viewer/#a-deepesdl-example-notebook","title":"A DeepESDL example notebook\u00b6","text":"<p>This notebook demonstrates how to use the features of the xcube JupyterLab integration. The notebook demonstrates three scenarios how xcube Viewer is utilized in JupyterLab. In particular, we open xcube Viewer for any <code>xarray.Dataset</code> instances</p> <ol> <li>persisted in team s3 storage (saved datasets)</li> <li>opened or otherwise created in this Notebook (in-memory datasets)</li> <li>using a configuration file for customising stlyes.</li> </ol> <p>To explore the xcube viewer functionalities, please checkout the documentation: https://xcube.readthedocs.io/en/latest/viewer.html#functionality</p> <p>Please, also refer to the DeepESDL documentation and visit the platform's website for further information!</p> <p>Brockmann Consult, 2024</p> <p>This notebook runs with the python environment <code>deepesdl-xcube-1.7.0</code>, please checkout the documentation for help on changing the environment.</p>"},{"location":"ml-toolkit/example/","title":"Example Use Case","text":""},{"location":"ml-toolkit/example/#example-use-cases","title":"Example Use Cases","text":"<p>This toolkit provides a series of tutorial notebooks designed to support geospatial data processing and machine learning tasks on Earth System Data Cubes (ESDCs). Key use cases include:</p> <ol> <li> <p>Land Surface Temperature Prediction: Demonstrates land surface temperature prediction. This serves as an introductory example for ESDC analysis.</p> </li> <li> <p>Distributed Machine Learning: Showcases efficient preparation and training of large datasets on distributed systems.</p> </li> <li> <p>Transfer Learning: Illustrates how to reuse pre-trained models for related tasks with limited data.</p> </li> <li> <p>Cube Insights: Explores the characteristics of data cubes, helping inform preprocessing and modeling decisions.</p> </li> <li> <p>Gap Filling: Provides techniques to fill missing data in remote sensing datasets using support vector regression (SVR).</p> </li> <li> <p>ML for Multidimensional Samples with missing values: Demonstrates predictions on multidimensional data, utilizing simpler data imputation methods to address gaps.</p> </li> </ol> <p>Each of these use cases is accompanied by a corresponding Jupyter notebook/Python script, providing step-by-step instructions, code examples, and visualizations to guide users through the entire workflow. Whether you're looking to explore your data, fill gaps, or train machine learning models, this toolkit offers a set of resources to help you achieve your goals.</p>"},{"location":"ml-toolkit/example/#1-land-surface-temperature-prediction","title":"1. Land Surface Temperature prediction","text":"<p>This generic use case aims at the prediction of land surface temperature values based on air temperature values derived from the ESDC (Sentinel 3 SLSTR and Terra MODIS sensor, s3 store).</p> <p>Satellite monitoring is highly sensitive to atmospheric conditions, in particular to cloud cover, leading to the loss of a significant part of data, especially at high latitudes. This may even affect some pixels of an image which are not cloudy, but strongly influenced by cloud cover, usually because they were cloudy shortly before the moment of sensing or because of cloud shadows (Sarafanov et al. 2020). Therefore, remotely sensed land surface temperature images are patchy and gaps need to be filled in to complete the data set. Here, we propose a shallow neural network (Linear Regression) to predict missing values of land surface temperature from consistent air temperature values.</p> <p> </p> <p> ML prediction of missing Land Surface Temperature values from Air Temperature values (xcube viewer) </p>"},{"location":"ml-toolkit/example/#demo-notebooks","title":"Demo Notebooks","text":"<p>All Jupyter Notebooks follow the same approach, involving five major sections supported by markdown cells, comments, and plots:</p> <ol> <li>Landsurface Temperature Prediction scikit-learn</li> <li>Landsurface Temperature Prediction PyTorch</li> <li>Landsurface Temperature Prediction TensorFlow</li> </ol>"},{"location":"ml-toolkit/example/#approach","title":"Approach","text":"<ol> <li>Import necessary libraries and mltools</li> <li>Load Earth System Data Cube (s3 object store)</li> <li>Initialize data mask</li> <li>Assign train/test split</li> <li>Preprocessing (filtering NaNs, standardization, normalization)</li> <li>Model set-up (linear regression with 1 node/ shallow neural network)</li> <li>Model training and validation over a number of epochs:</li> <li>Training:<ul> <li>Generate training batches using existing data loading and transformation mechanisms from Keras and PyTorch (DataGenerator, DataLoader)</li> <li>Train model, and compute average epoch training loss</li> </ul> </li> <li>Validation:<ul> <li>Generate testing batches using existing data loading and transformation mechanisms from Keras and PyTorch (DataGenerator, DataLoader)</li> <li>Test model, and compute average epoch validation loss</li> </ul> </li> <li>Use model to make predictions &amp; plot results</li> </ol> <p> Machine Learning workflow on Analysis Ready Data Cubes </p>"},{"location":"ml-toolkit/example/#preliminary-condition","title":"Preliminary Condition","text":"<p>As initially described in the demo cases, the missing values of land surface temperature are predicted from consistent air temperature values.</p> Air Temperature Land Surface Temperature"},{"location":"ml-toolkit/example/#machine-learning-approach","title":"Machine Learning Approach","text":"<p>In this section, the machine learning approach is briefly illustrated based on the TenorFlow notebook. For comprehensive implementations, refer to the demo notebooks to see the full implementations.</p>"},{"location":"ml-toolkit/example/#1-load-earth-system-data-cube","title":"1. Load Earth System Data Cube","text":"<p>First, the <code>zarr</code> data cube is loaded from the s3 data store. The ESDC consists of three dimensions: longitude, latitude, and time. The focus will be on two variables: \"land_surface_temperature\" and \"air_temperature_2m\".</p> <pre><code>from xcube.core.store import new_data_store\n\n# Initialize the data store for accessing the s3 bucket\ndata_store = new_data_store(\"s3\", root=\"esdl-esdc-v2.1.1\", storage_options=dict(anon=True))\n\n# Open the dataset\ndataset = data_store.open_data(\"esdc-8d-0.083deg-184x270x270-2.1.1.zarr\")\n\n# Select a smaller subset of the data for this demo case\nstart_time = \"2002-05-21\"\nend_time = \"2002-08-01\"\nds = dataset[[\"land_surface_temperature\", \"air_temperature_2m\"]].sel(time=slice(start_time, end_time))\n</code></pre>"},{"location":"ml-toolkit/example/#2-add-land-mask-variable","title":"2. Add land mask variable","text":"<p>Fir the prediction of the land surface temperature values only terrestrial regions are relevant. Therefore, a land variable is assigned to the ESDC to exclude the oceanic regions.</p> <pre><code>import numpy as np\nimport dask.array as da\nfrom global_land_mask import globe\nfrom ml4xcube.preprocessing import assign_mask\n\nlon_grid, lat_grid = np.meshgrid(ds.lon,ds.lat)\nlm0                = da.from_array(globe.is_land(lat_grid, lon_grid))\nxdsm               = assign_mask(ds, lm0)\nxdsm\n</code></pre>"},{"location":"ml-toolkit/example/#3-train-test-split-on-geo-data","title":"3. Train-/ Test Split on Geo-Data","text":"<p>The <code>ml4xcube.splits</code> module provides two methods to split the data into training and test sets: random split and block split.</p> <p>1. Random Split</p> <p>The random split is a straight forward procedure in classical machine learning application to divide data in a train and a test set. Every data sample is assigned randomly with a predefined probability either to the train or the test. This approach can lead to issues due to spatio-temporal distances and auto-correlation within chunks.</p> <p>2. Block Split</p> <p>It is therefore mandatory to utilize techniques that respects the basic principles of geo-data way beyond naive random split method in the Earth system context. To avoid auto-correlation during the training phase of the model, data splitting should rather be guided by the block split strategy, which segments data into contiguous blocks based on geographical and temporal proximity, assigning data points from these blocks to either training or test sets with a specific probability. This strategy keeps closely related data points together, reducing information leakage across the train-test divide and enhancing testing integrity.</p> Random Train-Test Assignment Balanced Stratified Train-Test Assignment <p>For this case, the <code>assign_block_split</code> method is employed to allocate each data point to either the training or test set:</p> <pre><code>from ml4xcube.splits import assign_rand_split, assign_block_split\n\n# random splitting\n\"\"\"\nxds = assign_rand_split(\n    ds    = xdsm,\n    split = 0.8\n)\n\"\"\"\n\n# block splitting\nxds = assign_block_split(\n    ds         = xdsm,\n    block_size = [(\"time\", 10), (\"lat\", 100), (\"lon\", 100)],\n    split      = 0.8\n)\nxds\n</code></pre>"},{"location":"ml-toolkit/example/#4-train-and-test-set-creation-and-preprocessing","title":"4. Train-/ and Test Set Creation and Preprocessing","text":"<p>In this step, data is preprocessed for training using the designated sampler. The dataset undergoes standardization and is segmented into manageable samples. The feature scaling strategy can be customized via the <code>scale_fn</code> parameter, which allows for normalization or can be set to None for manual adjustments. If <code>None</code>, a custom feature scaling function can be introduced using the <code>callback parameter</code>, enabling further preprocessing flexibility with costum functions.</p> <p>By default, missing values are omitted from the dataset. To apply alternative imputation strategies, adjust the <code>drop_nan</code> parameter of the <code>XrDataset</code>. For comprehensive guidance on these options, please consult the ml4xcube API description description.</p> <p>Following preprocessing, the data is allocated into training and testing sets based on the previously determined block split strategy, ensuring readiness for the subsequent training phase.</p> <pre><code>import tensorflow as tf\nfrom ml4xcube.datasets.xr_dataset import XrDataset\n\nsampler               = XrDataset(ds=xds, num_chunks=3, rand_chunk=False, to_pred='land_surface_temperature')\ntrain_data, test_data = sampler.get_datasets()\n\n# Create TensorFlow 6-datasets for 7-training and testing\ntrain_ds = tf.data.Dataset.from_tensor_slices(train_data).batch(32)\ntest_ds = tf.data.Dataset.from_tensor_slices(test_data).batch(32)\n</code></pre>"},{"location":"ml-toolkit/example/#5-model-setup-optimizer-and-loss-definition","title":"5. Model Setup, Optimizer and Loss Definition","text":"<p>A simple linear regression model using TensorFlow is defined, followed by the setup of the optimizer and the loss function definition.</p> <pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers as L\n\n# Define epoch and learning rate\nlr     = 0.1\nepochs = 10\n\n# Create model\ninputs      = L.Input(name=\"air_temperature_2m\", shape=(1,))\noutput      = L.Dense(1, activation=\"linear\", name=\"land_surface_temperature\")(inputs)\nmodel       = tf.keras.models.Model(inputs=inputs, outputs=output)\nmodel.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mae\"])\n\nmodel.optimizer.learning_rate.assign(lr)\n</code></pre>"},{"location":"ml-toolkit/example/#6-model-training-and-validation","title":"6. Model Training and Validation","text":"<p>Finally, the model is trained using <code>train_ds</code> and validated with the <code>test_ds</code> dataset. Early stopping is employed to prevent overfitting. The best model weights, according to the validation score, are saved, and the trained model is returned, ready for predictions.</p> <pre><code>from ml4xcube.training.tensorflow import Trainer\n\ntrainer = Trainer(\n    model=model,\n    train_data=train_ds,\n    test_data=test_ds,\n    early_stopping=True,\n    patience=5,\n    model_path=\"best_model.keras\",\n    mlflow_run=mlflow,\n    epochs=epochs,\n    create_loss_plot=True\n)\n\nmodel = trainer.train()\n</code></pre>"},{"location":"ml-toolkit/example/#results","title":"Results","text":"<p>After conducting the entire machine learning approach the trained model can be used to make predictions for the missing land surface temperature values:</p> <p> </p> <p> Land Surface Temperature Filled </p>"},{"location":"ml-toolkit/example/#model-tracking","title":"Model Tracking","text":"<p>Within the land surface temperature use cases model tracking is realized through the usage of TensorBoard and mlflow. These tools offer science teams an easy-to-use platform allowing to run and scale their Machine Learning workloads in a collaborative environment supporting versioning and sharing of parameters, models, artefacts, results, etc. within the team and potentially external users. Mlflow supports the MLOps pipelines particularly to log and evaluate experiment runs as well as to store models in a registry\u200b. Persistent mlflow deployments are made available on team level to allow each team member to compare their experiments with those of the other team members and to use the trained models of others. TensorBoard as another collaborative tool in this MLOPs space is currently evaluated by the science teams and available as part of the TensorFlow conda kernel to individual users within their JupyterLab session.</p> <p> </p> <p> Collaborative Experiment Tracking with mlflow. </p>"},{"location":"ml-toolkit/example/#2-distributed-machine-learning","title":"2. Distributed Machine Learning","text":"<p>Satellites continuously monitor various Earth parameters across, generating vast amounts of data ideal for training sophisticated machine learning models. However, preparing and training with such large datasets can be time-consuming and resource-intensive. The <code>ml4xcube</code> package facilitates efficient handling, preparation, and distributed training of large geospatial datasets, providing tools and workflows designed to optimize these processes. Below are demonstrations on efficient dataset preparation (4) and distributed machine learning (5). For simplicity the previous setup is leveraged to illustrate the functionality.</p>"},{"location":"ml-toolkit/example/#demo-scripts","title":"Demo Scripts","text":"<ol> <li>Distributed Dataset Creation.</li> <li>Distributed Machine Learning.</li> </ol>"},{"location":"ml-toolkit/example/#data-preparation","title":"Data Preparation","text":"<p>Before training machine learning models, data must be preprocessed and organized. This snippet is crucial for understanding how data, particularly large and complex datasets like those from satellites, is preprocessed before being used for machine learning. It demonstrates loading the data, computing statistics necessary for normalization, and applying these statistics to standardize the data with the help of a callback function. The callback function is used to apply transformations on-the-fly to each data chunk, ensuring that all data is processed uniformly. Further custom preprocessing steps can be added accordingly.</p> <pre><code>import xarray as xr\nfrom ml4xcube.preprocessing import get_statistics, standardize\nfrom ml4xcube.datasets.multiproc_sampler import MultiProcSampler\n\n# Load sample data\nds = xr.open_zarr('sample_data.zarr')\nds = ds['temperature']\n\n# Create a train and a test set and save them as train.zarr and test.zarr\ntrain_set, test_set = MultiProcSampler(\n    ds          = ds,\n    train_cube  = 'train.zarr',\n    test_cube   = 'test.zarr',\n    nproc       = 5,\n    chunk_batch = 10,\n).get_datasets()\n</code></pre> <p>In the next step, the environment for training must be prepared by converting datasets to a format compatible with PyTorch, setting up a basic neural network model, and configuring the training process. Since in this example 1D data points are utilized for training, the dimension names assigned correspond to a 1D Tuple as well. If the usage of multidimensional data samples is intended, please define the parameter sample_size of the <code>MultiProcSampler</code> class (e.g. <code>sample_size=[('time', 1), ('lat', 3), ('lon', 3)]</code>). Overlapping samples are also possible (<code>overlap=[('time', 0.), ('lat', 0.33), ('lon', 0.33)]</code>). For further details check out the corresponding definition in the ml4xcube API</p> <pre><code>import zarr\nimport torch\nimport xarray as xr\nimport dask.array as da\nfrom ml4xcube.datasets.pytorch import PTXrDataset\n\ndef load_train_objs():\n    train_store = zarr.open('train.zarr')\n    test_store = zarr.open('test.zarr')\n\n    train_set = xr.Dataset(train_data)\n    test_set  = xr.Dataset(test_data)\n\n    # Create PyTorch data sets\n    train_ds = PTXrDataset(train_set)\n    test_ds  = PTXrDataset(test_set)\n\n    # Initialize model and optimizer\n    model     = torch.nn.Linear(in_features=1, out_features=1, bias=True)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n    loss      = torch.nn.MSELoss(reduction='mean')\n\n    return train_ds, test_ds, model, optimizer, loss\n</code></pre> <p>This final snippet sets up and runs the distributed training process using PyTorch. It includes initializing the distributed data parallel training environment, preparing data loaders with parallel processing capabilities, and defining the training loop. This approach significantly enhances the training efficiency on large-scale datasets by leveraging multiple processing units.</p> <pre><code>from ml4xcube.datasets.pytorch import prepare_dataloader\nfrom ml4xcube.training.pytorch_distributed import ddp_init, Trainer, dist_train\n\n# Initialize distributed data parallel training\nddp_init()\n\n# Load training objects\ntrain_set, test_set, model, optimizer, loss = load_train_objs()\n\n# Prepare data loaders\ntrain_loader, test_loader = prepare_dataloader(train_set, test_set, batch_size, num_workers=5, parallel=True)\n\n# Initialize the trainer and start training\ntrainer = Trainer(\n    model                = model,\n    train_data           = train_loader,\n    test_data            = test_loader,\n    optimizer            = optimizer,\n    save_every           = save_every,\n    model_path           = best_model_path,\n    early_stopping       = True,\n    patience             = 3,\n    loss                 = loss,\n    validate_parallelism = True\n)\n</code></pre>"},{"location":"ml-toolkit/example/#3-transfer-learning","title":"3. Transfer Learning","text":"<p>Transfer learning corresponds to a way to reuse information obtained by previous model training for a second related task. This can be necessary when only a concise amount of data is available. Therefore, a PyTorch based Jupyter Notebook provides the implementation of Transfer Learning. This technique was illustrated for the same setting as the first example, predicting missing land surface temperature values.</p>"},{"location":"ml-toolkit/example/#demo-notebook","title":"Demo Notebook","text":"<ol> <li>Transfer Learning.</li> </ol> <p> The Basic Concept of Transfer Learning. </p>"},{"location":"ml-toolkit/example/#4-cube-insights","title":"4. Cube Insights","text":"<p>In order to decide which preprocessing steps are required by your machine learning application, the <code>insights</code> module offers tools for extracting and analyzing characteristics of an <code>xarray.DataArray</code> object. This module includes functions to assess the completeness and distribution of data within the cube.</p>"},{"location":"ml-toolkit/example/#demo-notebook_1","title":"Demo Notebook","text":"<p>The corresponding Jupyter notebook containing the entire workflow can be accessed here:</p> <ol> <li>Landsurface Temperature Insights</li> </ol> <p>The detailed workflow in order to analyze the specifics of a data cube is demonstrated in the following:</p> <pre><code>import xarray as xr\nfrom ml4xcube.insights import get_insights\n\n# Load sample data\nds = xr.open_zarr('sample_data.zarr')\nds = ds['temperature']\n\n# Get insights from the data cube\nget_insights(ds)\n</code></pre> <p>The <code>get_insights</code> function, prints the following statistics (example for a cube containing dimensions named time, latitude, and longitude):</p> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:09&lt;00:00,  1.10it/s]\nThe data cube has the following characteristics:\n\nVariable:             Land Surface Temperature\nShape:                (time: 10, lat: 2160, lon: 4320)\nTime range:           2002-05-21 - 2002-08-01\nLatitude range:       -89.958\u00b0 - 89.958\u00b0\nLongitude range:      -179.958\u00b0 - 179.958\u00b0\nTotal size:           93312000\nSize of each layer:   9331200\nTotal gap size:       74069847 -&gt; 79 %\nMaximum gap size:     87 % on 2002-06-06\nMinimum gap size:     75 % on 2002-08-01\nValue range:          222.99 - 339.32\n</code></pre> <p>Utiliting the get_gap_heat_map the amount of missing values over time can be computed for every latitude/longitude pixel:</p> <pre><code>import xarray as xr\nfrom ml4xcube.plotting import plot_slice\nfrom ml4xcube.insights import get_gap_heat_map\n\n# Load sample data\nds = xr.open_zarr('sample_data.zarr')\nds = ds['temperature']\n\n# Generate and visualize the gap heat map\ngap_heat_map = get_gap_heat_map(ds)\ndataset   = gap_heat_map.to_dataset(name='temperature')\n\nplot_slice(\n    ds          = dataset,\n    var_to_plot = 'temperature',\n    color_map   = \"plasma\",\n    title       = \"Filled artificial gaps matrix\",\n    label       = \"Number of gaps\",\n    xdim        = \"lon\",\n    ydim        = \"lat\"\n)\n</code></pre> <p>Running this example results in an the following illustration, showing a heatmap of data gaps in the land surface temperature variable over time. The number of available data ranges from 0 to 10, corresponding to the 10 frames in the analyzed cube:</p> <p> </p> <p> Heatmap of available data in the land surface temperature variable over time. </p>"},{"location":"ml-toolkit/example/#5-gapfilling","title":"5. Gapfilling","text":"<p>The gapfilling module provides a method for filling gaps in ESDCs, particularly tailored for remote sensing datasets (Sarafanov et al. 2020). This approach utilizes a support vector regression model to predict missing values based on available data.</p> <p>After examining the amount of missing values in the cube, the module can be applied to fill the corresponding areas in the cube as showcased in the following example:</p>"},{"location":"ml-toolkit/example/#demo-notebook_2","title":"Demo Notebook","text":"<ol> <li>Gap Filling Process.</li> </ol>"},{"location":"ml-toolkit/example/#6-predictions-for-multidimensional-samples","title":"6. Predictions for Multidimensional Samples","text":"<p>An alternative to gap filling can be using simpler methods. For example, missing values can be imputed by replacing them with the mean or a constant placeholder. After exploring the data, it might be evident that gaps are not frequent. In some cases, in environmental modeling for specific regions, missing values may be intentional. For instance, values may appear only in terrestrial regions.</p> <p>In such scenarios, data imputation can enable the effective use of the entire dataset, allowing for model training and analysis without the complications of incomplete data.</p> <p>The following notebooks demonstrate the workflow for land surface temperature prediction using multidimensional data with missing values.</p>"},{"location":"ml-toolkit/example/#demo-notebooks_1","title":"Demo Notebooks","text":"<ol> <li>Machine Learning for Multidimensional Samples (PyTorch).</li> <li>Machine Learning for Multidimensional Samples (TensorFlow).</li> </ol> <p> Filling areas outside the continent with constant value. </p>"},{"location":"ml-toolkit/getting-started/","title":"Getting Started","text":""},{"location":"ml-toolkit/getting-started/#getting-started","title":"Getting Started","text":"<p><code>ml4xcube</code> is a comprehensive Python-based toolkit designed for researchers and developers in the field of machine learning with an emphasis on <code>xarray</code> data cubes. This toolkit is engineered to provide specialized and robust support for data cube management and analysis, operating with the state-of-the-art machine learning libraries (1) <code>scikit-learn</code>, (2) <code>PyTorch</code> and (3) <code>TensorFlow</code>. </p>"},{"location":"ml-toolkit/getting-started/#installation","title":"Installation","text":"<p>Get started with <code>ml4xcube</code> effortlessly by installing it directly through pip: <pre><code>pip install ml4xcube\n</code></pre> or conda: <pre><code>conda install -c conda-forge ml4xcube\n</code></pre></p>"},{"location":"ml-toolkit/getting-started/#features","title":"Features","text":"<ul> <li>Data preprocessing and postprocessing functions</li> <li>Filling masked data and gap filling features</li> <li>Dataset creation and train-/ test splitting techniques</li> <li>Trainer classes for <code>sklearn</code>, <code>TensorFlow</code> and <code>PyTorch</code></li> <li>Distributed training framework compatible with <code>PyTorch</code></li> <li>chunk utilities for working with data cubes</li> </ul>"},{"location":"ml-toolkit/getting-started/#requirements","title":"Requirements","text":"Package Versions dask \u22652023.2.0 numpy \u22651.24 pandas \u22652.2 scikit-learn &gt;1.3.1 xarray &gt;2023.8.0 zarr &gt;2.11 rechunker \u22650.5.1 <p>Make sure you have Python version 3.8 or higher.</p> <p>If you're planning to use <code>ml4xcube</code> with TensorFlow or PyTorch, set up these frameworks properly in your conda environment. </p>"},{"location":"ml-toolkit/introduction/","title":"Introduction","text":""},{"location":"ml-toolkit/introduction/#deepesdl-ml4xcube-machine-learning-toolkits-for-data-cubes","title":"DeepESDL ml4xcube - Machine Learning Toolkits for Data Cubes","text":"<p>AI is becoming increasingly important in Earth observations as most parts  of the Earth system are continuously monitored by sensors and AI is able to  cope  with both the volume of data and the heterogeneous data  characteristics. For instance, satellites monitor the atmosphere, land, and  ocean with unprecedented accuracy. In course of DeepESDL, the Earth System  Data Lab (ESDL) capabilities have been extended to support the application of  machine learning (ML) methods on Earth System Data Cubes (ESDC). </p> <p>Various Python-based Jupyter Notebooks/Scripts illustrate a use cases to  demonstrate the capabilities of <code>ml4xcube</code>, utilizing state-of-the-art machine  learning libraries on ESDCs in the DeepESDL Hub environment. Each Jupyter  Notebook involves a self-contained workflow, markdown cells,  comments and plots for user-friendly application and guidance and is based  on one of the three well established open source ML libraries respectively:</p> <ol> <li>scikit-learn       For classical machine learning such as support vector machines, decision trees, regressions or clustering, scikit-learn provides a broad set of features that fulfils many basic requirements.</li> <li>PyTorch       For larger neural networks and support for Deep Learning additional ML toolchains are necessary, for example the python-based ML stack PyTorch. With PyTorch, experienced users are supported. It       provides low-level API and allows for flexibility to develop and customize deep learning models. It allows for GPU computation and supports transfer learning, domain adaptation, or diverse methods       for fine tuning of models.</li> <li>TensorFlow/Keras       Keras provides a high-level API that can be run on the popular execution backend TensorFlow. Due to its simplicity, it fits well to the requirements of those Earth system scientist that do not           require to newly develop neuronal network architectures. As PyTorch, TensorFlow is python-based, allows for GPU computation and it supports Deep Learning applications including transfer learning         or domain adaptation.</li> </ol>"},{"location":"ml-toolkit/introduction/#overview","title":"Overview","text":"<ol> <li>Getting Started </li> <li>Example Use Cases and Juypter Notebooks</li> <li>ml4xcube API</li> </ol>"},{"location":"ml-toolkit/api-reference/","title":"Overview","text":""},{"location":"ml-toolkit/api-reference/#ml4xcube-api","title":"ml4xcube API","text":""},{"location":"ml-toolkit/api-reference/#1-plotting","title":"1. plotting","text":"<p>The <code>Plotting</code> module provides functionality to visualize slices of data from <code>xarray.DataArray</code>. It facilitates the analysis of multidimensional data. The primary function in this module is <code>plot_slice</code>, which allows users to create visualizations with optional masks to highlight specific regions of interest.</p> <p>Functions:</p> <ul> <li>plot_slice - Renders a 2D slice of an <code>xarray.DataArray</code> with optional emphasis on specific features via masking.</li> </ul>"},{"location":"ml-toolkit/api-reference/#2-insights","title":"2. insights","text":"<p>The <code>insights</code> module offers tools for extracting and analyzing characteristics of multidimensional data cubes from <code>xarray.DataArray</code> objects. This module includes functions to assess the completeness and distribution of data within the cube, helping users understand the dataset's quality and spatial-temporal coverage. The detailed workflow in order to analyze the specifics of a data cube is demonstrated in the following Jupyter Notebook.</p> <p>Functions:</p> <ul> <li>get_insights - Extracts and prints detailed characteristics of a data cube, including dimensions, value ranges, and gaps in the data.</li> <li>get_gap_heat_map - Generates a heat map to visualize the distribution of non-<code>NaN</code> values across selected dimensions, revealing patterns of data availability or missingness.</li> </ul>"},{"location":"ml-toolkit/api-reference/#3-gapfilling","title":"3. gapfilling","text":"<p>The <code>ml4xcube.gapfilling</code> module is designed to address and rectify data gaps in multidimensional geospatial datasets, particularly those represented in <code>xarray.Dataset</code> formats. The gap filling process is divided into three main submodules, each playing a crucial role in the preparation, processing, and application of sophisticated machine learning algorithms to ensure accurate and efficient data imputation. he entire gapfilling process is showcased in the following Jupyter Notebook.</p>"},{"location":"ml-toolkit/api-reference/#31-helperpredictors","title":"3.1. helper.predictors","text":"<p>The <code>HelpingPredictor</code> class within the <code>helper.predictors</code> submodule facilitates the preparation of predictor data for gap filling applications. This submodule focuses on extracting and processing global predictor data, such as land cover classifications, matching them to the corresponding dimensions (e.g., latitude and longitude) of the target data cube. The prepared predictor is then stored in a <code>.zarr</code> dataset, ready to be used across various gap filling applications. If not leveraged during the gap filling process, a Support Vector Machine is trained on artificial gaps within the <code>Gapfiller</code> class.</p> <p>Classes:</p> <ul> <li>HelpingPredictor - Facilitates the preparation of predictor data for gap filling.</li> </ul>"},{"location":"ml-toolkit/api-reference/#32-gap_dataset","title":"3.2. gap_dataset","text":"<p>The <code>GapDataset</code> class in the <code>gap_dataset</code>submodule is designed to prepare data before performing the actual gap filling applications. This submodule focuses on slicing specific dimensions from a data cube, applying optional artificial gaps, and managing datasets for subsequent gap filling operations.</p> <p>Classes:</p> <ul> <li>GapDataset - Prepares data with artificial gaps optionally for training of a regressor and datasets with real gaps before gap filling can be performed.</li> </ul>"},{"location":"ml-toolkit/api-reference/#33-gap_filling","title":"3.3. gap_filling","text":"<p>The <code>Gapfiller</code> class within the <code>gap_filling</code> submodule is an integral part of the <code>ml4xcube.gapfilling</code> module designed to implement and manage the gap filling process using machine learning techniques, specifically focusing on Support Vector Regression (SVR) for now. It allows for the integration of different hyperparameters, and predictors to optimize the gap filling process. A prerequsite before gap filling can be applied, is a specific data preparation step, taken over by the functionalities of the <code>GapDataset</code> class.</p> <p>Classes:</p> <ul> <li>Gapfiller - Optionally trains a predictor to estimate actual values in gaps. Performs gap filling with SVR or a user-provided regressor.</li> </ul>"},{"location":"ml-toolkit/api-reference/#4-splits","title":"4. Splits","text":"<p>The <code>splits</code> module includes functions designed to divide an <code>xarray.Datasets</code> into a train and a test set. These functions use sampling strategies to ensure that data is split in a manner that respects the integrity of spatial and temporal data blocks, facilitating the development of machine learning models. Functions to assign split variables provide structured and random approaches to segmenting the dataset, which are then utilized by the <code>create_split</code> function to generate actual train-test splits.</p> <p>Functions:</p> <ul> <li>assign_block_split - Determines the assignment of data blocks to train or test sets using a deterministic approach based on the Cantor pairing function. This structured random sampling respects data locality and sets up the <code>splits</code> variable used by <code>create_split</code>.</li> <li>assign_rand_split - Randomly assigns a split indicator to each element in the dataset based on a specified proportion. This method provides a randomized approach to setting up the <code>splits</code> variable, which is also used by <code>create_split</code>.</li> <li>create_split - Generates train-test splits for machine learning models by utilizing a predefined <code>splits</code> variable within the dataset. Supports <code>xarray.Dataset</code> or a dictionary of <code>numpy</code> arrays and provides flexibility in specifying feature and target variables, effectively leveraging the split defined by the previous functions.</li> </ul>"},{"location":"ml-toolkit/api-reference/#5-preprocessing","title":"5. preprocessing","text":"<p>The <code>preprocessing</code> module provides a collection of functions for preparing and processing data from <code>xarray.Datasets</code>, particularly focusing on operations commonly required in data science and machine learning workflows. These functions include filtering, filling missing data, calculating statistics, and normalizing or standardizing data.</p> <p>Functions:</p> <ul> <li>apply_filter - Applies a specified filter to the data by setting all values to NaN which do not belong to the mask or dropping the entire sample.</li> <li>assign_mask - Assigns a mask to the dataset for later data division or filtering.</li> <li>drop_nan_values - Filters out samples from a dataset if they contain any <code>NaN</code> values, with an optional mask to determine sample validity. It handles both 1D and multi-dimensional samples.</li> <li>fill_nan_values - Fills <code>NaN</code> values in the dataset using a specified method.</li> <li>get_range - Computes the range (min and max) of the data.</li> <li>get_statistics - Computes the mean and standard deviation of a specified variable.</li> <li>normalize - Normalizes the data to the range [0,1].</li> <li>standardize - Standardizes the data to have a mean of 0 and variance of 1.</li> </ul>"},{"location":"ml-toolkit/api-reference/#6-datasets","title":"6. datasets","text":"<p>The datasets module is a comprehensive suite designed to handle, process, and prepare data cubes for machine learning applications. This module supports various data scales and integrates seamlessly with major deep learning frameworks like PyTorch and TensorFlow, ensuring that data stored in <code>xarray</code> datasets is optimally formatted and ready for training deep learning models.</p>"},{"location":"ml-toolkit/api-reference/#61-multiproc_sampler","title":"6.1. multiproc_sampler","text":"<p>The <code>MultiProcSampler</code> class is designed to process and sample large multidimensional training and testing datasets efficiently using parallel processing, specifically tailored for machine learning model training in the <code>ml4xcube</code> framework.</p> <p>Classes:</p> <ul> <li>MultiProcSampler - Samples train and test data as <code>.zarr</code> datasets.</li> </ul>"},{"location":"ml-toolkit/api-reference/#62-pytorch","title":"6.2. pytorch","text":"<p>The <code>datasets.pytorch</code> module integrates with <code>PyTorch</code> to manage and process large datasets efficiently. This module utilizes the power of <code>PyTorch</code>'s <code>Dataset</code> and <code>DataLoader</code> functionalities to prepare and iterate over chunks of data cubes for deep learning applications, ensuring that data management is scalable and performance-optimized.</p> <p>Classes:</p> <ul> <li>PTXrDataset - Corresponds to a subclass of PyTorch\u2019s Dataset, designed specifically to handle large datasets based on a provided <code>xarray.Dataset</code>.</li> </ul> <p>Functions:</p> <ul> <li>prep_dataloader - Sets up one or two <code>DataLoader</code>s from a PyTorch <code>Dataset</code> which was sampled from an <code>xarray.Dataset</code>. If a test set is provided, two <code>DataLoader</code>s are returned; otherwise, one.</li> </ul>"},{"location":"ml-toolkit/api-reference/#63-tensorflow","title":"6.3. tensorflow","text":"<p>The <code>datasets.tensorflow module</code> is specifically designed to handle and iterate over large <code>xarray</code> datasets and efficiently prepare them for use with TensorFlow models. This module provides a seamless interface to transform data stored in <code>xarray</code> datasets into structured TensorFlow datasets that can be directly utilized in training and inference pipelines. The core functionality is encapsulated in the <code>TFXrDataset</code> class, which leverages TensorFlow's capabilities to manage data flow dynamically, supporting scalable machine learning operations on large datasets.</p> <p>Classes:</p> <ul> <li>TFXrDataset - TensorFlow specific implementation to handle and iterate over large <code>xarray</code> datasets.</li> </ul>"},{"location":"ml-toolkit/api-reference/#64-xr_dataset","title":"6.4. xr_dataset","text":"<p>The <code>XrDataset</code> class within the <code>datasets/xr-dataset</code> module is tailored to efficiently manage and process smaller datasets directly within memory, leveraging in-memory operations to enhance both speed and performance.</p> <p>Classes:</p> <ul> <li>XrDataset - Creates small datasets manageable in memory.</li> </ul>"},{"location":"ml-toolkit/api-reference/#7-training","title":"7. training","text":"<p>The training module serves as a comprehensive suite for training machine learning models across various frameworks, designed to accommodate the unique demands of large-scale and high-dimensional datasets typically encountered in geospatial analysis and beyond. This module streamlines the training process, offering specialized support for PyTorch, TensorFlow, and scikit-learn, enabling users to leverage the strengths of these popular frameworks efficiently.</p>"},{"location":"ml-toolkit/api-reference/#71-pytorch","title":"7.1. pytorch","text":"<p>The <code>training.pytorch</code> module provides tools for training PyTorch models. It includes functionalities such as early stopping, model checkpointing, and performance logging, ensuring efficient training and optimization of models.</p> <p>Classes:</p> <ul> <li>Trainer - Tailored for the training of PyTorch models.</li> </ul>"},{"location":"ml-toolkit/api-reference/#72-pytorch_distributed","title":"7.2. pytorch_distributed","text":"<p>The training.pytorch_distributed module is designed to facilitate efficient distributed training of PyTorch models across multiple GPUs or nodes. This module leverages PyTorch's DistributedDataParallel (DDP) functionality, providing tools to handle complex distributed training tasks with ease, including setup, execution, and synchronization across multiple processes.</p> <p>Classes:</p> <ul> <li>Trainer - Crafted to perform distributed training for PyTorch models.</li> </ul> <p>Functions:</p> <ul> <li>ddp_init - Initializes the distributed process group for GPU-based distributed training.</li> </ul>"},{"location":"ml-toolkit/api-reference/#73-sklearn","title":"7.3. sklearn","text":"<p>The <code>training.sklearn</code> module is tailored to train scikit-learn models efficiently. It supports batch training for handling large datasets and provides tools for evaluating model performance using various metrics, catering to both supervised and unsupervised learning tasks.</p> <p>Classes:</p> <ul> <li>Trainer - Designed for training scikit-learn models.</li> </ul>"},{"location":"ml-toolkit/api-reference/#74-tensorflow","title":"7.4. tensorflow","text":"<p>The <code>training.tensorflow</code> module is specifically designed for training TensorFlow models. This module provides a comprehensive suite of tools for training, evaluating, and monitoring TensorFlow models, particularly those used in processing large datasets typically encountered in fields such as geospatial analysis.</p> <p>Classes:</p> <ul> <li>Trainer - Created to facilitate the training of TensorFlow models.</li> </ul>"},{"location":"ml-toolkit/api-reference/#8-postprocessing","title":"8. postprocessing","text":"<p>The <code>preprocessing</code> module provides functionalities, which are commonly required after machine learning operations to receive the final predictions.</p> <p>Functions:</p> <ul> <li>undo_normalizing - Reverts the normalization process to obtain the original data range.</li> <li>undo_standardizing - Reverts the standardization process to obtain the original data scale.</li> </ul>"},{"location":"ml-toolkit/api-reference/#9-evaluation","title":"9. evaluation","text":"<p>The evaluation module in the <code>ml4xcube</code> API is designed to support comprehensive metric evaluation for machine learning models across various frameworks including PyTorch, TensorFlow, and Scikit-learn. This module supports with assessing model performance during validation or testing phases, providing a range of metrics to evaluate accuracy, error rates, and other critical performance indicators. Providing a unified access to metrics from the different frameworks</p>"},{"location":"ml-toolkit/api-reference/#91-evaluator","title":"9.1. evaluator","text":"<p>The <code>Evaluator</code> class is tailored to handle metric evaluations, allowing users to measure and analyze model performance using metrics suited to their specific framework.</p> <p>Classes:</p> <ul> <li>Evaluator - Facilitates metric evaluation across different machine learning frameworks, enabling the assessment of various performance metrics during model validation or testing.</li> </ul>"},{"location":"ml-toolkit/api-reference/#10-utils","title":"10. utils","text":"<p>The <code>utils</code> module provides a set of utility functions for handling and processing <code>xarray.Datasets</code>. These functions facilitate tasks such as rechunking datasets, retrieving specific data chunks, and iterating over data blocks. They are particularly helpful for optimizing the performance of data operations and preparing datasets for machine learning tasks.</p> <p>Functions:</p> <ul> <li>assign_dims - Assign dimensions to each <code>dask.array</code> or <code>xarray.DataArray</code> within a dictionary.</li> <li>calculate_total_chunks - Compute the number of chunks of an <code>xarray.Dataset</code>.</li> <li>get_chunk_by_index - Retrieve a specific data chunk from an <code>xarray.Dataset</code>.</li> <li>get_chunk_sizes - Determine maximum chunk sizes of all data variables of the <code>xarray.Dataset</code>.</li> <li>get_dim_range - Calculates the dimension range of an <code>xarray.DataArray</code> dimension.</li> <li>iter_data_var_blocks - Create an iterator over chunks of an <code>xarray.Dataset</code>.</li> <li>rechunk_cube - Rechunks an <code>xarray.DataArray</code> to a new chunking scheme and stores the result at a specified path.</li> <li>split_chunk - Split a chunk into data samples for subsequent machine learning training.</li> </ul>"},{"location":"ml-toolkit/api-reference/1-plotting/plot-slice/","title":"Plot slice","text":""},{"location":"ml-toolkit/api-reference/1-plotting/plot-slice/#plot_slice","title":"plot_slice","text":"<pre><code>def plot_slice(\n    ds: xr.DataArray, var_to_plot: str, xdim: str, ydim: str, filter_var: str ='land_mask', title: str ='Slice Plot',\n    label: str ='Cube Slice', color_map: str ='viridis', xlabel: str ='Longitude', ylabel: str ='Latitude',\n    save_fig: bool = False, file_name: str ='plot.png', fig_size: Tuple[int, int] =(15, 10), vmin: float = None,\n    vmax: float = None, ticks: List[float] = None\n) -&gt; None\n</code></pre>"},{"location":"ml-toolkit/api-reference/1-plotting/plot-slice/#description","title":"Description","text":"<p>The <code>plot_slice</code> function plots a slice of data from an <code>xarray.DataArray</code> with an optional mask for context. It visualizes the specified variable using a heatmap and can highlight land borders if a mask is provided (e.g. land mask for ESDC). The function also supports saving the plot to a file.</p>"},{"location":"ml-toolkit/api-reference/1-plotting/plot-slice/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.DataArray</code>): DataArray containing the data to plot.</li> <li>var_to_plot (<code>str</code>): Name of the variable to visualize.</li> <li>xdim (<code>str</code>): Name of the x dimension to plot (e.g., longitude).</li> <li>ydim (<code>str</code>): Name of the y dimension to plot (e.g., latitude).</li> <li>filter_var (<code>str</code>): Name of the variable used for masking relevant areas for plotting. Defaults to <code>'land_mask'</code>.</li> <li>title (<code>str</code>): Title of the plot. Defaults to <code>'Cube Slice Plot'</code>.</li> <li>label (<code>str</code>): Legend label for the plot. Defaults to <code>'Cube Slice'</code>.</li> <li>color_map (<code>str</code>): Color map to use for the plot. Defaults to <code>'viridis'</code>.</li> <li>xlabel (<code>str</code>): Label for the x-axis. Defaults to <code>'Longitude'</code>.</li> <li>ylabel (<code>str</code>): Label for the y-axis. Defaults to <code>'Latitude'</code>.</li> <li>save_fig (<code>bool</code>): If <code>True</code>, saves the figure to a file. Defaults to <code>False</code>.</li> <li>file_name (<code>str</code>): Name of the file to save the plot to, if <code>save_fig</code> is <code>True</code>. Defaults to <code>'plot.png'</code>.</li> <li>fig_size (<code>tuple</code>): Size of the figure to create. Defaults to <code>(15, 10)</code>.</li> <li>vmin (<code>float</code>): Minimum value for the color bar. Defaults to <code>None</code>.</li> <li>vmax (<code>float</code>): MMaximum value for the color bar. Defaults to<code>None</code>.</li> <li>ticks (<code>List[float]</code>): List of tick values for the color bar. Defaults to <code>None</code>.</li> </ul>"},{"location":"ml-toolkit/api-reference/1-plotting/plot-slice/#returns","title":"Returns","text":"<ul> <li><code>None</code>: The function creates a plot but does not return any value.</li> </ul>"},{"location":"ml-toolkit/api-reference/1-plotting/plot-slice/#example","title":"Example","text":"<pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.xr_plots import plot_slice\n\n# Example usage with a sample dataset\nds = xr.Dataset({\n    'temperature': (('time', 'latitude', 'longitude'), np.random.rand(10, 20, 30)),\n    'precipitation': (('time', 'latitude', 'longitude'), np.random.rand(10, 20, 30)),\n    'land_mask': (('latitude', 'longitude'), np.random.choice([True, False], size=(20, 30)))\n})\n\n# Select a specific time slice (valid index within the example data range)\nds_slice = ds.isel(time=0)\n\n# Plot the slice\nplot_slice(\n    ds          = ds_slice,\n    var_to_plot = 'temperature',\n    xdim        = 'longitude',\n    ydim        = 'latitude',\n    filter_var  = 'land_mask',\n    title       = 'Temperature Plot',\n    label       = 'Temperature (\u00b0C)',\n    color_map   = 'coolwarm',\n    xlabel      = 'Longitude',\n    ylabel      = 'Latitude',\n    save_fig    = True,\n    file_name   = 'temperature_plot.png',\n    fig_size    = (12, 8),\n    vmin        = -10,\n    vmax        = 30,\n    ticks       = [-10, 0, 10, 20, 30]\n)\n</code></pre>"},{"location":"ml-toolkit/api-reference/1-plotting/plot-slice/#notes","title":"Notes","text":"<ul> <li>Ensure that the <code>filter_var</code> is present in the dataset to use it as a mask.</li> <li>Adjust the <code>fig_size</code>, <code>vmin</code>, <code>vmax</code>, and <code>ticks</code> parameters as needed to customize the plot appearance.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/assign-dims/","title":"Assign dims","text":""},{"location":"ml-toolkit/api-reference/10-utils/assign-dims/#assign_dims","title":"assign_dims","text":"<pre><code>def assign_dims(data: Dict[str, da.Array|xr.DataArray], dims: Tuple[str]) -&gt; Dict[str, xr.DataArray]\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/assign-dims/#description","title":"Description","text":"<p>The <code>assign_dims</code> function assigns dimension names to each variable in a dataset based on the provided dimension names. This function is useful for standardizing the dimensional metadata of dask arrays or xarray DataArrays within a dictionary and for the creation of xarray Datasets.</p>"},{"location":"ml-toolkit/api-reference/10-utils/assign-dims/#parameters","title":"Parameters","text":"<ul> <li>data (<code>Dict[str, dask.array | xarray.DataArray]</code>): A dictionary where keys are variable names and values are of type <code>dask.array</code> or <code>xarray.DataArray</code>.</li> <li>dims (<code>Tuple[str]</code>): A tuple of dimension names to assign to the arrays.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/assign-dims/#returns","title":"Returns","text":"<ul> <li><code>Dict[str, xarray.DataArray]</code>: A dictionary where keys are variable names and values are of type<code>xarray.DataArray</code> with assigned dimensions.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/assign-dims/#example","title":"Example","text":"<p><pre><code>import dask.array as da\nfrom ml4xcube.utils import assign_dims\n\n# Example data\ndata = {\n    'temperature': da.random.random((10, 20, 30)),\n    'precipitation': da.random.random((10, 20, 30))\n}\n\n# Assign dimensions\ndims = ('time', 'lat', 'lon')\nassigned_dims_data = assign_dims(data, dims)\n\n# Output the data with assigned dimensions\nfor var, dataarray in assigned_dims_data.items():\n    print(f\"{var}: {dataarray.dims}\")\n</code></pre> The dimensions 'time', 'lat', and 'lon' are assigned to the data arrays in the dictionary.</p>"},{"location":"ml-toolkit/api-reference/10-utils/calculate-total-chunks/","title":"Calculate total chunks","text":""},{"location":"ml-toolkit/api-reference/10-utils/calculate-total-chunks/#calculate_total_chunks","title":"calculate_total_chunks","text":"<pre><code>def calculate_total_chunks(ds: xr.Dataset, block_size: List[Tuple[str, int]] = None) -&gt; int\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/calculate-total-chunks/#description","title":"Description","text":"<p>The total number of chunks for an <code>xarray.Dataset</code> is calculated based on specified or default chunk sizes. </p>"},{"location":"ml-toolkit/api-reference/10-utils/calculate-total-chunks/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The dataset for which the total number of chunks will be calculated. The dataset should have dimensions that can be chunked.</li> <li>block_size (<code>Optional[List[Tuple[str, int]]]</code>): A sequence of tuples specifying the block size for each dimension. Each tuple should contain a dimension name and a block size for that dimension. If not provided, the function will use the dataset's default chunk sizes.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/calculate-total-chunks/#returns","title":"Returns","text":"<ul> <li><code>int</code>: The total number of chunks in the dataset based on the specified or default chunk sizes.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/calculate-total-chunks/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.utils import calculate_total_chunks\n\n# Example dataset\ndata = np.random.rand(100, 200, 300)\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), data),\n    'precipitation': (('time', 'lat', 'lon'), data)\n})\n\n# Define the block size (chunk size)\nblock_size = [('time', 10), ('lat', 20), ('lon', 30)]\n\n# Calculate total chunks\ntotal_chunks = calculate_total_chunks(ds, block_size=block_size)\nprint(f\"Total number of chunks: {total_chunks}\")\n</code></pre> Based on the specified blocks the number of chunks is calculated.</p>"},{"location":"ml-toolkit/api-reference/10-utils/calculate-total-chunks/#notes","title":"Notes","text":"<ul> <li>The <code>block_size</code> parameter allows you to specify custom chunk sizes. If not provided, the function will use the dataset's default chunk sizes.</li> <li>Ensure that the provided <code>block_size</code> values are appropriate for the dimensions of the dataset.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-by-index/","title":"Get chunk by index","text":""},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-by-index/#get_chunk_by_index","title":"get_chunk_by_index","text":"<pre><code>def get_chunk_by_index(ds: xr.Dataset, index: int, block_size: List[Tuple[str, int]] = None) -&gt; Dict[str, np.ndarray]\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-by-index/#description","title":"Description","text":"<p><code>get_chunk_by_index</code> retrieves a specific data chunk from an <code>xarray.Dataset</code> based on a given linear index. This way the extraction of subsets is feasible. Further the chunks of a cube can be iterated.</p>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-by-index/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The <code>xarray.Dataset</code> from which to retrieve a chunk.</li> <li>index (<code>int</code>): The index of the chunk to retrieve. This index will be converted to a multi-dimensional index based on the chunk sizes.</li> <li>block_size (<code>List[Tuple[str, int]]</code>): An optional list of tuples specifying the block size for each dimension. Each tuple should contain a dimension name and a block size for that dimension. A chunk with the specified block sizes will be returned. If not provided, the function will use the dataset's default chunk sizes.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-by-index/#returns","title":"Returns","text":"<ul> <li><code>Dict[str, np.ndarray]</code>: A dictionary where keys are variable names from the dataset and values are NumPy arrays containing the data of the specified chunk.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-by-index/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.utils import get_chunk_by_index\n\n# Example dataset\ndata = np.random.rand(100, 200, 300)\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), data),\n    'precipitation': (('time', 'lat', 'lon'), data)\n})\n\n# Define the block size (chunk size)\nblock_size = [('time', 10), ('lat', 20), ('lon', 30)]\n\n# Get the 5th chunk (index starts from 0)\nchunk_data = get_chunk_by_index(ds, index=5, block_size=block_size)\n\n# Output the chunk data\nfor var_name, chunk in chunk_data.items():\n    print(f\"{var_name} chunk shape: {chunk.shape}\")\n</code></pre> The <code>get_chunk_by_index</code> function retrieves the 5th chunk from the dataset, using the specified block sizes for each dimension.</p>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-sizes/","title":"Get chunk sizes","text":""},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-sizes/#get_chunk_sizes","title":"get_chunk_sizes","text":"<pre><code>def get_chunk_sizes(ds: xr.Dataset) -&gt; List[Tuple[str, int]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-sizes/#description","title":"Description","text":"<p>The maximum chunk sizes for all data variables in a given <code>xarray.Dataset</code> is determined. This allows to understand the chunking scheme of the dataset and for setting up consistent chunk sizes for processing.</p>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-sizes/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The dataset for which the maximum chunk sizes are to be determined. The dataset should have dimensions that can be chunked.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-sizes/#returns","title":"Returns","text":"<ul> <li><code>List[Tuple[str, int]]</code>: A list of tuples where each tuple contains a dimension name (str) and its corresponding maximum chunk size (int) over all variables.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-sizes/#example","title":"Example","text":"<pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.utils import get_chunk_sizes\n\n# Example dataset with chunking\ndata = np.random.rand(100, 200, 300)\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), data),\n    'precipitation': (('time', 'lat', 'lon'), data)\n}).chunk({'time': 10, 'lat': 20, 'lon': 30})\n\n# Get maximum chunk sizes\nchunk_sizes = get_chunk_sizes(ds)\nprint(chunk_sizes)\n</code></pre> <p>In this example, the <code>get_chunk_sizes</code> function returns the chunk sizes for each dimension in the dataset.</p>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-sizes/#notes","title":"Notes","text":"<ul> <li>The function iterates over all data variables in the dataset and retrieves their chunk sizes.</li> <li>If the variable has chunk sizes, it calculates the maximum chunk size for each dimension.</li> <li>The returned list contains tuples with dimension names and their maximum chunk sizes, which can be used for further processing.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-dim-range/","title":"Get dim range","text":""},{"location":"ml-toolkit/api-reference/10-utils/get-dim-range/#get_dim_range","title":"get_dim_range","text":"<pre><code>def get_dim_range(cube: xr.DataArray, dim: str) -&gt; Union[Tuple[float, float], Tuple[str, str]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/get-dim-range/#description","title":"Description","text":"<p>Calculates and returns the minimum and maximum values of a specified dimension within an <code>xarray.Dataset</code>. This function supports dimensions with numerical or datetime data types, making it versatile for a range of data analysis contexts.</p>"},{"location":"ml-toolkit/api-reference/10-utils/get-dim-range/#parameters","title":"Parameters","text":"<ul> <li>cube (<code>xarray.DataArray</code>): The input data cube from which the dimension range is to be calculated.</li> <li>dim (<code>str</code>): The name of the dimension for which the range is to be determined.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-dim-range/#returns","title":"Returns","text":"<ul> <li><code>Union[Tuple[float, float], Tuple[str, str]]</code>: The minimum and maximum values of the dimension. For datetime dimensions, these values are formatted as strings; for numerical dimensions, they are returned as numbers.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-dim-range/#example","title":"Example","text":"<p>Here's how you might use the <code>get_dim_range</code> function to find the range of the 'time' dimension in a dataset: <pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.utils import get_dim_range\n\n# Create a sample DataArray with datetime and numerical data\ntimes = np.array(['2020-01-01', '2020-01-02', '2020-01-03'], dtype='datetime64[D]')\ndata = np.random.rand(3, 2, 2)  # Random data for 3 days, 2 latitudes, and 2 longitudes\ncube = xr.DataArray(data, dims=['time', 'latitude', 'longitude'], coords={'time': times})\n\n# Get the range of the 'time' dimension\ntime_range = get_dim_range(cube, 'time')\nprint(\"Time Dimension Range:\", time_range)\n</code></pre> A simple <code>xarray.DataArray</code> with a 'time' dimension is created and <code>get_dim_range</code> is used to extract and print the date range.</p>"},{"location":"ml-toolkit/api-reference/10-utils/iter-data-var-blocks/","title":"Iter data var blocks","text":""},{"location":"ml-toolkit/api-reference/10-utils/iter-data-var-blocks/#iter_data_var_blocks","title":"iter_data_var_blocks","text":"<pre><code>def iter_data_var_blocks(ds: xr.Dataset, block_size: List[Tuple[str, int]] = None) -&gt; Iterator[Dict[str, np.ndarray]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/iter-data-var-blocks/#description","title":"Description","text":"<p>An iterator that provides all data blocks of all data variables in the given dataset is created. Allows iterating over chunks.</p>"},{"location":"ml-toolkit/api-reference/10-utils/iter-data-var-blocks/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The dataset to iterate over</li> <li>block_size (<code>List[Tuple[str, int]]</code>): A sequence of tuples specifying the block size for each dimension. Each tuple should contain a dimension name and a block size for that dimension. If not provided, the function will use the dataset's default chunk sizes.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/iter-data-var-blocks/#yields","title":"Yields","text":"<ul> <li><code>Iterator[Dict[str, numpy.ndarray]]</code>: An iterator of dictionaries where keys are variable names from the dataset and values are data blocks as NumPy arrays. Each iteration yields a dictionary representing a single data block for all variables.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/iter-data-var-blocks/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.utils import iter_data_var_blocks\n\n# Example dataset\ndata = np.random.rand(100, 200, 300)\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), data),\n    'precipitation': (('time', 'lat', 'lon'), data)\n})\n\n# Define the block size (chunk size)\nblock_size = [('time', 10), ('lat', 20), ('lon', 30)]\n\n# Iterate over data blocks\nfor block in iter_data_var_blocks(ds, block_size=block_size):\n    for var_name, chunk in block.items():\n        print(f\"{var_name} chunk shape: {chunk.shape}\")\n</code></pre> The <code>iter_data_var_blocks</code> function iterates over the dataset, yielding chunks of data according to the specified block sizes.</p>"},{"location":"ml-toolkit/api-reference/10-utils/iter-data-var-blocks/#notes","title":"Notes","text":"<ul> <li>The <code>block_size</code> parameter allows you to specify custom chunk sizes. If not provided, the function will use the dataset's default chunk sizes.</li> <li>Ensure that the provided <code>block_size</code> values are appropriate for the dimensions of the dataset.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/rechunk-cube/","title":"Rechunk cube","text":""},{"location":"ml-toolkit/api-reference/10-utils/rechunk-cube/#rechunk_cube","title":"rechunk_cube","text":"<pre><code>def rechunk_cube(source_cube: xr.DataArray, target_chunks: Dict[str, int] | Tuple[int] | List[int], target_path: str)\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/rechunk-cube/#description","title":"Description","text":"<p>The <code>rechunk_cube</code> function rechunks an xarray <code>DataArray</code> to a new chunking scheme and stores the result at the specified path. </p>"},{"location":"ml-toolkit/api-reference/10-utils/rechunk-cube/#parameters","title":"Parameters","text":"<ul> <li>source_cube (<code>xarray.DataArray</code>): The input <code>DataArray</code> that you want to rechunk. This <code>DataArray</code> should be already chunked or be capable of being chunked.</li> <li>target_chunks (<code>Dict[str, int] | Tuple[int] | List[int]</code>): The desired chunk sizes for the rechunking operation. This can be specified in different formats:</li> <li>Dictionary: Specify sizes for each named dimension, e.g., <code>{'lon': 60, 'lat': 1, 'time': 100}</code>. </li> <li>Tuple or List: Specify sizes by order, corresponding to the array's dimensions, e.g., <code>(60, 1, 100)</code>.</li> <li>target_path (<code>str</code>): The path where the rechunked <code>DataArray</code> should be stored, typically a path to a Zarr store. </li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/rechunk-cube/#returns","title":"Returns","text":"<ul> <li><code>None</code>: The function does not return any value. It prints a message upon successful completion of the rechunking process.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/rechunk-cube/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.utils import rechunk_cube\n\n# Example data\nsource_cube = xr.DataArray(\n    np.random.rand(100, 200, 300),\n    dims=['time', 'lat', 'lon'],\n    name='example_data'\n)\n\n# Desired chunk sizes\ntarget_chunks = {'time': 10, 'lat': 20, 'lon': 30}\n\n# Rechunk the DataArray\nrechunk_cube(source_cube, target_chunks=target_chunks, target_path='rechunked_data.zarr')\n</code></pre> In this example, the <code>source_cube</code> is rechunked according to the specified <code>target_chunks</code> and stored at the given <code>target_path</code>.</p>"},{"location":"ml-toolkit/api-reference/10-utils/split-chunk/","title":"Split chunk","text":""},{"location":"ml-toolkit/api-reference/10-utils/split-chunk/#split_chunk","title":"split_chunk","text":"<pre><code>def split_chunk(chunk: Dict[str, np.ndarray], sample_size: List[Tuple[str, int]] = None,\n                overlap: List[Tuple[str, float]] = None) -&gt; Dict[str, np.ndarray]\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/split-chunk/#description","title":"Description","text":"<p>The <code>split_chunk</code> function splits a given chunk of data into smaller data samples or points based on the provided sample size and optional overlap configurations. This function is useful for data preprocessing, particularly when dealing with large datasets that need to be divided into manageable parts for analysis or model training.</p>"},{"location":"ml-toolkit/api-reference/10-utils/split-chunk/#parameters","title":"Parameters","text":"<ul> <li>chunk (<code>Dict[str, numpy.ndarray]</code>): A dictionary where keys are variable names and values are NumPy arrays representing the data chunk to be split.</li> <li>sample_size (<code>List[Tuple[str, int]]</code>): A list of tuples specifying the sample size for each dimension. Each tuple consists of a dimension name (str) and the corresponding sample size (int). If <code>None</code> the chunk is split into points.</li> <li>overlap (<code>List[Tuple[str, float]]</code>): A list of tuples specifying the overlap for overlapping samples due to chunk splitting. Each tuple consists of a dimension name (str) and the overlap percentage (float) between 0 and 1. If <code>None</code>, the resulting samples don't overlap.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/split-chunk/#returns","title":"Returns","text":"<ul> <li><code>Dict[str, np.ndarray]</code>: A dictionary where keys are variable names and values are NumPy arrays containing the split data samples or points.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/split-chunk/#example","title":"Example","text":"<pre><code>import numpy as np\nfrom ml4xcube.utils import split_chunk\n\n# Example chunk data\nchunk = {\n    'temperature': np.random.rand(10, 10, 10),\n    'precipitation': np.random.rand(10, 10, 10)\n}\n\n# Split the chunk\nsplit_data = split_chunk(chunk, sample_size=[('time', 5), ('lat', 5), ('lon', 5)], overlap=[('time', 0.5), ('lat', 0.5), ('lon', 0.5)])\nprint(split_data)\n</code></pre> <p>The above code splits the chunk of data into smaller samples based on the specified sample size and optional overlap, resulting in a dictionary with the split data for each variable.</p>"},{"location":"ml-toolkit/api-reference/10-utils/split-chunk/#notes","title":"Notes","text":"<ul> <li>The <code>overlap</code> parameter allows you to specify overlapping regions between the samples, which can be useful for certain types of analyses or training machine learning models where context from neighboring samples is important.</li> <li>Ensure that the specified <code>sample_size</code> and overlap values are appropriate for the dimensions and size of the input chunk to avoid errors or unexpected behavior.</li> </ul>"},{"location":"ml-toolkit/api-reference/2-insights/get-count-heat-map/","title":"Get count heat map","text":""},{"location":"ml-toolkit/api-reference/2-insights/get-count-heat-map/#get_gap_heat_map","title":"get_gap_heat_map","text":"<pre><code>def get_gap_heat_map(cube: xr.DataArray, count_dim: str) -&gt; xr.DataArray\n</code></pre>"},{"location":"ml-toolkit/api-reference/2-insights/get-count-heat-map/#description","title":"Description","text":"<p>A heat map of value counts (non-<code>NaN</code> values) for each pixel of dimensions in an <code>xarray.DataArray</code> is genrated.  This heat map helps in visualizing the distribution and density of gaps across the spatial dimensions.</p>"},{"location":"ml-toolkit/api-reference/2-insights/get-count-heat-map/#parameters","title":"Parameters","text":"<ul> <li>cube (<code>xarray.DataArray</code>): The input data cube.</li> <li>count_dim (<code>str</code>): The dimension along which to count non-<code>NaN</code> values, typically spatial dimensions such as 'latitude' or 'longitude'.</li> </ul>"},{"location":"ml-toolkit/api-reference/2-insights/get-count-heat-map/#returns","title":"Returns","text":"<ul> <li><code>xarray.DataArray</code>: Heat map of non-<code>NaN</code> value counts for each pixel across one dimension.</li> </ul>"},{"location":"ml-toolkit/api-reference/2-insights/get-count-heat-map/#example","title":"Example","text":"<p><pre><code>import xarray as xr\nfrom ml4xcube.xr_plots import plot_slice\nfrom ml4xcube.cube_insights import get_gap_heat_map\n\n# Load sample data\nds = xr.open_zarr('sample_data.zarr')\nds = ds['temperature']\n\n# Generate and visualize the gap heat map\ngap_heat_map = get_gap_heat_map(ds)\ndataset   = gap_heat_map.to_dataset(name='temperature')\n\nplot_slice(\n    ds          = dataset,\n    var_to_plot = 'temperature', \n    color_map   = \"plasma\",\n    title       = \"Filled artificial gaps matrix\",\n    label       = \"Number of gaps\",\n    xdim        = \"lon\",\n    ydim        = \"lat\"\n)\n</code></pre> Running this example results in an illustration as the following:</p> <p> </p>"},{"location":"ml-toolkit/api-reference/2-insights/get-insights/","title":"Get insights","text":""},{"location":"ml-toolkit/api-reference/2-insights/get-insights/#get_insights","title":"get_insights","text":"<pre><code>def get_insights(cube: xr.Dataset, variable: str, layer_dim: str = None) -&gt; None\n</code></pre>"},{"location":"ml-toolkit/api-reference/2-insights/get-insights/#description","title":"Description","text":"<p>Various characteristics of a data cube represented by an <code>xarray.DataArray</code> are extracted and printed.  The function provides information such as the variable's name, dimension names and ranges, the size of the data cube, the number and percentage of <code>NaN</code> values, and the range of data values.  Additionally, it identifies the maximum and minimum gap sizes within the data cube layers</p>"},{"location":"ml-toolkit/api-reference/2-insights/get-insights/#parameters","title":"Parameters","text":"<ul> <li>cube (<code>xarray.Dataset</code>): The input data cube.</li> <li>variable (<code>str</code>): variable to extract the <code>DataArray</code> containing the data to receive insights from</li> <li>layer_dim (<code>str</code>): The dimension along which to iterate and extract detailed layer-specific information. First dimension is default if not specified. </li> </ul>"},{"location":"ml-toolkit/api-reference/2-insights/get-insights/#returns","title":"Returns","text":"<ul> <li><code>None</code>: The function prints the extracted characteristics of the data cube.</li> </ul>"},{"location":"ml-toolkit/api-reference/2-insights/get-insights/#example","title":"Example","text":"<p><pre><code>import xarray as xr\nfrom ml4xcube.cube_insights import get_insights\n\n# Load sample data\nds = xr.open_zarr('sample_data.zarr')\nds = ds['temperature']\n\n# Get insights from the data cube\nget_insights(ds)\n</code></pre> The <code>get_insights</code> function, prints the following statistics (example for a cube containing dimensions named Time, Latitude, and Longitude):</p> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:09&lt;00:00,  1.10it/s]\nThe data cube has the following characteristics:\n\nVariable:             Land Surface Temperature\nShape:                (time: 10, lat: 2160, lon: 4320)\nTime range:           2002-05-21 - 2002-08-01\nLatitude range:       -89.958\u00b0 - 89.958\u00b0\nLongitude range:      -179.958\u00b0 - 179.958\u00b0\nTotal size:           93312000\nSize of each layer:   9331200\nTotal gap size:       74069847 -&gt; 79 %\nMaximum gap size:     87 % on 2002-06-06\nMinimum gap size:     75 % on 2002-08-01\nValue range:          222.99 - 339.32\n</code></pre>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/","title":"Gap dataset","text":""},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/#class-gapdataset","title":"Class: GapDataset","text":"<p>The <code>GapDataset</code> class within the <code>helper.predictors</code> submodule is designed to manage and prepare predictor data that aids in the estimation of missing values within a target dataset. It initializes with a specific variable from a dataset and aligns predictor data accordingly, handling both the extraction and storage of the processed data.</p>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/#constructor","title":"Constructor","text":"<pre><code> def __init__(self, ds: xr.DataArray, ds_name: str = 'Test123',\n              dimensions: Dict[str, tuple] = None,\n              artificial_gaps: List[float] = None,\n              actual_matrix: Union[str, datetime.date] = 'Random',\n              predictor_path: str = None, layer_dim: str = 'time'):\n</code></pre>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.DataArray</code>): The input dataset that contains gaps.</li> <li>ds_name (<code>str</code>): The name of the dataset.</li> <li>dimensions (<code>Dict[str, tuple]</code>): Dict containing dimension ranges (e.g., lat, lon, times) in order to extract the subset of <code>ds</code> relevant for the prediction.</li> <li>artificial_gaps (<code>List[float]</code>):  List of artificial gap sizes (floats between 0 and 1) to create ground truth for training and subsequent gapfilling of real gaps. If None no artificial gaps are created. The predictor will estimate real gaps directly when utilizing the Gapfilling class. </li> <li>actual_matrix (<code>Union[str, datetime.date]</code>): Specifies the selection criterion for extracting a specific slice of the dataset based on the dimension defined by layer_dim. This parameter can be used in two ways:<ul> <li>As <code>str</code>: If set to 'Random', a random value from the available values within the specified dimension (layer_dim) is chosen. This is useful for stochastic approaches where randomness is needed for validation or testing.</li> <li>As <code>datetime.date</code> or specific value: Allows precise specification of the slice to be selected. When a date or specific value is provided, the dataset is sliced at this exact point, or the nearest available point if the exact value is not present in the dataset. </li> </ul> </li> <li>predictor_path (<code>str</code>): Path to the directory where predictor data is stored. If <code>None</code> a support vector regression is performed for every gap size defined in <code>artificial_gaps</code>.</li> <li>layer_dim (<code>str</code>): Dimension along which to iterate. First dimension is default if not specified. </li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/#get_data","title":"get_data","text":"<p>This method orchestrates several key operations essential for setting up the dataset for subsequent gap-filling tasks. It manages the data retrieval, processing, and preparation phases, ensuring that all necessary data transformations and setups are completed before the actual gap-filling process begins. <pre><code>def get_data(self) -&gt; None\n</code></pre></p>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/#returns","title":"Returns","text":"<ul> <li><code>None</code>: This method does not return any value.</li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/#example","title":"Example","text":"<pre><code>import datetime\nimport xarray as xr\nfrom ml4xcube.gapfilling.gap_dataset import GapDataset\n\n# Example data and dimensions\ndata = xr.open_dataarray('path_to_dataarray')\ndimensions = {\n    'lat': (30, 45), \n    'lon': (10, 25), \n    'time': (datetime.date(2020, 1, 1), datetime.date(2020, 12, 31))\n}\n\n# Initialize the GapDataset class\ngap_dataset = GapDataset(data, 'ExampleDataset', dimensions)\n\n# Process data to create a dataset ready for gap filling\ngap_dataset.get_data()\n</code></pre>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/","title":"Gap filling","text":""},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/#class-gapfiller","title":"Class: Gapfiller","text":""},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, ds_name: str = \"Test123\", hyperparameters: str = \"RandomGridSearch\", predictor: str = \"RandomPoints\")\n</code></pre>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/#parameters","title":"Parameters","text":"<ul> <li>ds_name (<code>str</code>): The name used to identify and reference the dataset throughout the gap filling process. This name is utilized for directory naming, which helps in organizing output results such as filled datasets or models.</li> <li>hyperparameters (<code>str</code>): Defines the approach for hyperparameter optimization, with options such as <code>RandomGridSearch</code>, <code>FullGridSearch</code>, or <code>Custom</code>. The selection influences how the underlying machine learning model, such as SVR, will tune its parameters to best fit the data.<ul> <li><code>RandomGridSearch</code>:  Utilizes a randomized search over a predefined grid of hyperparameters. This method is faster but might miss the optimal point, suitable for large datasets or when a good-enough solution is acceptable.</li> <li><code>FullGridSearch</code>: Performs an exhaustive search over the specified grid of hyperparameters. While comprehensive, it can be computationally intensive.</li> <li><code>Custom</code>: Allows for manual specification of hyperparameters, providing full control over the learning process, ideal for use cases where domain knowledge can guide specific settings.</li> </ul> </li> <li>predictor (<code>str</code>): Strategy for selecting predictors used in model training, options include <code>AllPoints</code>, <code>RandomPoints</code>, <code>lccs_class</code>, or any specified extra matrix predictors.<ul> <li><code>AllPoints</code>: Uses all available data points as predictors, maximizing the information available for model training.</li> <li><code>RandomPoints</code>: Randomly selects a subset of data points to be used as predictors, useful for reducing computational load or when data is too large.</li> <li><code>lccs_class</code>: Specifies the use of land cover value estimation, focusing on leveraging spatial or categorical similarities.</li> <li><code>Custom</code>: Utilizes different types of external or derived predictors.</li> </ul> </li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/#gapfill","title":"gapfill","text":"<p>Main method to execute the gap filling process. It orchestrates data retrieval, directory management, model training, and result processing. Prints the directory containing the application results when the processes is completed.</p> <pre><code>def gapfill(self) -&gt; None\n</code></pre>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/#returns","title":"Returns","text":"<ul> <li><code>None</code>: This method does not return any value.</li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/#example","title":"Example","text":"<pre><code>import datetime\nimport xarray as xr\nfrom ml4xcube.gapfilling.gap_filler import Gapfiller\n\n# Example of using the Gapfiller class to fill data gaps\ngap_filler = Gapfiller(\n    ds_name='ExampleDataset', \n    hyperparameters=\"RandomGridSearch\", \n    predictor=\"lccs_class\"\n)\ngap_filler.gapfill()\n</code></pre>"},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/","title":"Helper helpingpredictor","text":""},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/#class-helpingpredictor","title":"Class: HelpingPredictor","text":"<p>The <code>HelpingPredictor</code> class within the <code>helper.predictors</code> submodule is designed to manage and prepare predictor data that aids in the estimation of missing values within a target dataset. It initializes with a specific variable from a dataset and aligns predictor data accordingly, handling both the extraction and storage of the processed data.</p>"},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, ds: xr.Dataset, variable: str, ds_predictor: xr.DataArray, predictor_path: str,\n            predictor: str = 'lccs_class', layer_dim: str = None)\n</code></pre>"},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The dataset containing the target variable.</li> <li>variable (<code>str</code>): The target variable to estimate.</li> <li>ds_predictor (<code>xarray.DataArray</code>): The dataset containing the predictor variable.</li> <li>predictor_path (<code>str</code>): Path to save the processed predictor data.</li> <li>predictor (<code>str</code>): Name of the predictor variable. Defaults to <code>lccs_class</code>.</li> <li>layer_dim (<code>str</code>): Dimension along which to iterate. First dimension is default if not specified. </li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/#methods","title":"Methods","text":"<p>The <code>get_predictor_data</code> method is designed to fetch and prepare the predictor in order to conduct the gap filling process. It extracts relevant data that align with the target dataset, processes this data if necessary, and saves it in a format that is easily accessible for further analysis. <pre><code>def get_predictor_data(self) -&gt; str\n</code></pre></p>"},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/#returns","title":"Returns","text":"<ul> <li><code>str</code>: The file path of the saved <code>.zarr</code> file containing the processed predictor data.</li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/#example","title":"Example","text":"<pre><code>import xarray as xr\nfrom ml4xcube.gapfilling.helper.predictors import HelpingPredictor\n\n# Example data and paths\nds = xr.open_dataset('path_to_dataset')\nds_predictor = xr.open_dataarray('path_to_predictor_data')\npredictor_path = 'path_to_store_processed_data'\n\n# Initialize the HelpingPredictor\npredictor = HelpingPredictor(ds, 'temperature', ds_predictor, predictor_path)\n\n# Get and process predictor data\npredictor_data_path = predictor.get_predictor_data()\nprint(f'Processed predictor data saved to: {predictor_data_path}')\n</code></pre>"},{"location":"ml-toolkit/api-reference/4-splits/assign-block-split/","title":"Assign block split","text":""},{"location":"ml-toolkit/api-reference/4-splits/assign-block-split/#assign_block_split","title":"assign_block_split","text":"<pre><code>def assign_block_split(ds: xr.Dataset, block_size: List[Tuple[str, int]] = None, split: float = 0.8) -&gt; xr.Dataset\n</code></pre>"},{"location":"ml-toolkit/api-reference/4-splits/assign-block-split/#description","title":"Description","text":"<p>Assigns blocks of data to training or testing sets based on a specified split ratio.  This method uses a deterministic random seed generated from the indices of each block, ensuring that the same blocks are consistently assigned to the same subset across different runs, given the same initial conditions.</p>"},{"location":"ml-toolkit/api-reference/4-splits/assign-block-split/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The input dataset.</li> <li>block_size (<code>List[Tuple[str, int]]</code>): List of tuples specifying the dimensions and their respective sizes for block division. If <code>None</code>, chunk sizes are inferred from the dataset.</li> <li>split (<code>float</code>): The fraction of data to assign to the training set. The remainder is assigned to the testing set. Default is <code>0.8</code>.</li> </ul>"},{"location":"ml-toolkit/api-reference/4-splits/assign-block-split/#returns","title":"Returns","text":"<ul> <li><code>xarray.Dataset</code>: The dataset with an added 'split' variable that indicates whether each block belongs to the training set (<code>1.</code>) or the testing set (<code>0.</code>).</li> </ul>"},{"location":"ml-toolkit/api-reference/4-splits/assign-block-split/#example","title":"Example","text":"<pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.data_split import assign_block_split\n\n# Example dataset\ndata = xr.Dataset({'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 2, 3))})\nblock_size = [('time', 5), ('lat', 2), ('lon', 3)]\nsplit_dataset = assign_block_split(data, block_size)\nprint(split_dataset)\n</code></pre> <p> Block Assignment of Train/Test Split </p>"},{"location":"ml-toolkit/api-reference/4-splits/assign-rand-split/","title":"Assign rand split","text":""},{"location":"ml-toolkit/api-reference/4-splits/assign-rand-split/#assign_rand_split","title":"assign_rand_split","text":"<pre><code>def assign_rand_split(ds: xr.Dataset, split: float = 0.8) -&gt; xr.Dataset\n</code></pre>"},{"location":"ml-toolkit/api-reference/4-splits/assign-rand-split/#description","title":"Description","text":"<p>Randomly assigns a training/test split indicator to each element in an <code>xarray.Dataset</code> based on a specified split ratio. This method ensures that the distribution of training and testing data is balanced across the entire dataset.</p>"},{"location":"ml-toolkit/api-reference/4-splits/assign-rand-split/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The input dataset.</li> <li>split (<code>float</code>):  The proportion of the dataset to be used for training. Defaults to <code>0.8</code>.</li> </ul>"},{"location":"ml-toolkit/api-reference/4-splits/assign-rand-split/#returns","title":"Returns","text":"<ul> <li><code>xarray.Dataset</code>: The dataset with an additional 'split' variable indicating the random split, where <code>1.</code> represents training data and <code>0.</code> represents testing data.</li> </ul>"},{"location":"ml-toolkit/api-reference/4-splits/assign-rand-split/#example","title":"Example","text":"<pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.data_split import assign_rand_split\n\n# Example dataset\ndata = xr.Dataset({'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 2, 3))})\nsplit_dataset = assign_rand_split(data, 0.7)\nprint(split_dataset)\n</code></pre> <p>In the example, each point in the dataset is randomly assigned to the train set with a 70% probability or to the test set  with a 30% probability. The random split is demonstrated in the image below:</p> <p> </p> <p> Random Assignment of Train/Test Split </p>"},{"location":"ml-toolkit/api-reference/4-splits/create-split/","title":"Create split","text":""},{"location":"ml-toolkit/api-reference/4-splits/create-split/#create_split","title":"create_split","text":"<pre><code>def create_split(\n        data: Union[xr.Dataset, Dict[str, np.ndarray]], to_pred: Union[List[str], str] = None,\n        exclude_vars: List[str] = list(), feature_vars: List[str] = None, stack_axis: int = -1\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n</code></pre>"},{"location":"ml-toolkit/api-reference/4-splits/create-split/#description","title":"Description","text":"<p>The <code>create_split</code> function efficiently generates train-test splits for machine learning models by using a predefined  <code>split</code> variable within a dataset. This method supports inputs in the form of either <code>xarray.Dataset</code> or a dictionary of <code>numpy</code> arrays.  It allows for flexible specification of feature and target variables, along with optional exclusions and custom stacking  configurations for input dimensions.</p>"},{"location":"ml-toolkit/api-reference/4-splits/create-split/#parameters","title":"Parameters","text":"<ul> <li>data (<code>Union[xr.Dataset, Dict[str, np.ndarray]]</code>): The data set from which to generate the split, provided either as an xarray dataset or a dictionary of variables.</li> <li>to_pred (<code>Union[List[str], str]</code>):  Names of the variables to be used as targets. Can be a single string or a list of strings.</li> <li>exclude_vars (<code>List[str]</code>):  Names of the variables to exclude from the feature set. Defaults to an empty list.</li> <li>feature_vars (<code>List[str]</code>):  Explicit list of variable names to use as features. If <code>None</code>, the function automatically determines which variables to use based on exclusion criteria and target variables.</li> </ul>"},{"location":"ml-toolkit/api-reference/4-splits/create-split/#returns","title":"Returns","text":"<ul> <li><code>Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]</code>: A tuple containing numpy arrays for training features, testing features, training targets, and testing targets.</li> </ul>"},{"location":"ml-toolkit/api-reference/4-splits/create-split/#example","title":"Example","text":"<p><pre><code>import numpy as np\nfrom ml4xcube.splits import create_split\n\n# Example data setup\ndata = {\n    'temperature': np.random.rand(100, 10),\n    'humidity': np.random.rand(100, 10),\n    'split': np.random.choice([0., 1.], size=(100,))\n}\n\n# Specify the variables to predict and the feature set\nto_predict = 'temperature'\nfeatures = ['humidity']\n\n# Create train and test splits\nX_train, X_test, y_train, y_test = create_split(data, to_pred=to_predict, feature_vars=features)\n\nprint('Training features shape:', X_train.shape)\nprint('Training labels shape:', y_train.shape)\n</code></pre> This example illustrates the use of <code>create_split</code> to prepare data arrays for training and testing a model, where  <code>temperature</code> is the target variable and <code>humidity</code> serves as a feature variable.  The <code>split</code> variable within the data cube specifies which entries belong to the training set (1.) and which to the testing set (0.). It can be assigned using the assign_block_split or the assign_rand_split method.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/apply-filter/","title":"Apply filter","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/apply-filter/#apply_filter","title":"apply_filter","text":"<pre><code>def apply_filter(ds: Dict[str, np.ndarray], filter_var: str, drop_sample: bool = False) -&gt; Dict[str, np.ndarray]\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/apply-filter/#description","title":"Description","text":"<p>The <code>apply_filter</code> function applies a filter to a dataset. If <code>drop_sample</code> is True and any value in a sample does not belong to the mask (<code>False</code>), the function drops the entire sample. If <code>drop_sample</code> is <code>False</code>, it sets all values to <code>NaN</code> that do not belong to the mask (<code>False</code>). For lists of points, it keeps the current behavior of dropping single values.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/apply-filter/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>Dict[str, numpy.ndarray]</code>): The dataset to filter. It should be a dictionary where keys are variable names and values are numpy arrays.</li> <li>filter_var (<code>str</code>): The variable name to use as the filter mask, which must be contained in the dataset.</li> <li>drop_sample (<code>bool</code>): A boolean flag to determine whether to drop the entire subarray or set values to <code>NaN</code>.</li> <li></li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/apply-filter/#returns","title":"Returns","text":"<ul> <li><code>Dict[str, numpy.ndarray]</code>: The dataset after filtering NaN values as a dictionary where keys are variable names and values are filtered numpy arrays.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/apply-filter/#example","title":"Example","text":"<p><pre><code>import numpy as np\nfrom ml4xcube.preprocessing import apply_filter\nfrom ml4xcube.cube_utilities import split_chunk\n\n# Example dataset\nchunk = {\n    'temperature': np.random.rand(10, 20, 30),\n    'precipitation': np.random.rand(10, 20, 30),\n    'filter_mask': np.random.choice([True, False], size=(10, 20, 30))\n}\n\n# Split the chunk\nsplit_data = split_chunk(chunk, sample_size=[('time', 1), ('lat', 2), ('lon', 2)], overlap=[('time', 0.5), ('lat', 0.5), ('lon', 0.5)])\n\n# Apply the filter with drop_sample set to True\nfiltered_ds = apply_filter(split_data, filter_var='filter_mask', drop_sample=True)\n\n# Apply the filter with drop_sample set to False\nfiltered_ds_nan = apply_filter(split_data, filter_var='filter_mask', drop_sample=False)\n</code></pre> The <code>apply_filter</code> function applies the filter mask to the dataset. Depending on the value of <code>drop_sample</code>, it either drops entire subarrays or sets invalid values to <code>NaN</code>.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/apply-filter/#notes","title":"Notes","text":"<ul> <li>The <code>filter_var</code> parameter specifies the variable used as the filter mask. Ensure this variable exists in the dataset.</li> <li>If <code>drop_sample</code> is <code>True</code>, the function drops entire subarrays if any value does not belong to the mask.</li> <li>If <code>drop_sample</code> is <code>False</code>, the function sets invalid values to <code>NaN</code>.</li> <li>The function handles lists of points separately by dropping single values if they are <code>NaN</code>.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/assign-mask/","title":"Assign mask","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/assign-mask/#assign_mask","title":"assign_mask","text":"<pre><code>def assign_mask(ds: xr.Dataset, mask: da.Array, mask_name: str = None, stack_dim: str = 'time') -&gt; xr.Dataset\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/assign-mask/#description","title":"Description","text":"<p>The <code>assign_mask</code> function incorporates a mask into an <code>xarray.Dataset</code>, optionally expanding it along a specified  dimension. This is particularly useful when you need to apply the same mask across multiple data points in a dataset, such as across different time steps or other dimensions. The function ensures that the mask is properly aligned with  the dataset's dimensions and chunks, facilitating seamless integration.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/assign-mask/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset]</code>): TThe dataset to which the mask will be assigned.</li> <li>mask (<code>dask.array</code>):  The mask array to be integrated into the dataset. It must be compatible in shape or expandable to the dimensions of the dataset.</li> <li>mask_name (<code>str</code>): The name assigned to the mask variable within the dataset. Defaults to <code>filter_mask</code> if not provided.</li> <li>stack_dim (<code>str</code>): The dimension along which to expand the mask. If not specified, the mask will not be expanded. Defaults to expanding along the 'time' dimension if no value is provided.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/assign-mask/#returns","title":"Returns","text":"<ul> <li><code>xarray.Dataset</code>: The updated dataset containing the new mask variable.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/assign-mask/#example","title":"Example","text":"<pre><code>import xarray as xr\nimport dask.array as da\nfrom ml4xcube.preprocessing import assign_mask\n\n# Example dataset\ndata = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), da.random.random((10, 20, 30), chunks=(5, 10, 10)))\n})\n\n# Example mask\nmask_array = da.ones((10, 20, 30), chunks=(5, 10, 10))\n\n# Assign mask to dataset without expansion\ndataset_with_mask = assign_mask(data, mask_array, mask_name='custom_mask')\n\n# Example mask\nmask_array = da.ones((20, 30), chunks=(10, 10))\n\n# Assign mask to dataset with expansion along 'time'\ndataset_with_expanded_mask = assign_mask(data, mask_array, mask_name='custom_mask_2', stack_dim='time')\n\nprint(dataset_with_mask)\nprint(dataset_with_expanded_mask)\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/assign-mask/#notes","title":"Notes","text":"<ul> <li>If the specified <code>stack_dim</code> is specified but not a dimension within the dataset, a <code>ValueError</code> is raised. </li> <li>This function ensures that the mask is expanded and rechunked appropriately to match the dataset's dimensions and chunk sizes, facilitating efficient computations on large datasets. </li> <li>The mask is added to the dataset as a new data variable using the specified <code>mask_name</code>, or defaults to <code>filter_mask</code> if no name is provided</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/drop-nan-values/","title":"Drop nan values","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/drop-nan-values/#drop_nan_values","title":"drop_nan_values","text":"<pre><code>def drop_nan_values(ds: Dict[str, np.ndarray], vars: List[str], mode: str = 'auto', filter_var: str = 'filter_mask') -&gt; Dict[str, np.ndarray]\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/drop-nan-values/#description","title":"Description","text":"<p>The <code>drop_nan_values</code> function filters out samples containing <code>NaN</code> values from a dataset, using various modes to determine how NaNs affect the data inclusion. This function is applicable to datasets represented as dictionaries of numpy arrays. It can handle both lists of points and multi-dimensional arrays. Additionally, it can utilize a mask variable from the dataset to define valid data points, aligning the filtering process with specific validity conditions dictated by the mask.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/drop-nan-values/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>Dict[str, numpy.ndarray]</code>): The dataset to filter, provided as a dictionary where keys are variable names and values are numpy arrays.</li> <li>vars (<code>List[str]</code>): A list of variable names to check for <code>NaN</code> values.</li> <li>mode (<code>str</code>): If not <code>None</code>, specifies the method for handling areas with missing values:</li> <li>auto: Drops the entire sample if any part contains a <code>NaN</code>.</li> <li>if_all_nan (<code>List[str]</code>): Drops the sample only if it is entirely composed of <code>NaN</code>s.</li> <li>masked (<code>List[str]</code>): Drops valid regions (as defined by <code>filter_var</code>) containing any <code>NaN</code>s. Requires <code>filter_var</code> to be defined.</li> <li>filter_var (<code>str</code>):  An optional argument specifying the name of a mask variable in the dataset. If provided, this mask is used to determine the validity of a sample.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/drop-nan-values/#returns","title":"Returns","text":"<ul> <li><code>Dict[str, numpy.ndarray]</code>:  The dataset dictionary with the NaN-containing entries filtered out according to the specified mode.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/drop-nan-values/#example","title":"Example","text":"<p><pre><code>import numpy as np\nfrom ml4xcube.preprocessing import drop_nan_values\n\n# Creating a dataset with NaN values\ndataset = {\n    'temperature': np.random.rand(10, 20, 30),\n    'precipitation': np.random.rand(10, 20, 30),\n    'filter_mask': np.random.choice([True, False], size=(10, 20, 30))\n}\ndataset['temperature'][0, 0, 0] = np.nan\ndataset['precipitation'][1, 1, 1] = np.nan\n\n# Specifying the variables to check for NaNs\nvariables = ['temperature', 'precipitation']\n\n# Filter the dataset in 'auto' mode\nfiltered_ds_auto = drop_nan_values(dataset, vars=variables, mode='auto')\n\n# Filter the dataset in 'masked' mode using a filter mask\nfiltered_ds_masked = drop_nan_values(dataset, vars=variables, mode='masked', filter_var='filter_mask')\n</code></pre> The <code>drop_nan_values</code> function filters out samples containing <code>NaN</code> values from the dataset. If a <code>filter_var</code> is provided, it also uses this mask to determine the validity of the samples. If values are considered relevant according to the mask (<code>True</code>), samples containing containing <code>NaN</code> values in these specific regions are dropped.  If no relevant values according to the filter mask exist the sample is also dropped.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/drop-nan-values/#notes","title":"Notes","text":"<ul> <li>The function provides flexible handling of NaN values, with the ability to adjust the strictness of filtering through the <code>mode</code> parameter. </li> <li>When the <code>mode</code> is <code>masked</code>, the function checks for NaNs specifically in areas deemed valid by the <code>filter_var</code>. If any valid area contains a NaN, the whole subarray is considered invalid. </li> <li>For datasets with dimensions of 1 (lists of points), NaNs are dropped individually. For higher dimensions (2, 3, or 4), the function can operate by checking across the specified axes, contingent on the selected mode.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/","title":"Fill nan values","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/#fill_masked_data","title":"fill_masked_data","text":"<pre><code>def fill_nan_values(ds: Dict[str, np.ndarray], vars: List[str], method: str = 'mean', const: Union[float, str, bool] = None) -&gt;  Union[Dict[str, np.ndarray], xr.Dataset]\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/#description","title":"Description","text":"<p>The <code>fill_nan_values</code> function fills <code>NaN</code> values in the dataset using a specified method. The methods available are 'mean', 'noise', or 'constant'. Depending on the method, <code>NaN</code> values are replaced with the mean of non-<code>NaN</code> values, random noise within the range of non-<code>NaN</code> values, or a specified constant value. In some cases in certain areas no values are intended (e.g. where mask values <code>False</code>). To incorporate samples containing boundaries (like coastlines in ESDC), the <code>fill_masked_data</code> function can be utilized to prepare the data for masked machine learning. This approach is demonstrated in this jupyter notebook.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>Union[Dict[str, numpy.ndarray], xarray.Dataset]</code>): The dataset to fill. It should be a dictionary or <code>xarray.Dataset</code> where keys are variable names and with the values containing the data to fill.</li> <li>vars (<code>List[str]</code>): The list of variables for which to fill NaN values. These variables should be present in the dataset.</li> <li>method (<code>str</code>): The method to use for filling NaN values. Options are <code>mean</code>, <code>sample_mean</code>, <code>noise</code>, <code>constant</code>, or None.</li> <li>None: <code>NaN</code>s are not filled.</li> <li>mean: <code>NaN</code>s are filled with the mean value of the non-<code>NaN</code> values</li> <li>sample_mean: <code>NaN</code>s are filled with the sample mean value.</li> <li>noise: <code>NaN</code> are filled with random noise within the range of the non-<code>NaN</code> values.</li> <li>constant: <code>NaN</code>s are filled with the specified constant value (<code>const</code>).</li> <li> </li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/#const-unionfloat-str-bool-the-constant-value-to-use-for-filling-nan-values-when-the-method-is-constant-this-parameter-is-required-when-the-method-is-constant","title":"const (<code>Union[float, str, bool]</code>): The constant value to use for filling <code>NaN</code> values when the method is 'constant'. This parameter is required when the method is 'constant'.","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/#returns","title":"Returns","text":"<ul> <li><code>Union[Dict[str, numpy.ndarray], xarray.Dataset]</code>: The dataset with <code>NaN</code> values filled, where keys are variable names and values are NumPy arrays with filled data.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/#example","title":"Example","text":"<pre><code>import numpy as np\nfrom ml4xcube.preprocessing import fill_nan_values\n\n# Example dataset\nds = {\n    'temperature': np.random.rand(10, 20, 30),\n    'precipitation': np.random.rand(10, 20, 30)\n}\n\n# Introduce some NaN values\nds['temperature'][0, 0, 0] = np.nan\nds['precipitation'][1, 1, 1] = np.nan\n\n# Fill NaN values using the mean method\nfilled_ds_mean = fill_nan_values(ds, vars=['temperature', 'precipitation'], method='mean')\n\n# Fill NaN values using the noise method\nfilled_ds_noise = fill_nan_values(ds, vars=['temperature', 'precipitation'], method='noise')\n\n# Fill NaN values using a constant value\nfilled_ds_constant = fill_nan_values(ds, vars=['temperature', 'precipitation'], method='constant', const=0.0)\n</code></pre> <p>In this example, the <code>fill_nan_values</code> function fills the NaN values in the dataset using different methods: 'mean', 'noise', and 'constant'.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/#notes","title":"Notes","text":"<ul> <li>The <code>vars</code> parameter specifies the list of variables for which to fill <code>NaN</code> values. Ensure these variables exist in the dataset.</li> <li>When using the 'constant' method, the <code>const</code> parameter must be provided to specify the constant value for filling <code>NaNs</code>.</li> <li>The function handles both single-dimensional and multi-dimensional arrays for filling <code>NaN</code> values.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-range/","title":"Get range","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/get-range/#get_range","title":"get_range","text":"<pre><code>def get_range(ds: Union[xr.Dataset, Dict[str, np.ndarray]], exclude_vars:List[str] = list()) -&gt; Dict[str, List[float]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-range/#description","title":"Description","text":"<p>The <code>get_range</code> function computes the minimum and maximum values for all variables within an <code>xarray.Dataset</code> or a  dictionary of <code>numpy</code> arrays, excluding specified variables. This utility is useful for preprocessing  tasks like normalization where knowing the range of data values is crucial. Users can specify certain variables to  exclude from the range calculations, such as mask variables that do not represent the actual data range of interest.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-range/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>Union[xarray.Dataset, Dict[str, numpy.ndarray]]</code>): The dataset to analyze, provided either as an xarray dataset or a dictionary where keys are variable names and values are numpy arrays.</li> <li>exclude_vars (<code>List[str]</code>): List of variable names to exclude from the range calculations. Useful for ignoring auxiliary or non-data variables like masks or identifiers.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-range/#returns","title":"Returns","text":"<ul> <li><code>Dict[str, List[float]]</code>: A dictionary where each key is a variable name and the value is a list containing the minimum and maximum values of that variable.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-range/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.preprocessing import get_range\n\n# Example dataset\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'precipitation': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'land_mask': (('lat', 'lon'), np.random.randint(0, 2, size=(20, 30)))\n})\n\n# Exclude the 'land_mask' variable from range calculations\nranges = get_range(ds, exclude_vars=['land_mask'])\n\n# Output the calculated ranges\nfor var, r in ranges.items():\n    print(f\"{var} range: {r}\")\n\n# Example of how these ranges might be used for normalization\nnormalized_datasets = {var: (ds[var] - r[0]) / (r[1] - r[0]) for var, r in ranges.items()}\n\n# Display normalized datasets\nfor var, data in normalized_datasets.items():\n    print(f\"Normalized {var}: {data}\")\n</code></pre> In this example, the <code>get_range</code> function is used to retrieve the minimum and maximum values for the <code>temperature</code> and  <code>precipitation</code> variables in an <code>xarray.Dataset</code>. These ranges are then used by the <code>normalize</code> function to scale the  data to the range <code>[0, 1]</code>.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-statistics/","title":"Get statistics","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/get-statistics/#get_statistics","title":"get_statistics","text":"<pre><code>def get_statistics(ds: Union[xr.Dataset, Dict[str, np.ndarray]], exclude_vars:List[str] = list()) -&gt; Dict[str, List[float]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-statistics/#description","title":"Description","text":"<p>The <code>get_statistics</code> function calculates and returns the mean and standard deviation for all variables in an  <code>xarray.Dataset</code> or a dictionary of numpy arrays, except for specified variables to exclude. This function is  useful for statistical analysis and data standardization, where mean and standard deviation are essential for tasks  such as feature scaling.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-statistics/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>Union[xarray.Dataset, Dict[str, numpy.ndarray]]</code>):  The dataset to analyze, which can be provided either as an xarray dataset or a dictionary where keys are variable names and values are numpy arrays.</li> <li>exclude_vars (<code>List[str]</code>): A list of variable names to exclude from the statistical calculations. This can be useful for ignoring non-analytical variables like identifiers or masks.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-statistics/#returns","title":"Returns","text":"<ul> <li><code>Dict[str, List[float]]</code>: A dictionary where each key is a variable name and the value is a list containing the mean and standard deviation of that variable.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-statistics/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.preprocessing import get_statistics\n\n# Example dataset\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'precipitation': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'land_mask': (('lat', 'lon'), np.random.randint(0, 2, size=(20, 30)))\n})\n\n# Exclude the 'land_mask' variable from statistical calculations\nstats = get_statistics(ds, exclude_vars=['land_mask'])\n\n# Output the calculated statistics\nfor var, values in stats.items():\n    print(f\"{var} - Mean: {values[0]}, Standard Deviation: {values[1]}\")\n</code></pre> A random dataset is created. Subsequently the mean and standard deviation for the standardization of the variable <code>temperature</code>.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/normalize/","title":"Normalize","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/normalize/#normalize","title":"normalize","text":"<pre><code>def normalize(ds: Union[xr.Dataset, Dict[str, np.ndarray]], range_dict: Dict[str, List[float]], filter_var: str = None) -&gt; Union[xr.Dataset, Dict[str, np.ndarray]]:\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/normalize/#description","title":"Description","text":"<p>The <code>normalize</code>  function applies min-max scaling to each variable within an <code>xarray.Dataset</code> or a dictionary of numpy  arrays, contained within the <code>range_dict</code> dictionary. This dictionary specifies range values for this operation.  This scaling adjusts each data point to a [0, 1] range,  based on the minimum (<code>xmin</code>) and maximum (<code>xmax</code>) values for each variable. This method is essential for scaling input data in  various data processing and machine learning tasks, ensuring that each variable contributes equally without bias due  to different scales. A variable specified as <code>filter_var</code> is excluded from normalization, which can be useful for  variables, like mask indicators or filters.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/normalize/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>Union[xarray.Dataset, Dict[str, np.ndarray]]</code>): The dataset to normalize. It can either be an <code>xarray.Dataset</code> or a dictionary where keys are variable names and values are NumPy arrays.</li> <li>range_dict (<code>Dict[str, List[float]]</code>): A dictionary containing the minimum and maximum values (<code>xmin</code>, <code>xmax</code>) for each variable that requires normalization.</li> <li>filter_var (<code>str</code>): The name of a variable to exclude from normalization. This is useful for excluding non-data variables like mask or index fields.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/normalize/#returns","title":"Returns","text":"<ul> <li><code>numpy.ndarray</code>: The normalized array, with values scaled to the range [0, 1].</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/normalize/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom preprocessing import get_range, normalize\n\n# Creating an example dataset\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'humidity': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'land_mask': (('time', 'lat', 'lon'), np.random.randint(0, 2, size=(10, 20, 30)))\n})\n\n# Calculate the range for 'temperature' and 'humidity'\nranges = get_range(ds, exclude_vars=['land_mask'])\nprint(\"Ranges calculated:\", ranges)\n\n# Normalize the dataset, excluding 'land_mask' from normalization\nnormalized_ds = normalize(ds, ranges, filter_var='land_mask')\nprint(\"Normalized Dataset:\")\nprint(normalized_ds)\n</code></pre> In this example, the <code>get_range</code> function is used to retrieve the minimum and maximum values for the 'temperature' and 'precipitation' variables in an xarray dataset. These ranges are then used by the <code>normalize</code> function to scale the data to the range [0, 1].</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/standardize/","title":"Standardize","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/standardize/#standardize","title":"standardize","text":"<pre><code>def standardize(ds: Union[xr.Dataset, Dict[str, np.ndarray]], stats_dict: Dict[str, List[float]], filter_var: str = None) -&gt; Union[xr.Dataset, Dict[str, np.ndarray]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/standardize/#description","title":"Description","text":"<p>The <code>standardize</code> function performs standardization fpr all variables within an <code>xarray.Dataset</code> or a dictionary of  NumPy arrays, contained in the <code>stats_dict</code> dictionary. This dictionary provides mean and standard deviation values.  This standardization process adjusts each data point  so that the resulting distribution of each variable has a mean of 0 and a standard deviation of 1. This is crucial for  many statistical analyses and machine learning models to ensure that features have comparable scales without biasing  the model due to the variance in magnitude. Variables specified by <code>filter_var</code> are excluded from standardization, which  is beneficial for non-data variables like masks or indices.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/standardize/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>Union[xarray.Dataset, Dict[str, numpy.ndarray]]</code>): The dataset to standardize. It can either be an <code>xarray.Dataset</code> or a dictionary where keys are variable names and values are NumPy arrays.</li> <li>stats_dict (<code>Dict[str, List[float]]</code>): A dictionary containing the minimum and maximum values (<code>xmin</code>, <code>xmax</code>) for each variable that requires normalization.</li> <li>filter_var (<code>str</code>): The name of a variable to exclude from normalization. This is useful for excluding non-data variables like mask or index fields.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/standardize/#returns","title":"Returns","text":"<ul> <li><code>Union[xarray.Dataset, Dict[str, numpy.ndarray]]</code>: The standardized dataset. The data structure returned depends on the input; it will return an <code>xarray.Dataset</code> if provided with one, otherwise it will return a dictionary.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/standardize/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom preprocessing import get_statistics, standardize\n\n# Creating an example dataset\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'humidity': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'land_mask': (('time', 'lat', 'lon'), np.random.randint(0, 2, size=(10, 20, 30)))\n})\n\n# Calculate statistics for 'temperature' and 'humidity'\nstats = get_statistics(ds, exclude_vars=['land_mask'])\nprint(\"Statistics calculated:\", stats)\n\n# Standardize the dataset, excluding 'land_mask' from standardization\nstandardized_ds = standardize(ds, stats, filter_var='land_mask')\nprint(\"Standardized Dataset:\")\nfor var in ['temperature', 'humidity']:\n    print(f\"Standardized {var}: mean={np.mean(standardized_ds[var].values)}, std={np.std(standardized_ds[var].values)}\")\n</code></pre> A random dataset is created. Subsequently the mean and standard deviation are used for the standardization of  <code>temperature</code> and <code>humidity</code> variables.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/standardize/#notes","title":"Notes","text":"<ul> <li>Standardization is carried out by subtracting the mean and dividing by the standard deviation. If the standard deviation is zero (indicating no variability within the variable), the variable values are reduced by the mean alone since division by zero is not feasible.</li> <li>This function supports excluding specific variables from the standardization process, which is especially useful for preserving the integrity of certain types of data like binary masks or categorical indices. </li> <li>The function intelligently handles both <code>xarray.Dataset</code> and dictionary formats, making it versatile for different data handling contexts in scientific computing and machine learning.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/","title":"Multiproc sampler","text":""},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/#multiprocsampler","title":"MultiProcSampler","text":"<p>The <code>MultiProcSampler</code> class is a specialized component of the <code>datasets</code> module, designed to efficiently prepare large datasets for machine learning applications.  By leveraging Python's <code>multiprocessing</code> module, this class facilitates parallel processing to handle extensive datasets rapidly and effectively.  This class is particularly beneficial in environments where the handling and transformation of large volumes of data are required before training sophisticated machine learning models. It applies functionality to preprocess, scale, and partition data into training and testing sets, which are then stored in an efficient format for later retrieval.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n   self, ds: xr.Dataset, rand_chunk: bool = False, data_fraq: float = 1.0, nproc: int = 4,\n   apply_mask: bool = True, drop_sample: bool = True, fill_method: str = None, const: float = None,\n   filter_var: str = 'land_mask', chunk_size: Tuple[int, ...] = None, train_cube: str = 'train_cube.zarr',\n   test_cube: str = 'test_cube.zarr', drop_nan: str = 'auto', array_dims: Tuple[str, ...] = ('samples',),\n   data_split: float = 0.8, chunk_batch: int = None, callback: Callable = None,\n   block_size: List[Tuple[str, int]] = None, sample_size: List[Tuple[str, int]] = None,\n   overlap: List[Tuple[str, float]] = None, scale_fn: str = 'standardize'\n)\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The input dataset to extract train and test data from</li> <li>rand_chunk (<code>bool</code>): If True, chunks are selected randomly; otherwise, they are processed sequentially.</li> <li>data_fraq (<code>float</code>):  Fraction of the data to process, allowing for partial dataset processing.</li> <li>nproc (<code>int</code>): Number of processor cores to use for parallel processing.</li> <li>apply_mask (<code>bool</code>): Whether to apply a filtering condition to the data chunks.</li> <li>drop_sample (<code>bool</code>): If true, <code>NaN</code> values are dropped during filter application.</li> <li>fill_method (<code>str</code>): Method used to fill masked or <code>NaN</code> data.<ul> <li>None: <code>NaN</code>s are not filled.</li> <li>mean: <code>NaN</code>s are filled with the mean value of the non-<code>NaN</code> values</li> <li>sample_mean: <code>NaN</code>s are filled with the sample mean value.</li> <li>noise: <code>NaN</code> are filled with random noise within the range of the non-<code>NaN</code> values.</li> <li>constant: <code>NaN</code>s are filled with the specified constant value (<code>const</code>).</li> </ul> </li> <li>const (<code>float</code>): Constant value used when fill_method is 'constant'.</li> <li>filter_var (<code>str</code>): Variable name used for filtering data chunks.</li> <li>chunk_size (<code>Tuple[int, ...]</code>): The size of chunks in the generated training and testing data.</li> <li>train_cube (<code>str</code>): Path where training data are stored as <code>zarr</code> datasets.</li> <li>test_cube (<code>str</code>): Path where test data are stored as <code>zarr</code> datasets.</li> <li>drop_nan (<code>str</code>): If not <code>None</code>, specifies the method for handling areas with missing values:<ul> <li>auto: Drops the entire sample if any part contains a <code>NaN</code>.</li> <li>if_all_nan (<code>List[str]</code>): Drops the sample only if it is entirely composed of <code>NaN</code>s.</li> <li>masked (<code>List[str]</code>): Drops valid regions (as defined by <code>filter_var</code>) containing any <code>NaN</code>s. Requires <code>filter_var</code> to be defined.</li> </ul> </li> <li>array_dims (<code>Tuple[str, ...]</code>): Dimension names of the resulting <code>zarr</code>s with train and test data. </li> <li>data_split (<code>float</code>): Proportion of data allocated to training; remainder goes to testing.</li> <li>chunk_batch (<code>int</code>): Number of chunks to process in each batch during parallel execution.</li> <li>callback (<code>Callable</code>): Optional function applied to each chunk after initial processing.</li> <li>block_size (<code>List[Tuple[str, int]]</code>): Size of the chunks to process, which can define memory usage and performance. If <code>None</code> the chunk size of ds is utilized</li> <li>sample_size (<code>List[Tuple[str, int]]</code>): List of tuples specifying the dimensions and their respective sizes.</li> <li>overlap (<code>List[Tuple[str, float]]</code>): List of tuples specifying the dimensions and their respective overlap proportion.</li> <li>scale_fn (<code>str</code>): Feature scaling function to apply (<code>standardize</code>, <code>normalize</code>, or <code>None</code>).</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/#get_datasets","title":"get_datasets","text":"<p>This method retrieves the processed training and testing datasets, ensuring all data is ready for analysis or machine learning model training.</p> <pre><code>def get_datasets(self) -&gt; Tuple[xr.Dataset, xr.Dataset]\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/#returns","title":"Returns","text":"<ul> <li><code>Tuple[xarray.Dataset, xarray.Dataset]</code>: Tuple containing the training and testing dataset, each in the <code>xarray.Dataset</code> format</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.datasets.multiproc_sampler import MultiProcSampler\n\n# Example dataset with chunking\ndata = np.random.rand(100, 200, 300)\ndataset = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), data),\n    'precipitation': (('time', 'lat', 'lon'), data)\n}).chunk({'time': 10, 'lat': 50, 'lon': 50})\n\n# Create an instance of MultiProcSampler\nsampler = MultiProcSampler(\n    ds=dataset, rand_chunk=True, nproc=4, apply_mask=True, \n    drop_sample=True, fill_method='constant', const=0.0,\n    filter_var='land_mask', chunk_size=(100, 100, 10), \n    data_split=0.75, sample_size=[('time', 1),('lat', 5), ('lon', 5)], \n    overlap=[('time', 0.),('lat', 0.5), ('lon', 0.5)]\n)\n\n# Process the dataset and retrieve the 7-training and testing sets\ntrain_ds, test_ds = sampler.get_datasets()\n</code></pre> This documentation reflects the updated functionality and parameters of the <code>MultiProcSampler</code> class, providing a  comprehensive guide for users to utilize its capabilities in data processing workflows.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/prepare-dataloader/","title":"Prepare dataloader","text":""},{"location":"ml-toolkit/api-reference/6-datasets/prepare-dataloader/#prep_dataloader","title":"prep_dataloader","text":"<pre><code>def prep_dataloader(\n    train_ds: Dataset, test_ds: Dataset = None, batch_size: int = 1, callback: Callable = None, num_workers: int = 0, \n    parallel: bool = False, shuffle = True, drop_last=True\n) -&gt; Union[DataLoader, Tuple[DataLoader, DataLoader]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/prepare-dataloader/#description","title":"Description","text":"<p>This function sets up one or two <code>DataLoader</code>s' for PyTorch models, facilitating efficient and configurable data loading  for both training and optional testing phases. This function integrates best practices for data management in deep  learning applications, supporting parallel data loading, optional shuffling, and batch handling. It can handle single  and distributed computing environments, making it versatile for local training sessions or scalable, distributed  training across multiple GPUs or nodes.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/prepare-dataloader/#parameters","title":"Parameters","text":"<ul> <li>train_ds (<code>torch.utils.data.Dataset</code>): The PyTorch dataset for training data loading.</li> <li>test_ds (<code>torch.utils.data.Dataset</code>): The PyTorch dataset for testing data loading. If provided, the function returns a separate <code>DataLoader</code> for testing. Defaults to <code>None</code>.</li> <li>batch_size (<code>int</code>): Number of samples/chunks per batch to load.</li> <li>callback (<code>Callable</code>): A function to collate data into batches, or to perform custom operations during data loading.</li> <li>num_workers (<code>int</code>): Number of subprocesses used for data loading.</li> <li>shuffle (<code>bool</code>):  If <code>True</code>, the dataset is shuffled at every epoch to reduce model bias. Automatically set to <code>False</code> if parallel is <code>True</code>.</li> <li>parallel (<code>bool</code>): If set to True, enables distributed training mode, which is crucial for training across multiple GPUs or nodes.</li> <li>drop_last (<code>bool</code>): Whether to drop the last incomplete batch if the total number of samples isn't divisible by the batch size. This is often useful during training to ensure consistent batch sizes.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/prepare-dataloader/#returns","title":"Returns","text":"<ul> <li>Union[DataLoader, Tuple[DataLoader, DataLoader]]:  A single <code>DataLoader</code> for the training dataset if <code>test_ds</code> is <code>None</code>, or a tuple containing <code>DataLoader</code>s for both training and testing datasets if <code>test_ds</code> is provided.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/prepare-dataloader/#example","title":"Example","text":"<p><pre><code>from datasets.pytorch import prep_dataloader\n\n# Assuming 'MyDataset' is a custom class derived from torch.utils.data.Dataset\ntrain_dataset = MyDataset()\ntest_dataset = MyDataset()  # Optional test dataset\nbatch_size = 32\nnum_workers = 4\n\n# Prepare DataLoader for training, with an optional test DataLoader\ntrain_loader, test_loader = prep_dataloader(\n    train_ds=train_dataset,\n    test_ds=test_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    parallel=False,  # Set to True for distributed training\n    shuffle=True,\n    drop_last=True\n)\n\n# Use the DataLoader in a training loop\nfor data in train_loader:\n    # Training operations go here\n    pass\n</code></pre> In this example, the <code>prep_dataloader</code> function is used to set up <code>DataLoader</code>s for PyTorch datasets, specifying the  batch size and the number of worker subprocesses. This setup is typical for training machine learning models where  efficient data handling and processing are crucial. </p>"},{"location":"ml-toolkit/api-reference/6-datasets/prepare-dataloader/#notes","title":"Notes","text":"<p>The function checks if parallel processing is enabled: - If <code>parallel</code> is <code>True</code>, a <code>DistributedSampler</code> is used, which is essential for distributed training environments. This changes the sampling behavior of the dataset to ensure that each part of the dataset is handled by a different part of the model distributed across several nodes or GPUs. - The <code>DataLoader</code> is then configured with the specified parameters. Notably, <code>pin_memory</code> is set conditionally based on whether CUDA is available, which can enhance data transfer speeds to GPU.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/pt-large-scale-xr-dataset/","title":"Pt large scale xr dataset","text":""},{"location":"ml-toolkit/api-reference/6-datasets/pt-large-scale-xr-dataset/#ptxrdataset","title":"PTXrDataset","text":"<p><code>PTXrDataset</code> extends PyTorch's <code>Dataset</code> to provide specialized handling of <code>xarray.Dataset</code> for training neural  networks in PyTorch. It supports dynamic processing and iteration over chunks of large datasets that cannot be fully  loaded into memory, making it ideal for environments with significant data volumes. The class allows for flexible data  manipulation including optional random chunk selection, dropping of <code>NaN</code> values, application of filtering masks, and  data filling strategies, contained in the preprocessing module.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/pt-large-scale-xr-dataset/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n   self, ds: xr.Dataset, rand_chunk: bool = True, drop_nan: str = 'auto', drop_sample: bool = False,\n  chunk_indices: List[int] = None, apply_mask: bool = True, fill_method: str = None,\n  const: float = None, filter_var: str = 'filter_mask', num_chunks: int = None, callback = None,\n  block_sizes: List[Tuple[str, int]] = None, sample_size: List[Tuple[str, int]] = None,\n  overlap: List[Tuple[str, float]] = None, process_chunks: bool = False\n):\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/pt-large-scale-xr-dataset/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The dataset from which data chunks are processed.</li> <li>rand_chunk (<code>bool</code>): If True, chunks are selected randomly; otherwise, they are processed sequentially.</li> <li>drop_nan (<code>str</code>): If not <code>None</code>, specifies the method for handling areas with missing values:<ul> <li>auto: Drops the entire sample if any part contains a <code>NaN</code>.</li> <li>if_all_nan (<code>List[str]</code>): Drops the sample only if it is entirely composed of <code>NaN</code>s.</li> <li>masked (<code>List[str]</code>): Drops valid regions (as defined by <code>filter_var</code>) containing any <code>NaN</code>s. Requires <code>filter_var</code> to be defined.</li> </ul> </li> <li>drop_sample (<code>bool</code>): If true, <code>NaN</code> values are dropped during filter application.</li> <li>chunk_indices (<code>List[int]</code>): Specifies indices of chunks to be processed if not randomly selected.</li> <li>apply_mask (<code>bool</code>): Whether to apply a filtering condition defined by the <code>filter_var</code> to the data chunks.</li> <li>fill_method (<code>str</code>): Method used to fill masked or NaN data.<ul> <li>None: <code>NaN</code>s are not filled.</li> <li>mean: <code>NaN</code>s are filled with the mean value of the non-<code>NaN</code> values</li> <li>sample_mean: <code>NaN</code>s are filled with the sample mean value.</li> <li>noise: <code>NaN</code> are filled with random noise within the range of the non-<code>NaN</code> values.</li> <li>constant: <code>NaN</code>s are filled with the specified constant value (<code>const</code>).</li> </ul> </li> <li>const (<code>float</code>): Constant value used when <code>fill_method</code> is <code>constant</code>.</li> <li>filter_var (<code>str</code>): Variable used for filtering data chunks.</li> <li>num_chunk (<code>int</code>): Specifies the number of chunks to process if not processing all.</li> <li>block_size (<code>List[Tuple[str, int]]</code>): Size of the chunks to process, which can define memory usage and performance. If <code>None</code> the chunk size of ds is utilized</li> <li>sample_size (<code>List[Tuple[str, int]]</code>): List of tuples specifying the dimensions and their respective sizes.</li> <li>overlap (<code>List[Tuple[str, float]]</code>): List of tuples specifying the dimensions and their respective overlap proportion.</li> <li>process_chunks (<code>bool</code>): Whether to preprocess each chunk before returning.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/pt-large-scale-xr-dataset/#example","title":"Example","text":"<p><pre><code>import xarray as xr\nfrom ml4xcube.datasets.pytorch import PTXrDataset, prep_dataloader  \n\n# Initializing the dataset\ndataset = PTXrDataset(\n    ds=my_xarray_dataset,\n    rand_chunk=True,\n    use_filter=True,\n    filter_var='land_mask',\n    sample_size=[('time', 2), ('lat', 10), ('lon', 10)],\n    overlap=[('time', 0.5), ('lat', 0.5), ('lon', 0.5)]\n)\n\n# Creating a DataLoader for batch processing\ndata_loader = prep_dataloader(dataset, batch_size=10, shuffle=True, callback=map_fn)\n\n# Using DataLoader in a training loop\nfor batch in data_loader:\n    inputs, targets = batch\n    outputs = model(inputs)  # Assuming 'model' is an instance of a PyTorch model\n    # Continue with training steps\n</code></pre> This setup demonstrates how <code>PTXrDataset</code> is integrated into a PyTorch training loop using <code>DataLoader</code>, facilitating  efficient and scalable processing of geospatial datasets for deep learning applications.  This functionality is critical for leveraging high-performance computing resources effectively, ensuring that large  datasets are handled in a manner that optimizes both memory usage and computational speed. The <code>map_fn</code> is a callback function as defined in prep_dataloader.  It allows to define the features as well as the dependent variable for the training process and include further preprocessing steps.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/pt-large-scale-xr-dataset/#notes","title":"Notes","text":"<ul> <li><code>training.pytorch.Trainer</code> is able to handle empty chunks. Therefore raw data can be handed over to the <code>PTXrDataset</code> despite of gaps in the data.</li> <li>Samples obtained from a chunk serve as a batch of data. If a consistent batch size is required leverage the XrDataset or the MultiProcSampler to prepare data accordingly.</li> <li>This class efficiently handles large datasets by enabling the selective loading and processing of manageable data chunks.</li> <li><code>PTXrDataset</code> allows for high customization in how data is processed, which is vital for training deep learning models that require specific data formats or preprocessing steps.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/","title":"Tf large scale xr dataset","text":""},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#largescalexrdataset","title":"LargeScaleXrDataset","text":"<p><code>TFXrDataset</code> is specifically designed to integrate <code>xarray.Dataset</code>s with TensorFlow machine learning workflows. It efficiently processes and streams data chunks for training or inference, making it particularly well-suited for h andling extensive datasets that exceed memory capacities. This class enables dynamic data preprocessing and chunk-based iteration, facilitating performance optimization and seamless integration into TensorFlow pipelines.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, ds: xr.Dataset, rand_chunk: bool = True, drop_nan: str = 'auto', chunk_indices: list = None,\n            apply_mask: bool = True, drop_sample: bool = False, fill_method: str = None, const: float = None,\n            filter_var: str = 'filter_mask', num_chunks: int = None, callback_fn = None,\n            block_size: List[Tuple[str, int]] = None, sample_size: List[Tuple[str, int]] = None,\n            overlap: List[Tuple[str, float]] = None, process_chunks: bool = False):\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The dataset from which data chunks are processed.</li> <li>rand_chunk (<code>bool</code>): If True, chunks are selected randomly; otherwise, they are processed sequentially.</li> <li>drop_nan (<code>str</code>): If not <code>None</code>, specifies the method for handling areas with missing values:</li> <li>auto: Drops the entire sample if any part contains a <code>NaN</code>.</li> <li>if_all_nan (<code>List[str]</code>): Drops the sample only if it is entirely composed of <code>NaN</code>s.</li> <li>masked (<code>List[str]</code>): Drops valid regions (as defined by <code>filter_var</code>) containing any <code>NaN</code>s. Requires <code>filter_var</code> to be defined.</li> <li>chunk_indices (<code>List[int]</code>): Specifies indices of chunks to be processed if not randomly selected.</li> <li>apply_mask (<code>bool</code>): Whether to apply a filtering condition defined by the <code>filter_var</code> to the data chunks.</li> <li>drop_sample (<code>bool</code>): If True, drops any samples that doesn't contain relevant values according to the <code>filter_var</code> criteria completely.</li> <li>fill_method (<code>str</code>): Method used to fill masked or NaN data.</li> <li>None: <code>NaN</code>s are not filled.</li> <li>mean: <code>NaN</code>s are filled with the mean value of the non-<code>NaN</code> values</li> <li>sample_mean: <code>NaN</code>s are filled with the sample mean value.</li> <li>noise: <code>NaN</code> are filled with random noise within the range of the non-<code>NaN</code> values.</li> <li>constant: <code>NaN</code>s are filled with the specified constant value (<code>const</code>).</li> <li>const (<code>float</code>): Constant value used when fill_method is 'constant'.</li> <li>filter_var (<code>str</code>): Filter mask name used for filtering data chunks.</li> <li>num_chunk (<code>int</code>): Specifies the number of chunks to process if not processing all.</li> <li>callback (<code>int</code>): Optional function applied to each chunk after initial processing.</li> <li>block_size (<code>List[Tuple[str, int]]</code>): Size of the chunks to process, which can define memory usage and performance. If <code>None</code> the chunk size of ds is utilized</li> <li>sample_size (<code>List[Tuple[str, int]]</code>): List of tuples specifying the dimensions and their respective sizes.</li> <li>overlap (<code>List[Tuple[str, float]]</code>): List of tuples specifying the dimensions and their respective overlap proportion.</li> <li>process_chunks (<code>bool</code>): Whether to preprocess each chunk before returning.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#len","title":"len","text":"<p>Returns the number of chunks, providing insights into the volume of data being processed.</p> <pre><code>def __len__(self) -&gt; int\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#returns","title":"Returns","text":"<ul> <li><code>int</code>: number of chunks</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#get_datasets","title":"get_datasets","text":"<p>Creates a <code>TensorFlow</code> dataset from the generator.</p> <pre><code>get_dataset(self) -&gt; tf.data.Dataset\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#parameters_2","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#returns_1","title":"Returns","text":"<ul> <li><code>tf.data.Dataset</code>: <code>TensorFlow</code> dataset object, yielding chunks of data.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#example","title":"Example","text":"<pre><code># Create an instance of LargeScaleXrDataset\ndataset_processor = LargeScaleXrDataset(\n    xr_dataset=my_xarray_dataset,\n    rand_chunk=True,\n    drop_nan=True,\n    use_filter=True,\n    filter_var='land_mask',\n    sample_size=[('time', 24)],\n    overlap=[('time', 1)]\n)\n\n# Use the generator to feed data into a TensorFlow model\nfor data_chunk in dataset_processor.generate():\n    # Assuming 'model' is an instance of a TensorFlow model\n    predictions = model.predict(data_chunk['input_data'])\n    print(\"Processed predictions:\", predictions)\n</code></pre> <p>This example demonstrates how to initialize the <code>LargeScaleXrDataset</code> class and use its generate method to continuously supply data to a TensorFlow model. The class is essential for applications where large datasets are common, and chunks must be loaded to memory and processed successively.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#notes","title":"Notes","text":"<ul> <li>If instances of <code>LargeScaleXrDataset</code> are handed over to the <code>training.tensorflow.Trainer</code> every processed chunk must contain valid data</li> <li>If validity of data samples can not be guaranteed after preprocessing a chunk, prepare an appropriate datasets using the XrDataset or the MultiProcSampler.</li> <li>Samples obtained from a chunk serve as a batch of data. If a consistent batch size is required leverage the [XrDataset]xr-dataset.md) or the MultiProcSampler to prepare data accordingly.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/","title":"Xr dataset","text":""},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#xrdataset","title":"XrDataset","text":"<p>The <code>XrDataset</code> class within the <code>datasets.xr_dataset</code> module is specifically tailored to sample smaller datasets that are manageable in memory.  It is created for scenarios where direct, rapid manipulation of data in memory is feasible and preferable over the more complex, disk-based operations typical of very large datasets.  This capability makes it ideal for machine learning applications where dataset size allows for quick iterations and immediate feedback on preprocessing and modeling efforts. This class aims provides tools to selectively process data chunks based on specified criteria and apply data cleaning operations such as NaN value handling and optional filtering based on specific variables. It efficiently concatenates processed chunks into a single dataset for further analysis or model training.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, ds: xr.Dataset, chunk_indices: List[int] = None, rand_chunk: bool = True, drop_nan: str = 'auto',\n             apply_mask: bool = True, drop_sample: bool = False, fill_method: str = None, const: float = None,\n             filter_var: str = 'filter_mask', patience: int = 500, block_size: List[Tuple[str, int]] = None,\n             sample_size: List[Tuple[str, int]] = None, overlap: List[Tuple[str, float]] = None, callback: Callable = None,\n             num_chunks: int = None, to_pred: Union[str, List[str]] = None, scale_fn: str = 'standardize'):\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The input dataset to extract the training samples from.</li> <li>chunk_indices (<code>List[int]</code>): List of indices specifying which chunks to process.</li> <li>rand_chunk (<code>bool</code>): If True, chunks are selected randomly; otherwise, they are processed sequentially.</li> <li>drop_nan (<code>str</code>): If not <code>None</code>, specifies the method for handling areas with missing values:<ul> <li>auto: Drops the entire sample if any part contains a <code>NaN</code>.</li> <li>if_all_nan (<code>List[str]</code>): Drops the sample only if it is entirely composed of <code>NaN</code>s.</li> <li>masked (<code>List[str]</code>): Drops valid regions (as defined by <code>filter_var</code>) containing any <code>NaN</code>s. Requires <code>filter_var</code> to be defined.</li> </ul> </li> <li>apply_mask (<code>bool</code>): Whether to apply a filtering condition to the data chunks.</li> <li>drop_sample (<code>bool</code>): If True, drops any samples that doesn't contain relevant values according to the <code>filter_var</code> criteria completely.</li> <li>fill_method (<code>str</code>): Method used to fill masked or NaN data.<ul> <li>None: <code>NaN</code>s are not filled.</li> <li>mean: <code>NaN</code>s are filled with the mean value of the non-<code>NaN</code> values</li> <li>sample_mean: <code>NaN</code>s are filled with the sample mean value.</li> <li>noise: <code>NaN</code> are filled with random noise within the range of the non-<code>NaN</code> values.</li> <li>constant: <code>NaN</code>s are filled with the specified constant value (<code>const</code>).</li> </ul> </li> <li>const (<code>float</code>): Constant value used when fill_method is 'constant'.</li> <li>filter_var (<code>str</code>): Filter mask name used for filtering data chunks.</li> <li>patience (<code>int</code>): The number of consecutive iterations without a valid chunk before stopping.</li> <li>block_size (<code>List[Tuple[str, int]]</code>): Size of the chunks to process, which can define memory usage and performance. If <code>None</code> the chunk size of ds is utilized.</li> <li>sample_size (<code>List[Tuple[str, int]]</code>): List of tuples specifying the sizes of the resulting dataset's samples</li> <li>overlap (<code>List[Tuple[str, float]]</code>): List of tuples specifying the overlap proportion of the resulting dataset's samples</li> <li>callback (<code>Callable</code>): Optional function applied to each chunk after initial processing.</li> <li>num_chunk (<code>int</code>):  Specifies the number of chunks to process if not processing all.</li> <li>to_pred (<code>Union[str, List[str]]</code>): Variable or list of variables to construct the dependent variable or sample to predict.</li> <li>scale_fn (<code>str</code>): Feature scaling function to apply (<code>standardize</code>, <code>normalize</code>, or <code>None</code>).</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#get_datasets","title":"get_datasets","text":"<p>Retrieves the fully processed dataset, ready for use in applications or further analysis. This method ensures that all preprocessing steps are applied and the data is returned in a manageable format.</p> <pre><code>def get_datasets(self) -&gt; Union[Dict[str, np.ndarray], Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#returns","title":"Returns","text":"<ul> <li><code>Union[Dict[str, np.ndarray], Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]]</code>: Returns the processed dataset.</li> <li>If <code>to_pred</code> is <code>None</code>: Returns a dictionary where keys are variable names and values are concatenated numpy arrays representing the dataset.</li> <li>If <code>to_pred</code> is provided: Returns a tuple containing:       (X_train, y_train): Training features and targets.       (X_test, y_test): Testing features and targets.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.splits import assign_block_split\nfrom ml4xcube.datasets.xr_dataset import XrDataset\n\n# Example dataset with chunking\ndata = np.random.rand(100, 200, 300)\ndataset = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), data),\n    'precipitation': (('time', 'lat', 'lon'), data)\n}).chunk({'time': 10, 'lat': 50, 'lon': 50})\n\n# Initialize the XrDataset\nxr_dataset = XrDataset(\n    ds          = dataset,\n    rand_chunk  = True,\n    drop_nan    = 'if_all_nan',\n    fill_method = 'mean',\n    sample_size = [('time', 1),('lat', 10), ('lon', 10)], \n    overlap     = [('time', 0.),('lat', 0.8), ('lon', 0.8)],\n    to_pred     = 'precipitation',\n    num_chunks  = 3\n)\n\n# Retrieve the processed dataset\ntrain_data, test_data = xr_dataset.get_datasets()\n</code></pre> This example demonstrates initializing the <code>XrDataset</code> class with a dataset, where chunks are randomly selected.  <code>NaN</code> values are managed by replacing them with the sample mean, and specific sample sizes and overlaps are defined to  create the datasets. precipitation is predicted based on temperature. The processed train and test data is then retrieved, showcasing the class's capability to efficiently manage and  preprocess smaller datasets suitable for machine learning models and other analytical tasks. </p>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#notes","title":"Notes","text":"<ul> <li><code>XrDataset</code> is designed in order to obtain data samples from <code>num_chunks</code> unique chunks.</li> <li>If <code>num_chunks</code> is not provided it will be set automatically determined, using <code>chunk_indices</code> if assigned or the number of total chunks in the dataset</li> <li>until <code>num_chunks</code> chunks are found containing valid data samples the <code>patience</code> parameter is used. It manages the number of attempts to find the next valid chunk before stopping.</li> <li>Sampling with <code>XrDataset</code> provides the following options:</li> <li>Use specific chunks if <code>chunk_indices</code>are set.</li> <li>Limit <code>num_chunks</code> to the total chunks if exceeded.</li> <li>Default <code>num_chunks</code> to total chunks if unspecified.</li> <li>Ensure all chunks in <code>num_chunks</code> contain valid, non-<code>NaN</code> data.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/ddp-init/","title":"Ddp init","text":""},{"location":"ml-toolkit/api-reference/7-training/ddp-init/#ddp_init","title":"ddp_init","text":"<pre><code>def ddp_init() -&gt; None \n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/ddp-init/#description","title":"Description","text":"<p><code>ddp_init</code> initializes the distributed process group for GPU-based distributed training using NCCL (NVIDIA Collective Communications Library).  It configures the environment to ensure each process operates on its designated GPU.</p>"},{"location":"ml-toolkit/api-reference/7-training/ddp-init/#parameters","title":"Parameters","text":"<ul> <li>(<code>None</code>): This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/ddp-init/#returns","title":"Returns","text":"<ul> <li><code>None</code>: The function does not return any value. It initializes the distributed training process</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/ddp-init/#example","title":"Example","text":"<pre><code>ddp_init()  # Initialize distributed environment\nmodel = MyModel()\ntrainer = Trainer(model=model, ...)\ntrainer.train()  # Manage distributed training\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/ddp-init/#notes","title":"Notes","text":"<ul> <li>NCCL Backend: Optimizes GPU communication in multi-GPU settings, enhancing the speed and efficiency of model training.</li> <li>Environment Configuration: Automatically sets the CUDA device to the local rank provided by the environment, aligning the process-to-GPU mapping.</li> <li>Usage Scenario: This function should be called at the beginning of your script to set up the necessary environment for distributed training.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/","title":"Ddp trainer","text":""},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/#trainer","title":"Trainer","text":"<p>The <code>Trainer</code> class in the <code>training.pytorch_distributed</code> module is meticulously crafted to optimize the distributed training of PyTorch models on systems equipped with multiple GPUs.  It supports a range of functionalities such as distributed data parallel processing, early stopping, metrics evaluation, snapshot saving, and optional loss plotting in a distributed environment.</p>"},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self, model: torch.nn.Module, train_data: DataLoader, test_data: DataLoader,\n    optimizer: torch.optim.Optimizer, save_every: int, best_model_path: str,\n    snapshot_path: str = None, early_stopping: bool = True, patience: int = 10,\n    loss: Callable = None, metrics: Dict[str, Callable] = None, epochs: int = 10,\n    validate_parallelism: bool = False, create_loss_plot: bool = False\n)\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/#parameters","title":"Parameters","text":"<ul> <li>model (<code>torch.nn.Module</code>): The model to be trained, which will be wrapped within a DistributedDataParallel (DDP) container.</li> <li>train_data (<code>DataLoader</code>): The DataLoader for the training dataset, appropriately set up to work in a distributed manner.</li> <li>test_data (<code>DataLoader</code>): The DataLoader for the validation dataset, also configured for distributed usage.</li> <li>optimizer (<code>torch.optim.Optimizer</code>): Optimizer used for training the model.</li> <li>save_every (<code>int</code>): Epoch frequency at which to save training snapshots.</li> <li>best_model_path (<code>str</code>): Path where the best model according to validation loss is saved.</li> <li>snapshot_path (<code>str</code>): Path to save periodic training snapshots; helpful for long training sessions.</li> <li>early_stopping (<code>bool</code>): Indicates whether training should stop early if there's no improvement, with a default setting of True</li> <li>patience (<code>int</code>): Number of epochs to wait for improvement in validation loss before early stopping. Defaults to <code>10</code>.</li> <li>loss (<code>Callable</code>): Loss function used during training. Must be specified.</li> <li>metrics (<code>Dict[str, Callable]</code>): Dictionary containing metrics to be evaluated during validation.</li> <li>epochs (<code>int</code>): The number of maximum training epochs.</li> <li>validate_parallelism (<code>bool</code>): If set to True, prints loss information from each GPU, useful for debugging and performance tuning.</li> <li>create_loss_plot (<code>bool</code>): Enables the creation of a plot that displays the training and validation loss progress over epochs.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/#train","title":"train","text":"<p>Manages the distributed training process across all epochs, handles early stopping, and loads the best model state at the end. It encapsulates the training process within a recorded session to handle potential errors and ensure proper cleanup of resources.</p> <pre><code>train(self) -&gt; torch.nn.Module\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/#returns","title":"Returns","text":"<ul> <li><code>torch.nn.Module</code>: The trained model with the best performance on validation data after distributed training.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/#example","title":"Example","text":"<p><pre><code>import torch\nimport xarray as xr\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\nfrom ml4xcube.splits import assign_block_split\nfrom ml4xcube.datasets.xr_dataset import XrDataset\nfrom ml4xcube.datasets.pytorch import prep_dataloader\nfrom ml4xcube.training.pytorch_distributed import ddp_init, Trainer\n\n# Example model\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.linear = nn.Linear(...)\n\n    def forward(self, x):\n        return self.linear(x)\n\n# Setting up distributed 7-training environment\nddp_init()\n\n# Load sample data\nxds = xr.open_zarr('sample_data.zarr')\n\n# Assign a train test split\nxds = assign_block_split(ds=xds, block_size=[(\"time\", 10), (\"lat\", 100), (\"lon\", 100)], split=0.8)\n\n# Extract a subset for training and testing, \ntrain_data, test_data = XrDataset(ds=xds, num_chunks=5, rand_chunk=False, to_pred='variable1').get_datasets()\n\n# Extract X_train and y_train, as well as X_test and y_test from train_data and test_data tuples and potentially reshape\n# ...\n\n# Load data and prepare it for parallel training\ntrain_ds     = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\ntest_ds      = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n\ntrain_loader, test_loader = prep_dataloader(train_ds, test_ds, batch_size=64, parallel=True)\n\n# Model, optimizer, and loss\nmodel = SimpleModel()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\nloss_fn = nn.CrossEntropyLoss()\n\n# Trainer\ntrainer = Trainer(model, train_loader, test_loader, optimizer, model_path=\"best_model.pth\", loss=loss_fn, epochs=10)\n\n# Train the model\ntrained_model = trainer.train()\n</code></pre> This setup demonstrates the workflow of the of handling a distributed training tasks within <code>ml4xcube</code>. This module provides a tool for training with large datasets, requiring high computational resources.</p>"},{"location":"ml-toolkit/api-reference/7-training/pytorch/","title":"Pytorch","text":""},{"location":"ml-toolkit/api-reference/7-training/pytorch/#class-trainer","title":"Class: Trainer","text":"<p>The <code>Trainer</code> class in the <code>training.pytorch</code> module is designed to facilitate efficient and effective training of PyTorch models, particularly on single or no GPU systems. It incorporates various functionalities such as early stopping, model saving, metrics evaluation, and optional loss plotting to streamline the training process.</p>"},{"location":"ml-toolkit/api-reference/7-training/pytorch/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self, model: torch.nn.Module, train_data: DataLoader, test_data: DataLoader,\n    optimizer: torch.optim.Optimizer, best_model_path: str,\n    early_stopping: bool = True, patience: int = 10, loss: Callable = None,\n    metrics: Dict[str, Callable] = None, epochs: int = 10, mlflow_run=None,\n    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    create_loss_plot: bool = False,\n)\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/pytorch/#parameters","title":"Parameters","text":"<ul> <li>model (<code>torch.nn.Module</code>): The PyTorch model to be trained.</li> <li>train_data (<code>DataLoader</code>): DataLoader for the training dataset.</li> <li>test_data (<code>DataLoader</code>): DataLoader for the validation/test dataset.</li> <li>optimizer (<code>torch.optim.Optimizer</code>): Optimizer used for training the model.</li> <li>best_model_path (<code>str</code>): Path where the best model according to validation loss is saved.</li> <li>early_stopping (<code>bool</code>): Indicates whether training should stop early if there's no improvement, with a default setting of True</li> <li>patience (<code>int</code>): Number of epochs to wait for improvement in validation loss before early stopping. Defaults to <code>10</code>.</li> <li>loss (<code>Callable</code>): Loss function used during training. Must be specified.</li> <li>metrics (<code>Dict[str, Callable]</code>): Dictionary containing metrics to be evaluated during validation.</li> <li>epochs (<code>int</code>): Total number of epochs to train. Defaults to <code>10</code>.</li> <li>mlflow_run (optional): Optional MLflow run instance to log training parameters, metrics, and models. Default is <code>None</code>.</li> <li>device (<code>torch.device</code>): Device on which to train the model (<code>cuda</code> or <code>cpu</code>). Automatically set based on availability.</li> <li>create_loss_plot (<code>bool</code>): If <code>True</code>, generates a plot for training and validation losses after training. Defaults to <code>False</code>.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/pytorch/#train","title":"train","text":"<p>Conducts the training process across all epochs, handles early stopping, and loads the best model state at the end.</p> <pre><code>train(self) -&gt; torch.nn.Module\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/pytorch/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/pytorch/#returns","title":"Returns","text":"<ul> <li><code>torch.nn.Module</code>: The trained model with the best performance on validation data.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/pytorch/#example","title":"Example","text":"<p><pre><code># Assuming model, train_loader, and test_loader are predefined:\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ntrainer = Trainer(model, train_loader, test_loader, optimizer, \"path/to/save/best_model.pth\")\ntrained_model = trainer.train()\n</code></pre> This class is integral to the ml4xcube framework, providing a structured and efficient way to train PyTorch models, especially suited for handling large-scale datasets typically used in geospatial analysis.</p>"},{"location":"ml-toolkit/api-reference/7-training/sklearn/","title":"Sklearn","text":""},{"location":"ml-toolkit/api-reference/7-training/sklearn/#class-trainer","title":"Class: Trainer","text":"<p>The Trainer class is designed for training scikit-learn models using versatile data inputs, such as PyTorch DataLoaders or numpy arrays.  This class is particularly useful for handling large datasets that may not fit into memory, as well as for leveraging the speed of batch training.</p>"},{"location":"ml-toolkit/api-reference/7-training/sklearn/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n        self,\n        model: BaseEstimator, train_data: Union[Any, Tuple[np.ndarray, np.ndarray]],\n        test_data: Union[Any, Tuple[np.ndarray, np.ndarray]] = None, metrics: Dict[str, Callable] = None,\n        model_path: str = None, batch_training: bool = False, mlflow_run=None, task_type: str = 'supervised'\n    )\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/sklearn/#parameters","title":"Parameters","text":"<ul> <li>model (<code>sklearn.base.BaseEstimator</code>): A scikit-learn estimator capable of partial_fit for incremental learning or fit for standard full-batch training.</li> <li>train_data (<code>Union[DataLoader, Tuple[numpy.ndarray, numpy.ndarray]]</code>):  Training data can be provided as a PyTorch DataLoader for batch training or a tuple of numpy arrays (X_train, y_train).</li> <li>test_data (<code>Union[DataLoader, Tuple[numpy.ndarray, numpy.ndarray]]</code>): Similar to train_data, validation/testing data can also be provided either as a DataLoader or a tuple of numpy arrays (X_test, y_test).</li> <li>metrics (<code>Dict[str, Callable]</code>): Dictionary containing metric functions that compute a performance score between predictions and true labels.</li> <li>model_path (<code>str</code>): File path to save the trained model.</li> <li>batch_training (<code>bool</code>): : Specifies whether to train the model using batches (if True) or on the complete dataset at once (if False).</li> <li>mlflow_run (optional): Optional MLflow run instance to log training parameters, metrics, and models. Default is <code>None</code>.</li> <li>task_type (<code>str</code>):  Specifies whether the training is 'supervised' or 'unsupervised'. Default is 'supervised'.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/sklearn/#train","title":"train","text":"<p>Conducts the training process, using batched training or on the complete dataset at once and returns the model. <pre><code>train(self) -&gt; BaseEstimator:\n</code></pre></p>"},{"location":"ml-toolkit/api-reference/7-training/sklearn/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/sklearn/#returns","title":"Returns","text":"<ul> <li><code>sklearn.base.BaseEstimator</code>: The trained model.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/sklearn/#example","title":"Example","text":"<p><pre><code>import numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import SGDClassifier\nfrom ml4xcube.training.sklearn import Trainer\n\n# Define a simple model and metrics\nmodel = SGDClassifier()\nmetrics = {'Accuracy': accuracy_score}\n\n# Dummy data\nX_train = np.random.rand(100, 10)\ny_train = np.random.randint(0, 2, 100)\nX_test = np.random.rand(20, 10)\ny_test = np.random.randint(0, 2, 20)\n\n# Initialize trainer\ntrainer = Trainer(\n    model=model,\n    train_data=(X_train, y_train),\n    test_data=(X_test, y_test),\n    metrics=[(\"Accuracy\", accuracy_score)],\n    model_path=\"best_model.pkl\",\n    batch_training=False\n)\n\n# Train the model\ntrained_model = trainer.train()\n\n# Evaluate the model\npredictions = trained_model.predict(X_test)\nprint(\"Test Accuracy:\", accuracy_score(y_test, predictions))\n</code></pre> This setup offers an approach to train scikit-learn models, accommodating both large-scale and in-memory datasets effectively. The Trainer class not only facilitates extensive training configurations but also integrates model evaluation and saving mechanisms, making it a robust tool for machine learning training in diverse data-intensive environments.</p>"},{"location":"ml-toolkit/api-reference/7-training/tensorflow/","title":"Tensorflow","text":""},{"location":"ml-toolkit/api-reference/7-training/tensorflow/#class-trainer","title":"Class: Trainer","text":"<p>The Trainer class in the training.tensorflow module is tailored to facilitate the efficient training of TensorFlow models, especially on systems equipped with a single GPU or none. It provides comprehensive support for training session management, including early stopping, model checkpointing, and integration with TensorBoard for monitoring.</p>"},{"location":"ml-toolkit/api-reference/7-training/tensorflow/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self, model: tf.keras.Model, train_data: tf.data.Dataset, test_data: tf.data.Dataset, \n    best_model_path: str, early_stopping: bool = True, patience: int = 10, \n    tf_log_dir: str = './logs', mlflow_run=None, epochs: int = 100, \n    train_epoch_steps: int = None, val_epoch_steps: int = None, create_loss_plot: bool = False,\n)\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/tensorflow/#parameters","title":"Parameters","text":"<ul> <li>model (<code>tensorflow.keras.Model</code>): The TensorFlow model to be trained.</li> <li>train_data (<code>tensorflow.data.Dataset</code>): TensorFlow Dataset containing the training data.</li> <li>test_data (<code>tensorflow.data.Dataset</code>): TensorFlow Dataset containing the validation data.</li> <li>best_model_path (<code>str</code>): Path where the best model according to validation loss is saved.</li> <li>early_stopping (<code>bool</code>): Indicates whether training should stop early if there's no improvement, with a default setting of True</li> <li>patience (<code>int</code>): Number of epochs to wait for improvement in validation loss before early stopping. Defaults to <code>10</code>.</li> <li>tf_log_dir (<code>str</code>): Directory path for saving TensorBoard logs, defaulted to './logs'.</li> <li>mlflow_run (optional): Optional MLflow run instance to log training parameters, metrics, and models. Default is <code>None</code>.</li> <li>epochs (<code>int</code>): Total number of epochs to train. Defaults to <code>10</code>.</li> <li>train_epoch_steps (<code>int</code>): Number of steps to run each training epoch, calculated dynamically if not set.</li> <li>val_epoch_steps (<code>int</code>): Number of steps to run each validation epoch, calculated dynamically if not set.</li> <li>create_loss_plot (<code>bool</code>): If <code>True</code>, generates a plot for training and validation losses after training. Defaults to <code>False</code>.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/tensorflow/#train","title":"train","text":"<p>Conducts the training process across all epochs, handles early stopping, and loads the best model state at the end.</p> <pre><code>train(self) -&gt; tf.keras.Model\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/tensorflow/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/tensorflow/#returns","title":"Returns","text":"<ul> <li><code>torch.nn.Module</code>:  The trained model, equipped with the best weights found during the training if early stopping was triggered.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/tensorflow/#example","title":"Example","text":"<p><pre><code>import tensorflow as tf\nfrom ml4xcube.training.tensorflow import Trainer\n\n# Define the model\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(10, activation='relu', input_shape=(feature_size,)),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Prepare data\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n\n# Create a trainer instance\ntrainer = Trainer(\n    model=model,\n    train_data=train_dataset,\n    test_data=test_dataset,\n    best_model_path='path/to/save/best_model.h5',\n    tf_log_dir='path/to/save/logs',\n    epochs=50,\n    create_loss_plot=True\n)\n\n# Train the model\ntrained_model = trainer.train()\n</code></pre> This class offers a robust solution for training complex TensorFlow models with high efficiency, providing tools necessary for handling large-scale data and optimizing computational resources.</p>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-normalizing/","title":"Undo normalizing","text":""},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-normalizing/#undo_normalizing","title":"undo_normalizing","text":"<pre><code>def undo_normalizing(x: np.ndarray, xmin: float, xmax: float) -&gt; np.ndarray\n</code></pre>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-normalizing/#description","title":"Description","text":"<p>This function performs the inverse operation of normalization, transforming normalized data back to its original scale.</p>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-normalizing/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>numpy.ndarray</code>): The normalized array.</li> <li>xmin (<code>str</code>): The minimum value used for the original normalization.</li> <li>xmax (<code>bool</code>): The maximum value used for the original normalization.</li> <li></li> </ul>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-normalizing/#returns","title":"Returns","text":"<ul> <li><code>numpy.ndarray</code>:  The denormalized array.</li> </ul>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-normalizing/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.postprocessing import undo_normalization\nfrom ml4xcube.preprocessing import get_range, normalize\n\n# Example dataset\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'precipitation': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30))\n})\n\n# Get the range of the 'temperature' variable\ntemperature_range = get_range(ds, 'temperature')\nprint(f\"Temperature range: {temperature_range}\")\n\n# Normalize the 'temperature' variable\nnormalized_temperature = normalize(ds['temperature'].values, *temperature_range)\nprint(f\"Normalized temperature: {normalized_temperature}\")\n\n# Revert the normalization\noriginal_temperature = undo_normalization(normalized_temperature, *temperature_range)\n</code></pre> This example demonstrates how to revert normalized data back to its original scale using the <code>undo_normalizing</code> function.</p>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-standardizing/","title":"Undo standardizing","text":""},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-standardizing/#undo_normalizing","title":"undo_normalizing","text":"<pre><code>def undo_standardizing(x: np.ndarray, xmean: float, xstd: float) -&gt; np.ndarray\n</code></pre>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-standardizing/#description","title":"Description","text":"<p>This function performs the inverse operation of standardization, transforming standardized data back to its original scale.</p>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-standardizing/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>numpy.ndarray</code>): The standardized array.</li> <li>xmean (<code>str</code>): The mean value used for the original standardization.</li> <li>xstd (<code>bool</code>): The standard deviation value used for the original standardization.</li> <li></li> </ul>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-standardizing/#returns","title":"Returns","text":"<ul> <li><code>numpy.ndarray</code>:  The destandardized array.</li> </ul>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-standardizing/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.postprocessing import undo_standardization\nfrom ml4xcube.preprocessing import get_statistics, standardize\n\n# Example dataset\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n})\n\n# Calculate statistics\nstatistics = get_statistics(ds, 'temperature')\n\nprint(f\"Mean: {statistics[0]}, Standard Deviation: {statistics[1]}\")\n\n# Standardize the 'temperature' variable\nstandardized_temperature = standardize(ds['temperature'].values, *statistics)\nprint(f\"Normalized temperature: {standardized_temperature}\")\n\n# Revert the standardization\noriginal_temperature = undo_standardization(standardized_temperature, *statistics)\n</code></pre> This example demonstrates how to revert standardized data back to its original scale using the <code>undo_standardizing</code> function.</p>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/","title":"Evaluation","text":""},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#evaluator","title":"Evaluator","text":"<p>The <code>Evaluator</code> class in the <code>ml4xcube.evaluation.metrics</code> module is designed to handle metric evaluation  for machine learning frameworks (PyTorch, TensorFlow, and Scikit-learn), allowing users to evaluate various  metrics during model validation or testing.</p>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, framework: str):\n</code></pre>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#parameters","title":"Parameters","text":"<ul> <li>framework (<code>str</code>): The deep learning framework being used. Supported values are:</li> <li><code>'pytorch'</code></li> <li><code>'tensorflow'</code></li> <li><code>'sklearn'</code></li> </ul>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#get_metrics","title":"get_metrics","text":"<pre><code>def get_metrics(self, metric_names: List[str], average: str = 'macro', delta: float = 1.0) -&gt; Dict[str, Callable]:\n</code></pre> <p>This method returns a dictionary of metric functions based on the selected framework, metric names, and  optional parameters. <code>average</code> is </p>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#parameters_1","title":"Parameters","text":"<ul> <li>metric_names (<code>List[str]</code>): A list of metric names to retrieve. These names should correspond to the keys in the <code>metric_functions</code> attribute.</li> <li>average (<code>str</code>): The averaging method for precision, recall, and F1 score. Default is <code>'macro'</code>. Other possible values include <code>'micro'</code> and <code>'weighted'</code>.</li> <li>delta (<code>float</code>): The delta parameter for Huber loss. Default is <code>1.0</code>.</li> </ul>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#returns","title":"Returns","text":"<ul> <li>metrics (<code>Dict[str, Callable]</code>): A dictionary where the keys are metric names and the values are the corresponding metric functions.</li> </ul>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#supported-metrics","title":"Supported Metrics","text":"<ul> <li>'mae': Mean Absolute Error</li> <li>'mse': Mean Squared Error</li> <li>'rmse': Root Mean Squared Error</li> <li>'r2': R-squared</li> <li>'huber_loss': Huber Loss</li> <li>'mape': Mean Absolute Percentage Error</li> <li>'med_ae': Median Absolute Error</li> <li>'explained_variance': Explained Variance</li> <li>'accuracy': Accuracy</li> <li>'roc_auc': ROC AUC score</li> <li>'cross_entropy': Cross-Entropy Loss</li> <li>'precision': Precision score</li> <li>'recall': Recall score</li> <li>'f1_score': F1 Score</li> </ul>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#example-usage","title":"Example Usage","text":"<pre><code>from ml4xcube.training.sklearn import Trainer\nfrom ml4xcube.evaluation.evaluator import Evaluator\n\nevaluator = Evaluator(framework='sklearn')\nmetrics = evaluator.get_metrics(metric_names=['recall', 'accuracy', 'precision'])\n\n# Access a specific metric function and use it\nmae_fn = metrics['recall']\n\n# Use the dictionary for model validation during training\ntrainer = Trainer(\n  model   = model,\n  ...\n  metrics = metrics\n)\n\ntrained_model = trainer.train()\n</code></pre>"},{"location":"science_cases/drought_effects/","title":"Drought Effects","text":""},{"location":"science_cases/drought_effects/#linear-estimation-of-drought-effects-on-productivity","title":"Linear estimation of drought effects on productivity","text":"<p>Authors: Chaonan Ji</p> <p>The rationale for the demonstration case on climate extremes is the following: especially excess drought and heatwave events (DHEs) are increasingly co-occurring, severely reducing the productivity of both semi-natural vegetation and crops and, therefore, carbon sequestration. Although the impacts of individual extreme events are partly very well investigated, we still lack a general understanding of the impacts of compound event years on ecosystems. Specifically, it remains unclear, firstly, whether DHEs systematically reduce vegetation\u2019s productivity potential of the entire growing season, if temporal compensations undo these effects, and secondly, how such responses differ among vegetation types. In a first exploratory phase (Ji et al. in prep \u2013 figures below), we analysed European climate conditions from 2001 to 2022 and used a rank-based method to identify the hottest and driest compound event per year for the entire continent. We used vegetation greenness data per vegetation classes to assess the integral growing season greenness anomalies across events and climate zones. Our results clearly show the large-scale signatures of DHEs on the annual growing conditions for vegetation, which lead to clear impacts on vegetation growing conditions that cannot be compensated. However, the effect is much more pronounced for grasslands than forests, which seem to have much higher seasonal resilience to DHE events. In the case of subsequent DHE years, the effects on forests are, however, much more pronounced indicating the risks of clustered extremes as we expect them in the near future. Given that this study investigated growing-season integrals, it corroborates earlier findings that individual extreme events have the potential to affect the inter-annual variability of the terrestrial carbon cycle. Future land-management strategies should consider such effects in landscape planning for buffering the impacts of climate extremes, reducing the volatility of the carbon sequestration potential of ecosystems, and regulating regional climate feedback.</p>          Ranking results of CHD years for Europe identifies regions experiencing         sweltering summers, defined by the top three temperature years in the ranking,         shown in reddish tones and highlighted with green edges. Similar analyses were         done for all relevant climate variales.               kNDVI value reductions by temperature but in different vegetation types. Clear         reductions are seen in grasslands for very high temperature regimes, showing the         susceptibility of these ecosystems to DHS.               Vegetation responses in climate space.       <p>Figures 4-6 aim to investigate the responses of different vegetation types (e.g., grasslands, conifers) in hot and dry years worldwide. Specifically, we would like to address (1) the rapid detection of extreme years, (2) the spatial trend of vegetation responses, and (3) the different responses of different vegetation types. We used ERA5 T, P, and soil moisture data, spectral indices, and land cover maps to investigate these points. Based on these preliminary results, we embarked with developing deep learning frameworks that can effectively predict the responses especially in ecosystems that do not show obvious results i.e., forest ecosystems. One first methodological idea towards deep learning was using Echo State Networks, an advanced variant of recurrent neural networks. The results by Martinuzzi et al. (accepted) has not been as conclusive in the sense of gaining significant improvements over existing RNNs as expected. In Echo State Networks (ESNs) only the last layer is trained through linear regression. The absence of derivatives guarantees no vanishing or exploding gradients, offering an alternative solution to gating. To ensure a comprehensive comparison, we also investigate the performance of other RNN architectures. The comparison of these models has a strong focus on the extreme responses of vegetation indices to climate drivers. The primary focus of this model comparison lies in understanding vegetation\u2019s extreme responses to climate drivers. We conducted a comprehensive analysis of recurrent neural networks in the context of modeling biosphere dynamics in response to climate factors. By using daily data, we assessed the effectiveness of these network architectures in capturing extreme events within vegetation dynamics. To discern variations in performance across different scenarios, we employed various metrics. Echo State Networks (ESNs) slightly outperformed other RNNs, but the improvements are relatively minor, despite multiple theoretical arguments in favour of ESN. This is why we still started to explore other methodological avenues before going to a continent-wide deep analysis of extremes in DeepESDL.</p>"},{"location":"science_cases/land_biosphere/","title":"Land-Biosphere-Society","text":""},{"location":"science_cases/land_biosphere/#science-demonstration-case-land-biosphere-society","title":"Science Demonstration Case \u201cLand-Biosphere-Society\u201d","text":"<p>Authors: Wanton Li, Gregory Duveiller, Fabian Gans, Jeroen Smits, Guido Kraemer, Dorothea Frank, Miguel Mahecha, Ulrich Weber, Mirco Migliavacca, Andrey Ceglar, Markus Reichstein: Diagnosing syndroms of biosphere-atmosphere-socioeconomic change.</p> <p>While previous work (Kraemer et al. 2020) to create an index of the Earth System focussed on describing the different spheres (Atmosphere - Biosphere - Society) separately this use case focuses on describing the interaction between these individual spheres. To obtain a first understanding on the main signal that can be extracted from the ESDC we applied canonical correlation analysis (CCA) on annually aggregated time series per country. Applying a linear method first\u202f can be seen as creating a baseline to understand the relationships between variables before applying nonlinear deep-learning methods. The main difference of using CCA in comparison to PCA is that instead of maximizing the explanation of variance in a dataset itself, the CCA tries to explain as much variance as possible for an independent dataset. This method can be applied as a 3-way CCA to sub-datasets of the ESDC from the biosphere and atmosphere as well as to a compiled dataset based on World-Bank socioeconomic indicators. In order to remove confounding spatial patterns, we spatially detrended the input data to concentrate the analysis on the temporal evolution of the country-based data.</p> <p>Finally, the result of our analysis is a time-dependent interaction index for each country and every pair of variables that encodes the possible interaction between these spheres. This can be interesting from two viewpoints. First one can examine certain known events for single countries and test if there is a signal in multiple of the interaction data streams. This can be an indication that an event had an effect on multiple spheres and hypotheses can be generated about the possible interactions and causal effects.</p> <p></p> <p>We use canonical correlation analysis (CCA) to construct interactive socio-biosphere-atmosphere indices and monitor their temporal changes across different countries. The left plot shows a 3d scatter plot of the first component of every sphere for all countries and years. Outliers points are marked in red color. For two of the outliers (Niger and Vanuatu) the time evolution of these indices is shown and the outliers can be related to known events (2017 Niger soil drought and 2015 Cyclone in Vanuatu).</p> <p>Another approach to investigate the data is to summarize the long-term trajectories different countries take on decadal time scales. For example, it is possible to define clusters of countries with similar co-evolution of different indices based on\u202f trend and standard deviation of their index time series.</p> <p></p> <p>The upper figure (a) shows that global countries are distinguished into seven common groups based on clustering on the CCA constructed components. The button figure (b) shows the mean trajectories of CCA constructed socio-biosphere-atmosphere indices across the groups.  </p> <p>We conclude that our results demonstrate the possibilities to explore the interactions of different Earth System components by using dimensionality reduction techniques that aim to summarize the interaction between different data domains. Since these interactions can be very complex and nonlinear there will be future possibilities to explore nonlinear Deep-Learning based extensions of our methods to improve the robustness of our results and provide more capabilities of diagnosing data-driven trends and generating hypotheses on interactions in the Earth System. Work is also foreseen in collaboration with the EU funded Open-Earth-Monitor (OEMC) project, in which the concept is being further developed with a specific use case involving the European Central Bank to diagnose interactions between the financial sector and the natural system.</p>"},{"location":"science_cases/ocean/","title":"Ocean Carbon Cycle","text":""},{"location":"science_cases/ocean/#science-demonstration-case-ocean","title":"Science Demonstration Case \u201cOcean\u201d","text":"<p>Authors: Julia Klima, Jannes Kruse, Jonas Neumann</p> <p>A novel approach was explored to understanding the ocean carbon cycle through the utilization of a recently developed, uniformly structured data cube encompassing key ocean carbon cycle variables. The methodology involves a combination of dimensionality reduction and feature selection techniques, enabling to unravel the intricacies of the carbon cycle across various geographic locations. The study spans a period from 1998 to 2020, during which the complexity of the ocean's carbon cycle was analysed. We find that the originally eight-dimensional feature space can be effectively condensed into three to four dimensions. This reduction not only simplifies the representation but also enhances our understanding of the underlying processes. A significant aspect of our analysis is the identification of geographical patterns in the carbon cycle's complexity. These patterns are closely linked to the dynamics of thermohaline circulation and the movement of water masses. Specifically, areas of intense upwelling and the warm, surface-near currents of the Global Conveyor Belt are highlighted, along with their anomalies and regional extreme events.</p> <p>This research pinpoints several key variables, such as Particulate Inorganic Carbon and Mixed Layer Depth, as critical in both a global and regional context for representing the simplified dimensions of the carbon cycle. These findings are pivotal in deepening our understanding of the carbon cycle's regional behaviours. We will now iterate the findings with the data providers in order to understand how to enhance the predictability and effectiveness of future research in carbon modelling and oceanic pathways.</p>          Study design using a data cube of oceanic carbon indicators.               Steps performed to obtain the dimensionally reduced and clustered new         data cubes               Results of the Principal Component Analysis per pixel.       <p>The study presented here is still in preliminatry state. However, we find that using Principal Component Analysis (PCA) and Principal Feature Analysis (PFA) is very helpful to analyze ocean carbon cycle variables from a data cube. The key findings are summarized as follows:</p> <ul> <li>Dimensionality Reduction: The PCA results indicated that most of the ocean's   carbon cycle can be described by three to four dimensions, significantly   reducing the original eight-dimensional feature space. This suggests that many   ocean carbon pump variables are interrelated and exhibit coordinated   variations over time.</li> <li>Geographical Patterns: The study revealed distinct geographical patterns in   the number of dimensions per pixel. These patterns are associated with ocean   currents, upwelling regions, wind zones, and differences between coastal and   open-sea areas. In open-sea regions, the data cube variables often require   four dimensions for description, while near coastlines, three dimensions are   generally sufficient.</li> <li>Regional Variations: The analysis identified specific regions with unique   dimensional patterns. For example, large areas above 30\u00b0 N and 30\u00b0 S in the   westerly wind zone predominantly show three dimensions. Notable regions like   the equatorial-subtropical zone, the Pacific-Indian Ocean transition, and   areas around the Global Conveyor Belt also exhibit distinct patterns.</li> <li>El Ni\u00f1o-Southern Oscillation Area: This area is mostly represented by four   dimensions, with patterns influenced by trade winds, upwelling regions, and   warm ocean currents. The equatorial-subtropical zone is the only area where   five dimensions are occasionally present.</li> <li>Principal Component Variability: The first principal component (PC1) explains   a significant portion of the variability in many regions, especially around   the equator. However, in areas like the East Pacific Rise, the variability is   spread across multiple dimensions.</li> <li>Principal Feature Analysis (PFA) Results: PFA identified key variables that   describe the reduced dimensionality of the ocean carbon cycle. Particulate   Inorganic Carbon (PIC) is a major variable, present in almost all pixels,   indicating its importance in describing the variability of the ocean carbon   pump.</li> <li>Phytoplankton Variability: Different types of phytoplankton (micro, pico, and   nano) show varying presence across different regions, indicating their role in   the ocean carbon cycle's variability.</li> <li>Mixed Layer Depth (MLD) and Other Variables: MLD is generally important for   describing variability, but its presence varies geographically. Other   variables like Primary Production (PP) and Particulate Organic Carbon (POC)   also show varying importance across different locations.</li> <li>Cluster Analysis: The clustered PFA results reveal patterns in variable   importance across different ocean regions. These clusters help in   understanding the regional differences in the ocean carbon cycle</li> <li>Implications and Limitations: The study provides insights into the   interconnected nature of oceanic processes affecting the carbon cycle.   However, it also acknowledges limitations such as the exclusion of certain   variables, the lack of a comprehensive theoretical framework, and the absence   of data in certain latitudes.</li> </ul> <p>Overall, the study offers a nuanced understanding of the ocean carbon cycle's complexity and its geographical variability, highlighting the interconnectedness of various oceanic processes and their impact on carbon cycling.</p>"}]}