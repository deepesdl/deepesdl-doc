{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#about-deepesdl","title":"About DeepESDL","text":"<p>Welcome to the online documentation of DeepESDL \u2013 ESA\u2019s Deep Earth System Data Laboratory, a platform providing analysis-ready data cube in a powerful, virtual laboratory to the Earth Science research community. DeepESDL offers a full suite of services to facilitate data exploitation, share data and source code, and publish results. Special emphasize is put on improving the support for machine learning and artificial intelligence approaches, which includes the preparation of AI-ready datasets, providing a programming environment with relevant libraries and packages, and the resources to execute processing pipelines. For more information and access to the lab please visit the DeepESDL website.</p> <p>The DeepESDL documentation contains:</p> <ul> <li>The User Guide for all DeepESDL services.   This is the starting point for new users.</li> <li>An overview of public, pre-generated data cubes   available in DeepESDL with detailed metadata and specifications.</li> <li>A description of DeepESDL\u2019s architecture.</li> </ul>"},{"location":"about/","title":"Overview","text":""},{"location":"about/#about-deepesdl","title":"About DeepESDL","text":"<p>Welcome to the online documentation of DeepESDL \u2013 ESA\u2019s Deep Earth System Data Laboratory, a platform providing analysis-ready data cube in a powerful, virtual laboratory to the Earth Science research community. DeepESDL offers a full suite of services to facilitate data exploitation, share data and source code, and publish results. Special emphasize is put on improving the support for machine learning and artificial intelligence approaches, which includes the preparation of AI-ready datasets, providing a programming environment with relevant libraries and packages, and the resources to execute processing pipelines. For more information and access to the lab please visit the DeepESDL website.</p> <p>The DeepESDL documentation contains:</p> <ul> <li>The User Guide for all DeepESDL services.   This is the starting point for new users.</li> <li>An overview of public, pre-generated data cubes   available in DeepESDL with detailed metadata and specifications.</li> <li>A description of DeepESDL\u2019s architecture.</li> </ul>"},{"location":"become_user/","title":"Become a User","text":""},{"location":"become_user/#via-esa-sponsorship","title":"via ESA Sponsorship","text":"<p>To become a user of DeepESD apply for an ESA sponsorship via the Network of Ressources (NoR).</p> <p>Take the following steps for the application process:</p> <ol> <li>Search for DeepESDL in the NoR portfolio.</li> <li>Read the provided information about available VMs and other services.</li> <li>Use the Prizing Wizard to configure your application.</li> <li>Apply.</li> </ol> <p>Detailed information about the application process can be found in the NoR Portal. For more support write an email to <code>esdl-support@brockmann-consult.de</code>.</p>"},{"location":"become_user/#without-esa-sponsorship","title":"without ESA Sponsorship","text":"<p>To use DeepESDL without an ESA Sponsorship write an email to <code>esdl-support@brockmann-consult.de</code> with your requirements and the colleagues from Brockmann Consult GmbH will come back to you with an offer.</p>"},{"location":"datasets/ESDC/","title":"Earth System Data Cube","text":""},{"location":"datasets/ESDC/#earth-system-data-cube-esdc-v301","title":"Earth System Data Cube (ESDC) v3.0.1","text":""},{"location":"datasets/ESDC/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('esdc-8d-0.25deg-1x720x1440-3.0.1.zarr')\n</code></pre>"},{"location":"datasets/ESDC/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/ESDC/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180.0 to 180.0 Bounding box latitude (\u00b0) -90.0 to 90.0 Time range 1979-01-05 to 2021-12-31 Publisher DeepESDL Team <p>Click here for full dataset metadata.</p>"},{"location":"datasets/ESDC/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units aerosol_optical_thickness_550 Aerosol Optical Thickness at 550 nm 1 air_temperature_2m Mean Air Temperature at 2 m \u00b0C bare_soil_evaporation Bare Soil Evaporation mm d^-1 burnt_area Monthly Burnt Area hectares cot Cloud Optical Thickness 1 cth Cloud Top Height km ctt Cloud Top Temperature K evaporation Actual Evaporation mm d^-1 evaporation_era5 Evaporation mm d^-1 evaporative_stress Evaporative Stress 1 gross_primary_productivity Gross Primary Productivity g C m^-2 d^-1 interception_loss Interception Loss mm d^-1 kndvi Kernel Normalized Difference Vegetation Index 1 latent_energy Latent Energy MJ m^-2 d^-1 max_air_temperature_2m Maximum Air Temperature at 2 m \u00b0C min_air_temperature_2m Minimum Air Temperature at 2 m \u00b0C nbar_blue Nadir BRDF Adjusted Reflectance of Band 3 (blue) 1 nbar_green Nadir BRDF Adjusted Reflectance of Band 4 (green) 1 nbar_nir Nadir BRDF Adjusted Reflectance of Band 2 (NIR) 1 nbar_red Nadir BRDF Adjusted Reflectance of Band 1 (red) 1 nbar_swir1 Nadir BRDF Adjusted Reflectance of Band 5 (SWIR1) 1 nbar_swir2 Nadir BRDF Adjusted Reflectance of Band 6 (SWIR2) 1 nbar_swir3 Nadir BRDF Adjusted Reflectance of Band 7 (SWIR3) 1 ndvi Normalized Difference Vegetation Index 1 net_ecosystem_exchange Net Ecosystem Exchange g C m^-2 d^-1 net_radiation Net Radiation MJ m^-2 d^-1 nirv Near Infrared Reflectance of Vegetation 1 open_water_evaporation Open-water Evaporation mm d^-1 potential_evaporation Potential Evaporation mm d^-1 precipitation_era5 Total Precipitation mm d^-1 radiation_era5 Surface Net Solar Radiation J m^-2 root_moisture Root-zone Soil Moisture mm d^-1 sensible_heat Sensible Heat MJ m^-2 d^-1 sif_gome2_jj Downscaled Daily Corrected Sun-Induced Chlorophyll Fluorescence at 740 nm m W m^-2 sr^-1 nm^-1 sif_gome2_pk Downscaled Daily Corrected Sun-Induced Chlorophyll Fluorescence at 740 nm m W m^-2 sr^-1 nm^-1 sif_gosif Sun-Induced Chlorophyll Fluorescence at 757 nm W m^-2 sr^-1 um^-1 sif_rtsif Sun-Induced Chlorophyll Fluorescence at 740 nm m W m^-2 sr^-1 um^-1 sm Volumetric Soil Moisture m^3 m^-3 snow_sublimation Snow Sublimation mm d^-1 surface_moisture Surface Soil Moisture mm d^-1 terrestrial_ecosystem_respiration Terrestrial Ecosystem Respiration g C m^-2 d^-1 transpiration Transpiration mm d^-1"},{"location":"datasets/ESDC/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/ESDC/#aerosol_optical_thickness_550","title":"aerosol_optical_thickness_550","text":"Field Value acknowledgment ESA Aerosol Climate Change Initiative (Aerosol_cci) date_modified 2022-10-13 03:15:18.312024 description ESA Aerosol Climate Change Initiative (Aerosol_cci): Level 3 aerosol products from AATSR (ensemble product), Version 2.6 geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Aerosol Optical Thickness at 550 nm original_add_offset 0.0 original_name AOD550_mean original_scale_factor 1.0 processing_steps Loading data using the cciodp xcube data store, Resampling by 8-day mean, Upsampling to 0.25 degrees using nearest neighbor project DeepESDL references https://doi.org/10.5194/amt-6-1919-2013 reported_day 5.0 source https://catalogue.ceda.ac.uk/uuid/c183044b88734442b6d37f5c4f6b0092 standard_name atmosphere_optical_thickness_due_to_ambient_aerosol temporal_resolution 8D time_coverage_end 2012-04-10T00:00:00.000000000 time_coverage_start 2002-05-21T00:00:00.000000000 time_period 8D units 1"},{"location":"datasets/ESDC/#air_temperature_2m","title":"air_temperature_2m","text":"Field Value acknowledgment ERA5 hourly data on single levels from 1959 to present date_modified 2022-11-04 15:41:36.233472 description ERA5 Reanalysis Products geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Mean Air Temperature at 2 m original_add_offset 0.0 original_name t2m original_scale_factor 1.0 processing_steps Merging nc files, Resampling by daily mean, Converting to \u00b0C from K, Resampling by 8-day mean, Resampling to 0.25 degrees using bilinear interpolation project DeepESDL references https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation reported_day 5.0 source https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels standard_name mean_air_temperature_2m temporal_resolution 8D time_coverage_end 2021-12-27T00:00:00.000000000 time_coverage_start 1979-01-01T00:00:00.000000000 time_period 8D units \u00b0C"},{"location":"datasets/ESDC/#bare_soil_evaporation","title":"bare_soil_evaporation","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Bare Soil Evaporation original_add_offset 0.0 original_name Eb original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name bare_soil_evaporation temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#burnt_area","title":"burnt_area","text":"Field Value acknowledgment https://www.globalfiredata.org/ date_modified 2022-10-13 14:55:35.002779 description Global Fire Emissions Database (GFED) 4 Monthly Burnt Area geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Monthly Burnt Area original_add_offset 0.0 original_name burnt_area original_scale_factor 0.01 processing_steps Merging hdf files, Resampling by 8-day nearest neighbor project DeepESDL references https://doi.org/10.1002/jgrg.20042 reported_day 5.0 source https://www.globalfiredata.org/ standard_name burnt_area temporal_resolution 8D time_coverage_end 2016-12-30T00:00:00.000000000 time_coverage_start 1995-06-06T00:00:00.000000000 time_period 8D units hectares"},{"location":"datasets/ESDC/#cot","title":"cot","text":"Field Value acknowledgment ESA Cloud Climate Change Initiative (Cloud_cci) date_modified 2022-11-04 13:33:18.450458 description ESA Cloud Climate Change Initiative (Cloud_cci): MODIS-TERRA monthly gridded cloud properties, version 2.0 geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Cloud Optical Thickness original_add_offset 0.0 original_name cot original_scale_factor 1.0 processing_steps Loading data using the cciodp xcube data store, Resampling by 8-day nearest neighbor, Upsampling to 0.25 degrees using nearest neighbor project DeepESDL references https://public.satproj.klima.dwd.de/data/ESA_Cloud_CCI/CLD_PRODUCTS/v2.0/DOIs/DOI_ESA_Cloud_cci_MODIS-Terra_v2.0_landingpage.html, https://dap.ceda.ac.uk/neodc/esacci/cloud/docs/DataSet_Desc_ESA_Cloud_cci_CC4CL_1.5.pdf reported_day 5.0 source https://catalogue.ceda.ac.uk/uuid/f1ab07b5292f4813bd3090b51d270aa8 standard_name atmosphere_optical_thickness_due_to_cloud temporal_resolution 8D time_coverage_end 2014-12-15T00:00:00.000000000 time_coverage_start 2000-02-22T00:00:00.000000000 time_period 8D units 1"},{"location":"datasets/ESDC/#cth","title":"cth","text":"Field Value acknowledgment ESA Cloud Climate Change Initiative (Cloud_cci) date_modified 2022-11-04 13:33:18.450458 description ESA Cloud Climate Change Initiative (Cloud_cci): MODIS-TERRA monthly gridded cloud properties, version 2.0 geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Cloud Top Height original_add_offset 0.0 original_name cth original_scale_factor 1.0 processing_steps Loading data using the cciodp xcube data store, Resampling by 8-day nearest neighbor, Upsampling to 0.25 degrees using nearest neighbor project DeepESDL references https://public.satproj.klima.dwd.de/data/ESA_Cloud_CCI/CLD_PRODUCTS/v2.0/DOIs/DOI_ESA_Cloud_cci_MODIS-Terra_v2.0_landingpage.html, https://dap.ceda.ac.uk/neodc/esacci/cloud/docs/DataSet_Desc_ESA_Cloud_cci_CC4CL_1.5.pdf reported_day 5.0 source https://catalogue.ceda.ac.uk/uuid/f1ab07b5292f4813bd3090b51d270aa8 standard_name cloud_top_altitude temporal_resolution 8D time_coverage_end 2014-12-15T00:00:00.000000000 time_coverage_start 2000-02-22T00:00:00.000000000 time_period 8D units km"},{"location":"datasets/ESDC/#ctt","title":"ctt","text":"Field Value acknowledgment ESA Cloud Climate Change Initiative (Cloud_cci) date_modified 2022-11-04 13:33:18.450458 description ESA Cloud Climate Change Initiative (Cloud_cci): MODIS-TERRA monthly gridded cloud properties, version 2.0 geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Cloud Top Temperature original_add_offset 0.0 original_name ctt original_scale_factor 1.0 processing_steps Loading data using the cciodp xcube data store, Resampling by 8-day nearest neighbor, Upsampling to 0.25 degrees using nearest neighbor project DeepESDL references https://public.satproj.klima.dwd.de/data/ESA_Cloud_CCI/CLD_PRODUCTS/v2.0/DOIs/DOI_ESA_Cloud_cci_MODIS-Terra_v2.0_landingpage.html, https://dap.ceda.ac.uk/neodc/esacci/cloud/docs/DataSet_Desc_ESA_Cloud_cci_CC4CL_1.5.pdf reported_day 5.0 source https://catalogue.ceda.ac.uk/uuid/f1ab07b5292f4813bd3090b51d270aa8 standard_name air_temperature_at_cloud_top temporal_resolution 8D time_coverage_end 2014-12-15T00:00:00.000000000 time_coverage_start 2000-02-22T00:00:00.000000000 time_period 8D units K"},{"location":"datasets/ESDC/#evaporation","title":"evaporation","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Actual Evaporation original_add_offset 0.0 original_name E original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name actual_evaporation temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#evaporation_era5","title":"evaporation_era5","text":"Field Value acknowledgment ERA5 hourly data on single levels from 1959 to present date_modified 2022-11-04 15:41:36.233472 description ERA5 Reanalysis Products geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Evaporation original_add_offset 0.0 original_name e original_scale_factor 1.0 processing_steps Merging nc files, Resampling by daily sum, Converting to mm from m, Resampling by 8-day mean, Resampling to 0.25 degrees using bilinear interpolation project DeepESDL references https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation reported_day 5.0 source https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels standard_name lwe_thickness_of_water_evaporation_amount temporal_resolution 8D time_coverage_end 2021-12-27T00:00:00.000000000 time_coverage_start 1979-01-01T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#evaporative_stress","title":"evaporative_stress","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Evaporative Stress original_add_offset 0.0 original_name S original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name evaporative_stress temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units 1"},{"location":"datasets/ESDC/#gross_primary_productivity","title":"gross_primary_productivity","text":"Field Value acknowledgment FLUXCOM date_modified 2022-10-17 22:14:30.401506 description FLUXCOM geospatial_lat_max 89.87499928049999 geospatial_lat_min -89.8750000005 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87499856049996 geospatial_lon_min -179.8750000005 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Gross Primary Productivity original_add_offset 0.0 original_name GPP original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.5194/bg-13-4291-2016, https://doi.org/10.1038/s41597-019-0076-8 reported_day 5.0 source https://www.fluxcom.org/ standard_name gross_primary_productivity_of_carbon temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 2001-01-05T00:00:00.000000000 time_period 8D units g C m^-2 d^-1"},{"location":"datasets/ESDC/#interception_loss","title":"interception_loss","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Interception Loss original_add_offset 0.0 original_name Ei original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name interception_loss temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#kndvi","title":"kndvi","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Kernel Normalized Difference Vegetation Index original_add_offset 0.0 original_name kNDVI original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.1126/sciadv.abc7447, https://github.com/awesome-spectral-indices/awesome-spectral-indices, https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://github.com/awesome-spectral-indices/spyndex, https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name kNDVI temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1"},{"location":"datasets/ESDC/#latent_energy","title":"latent_energy","text":"Field Value acknowledgment FLUXCOM date_modified 2022-10-17 22:14:30.401506 description FLUXCOM geospatial_lat_max 89.87499928049999 geospatial_lat_min -89.8750000005 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87499856049996 geospatial_lon_min -179.8750000005 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Latent Energy original_add_offset 0.0 original_name LE original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.5194/bg-13-4291-2016, https://doi.org/10.1038/s41597-019-0076-8 reported_day 5.0 source https://www.fluxcom.org/ standard_name surface_upward_latent_heat_flux temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 2001-01-05T00:00:00.000000000 time_period 8D units MJ m^-2 d^-1"},{"location":"datasets/ESDC/#max_air_temperature_2m","title":"max_air_temperature_2m","text":"Field Value acknowledgment ERA5 hourly data on single levels from 1959 to present date_modified 2022-11-04 15:41:36.233472 description ERA5 Reanalysis Products geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Maximum Air Temperature at 2 m original_add_offset 0.0 original_name t2m_max original_scale_factor 1.0 processing_steps Merging nc files, Resampling by daily max, Converting to \u00b0C from K, Resampling by 8-day max, Resampling to 0.25 degrees using bilinear interpolation project DeepESDL references https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation reported_day 5.0 source https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels standard_name max_air_temperature_2m temporal_resolution 8D time_coverage_end 2021-12-27T00:00:00.000000000 time_coverage_start 1979-01-01T00:00:00.000000000 time_period 8D units \u00b0C"},{"location":"datasets/ESDC/#min_air_temperature_2m","title":"min_air_temperature_2m","text":"Field Value acknowledgment ERA5 hourly data on single levels from 1959 to present date_modified 2022-11-04 15:41:36.233472 description ERA5 Reanalysis Products geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Minimum Air Temperature at 2 m original_add_offset 0.0 original_name t2m_min original_scale_factor 1.0 processing_steps Merging nc files, Resampling by daily min, Converting to \u00b0C from K, Resampling by 8-day min, Resampling to 0.25 degrees using bilinear interpolation project DeepESDL references https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation reported_day 5.0 source https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels standard_name min_air_temperature_2m temporal_resolution 8D time_coverage_end 2021-12-27T00:00:00.000000000 time_coverage_start 1979-01-01T00:00:00.000000000 time_period 8D units \u00b0C"},{"location":"datasets/ESDC/#nbar_blue","title":"nbar_blue","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ bandwidth 20.0 bandwidth_units nm date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Nadir BRDF Adjusted Reflectance of Band 3 (blue) original_add_offset 0.0 original_name Nadir_Reflectance_Band3 original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name nadir_reflectance_band3 temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1 wavelength 469.0 wavelength_units nm"},{"location":"datasets/ESDC/#nbar_green","title":"nbar_green","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ bandwidth 20.0 bandwidth_units nm date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Nadir BRDF Adjusted Reflectance of Band 4 (green) original_add_offset 0.0 original_name Nadir_Reflectance_Band4 original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name nadir_reflectance_band4 temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1 wavelength 555.0 wavelength_units nm"},{"location":"datasets/ESDC/#nbar_nir","title":"nbar_nir","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ bandwidth 35.0 bandwidth_units nm date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Nadir BRDF Adjusted Reflectance of Band 2 (NIR) original_add_offset 0.0 original_name Nadir_Reflectance_Band2 original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name nadir_reflectance_band2 temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1 wavelength 858.5 wavelength_units nm"},{"location":"datasets/ESDC/#nbar_red","title":"nbar_red","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ bandwidth 50.0 bandwidth_units nm date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Nadir BRDF Adjusted Reflectance of Band 1 (red) original_add_offset 0.0 original_name Nadir_Reflectance_Band1 original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name nadir_reflectance_band1 temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1 wavelength 645.0 wavelength_units nm"},{"location":"datasets/ESDC/#nbar_swir1","title":"nbar_swir1","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ bandwidth 20.0 bandwidth_units nm date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Nadir BRDF Adjusted Reflectance of Band 5 (SWIR1) original_add_offset 0.0 original_name Nadir_Reflectance_Band5 original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name nadir_reflectance_band5 temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1 wavelength 1240.0 wavelength_units nm"},{"location":"datasets/ESDC/#nbar_swir2","title":"nbar_swir2","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ bandwidth 24.0 bandwidth_units nm date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Nadir BRDF Adjusted Reflectance of Band 6 (SWIR2) original_add_offset 0.0 original_name Nadir_Reflectance_Band6 original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name nadir_reflectance_band6 temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1 wavelength 1640.0 wavelength_units nm"},{"location":"datasets/ESDC/#nbar_swir3","title":"nbar_swir3","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ bandwidth 50.0 bandwidth_units nm date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Nadir BRDF Adjusted Reflectance of Band 7 (SWIR3) original_add_offset 0.0 original_name Nadir_Reflectance_Band7 original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name nadir_reflectance_band7 temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1 wavelength 2130.0 wavelength_units nm"},{"location":"datasets/ESDC/#ndvi","title":"ndvi","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Normalized Difference Vegetation Index original_add_offset 0.0 original_name NDVI original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://ntrs.nasa.gov/citations/19740022614, https://github.com/awesome-spectral-indices/awesome-spectral-indices, https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://github.com/awesome-spectral-indices/spyndex, https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name NDVI temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1"},{"location":"datasets/ESDC/#net_ecosystem_exchange","title":"net_ecosystem_exchange","text":"Field Value acknowledgment FLUXCOM date_modified 2022-10-17 22:14:30.401506 description FLUXCOM geospatial_lat_max 89.87499928049999 geospatial_lat_min -89.8750000005 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87499856049996 geospatial_lon_min -179.8750000005 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Net Ecosystem Exchange original_add_offset 0.0 original_name NEE original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.5194/bg-13-4291-2016, https://doi.org/10.1038/s41597-019-0076-8 reported_day 5.0 source https://www.fluxcom.org/ standard_name net_primary_productivity_of_carbon temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 2001-01-05T00:00:00.000000000 time_period 8D units g C m^-2 d^-1"},{"location":"datasets/ESDC/#net_radiation","title":"net_radiation","text":"Field Value acknowledgment FLUXCOM date_modified 2022-10-17 22:14:30.401506 description FLUXCOM geospatial_lat_max 89.87499928049999 geospatial_lat_min -89.8750000005 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87499856049996 geospatial_lon_min -179.8750000005 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Net Radiation original_add_offset 0.0 original_name Rn original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.5194/bg-13-4291-2016, https://doi.org/10.1038/s41597-019-0076-8 reported_day 5.0 source https://www.fluxcom.org/ standard_name surface_net_radiation_flux temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 2001-01-05T00:00:00.000000000 time_period 8D units MJ m^-2 d^-1"},{"location":"datasets/ESDC/#nirv","title":"nirv","text":"Field Value acknowledgment https://lpdaac.usgs.gov/products/mcd43c4v061/ date_modified 2022-10-11 23:51:00.603768 description MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Reflectance Daily L3 Global 0.05 Deg CMG and Vegetation Indices geospatial_lat_max 89.875 geospatial_lat_min -89.87499999998977 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000008183 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Near Infrared Reflectance of Vegetation original_add_offset 0.0 original_name NIRv original_scale_factor 1.0 processing_steps Merging hdf files, Computing NDVI, NIRv, and kNDVI, resampling by 8-day mean, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation, Masking water using GLEAM as reference project DeepESDL references https://doi.org/10.1126/sciadv.1602244, https://github.com/awesome-spectral-indices/awesome-spectral-indices, https://doi.org/10.5067/MODIS/MCD43C4.061, https://www.umb.edu/spectralmass/terra_aqua_modis/v006 reported_day 5.0 source https://github.com/awesome-spectral-indices/spyndex, https://lpdaac.usgs.gov/products/mcd43c4v061/ standard_name NIRv temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units 1"},{"location":"datasets/ESDC/#open_water_evaporation","title":"open_water_evaporation","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Open-water Evaporation original_add_offset 0.0 original_name Ew original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name open_water_evaporation temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#potential_evaporation","title":"potential_evaporation","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Potential Evaporation original_add_offset 0.0 original_name Ep original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name potential_evaporation temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#precipitation_era5","title":"precipitation_era5","text":"Field Value acknowledgment ERA5 hourly data on single levels from 1959 to present date_modified 2022-11-04 15:41:36.233472 description ERA5 Reanalysis Products geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Total Precipitation original_add_offset 0.0 original_name tp original_scale_factor 1.0 processing_steps Merging nc files, Resampling by daily sum, Converting to mm from m, Resampling by 8-day mean, Resampling to 0.25 degrees using bilinear interpolation project DeepESDL references https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation reported_day 5.0 source https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels standard_name total_precipitation temporal_resolution 8D time_coverage_end 2021-12-27T00:00:00.000000000 time_coverage_start 1979-01-01T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#radiation_era5","title":"radiation_era5","text":"Field Value acknowledgment ERA5 hourly data on single levels from 1959 to present date_modified 2022-11-04 15:41:36.233472 description ERA5 Reanalysis Products geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Surface Net Solar Radiation original_add_offset 0.0 original_name ssr original_scale_factor 1.0 processing_steps Merging nc files, Resampling by daily mean, Resampling by 8-day mean, Resampling to 0.25 degrees using bilinear interpolation project DeepESDL references https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation reported_day 5.0 source https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels standard_name surface_net_downward_shortwave_flux temporal_resolution 8D time_coverage_end 2021-12-27T00:00:00.000000000 time_coverage_start 1979-01-01T00:00:00.000000000 time_period 8D units J m^-2"},{"location":"datasets/ESDC/#root_moisture","title":"root_moisture","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Root-zone Soil Moisture original_add_offset 0.0 original_name SMroot original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name root_zone_soil_moisture temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#sensible_heat","title":"sensible_heat","text":"Field Value acknowledgment FLUXCOM date_modified 2022-10-17 22:14:30.401506 description FLUXCOM geospatial_lat_max 89.87499928049999 geospatial_lat_min -89.8750000005 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87499856049996 geospatial_lon_min -179.8750000005 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Sensible Heat original_add_offset 0.0 original_name H original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.5194/bg-13-4291-2016, https://doi.org/10.1038/s41597-019-0076-8 reported_day 5.0 source https://www.fluxcom.org/ standard_name surface_upward_sensible_heat_flux temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 2001-01-05T00:00:00.000000000 time_period 8D units MJ m^-2 d^-1"},{"location":"datasets/ESDC/#sif_gome2_jj","title":"sif_gome2_jj","text":"Field Value acknowledgment https://doi.org/10.5194/essd-12-1101-2020 date_modified 2022-10-11 22:36:53.583022 description Spatially Downscaled Sun-Induced Fluorescence (JJ Method) geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000000003 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Downscaled Daily Corrected Sun-Induced Chlorophyll Fluorescence at 740 nm original_add_offset 0.0 original_name SIF original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation project DeepESDL references https://doi.org/10.5194/essd-12-1101-2020 reported_day 9.0 source https://data.jrc.ec.europa.eu/dataset/21935ffc-b797-4bee-94da-8fec85b3f9e1 standard_name sif temporal_resolution 16D time_coverage_end 2018-10-04T00:00:00.000000000 time_coverage_start 2007-01-21T00:00:00.000000000 time_period 8D units m W m^-2 sr^-1 nm^-1"},{"location":"datasets/ESDC/#sif_gome2_pk","title":"sif_gome2_pk","text":"Field Value acknowledgment https://doi.org/10.5194/essd-12-1101-2020 date_modified 2022-10-11 22:43:08.258033 description Spatially Downscaled Sun-Induced Fluorescence (PK Method) geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000000003 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Downscaled Daily Corrected Sun-Induced Chlorophyll Fluorescence at 740 nm original_add_offset 0.0 original_name SIF original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean, Interpolating NA with linear interpolation project DeepESDL references https://doi.org/10.5194/essd-12-1101-2020 reported_day 9.0 source https://data.jrc.ec.europa.eu/dataset/21935ffc-b797-4bee-94da-8fec85b3f9e1 standard_name sif temporal_resolution 16D time_coverage_end 2018-12-31T00:00:00.000000000 time_coverage_start 2007-01-21T00:00:00.000000000 time_period 8D units m W m^-2 sr^-1 nm^-1"},{"location":"datasets/ESDC/#sif_gosif","title":"sif_gosif","text":"Field Value acknowledgment https://doi.org/10.3390/rs11050517 date_modified 2022-10-11 22:20:05.841847 description GOSIF Solar-Induced Chlorophyll Fluorescence Derived from OCO-2, MODIS, and Reanalysis Data geospatial_lat_max 89.87499999999999 geospatial_lat_min -89.87500000000001 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000000003 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Sun-Induced Chlorophyll Fluorescence at 757 nm original_add_offset 0.0 original_name sif original_scale_factor 0.0001 processing_steps Merging tif files, Converting water bodies and snow covered areas to NaN, Applying original scale factor, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.3390/rs11050517 reported_day 5.0 source https://globalecology.unh.edu/data.html standard_name sif temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 2000-03-01T00:00:00.000000000 time_period 8D units W m^-2 sr^-1 um^-1"},{"location":"datasets/ESDC/#sif_rtsif","title":"sif_rtsif","text":"Field Value acknowledgment https://doi.org/10.1038/s41597-022-01520-1 date_modified 2022-10-12 14:26:02.972963 description Long-term Reconstructed TROPOMI Solar-Induced Fluorescence (RTSIF) geospatial_lat_max 89.87499999999999 geospatial_lat_min -89.87500000000001 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87500000000003 geospatial_lon_min -179.87499999999997 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Sun-Induced Chlorophyll Fluorescence at 740 nm original_add_offset 0.0 original_name sif original_scale_factor 1.0 processing_steps Merging tif files, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.1038/s41597-022-01520-1 reported_day 5.0 source https://figshare.com/articles/dataset/RTSIF_dataset/19336346/2 standard_name sif temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 2001-01-05T00:00:00.000000000 time_period 8D units m W m^-2 sr^-1 um^-1"},{"location":"datasets/ESDC/#sm","title":"sm","text":"Field Value acknowledgment ESA Soil Moisture Climate Change Initiative (Soil_Moisture_cci) date_modified 2022-10-13 20:42:41.277132 description ESA Soil Moisture Climate Change Initiative (Soil_Moisture_cci): COMBINED product, Version 06.1 geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Volumetric Soil Moisture original_add_offset 0.0 original_name sm original_scale_factor 1.0 processing_steps Merging nc files, Resampling by 8-day mean project DeepESDL references https://data.cci.ceda.ac.uk/thredds/fileServer/esacci/soil_moisture/docs/v06.1/ESA_CCI_SM_RD_D2.1_v2_ATBD_v06.1_issue_1.1.pdf, https://doi.org/10.5194/essd-11-717-2019, https://doi.org/10.1016/j.rse.2017.07.001 reported_day 5.0 source https://catalogue.ceda.ac.uk/uuid/43d73291472444e6b9c2d2420dbad7d6 standard_name volumetric_soil_moisture temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 1979-01-05T00:00:00.000000000 time_period 8D units m^3 m^-3"},{"location":"datasets/ESDC/#snow_sublimation","title":"snow_sublimation","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Snow Sublimation original_add_offset 0.0 original_name Es original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name snow_sublimation temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#surface_moisture","title":"surface_moisture","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Surface Soil Moisture original_add_offset 0.0 original_name SMsurf original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name surface_soil_moisture temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#terrestrial_ecosystem_respiration","title":"terrestrial_ecosystem_respiration","text":"Field Value acknowledgment FLUXCOM date_modified 2022-10-17 22:14:30.401506 description FLUXCOM geospatial_lat_max 89.87499928049999 geospatial_lat_min -89.8750000005 geospatial_lat_resolution 0.25 geospatial_lon_max 179.87499856049996 geospatial_lon_min -179.8750000005 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Terrestrial Ecosystem Respiration original_add_offset 0.0 original_name TER original_scale_factor 1.0 processing_steps Merging nc files, Downsampling to 0.25 deg with mean project DeepESDL references https://doi.org/10.5194/bg-13-4291-2016, https://doi.org/10.1038/s41597-019-0076-8 reported_day 5.0 source https://www.fluxcom.org/ standard_name ecosystem_respiration_carbon_flux temporal_resolution 8D time_coverage_end 2020-12-30T00:00:00.000000000 time_coverage_start 2001-01-05T00:00:00.000000000 time_period 8D units g C m^-2 d^-1"},{"location":"datasets/ESDC/#transpiration","title":"transpiration","text":"Field Value acknowledgment https://www.gleam.eu/ date_modified 2022-10-11 16:47:36.234042 description Global Land Evaporation Amsterdam Model (GLEAM) v3.6a geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution -0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 license Terms and conditions of the DeepESDL data distribution long_name Transpiration original_add_offset 0.0 original_name Et original_scale_factor 1.0 processing_steps Merging nc files, resampling by 8-day mean project DeepESDL references https://doi.org/10.5194/gmd-10-1903-2017, https://doi.org/10.5194/hess-15-453-2011 reported_day 5.0 source GLEAM v3.6a, https://www.gleam.eu/ standard_name transpiration temporal_resolution 8D time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1980-01-05T00:00:00.000000000 time_period 8D units mm d^-1"},{"location":"datasets/ESDC/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.9 acknowledgment All ESDC data providers are acknowledged inside each variable contributor_name University of Leipzig, Max Planck Institute, Brockmann Consult GmbH contributor_url https://www.uni-leipzig.de/, https://www.mpg.de/en, https://www.brockmann-consult.de/ creator_name University of Leipzig, Brockmann Consult GmbH creator_url https://www.uni-leipzig.de/, https://www.brockmann-consult.de/ date_modified 2022-11-25 23:13:03.350030 geospatial_lat_max 89.875 geospatial_lat_min -89.875 geospatial_lat_resolution 0.25 geospatial_lon_max 179.875 geospatial_lon_min -179.875 geospatial_lon_resolution 0.25 id esdc-8d-0.25deg-256x128x128-3.0.1 license Terms and conditions of the DeepESDL data distribution project DeepESDL publisher_name DeepESDL Team publisher_url https://www.earthsystemdatalab.net/ time_coverage_end 2021-12-31T00:00:00.000000000 time_coverage_start 1979-01-05T00:00:00.000000000 time_period 8D time_period_reported_day 5.0 title Earth System Data Cube (ESDC) v3.0.1"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/","title":"Land Cover Cube","text":""},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#land-cover-map-of-esa-cci-brokered-by-cds","title":"Land Cover Map of ESA CCI brokered by CDS","text":""},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\n# The cube is saved as a multilevel cube, the level 0 is the base layer with \n# the highest resolution\nml_dataset = store.open_data('LC-1x2025x2025-2.0.0.levels')\n# Chek how many levels are present\nml_dataset.num_levels\n# Open dataset at a certain level, here level 0\nds = ml_dataset.get_dataset(0)\n</code></pre>"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180.0 to 180.0 Bounding box latitude (\u00b0) -90.0 to 90.0 Time range 1992-01-01 to 2022-12-31 <p>Click here for full dataset metadata.</p>"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units change_count number of class changes [none] current_pixel_state LC pixel type mask [none] lat_bounds [none] [none] lccs_class Land cover class defined in LCCS [none] lon_bounds [none] [none] observation_count number of valid observations [none] processed_flag LC map processed area flag [none] time_bounds [none] [none]"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#change_count","title":"change_count","text":"Field Value long_name number of class changes valid_max 100 valid_min 0"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#crs","title":"crs","text":"Field Value i2m 0.002777777777778,0.0,0.0,-0.002777777777778,-180.0,90.0 wkt GEOGCS[\"WGS 84\",    DATUM[\"World Geodetic System 1984\",      SPHEROID[\"WGS 84\", 6378137.0, 298.257223563, AUTHORITY[\"EPSG\",\"7030\"]],      AUTHORITY[\"EPSG\",\"6326\"]],    PRIMEM[\"Greenwich\", 0.0, AUTHORITY[\"EPSG\",\"8901\"]],    UNIT[\"degree\", 0.017453292519943295],    AXIS[\"Geodetic longitude\", EAST],    AXIS[\"Geodetic latitude\", NORTH],    AUTHORITY[\"EPSG\",\"4326\"]]"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#current_pixel_state","title":"current_pixel_state","text":"Field Value flag_meanings invalid clear_land clear_water clear_snow_ice cloud cloud_shadow flag_values 0, 1, 2, 3, 4, 5 long_name LC pixel type mask standard_name land_cover_lccs status_flag valid_max 5 valid_min 0"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#lat_bounds","title":"lat_bounds","text":""},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#lccs_class","title":"lccs_class","text":"Field Value ancillary_variables processed_flag current_pixel_state observation_count change_count flag_colors #ffff64 #ffff64 #ffff00 #aaf0f0 #dcf064 #c8c864 #006400 #00a000 #00a000 #aac800 #003c00 #003c00 #005000 #285000 #285000 #286400 #788200 #8ca000 #be9600 #966400 #966400 #966400 #ffb432 #ffdcd2 #ffebaf #ffc864 #ffd278 #ffebaf #00785a #009678 #00dc82 #c31400 #fff5d7 #dcdcdc #fff5d7 #0046c8 #ffffff flag_meanings no_data cropland_rainfed cropland_rainfed_herbaceous_cover cropland_rainfed_tree_or_shrub_cover cropland_irrigated mosaic_cropland mosaic_natural_vegetation tree_broadleaved_evergreen_closed_to_open tree_broadleaved_deciduous_closed_to_open tree_broadleaved_deciduous_closed tree_broadleaved_deciduous_open tree_needleleaved_evergreen_closed_to_open tree_needleleaved_evergreen_closed tree_needleleaved_evergreen_open tree_needleleaved_deciduous_closed_to_open tree_needleleaved_deciduous_closed tree_needleleaved_deciduous_open tree_mixed mosaic_tree_and_shrub mosaic_herbaceous shrubland shrubland_evergreen shrubland_deciduous grassland lichens_and_mosses sparse_vegetation sparse_tree sparse_shrub sparse_herbaceous tree_cover_flooded_fresh_or_brakish_water tree_cover_flooded_saline_water shrub_or_herbaceous_cover_flooded urban bare_areas bare_areas_consolidated bare_areas_unconsolidated water snow_and_ice flag_values 0, 10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100, 110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180, 190, 200, 201, 202, 210, 220 long_name Land cover class defined in LCCS standard_name land_cover_lccs valid_max 220 valid_min 1"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#lon_bounds","title":"lon_bounds","text":""},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#observation_count","title":"observation_count","text":"Field Value long_name number of valid observations standard_name land_cover_lccs number_of_observations valid_max 32767 valid_min 0"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#processed_flag","title":"processed_flag","text":"Field Value flag_meanings not_processed processed flag_values 0, 1 long_name LC map processed area flag standard_name land_cover_lccs status_flag valid_max 1 valid_min 0"},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#time_bounds","title":"time_bounds","text":""},{"location":"datasets/LC-1x2025x2025-2-0-0-levels/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.6 TileSize 2025:2025 cdm_data_type grid comment contact https://www.ecmwf.int/en/about/contact-us/get-support creation_date 20181130T095451Z creator_email landcover-cci@uclouvain.be creator_name UCLouvain creator_url http://www.uclouvain.be/ geospatial_lat_max 90.0 geospatial_lat_min -90.0 geospatial_lat_resolution 0.002778 geospatial_lat_units degrees_north geospatial_lon_max 180 geospatial_lon_min -180 geospatial_lon_resolution 0.002778 geospatial_lon_units degrees_east history amorgos-4,0, lc-sdr-1.0, lc-sr-1.0, lc-classification-1.0,lc-user-tools-3.13,lc-user-tools-4.3 id ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992-v2.0.7cds institution UCLouvain keywords land cover classification,satellite,observation keywords_vocabulary NASA Global Change Master Directory (GCMD) Science Keywords license ESA CCI Data Policy: free and open access naming_authority org.esa-cci product_version 2.0.7cds project Climate Change Initiative - European Space Agency references http://www.esa-landcover-cci.org/ source MERIS FR L1B version 5.05, MERIS RR L1B version 8.0, SPOT VGT P spatial_resolution 300m standard_name_vocabulary NetCDF Climate and Forecast (CF) Standard Names version 21 summary This dataset characterizes the land cover of a particular year (see time_coverage). The land cover was derived from the analysis of satellite data time series of the full period. time_coverage_duration P1Y time_coverage_end 19921231 time_coverage_resolution P1Y time_coverage_start 19920101 title Land Cover Map of ESA CCI brokered by CDS tracking_id 61b96fd7-42c3-4374-9de1-0dc3b0bcae2a type ESACCI-LC-L4-LCCS-Map-300m-P1Y"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/","title":"SMOS L2C OS 20230101 20231231 1W res0 1x1000x1000 levels","text":""},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#smos-ocean-salinity-data-cube","title":"SMOS Ocean Salinity Data Cube","text":""},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000.levels')\n</code></pre>"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180 to 180 Bounding box latitude (\u00b0) -89 to 89 Time range 2023-01-01 to 2024-01-06 Time period 7D Publisher Brockmann Consult GmbH <p>Click here for full dataset metadata.</p>"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units Coast_distance [none] [none] Dg_RFI_X [none] [none] Dg_RFI_Y [none] [none] Dg_chi2_corr [none] [none] Dg_quality_SSS_anom [none] [none] Dg_quality_SSS_corr [none] [none] Mean_acq_time [none] dd SSS_anom [none] psu SSS_corr Sea Surface Salinity psu Sigma_SSS_anom [none] psu Sigma_SSS_corr [none] psu X_swath [none] m"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#coast_distance","title":"Coast_distance","text":"Field Value scale_offset 0.0"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#dg_rfi_x","title":"Dg_RFI_X","text":""},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#dg_rfi_y","title":"Dg_RFI_Y","text":""},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#dg_chi2_corr","title":"Dg_chi2_corr","text":"Field Value scale_offset 0.0"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#dg_quality_sss_anom","title":"Dg_quality_SSS_anom","text":""},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#dg_quality_sss_corr","title":"Dg_quality_SSS_corr","text":""},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#mean_acq_time","title":"Mean_acq_time","text":"Field Value units dd"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#sss_anom","title":"SSS_anom","text":"Field Value units psu"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#sss_corr","title":"SSS_corr","text":"Field Value color_bar_name haline color_value_max 42 color_value_min 0 long_name Sea Surface Salinity units psu"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#sigma_sss_anom","title":"Sigma_SSS_anom","text":"Field Value units psu"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#sigma_sss_corr","title":"Sigma_SSS_corr","text":"Field Value units psu"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#x_swath","title":"X_swath","text":"Field Value units m"},{"location":"datasets/SMOS-L2C-OS-20230101-20231231-1W-res0-1x1000x1000-levels/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.9 FH-File_Class OPER FH-File_Description L2 Ocean Salinity Output User Data Product. FH-File_Type MIR_OSUDP2 FH-File_Version 0001 FH-Mission SMOS FH-Notes The UDP (User Data Product) is designed for oceanographics and high level centers, it includes geophysical parameters, a theoretical estimate of their accuracy, flags and descriptors of the product quality. FH-Source-Creator L2OP FH-Source-Creator_Version 700 FH-Source-System DPGS VH-MPH-Acquisition_Station SVLD VH-MPH-Logical_Proc_Centre FPC VH-MPH-Processing_Centre ESAC VH-MPH-Product_Confidence NOMINAL VH-MPH-Ref_Doc SO-TN-IDR-GS-0006 acknowledgment ESA SMOS, DeepESDL project creator_email info@brockmann-consult.de creator_name Brockmann Consult GmbH creator_url www.brockmann-consult.de data_id SMOS-L2C-OS-20230101-20231231-1W-res0 date_modified 2024-08-19 16:19:15.359970 description Weekly means SMOS Ocean Salinity 2023 geospatial_lat_max 89.0 geospatial_lat_min -89.0 geospatial_lon_max 180 geospatial_lon_min -180 institution Brockmann Consult GmbH license Creative Commons Attribution 4.0 International (CC BY 4.0) license_url https://creativecommons.org/licenses/by/4.0/ project DeepESDL publisher_email info@brockmann-consult.de publisher_name Brockmann Consult GmbH source ESA SMOS Ocean Salinity temporal_coverage_end 2023-12-31 23:59:59 temporal_coverage_start 2023-01-01 00:00:00 temporal_resolution 1W title SMOS Ocean Salinity Data Cube version 1.0.0"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/","title":"SMOS L2C SM 20230101 20231231 1W res0 1x1000x1000 levels","text":""},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#smos-soil-moisture-data-cube","title":"SMOS Soil Moisture Data Cube","text":""},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000.levels')\n</code></pre>"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180 to 180 Bounding box latitude (\u00b0) -89 to 89 Time range 2023-01-01 to 2024-01-06 Time period 7D Publisher Brockmann Consult GmbH <p>Click here for full dataset metadata.</p>"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units Chi_2 [none] [none] Chi_2_P [none] [none] N_RFI_X [none] [none] N_RFI_Y [none] [none] RFI_Prob [none] [none] Soil_Moisture [none] m3 m-3 Soil_Moisture_DQX [none] m3 m-3"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#chi_2","title":"Chi_2","text":"Field Value scale_offset 0.0"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#chi_2_p","title":"Chi_2_P","text":"Field Value scale_offset 0.0"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#n_rfi_x","title":"N_RFI_X","text":""},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#n_rfi_y","title":"N_RFI_Y","text":""},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#rfi_prob","title":"RFI_Prob","text":"Field Value scale_offset 0.0"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#soil_moisture","title":"Soil_Moisture","text":"Field Value color_bar_name YlGnBu color_value_max 1 color_value_min 0 units m3 m-3"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#soil_moisture_dqx","title":"Soil_Moisture_DQX","text":"Field Value units m3 m-3"},{"location":"datasets/SMOS-L2C-SM-20230101-20231231-1W-res0-1x1000x1000-levels/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.9 FH-File_Class OPER FH-File_Description L2 Soil Moisture Output User Data Product FH-File_Type MIR_SMUDP2 FH-File_Version 0001 FH-Mission SMOS FH-Source-Creator L2OP FH-Source-Creator_Version 700 FH-Source-System DPGS VH-MPH-Acquisition_Station SVLD VH-MPH-Logical_Proc_Centre FPC VH-MPH-Processing_Centre ESAC VH-MPH-Product_Confidence NOMINAL VH-MPH-Ref_Doc SO-TN-IDR-GS-0006 acknowledgment ESA SMOS, DeepESDL project creator_email info@brockmann-consult.de creator_name Brockmann Consult GmbH creator_url www.brockmann-consult.de data_id SMOS-L2C-SM-20230101-20231231-1W-res0 date_modified 2024-08-19 16:19:15.359970 description Weekly means SMOS Soil Moisture 2023 geospatial_lat_max 89.0 geospatial_lat_min -89.0 geospatial_lon_max 180 geospatial_lon_min -180 institution Brockmann Consult GmbH license Creative Commons Attribution 4.0 International (CC BY 4.0) license_url https://creativecommons.org/licenses/by/4.0/ project DeepESDL publisher_email info@brockmann-consult.de publisher_name Brockmann Consult GmbH source ESA SMOS Soil Moisture temporal_coverage_end 2023-12-31 23:59:59 temporal_coverage_start 2023-01-01 00:00:00 temporal_resolution 1W title SMOS Soil Moisture Data Cube version 1.0.0"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/","title":"SMOS freeze/thaw Cube","text":""},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#smos-freeze-and-thaw-processing-and-dissemination-service","title":"SMOS Freeze and Thaw Processing and Dissemination Service","text":""},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('SMOS-snow-1x720x720-1.0.1.zarr')\n</code></pre>"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180 to 180 Bounding box latitude (\u00b0) 0 to 85 Time range 2010-07-01 to 2023-03-09 <p>Click here for full dataset metadata.</p>"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units L3FT SMOS Level 3 Freeze Thaw Estimates [none] PM Processing Mask [none] quality_flag Quality Flag [none] uncertainty Uncertainty [none]"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#l3ft","title":"L3FT","text":"Field Value flag_meanings Thaw Partial Frozen flag_values 1, 2, 3 grid_mapping crs long_name SMOS Level 3 Freeze Thaw Estimates valid_range 1, 3"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#pm","title":"PM","text":"Field Value grid_mapping crs long_name Processing Mask valid_range 0, 8"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#crs","title":"crs","text":"Field Value GeoTransform -9000000 25000 0 9000000 0 -25000 false_easting 0 false_northing 0 grid_mapping_name lambert_azimuthal_equal_area inverse_flattening 298 latitude_of_projection_origin 90 longitude_of_prime_meridian 0 longitude_of_projection_origin 0 semi_major_axis 6380000.0 spatial_ref PROJCS[\"unnamed\",GEOGCS[\"WGS 84\",DATUM[\"unknown\",SPHEROID[\"WGS84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"latitude_of_center\",90],PARAMETER[\"longitude_of_center\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]]]"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#quality_flag","title":"quality_flag","text":"Field Value grid_mapping crs long_name Quality Flag valid_range 0, 255"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#uncertainty","title":"uncertainty","text":"Field Value grid_mapping crs long_name Uncertainty valid_range 0, 100"},{"location":"datasets/SMOS-snow-1x720x720-1-0-1-zarr/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value ancillarydata_2mair Ancillary data for 2m airtemperature: ECMWF ERA Interim reanalysis (1-Jul-2010 - 31-Jul-2018) and ECMWF NRT data (1-Aug-2018 onwards) ancillarydata_snowcover Ancillary data for snow cover: IMS DAILY NORTHERN HEMISPHERE SNOW AND ICE ANALYSIS AT 4 KM contact Kimmo Rautiainen &lt;kimmo.rautiainen@fmi.fi&gt; coordinate_system Equal-Area Scalable Earth Grid 2.0 (EASE-Grid 2.0) - Northern Hemisphere data_date 20100701 incidence_angle_range 50-55 degrees latitude_range 0N - 85N longitude_range 180W - 180E moving_average 20 days orbits_included Currently only descending orbits used processing_date 2019-05-22 processing_organisation Finnish Meteorological Institute processing_software_name FMI SMOS F/T processing sowtware (Rautiainen, Cohen, Hiltunen, Ikonen, Parkkinen, Moisander, Takala 2016-2019) processing_software_version v_2.00 project_id SMOS Freeze and Thaw Processing and Dissemination Service sensor SMOS smosinputdataversion SMOS input data: CATDS v620 spatial_resolution 25 X 25 sq.km title SMOS Freeze and Thaw Processing and Dissemination Service"},{"location":"datasets/SeasFireCube_v3-zarr/","title":"SeasFireCube v3 zarr","text":""},{"location":"datasets/SeasFireCube_v3-zarr/#seasfire-cube-a-global-dataset-for-seasonal-fire-modeling-in-the-earth-system","title":"SeasFire Cube: A Global Dataset for Seasonal Fire Modeling in the Earth System","text":""},{"location":"datasets/SeasFireCube_v3-zarr/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('SeasFireCube_v3.zarr')\n</code></pre>"},{"location":"datasets/SeasFireCube_v3-zarr/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/SeasFireCube_v3-zarr/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180.0 to 180.0 Bounding box latitude (\u00b0) -90.0 to 90.0 Time range 2001-01-01 to 2021-12-27 <p>Click here for full dataset metadata.</p>"},{"location":"datasets/SeasFireCube_v3-zarr/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units area [none] m2 biomes [none] [none] cams_co2fire Wildfire flux of carbon dioxide kg m-2 cams_frpfire Wildfire radiative power W m-2 drought_code_max Drought Code, Maximum Dimensionless drought_code_mean Drought Code, Average Dimensionless fcci_ba Burned Areas from Fire Climate Change Initiative (FCCI) hectares (ha) fcci_ba_valid_mask [none] [none] fcci_fraction_of_burnable_area fraction of burnable area 0 to 1 fcci_fraction_of_observed_area fraction of observed area 0 to 1 fcci_number_of_patches number of burn patches 0 to N fwi_max Fire Weather Index, Maximum Dimensionless fwi_mean Fire Weather Index, Average Dimensionless gfed_ba Burned Areas from GFED hectares (ha) gfed_ba_valid_mask [none] [none] gfed_region GFED basis regions [none] gwis_ba Burned Areas from GWIS hectares (ha) gwis_ba_valid_mask [none] [none] lai Leaf Area Index m\u00b2/m\u00b2 lccs_class_0 Land Cover Class 0 - No data % lccs_class_1 Land Cover Class 1 - Agriculture % lccs_class_2 Land Cover Class 2 - Forest % lccs_class_3 Land Cover Class 3 - Grassland % lccs_class_4 Land Cover Class 4 - Wetland % lccs_class_5 Land Cover Class 5 - Settlement % lccs_class_6 Land Cover Class 6 - Shrubland % lccs_class_7 Land Cover Class 4 - Sparse vegetation, bare areas, permanent snow and ice % lccs_class_8 Land Cover Class 4 - Sparse vegetation, bare areas, permanent snow and ice % lsm Land-sea mask (0 - 1) lst_day Day Land Surface Temperature for Climate Modeling Grid (CMG) Kelvin (K) mslp Mean sea level pressure Pa ndvi CMG 0.05 Deg 16 days NDVI -1 to 1 oci_ao Arctic Oscillation Dimensionless oci_censo Bivariate ENSO Timeseries Dimensionless oci_ea Eastern Asia/Western Russia Dimensionless oci_epo East Pacific/North Pacific Oscillation Dimensionless oci_gmsst Global Mean Land/Ocean Temperature Dimensionless oci_nao North Atlantic Oscillation Dimensionless oci_nina34_anom Ni\u00f1o 3.4 region Dimensionless oci_pdo Pacific Decadal Oscillation Dimensionless oci_pna Pacific North American Index Dimensionless oci_soi Southern Oscillation Index Dimensionless oci_wp Western Pacific Index Dimensionless pop_dens UN WPP-Adjusted Population Density Persons per square kilometer rel_hum Relative humidity % skt Skin Temperature Kelvin (K) ssr Surface net solar radiation MJ m-2 ssrd Surface net solar radiation downwards MJ m-2 sst Sea surface temperature K swvl1 Volumetric soil water layer 1 m3 m-3 swvl2 Volumetric soil water layer 2 m3 m-3 swvl3 Volumetric soil water layer 3 m3 m-3 swvl4 Volumetric soil water layer 4 m3 m-3 t2m_max 2 meters temperature - Maximum value Kelvin (K) t2m_mean 2 meters temperature - Mean value Kelvin (K) t2m_min 2 meters temperature - Minimum Value Kelvin (K) tp Total precipitation mm vpd Vapour Pressure Deficit hPa ws10 Windspeed of the 10m wind [sqrt(u10^2+v10^2)] m*s-2"},{"location":"datasets/SeasFireCube_v3-zarr/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/SeasFireCube_v3-zarr/#area","title":"area","text":"Field Value description Square meters of each grid cell, calculated in crs 8857, WGS 84 / Equal Earth Greenwich units m2"},{"location":"datasets/SeasFireCube_v3-zarr/#biomes","title":"biomes","text":"Field Value 1 Flooded Grasslands &amp; Savannas 10 Tundra 11 Tropical &amp; Subtropical Coniferous Forests 12 Tropical &amp; Subtropical Dry Broadleaf Forests 13 Tropical &amp; Subtropical Moist Broadleaf Forests 14 Montane Grasslands &amp; Shrublandss 15 Temperate Broadleaf &amp; Mixed Forests 2 Tropical &amp; Subtropical Grasslands, Savannas &amp; Shrublands 3 Mediterranean Forests, Woodlands &amp; Scrub 4 Rock &amp; Ice 5 Mangroves 6 Temperate Grasslands, Savannas &amp; Shrublands 7 Temperate Conifer Forests 8 Deserts &amp; Xeric Shrublandss 9 Boreal Forests/Taiga biome_number biome_name description The RESOLVE Ecoregions dataset, updated in 2017, offers a depiction of the 846 terrestrial ecoregions that represent our living planet.Ecoregions, in the simplest definition, are ecosystems of regional extent. Specifically, ecoregions represent distinct assemblages of biodiversity-all taxa, not just vegetation-whose boundaries include the space required to sustain ecological processes. Ecoregions provide a useful basemap for conservation planning in particular because they draw on natural, rather than political, boundaries, define distinct biogeographic assemblages and ecological habitats within biomes, and assist in representation of Earth's biodiversity.This dataset is based on recent advances in biogeography - the science concerning the distribution of plants and animals. The original ecoregions dataset has been widely used since its introduction in 2001, underpinning the most recent analyses of the effects of global climate change on nature by ecologists to the distribution of the world's beetles to modern conservation planning.The 846 terrestrial ecoregions are grouped into 14 biomes provider RESOLVE Biodiversity and Wildlife Solutions"},{"location":"datasets/SeasFireCube_v3-zarr/#cams_co2fire","title":"cams_co2fire","text":"Field Value creator_notes Missing years filled with Nan. To convert to kg multiply by the variable square_meters. description The Global Fire Assimilation System (GFAS) assimilates fire radiative power (FRP) observations from satellite-based sensors to produce daily estimates of biomass burning emissions downloaded from https://confluence.ecmwf.int/display/CKB/CAMS%3A+Global+Fire+Assimilation+System+%28GFAS%29+data+documentation long_name Wildfire flux of carbon dioxide missing_years 2001 &amp; 2002, see Creators Notes provider ECMWF CAMS Global Fire Assimilation System (GFAS) units kg m-2"},{"location":"datasets/SeasFireCube_v3-zarr/#cams_frpfire","title":"cams_frpfire","text":"Field Value creator_notes Missing years filled with Nan. To convert to W multiply by the variable square_meters. description FRP observations currently assimilated in GFAS are the NASA Terra MODIS and Aqua MODIS active fire products (http://modis-fire.umd.edu/) downloaded_from https://confluence.ecmwf.int/display/CKB/CAMS%3A+Global+Fire+Assimilation+System+%28GFAS%29+data+documentation long_name Wildfire radiative power missing_years 2001 &amp; 2002, see creators notes provider ECMWF CAMS Global Fire Assimilation System (GFAS) units W m-2"},{"location":"datasets/SeasFireCube_v3-zarr/#drought_code_max","title":"drought_code_max","text":"Field Value description The Drought code is an indicator of the moisture content in deep compact organic layers. This code represents a fuel layer at approximately 10-20 cm deep. The Drought code fuels have a very slow drying rate, with a time lag of 52 days. The Drought code scale is open-ended, although the maximum value is about 800 downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/cems-fire-historical?tab=overview long_name Drought Code, Maximum provider Copernicus-CEMS units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#drought_code_mean","title":"drought_code_mean","text":"Field Value description The Drought code is an indicator of the moisture content in deep compact organic layers. This code represents a fuel layer at approximately 10-20 cm deep. The Drought code fuels have a very slow drying rate, with a time lag of 52 days. The Drought code scale is open-ended, although the maximum value is about 800 downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/cems-fire-historical?tab=overview long_name Drought Code, Average provider Copernicus CEMS units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#fcci_ba","title":"fcci_ba","text":"Field Value creator_notes Masked ocean with lsm variable. Missing years filled with Nan. USE ONLY for monthly modeling! FireCCI is a monthly product that could not be fairly distributed to weekly time range. Each week of the same month is filled with the same monthly value. To acquire the monthly value take the average of each month description The ESA Fire Disturbance Climate Change Initiative (CCI) project has produced maps of global burned area derived from satellite observations. The MODIS Fire_cci v5.1 grid product described here contains gridded data on global burned area derived from the MODIS instrument onboard the TERRA satellite at 250m resolution for the period 2001 to 2019. This product supercedes the previously available MODIS v5.0 product. The v5.1 dataset was initially published for 2001-2017, and has been periodically extended to include 2018 to 2020. downloaded_from https://catalogue.ceda.ac.uk/uuid/3628cb2fdba443588155e15dee8e5352 long_name Burned Areas from Fire Climate Change Initiative (FCCI) missing_years 2021 provider ESA CCI units hectares (ha)"},{"location":"datasets/SeasFireCube_v3-zarr/#fcci_ba_valid_mask","title":"fcci_ba_valid_mask","text":"Field Value description fcci_ba_valid_mask signifies the time period for which variable fcci_ba is valid. 1 valid, 0 invalid."},{"location":"datasets/SeasFireCube_v3-zarr/#fcci_fraction_of_burnable_area","title":"fcci_fraction_of_burnable_area","text":"Field Value description The fraction of burnable area is the fraction of the cell that corresponds to vegetated land covers that could burn. The land cover classes are those from CCI Land Cover, http://www.esa-landcover-cci.org/ long_name fraction of burnable area units 0 to 1"},{"location":"datasets/SeasFireCube_v3-zarr/#fcci_fraction_of_observed_area","title":"fcci_fraction_of_observed_area","text":"Field Value description The fraction of the total burnable area in the cell (fraction_of_burnable_area variable of this file) that was observed during the time interval, and was not marked as unsuitable/not observable. The latter refers to the area where it was not possible to obtain observational burned area information for the whole time interval because of lack of input data (non-existing data for that location and period). long_name fraction of observed area units 0 to 1"},{"location":"datasets/SeasFireCube_v3-zarr/#fcci_number_of_patches","title":"fcci_number_of_patches","text":"Field Value description Number of contiguous groups of burned pixels. long_name number of burn patches units 0 to N"},{"location":"datasets/SeasFireCube_v3-zarr/#fwi_max","title":"fwi_max","text":"Field Value description The Fire weather index (max and mean) is a combination of Initial spread index and Build-up index, and is a numerical rating of the potential frontal fire intensity. In effect, it indicates fire intensity by combining the rate of fire spread with the amount of fuel being consumed. Fire weather index values are not upper bounded however a value of 50 is considered as extreme in many places. The Fire weather index is used for general public information about fire danger conditions downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/cems-fire-historical?tab=overview long_name Fire Weather Index, Maximum provider Copernicus-CEMS units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#fwi_mean","title":"fwi_mean","text":"Field Value description The Fire weather index (max and mean) is a combination of Initial spread index and Build-up index, and is a numerical rating of the potential frontal fire intensity. In effect, it indicates fire intensity by combining the rate of fire spread with the amount of fuel being consumed. Fire weather index values are not upper bounded however a value of 50 is considered as extreme in many places. The Fire weather index is used for general public information about fire danger conditions downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/cems-fire-historical?tab=overview long_name Fire Weather Index, Average provider Copernicus-CEMS units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#gfed_ba","title":"gfed_ba","text":"Field Value aggregation Temporal creator_notes Masked ocean with lsm variable. Missing years filled with Nan description GFED4 dataset contains information about large fires only downloaded_from http://www.globalfiredata.org/data.html long_name Burned Areas from GFED missing years 2016-2021 provider Global Fire Emissions Database (GFED) units hectares (ha)"},{"location":"datasets/SeasFireCube_v3-zarr/#gfed_ba_valid_mask","title":"gfed_ba_valid_mask","text":"Field Value description gfed_ba_valid_mask signifies the time period for which variable gfed_ba is valid. 1 valid, 0 invalid."},{"location":"datasets/SeasFireCube_v3-zarr/#gfed_region","title":"gfed_region","text":"Field Value description 0-OCEAN, 1-BONA, 2-TENA, 3-CEAM, 4-NHSA, 5-SHSA, 6-EURO, 7-MIDE, 8-NHAF, 9-SHAF, 10-BOAS, 11-CEAS, 12-SEAS, 13-EQAS, 14-AUST. For more information visit http://globalfiredata.org/pages/data/ long_name GFED basis regions"},{"location":"datasets/SeasFireCube_v3-zarr/#gwis_ba","title":"gwis_ba","text":"Field Value aggregation Spatio-Temporal creator_notes Masked ocean with lsm variable. Missing years filled with Nan. Dataset created by using the ignition date of final burned areas description Global dataset of individual fire perimeters for 2001-2020.The dataset is in ESRI shapefile format and is derived from the MCD64A1 burned area product. Each fire shapefile has a unique fire identification code, the initial date, the final date, the geometry and a field specifying if it is a daily burned area or a final burned area. downloaded_from: https://gwis.jrc.ec.europa.eu/apps/country.profile/downloads long_name Burned Areas from GWIS missing_years 2021 provider Global Wildfire Information System (GWIS) units hectares (ha)"},{"location":"datasets/SeasFireCube_v3-zarr/#gwis_ba_valid_mask","title":"gwis_ba_valid_mask","text":"Field Value description gwis_ba_valid_mask signifies the time period for which variable gwis_ba is valid. 1 valid, 0 invalid."},{"location":"datasets/SeasFireCube_v3-zarr/#lai","title":"lai","text":"Field Value aggregation Temporal creator_notes Seasonality in Nan values for high latitudes, due to seasonality in the data availability description The MCD15A2H Version 6 Moderate Resolution Imaging Spectroradiometer (MODIS) Level 4, Combined Fraction of Photosynthetically Active Radiation (FPAR), and Leaf Area Index (LAI) product is an 8-day composite dataset with 500 meter pixel size. The algorithm chooses the best pixel available from all the acquisitions of both MODIS sensors located on NASA\u2019s Terra and Aqua satellites from within the 8-day period.LAI is defined as the one-sided green leaf area per unit ground area in broadleaf canopies and as one-half the total needle surface area per unit ground area in coniferous canopies. FPAR is defined as the fraction of incident photosynthetically active radiation (400-700 nm) absorbed by the green elements of a vegetation canopy. downloaded_from https://lpdaac.usgs.gov/products/mcd15a2hv006/ long_name Leaf Area Index provider NASA units m\u00b2/m\u00b2"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_0","title":"lccs_class_0","text":"Field Value aggregation Spatial description Class 0, contains the percentage of LCCS code [0] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 0 - No data provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_1","title":"lccs_class_1","text":"Field Value aggregation Spatial description Class 1, contains only the percentage of LCCS codes [10,11,12,20,30,40] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 1 - Agriculture provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_2","title":"lccs_class_2","text":"Field Value aggregation Spatial description Class 2, contains only the percentage of LCCS codes  [50,60,61,62,70,71,72,80,81,82,90,100] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 2 - Forest provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_3","title":"lccs_class_3","text":"Field Value aggregation Spatial description Class 3, contains only the percentage of LCCS codes [110,130] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 3 - Grassland provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_4","title":"lccs_class_4","text":"Field Value aggregation Spatial description Class 4, contains only the percentage of LCCS codes [160,170,180] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 4 - Wetland provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_5","title":"lccs_class_5","text":"Field Value aggregation Spatial description Class 5, contains only the percentage with LCCS codes [190] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 5 - Settlement provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_6","title":"lccs_class_6","text":"Field Value aggregation Spatial description Class 6, contains only the percentage of Water Bodies with LCCS codes [120,121,122] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 6 - Shrubland provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_7","title":"lccs_class_7","text":"Field Value aggregation Spatial description Class 7, contains only the percentage with LCCS codes [140,150,151,152,153,200,201,202,220] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 4 - Sparse vegetation, bare areas, permanent snow and ice provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lccs_class_8","title":"lccs_class_8","text":"Field Value aggregation Spatial description Class 8, contains only the percentage with LCCS codes [210] downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-land-cover?tab=form long_name Land Cover Class 4 - Sparse vegetation, bare areas, permanent snow and ice provider Copernicus units %"},{"location":"datasets/SeasFireCube_v3-zarr/#lsm","title":"lsm","text":"Field Value long_name Land-sea mask provider ERA5 units (0 - 1)"},{"location":"datasets/SeasFireCube_v3-zarr/#lst_day","title":"lst_day","text":"Field Value aggregation Temporal creator_notes Seasonality in Nan values for high latitudes, due to seasonality in the data availability description The MOD11C1 Version 6 product provides daily Land Surface Temperature and Emissivity (LST&amp;E) values in a 0.05 degree (5,600 meters at the equator) latitude/longitude Climate Modeling Grid (CMG). The MOD11C1 product is directly derived from the MOD11B1 product. A CMG granule follows a Geographic grid, having 7,200 columns and 3,600 rows, which represent the entire globe. Each MOD11C1 product consists of the following layers for daytime and nighttime observations: LSTs, quality control assessments, observation times, view zenith angles, number of clear-sky observations, and emissivities from bands 20, 22, 23, 29, 31, and 32 (bands 31 and 32 are daytime only) along with the percentage of land in the grid downloaded_from https://lpdaac.usgs.gov/products/mod11c1v006/ long_name Day Land Surface Temperature for Climate Modeling Grid (CMG) provider NASA units Kelvin (K)"},{"location":"datasets/SeasFireCube_v3-zarr/#mslp","title":"mslp","text":"Field Value description This parameter is the pressure (force per unit area) of the atmosphere at the surface of the Earth, adjusted to the height of mean sea level. It is a measure of the weight that all the air in a column vertically above a point on the Earth's surface would have, if the point were located at mean sea level. It is calculated over all surfaces - land, sea and inland water. Maps of mean sea level pressure are used to identify the locations of low and high pressure weather systems, often referred to as cyclones and anticyclones. Contours of mean sea level pressure also indicate the strength of the wind. Tightly packed contours show stronger winds. The units of this parameter are pascals (Pa). Mean sea level pressure is often measured in hPa and sometimes is presented in the old units of millibars, mb (1 hPa = 1 mb = 100 Pa). downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-pressure-levels?tab=overview long_name Mean sea level pressure provider ERA5 units Pa"},{"location":"datasets/SeasFireCube_v3-zarr/#ndvi","title":"ndvi","text":"Field Value aggregation Temporal creator_notes Seasonality in Nan values for high latitudes, due to seasonality in the data availability description The MOD13C1 Version 6 product provides a Vegetation Index (VI) value at a per pixel basis. There are two primary vegetation layers. The first is the Normalized Difference Vegetation Index (NDVI) which is referred to as the continuity index to the existing National Oceanic and Atmospheric Administration-Advanced Very High Resolution Radiometer (NOAA-AVHRR) derived NDVI. The second vegetation layer is the Enhanced Vegetation Index (EVI), which has improved sensitivity over high biomass regions.The Climate Modeling Grid (CMG) consists 3,600 rows and 7,200 columns of 5,600 meter (m) pixels. Global MOD13C1 data are cloud-free spatial composites of the gridded 16-day 1 kilometer MOD13A2 data, and are provided as a Level 3 product projected on a 0.05 degree (5,600 m) geographic CMG. The MOD13C1 has data fields for NDVI, EVI, VI QA, reflectance data, angular information, and spatial statistics such as mean, standard deviation, and number of used input pixels at the 0.05 degree CMG resolution. downloaded_from https://lpdaac.usgs.gov/products/mod13c1v006/ long_name CMG 0.05 Deg 16 days NDVI provider NASA units -1 to 1"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_ao","title":"oci_ao","text":"Field Value creators Notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The Arctic Oscillation (AO) is a large scale mode of climate variability, also referred to as the Northern Hemisphere annular mode. The AO is a climate pattern characterized by winds circulating counterclockwise around the Arctic at around 55\u00b0N latitude. When the AO is in its positive phase, a ring of strong winds circulating around the North Pole acts to confine colder air across polar regions. This belt of winds becomes weaker and more distorted in the negative phase of the AO, which allows an easier southward penetration of colder, arctic airmasses and increased storminess into the mid-latitudes.AO index is obtained by projecting the AO loading pattern to the daily anomaly 1000 millibar height field over 20\u00b0N-90\u00b0N latitude. The AO loading pattern has been chosen as the first mode of EOF analysis using monthly mean 1000 millibar height anomaly data from 1979 to 2000 over 20\u00b0N-90\u00b0N. downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Arctic Oscillation provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_censo","title":"oci_censo","text":"Field Value creators_notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The index was designed to be simple to calculate and to provide a long time period ENSO index for research purposes. Based on 1871-2001 SST and SOI indices downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Bivariate ENSO Timeseries provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_ea","title":"oci_ea","text":"Field Value creators_notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The East Atlantic/ West Russia (EATL/WRUS) pattern is one of three prominent teleconnection patterns that affects Eurasia throughout the year. This pattern has been referred to as the Eurasia-2 pattern by Barnston and Livezey [18]. The East Atlantic/ West Russia pattern consists of four main anomaly centres. The positive phase is associated with positive height anomalies located over Europe and northern China, and negative height anomalies located over the central North Atlantic and north of the Caspian Sea downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Eastern Asia/Western Russia provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_epo","title":"oci_epo","text":"Field Value creators_notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The East Pacific - North Pacific (EP- NP) pattern is a Spring-Summer-Fall pattern with three main anomaly centers.The positive phase of this pattern features positive height anomalies located over Alaska/ Western Canada, and negative anomalies over the central North Pacific and eastern North America fownloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name East Pacific/North Pacific Oscillation provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_gmsst","title":"oci_gmsst","text":"Field Value creators Notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description Data values are from NASA/GISS. Please read and refer to this web page plus the main web page describing various temperature indices at the main NASA/GISTEMP webpage. Note, the index is an anomaly index downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Global Mean Land/Ocean Temperature provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_nao","title":"oci_nao","text":"Field Value creators Notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The North Atlantic Oscillation Index describes changes in the strength of two recurring pressure patterns in the atmosphere over the North Atlantic: a low near Iceland, and a high near the Azores Islands. Positive NAO values indicate these features are strong, creating a big pressure difference between them. Strongly positive values are linked to warm conditions across the U.S. East and Northern Europe, and cold conditions across southern Europe.Negative NAOI indicate these features are relatively weak, and the pressure difference between them is smaller. Strongly negative values are linked to cold conditions in the U.S. East and Northern Europe, and warm conditions in Southern Europe downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name North Atlantic Oscillation provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_nina34_anom","title":"oci_nina34_anom","text":"Field Value creators Notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The Ni\u00f1o 3.4 index typically uses a 5-month running mean, and El Ni\u00f1o or La Ni\u00f1a events are defined when the Ni\u00f1o 3.4 SSTs exceed +/- 0.4C for a period of six months or more downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Ni\u00f1o 3.4 region provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_pdo","title":"oci_pdo","text":"Field Value Provider National Oceanic and Atmospheric Administration (NOAA) creator_notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The Pacific Decadal Oscillation (PDO) is often described as a long-lived El Ni\u00f1o-like pattern of Pacific climate variability. It is a pattern of Pacific climate variability similar to ENSO in character, but which varies over a much longer time scale. The PDO can remain in the same phase for 20 to 30 years, while ENSO cycles typically only last 6 to 18 months. The PDO, like ENSO, consists of a warm and cool phase which alters upper level atmospheric winds. Shifts in the PDO phase can have significant implications for global climate, affecting Pacific and Atlantic hurricane activity, droughts and flooding around the Pacific basin, the productivity of marine ecosystems, and global land temperature patterns. Experts also believe the PDO can intensify or diminish the impacts of ENSO according to its phase. If both ENSO and the PDO are in the same phase, it is believed that El Ni\u00f1o/La Nina impacts may be magnified. Conversely, if ENSO and the PDO are out of phase, it has been proposed that they may offset one another, preventing 'true' ENSO impacts from occurring downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Pacific Decadal Oscillation units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_pna","title":"oci_pna","text":"Field Value creator_notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The Pacific\u2013North American teleconnection pattern (PNA) is a climatological term for a large-scale weather pattern with two modes, denoted positive and negative, and which relates the atmospheric circulation pattern over the North Pacific Ocean with the one over the North American continent downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Pacific North American Index provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_soi","title":"oci_soi","text":"Field Value creator_notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The Southern Oscillation Index (SOI) is a standardised index based on the observed sea level pressure (SLP) differences between Tahiti and Darwin, Australia. The SOI is one measure of the large-scale fluctuations in air pressure occurring between the western and eastern tropical Pacific (i.e., the state of the Southern Oscillation) during El Ni\u00f1o and La Ni\u00f1a episodes downloaded_from https://psl.noaa.gov/data/climateindices/list/ long_name Southern Oscillation Index provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#oci_wp","title":"oci_wp","text":"Field Value creator_notes Added as a static variable. Only changing through time. No latitude and longitude dimensions description The WP pattern is a primary mode of low-frequency variability over the North Pacific in all months, and has been previously described by both Barnston and Livezey and Wallace and Gutzler downloaded from https://psl.noaa.gov/data/climateindices/list/ long_name Western Pacific Index provider National Oceanic and Atmospheric Administration (NOAA) units Dimensionless"},{"location":"datasets/SeasFireCube_v3-zarr/#pop_dens","title":"pop_dens","text":"Field Value creator_notes Data available every five years (2000,2005,2010,2015,2020). Each time dimension is filled with the method pad. description The Gridded Population of the World, Version 4 (GPWv4): Population Density Adjusted to Match 2015 Revision of UN WPP Country Totals, Revision 11 consists of estimates of human population density (number of persons per square kilometer) based on counts consistent with national censuses and population registers with respect to relative spatial distribution, but adjusted to match the 2015 Revision of the United Nation's World Population Prospects (UN WPP) country totals, for the years 2000, 2005, 2011, 2015, and 2020. A proportional allocation gridding algorithm, utilizing approximately 13.5 million national and sub-national administrative units, was used to assign UN WPP-adjusted population counts to 30 arc-second grid cells. The density rasters were created by dividing the UN WPP-adjusted population count raster for a given target year by the land area raster. The data files were produced as global rasters at 30 arc-second (~1 km at the equator) resolution. To enable faster global processing, and in support of research communities, the 30 arc-second adjusted count data were aggregated to 2.5 arc-minute, 15 arc-minute, 30 arc-minute and 1 degree resolutions to produce density rasters at these resolutions downloaded_from https://sedac.ciesin.columbia.edu/data/set/gpw-v4-population-density-adjusted-to-2015-unwpp-country-totals-rev11 long_name UN WPP-Adjusted Population Density provider Socioeconomic Data and Applications Center (sedac) units Persons per square kilometer"},{"location":"datasets/SeasFireCube_v3-zarr/#rel_hum","title":"rel_hum","text":"Field Value aggregation Temporal creator_notes This variable is calculated in-house, with Tetens formula for calculation of the saturation vapour pressure of water over liquid and ice(es) and surface pressure (Spressure).eq: Rh=(q \u00d7Spresure)/((0.622 + q)*es)*100 description This parameter is the water vapour pressure as a percentage of the value at which the air becomes saturated (the point at which water vapour begins to condense into liquid water or deposition into ice). For temperatures over 0\u00b0C (273.15 K) it is calculated for saturation over water. At temperatures below -23\u00b0C it is calculated for saturation over ice. Between -23\u00b0C and 0\u00b0C this parameter is calculated by interpolating between the ice and water values using a quadratic function. long_name Relative humidity provider ERA5 units %"},{"location":"datasets/SeasFireCube_v3-zarr/#skt","title":"skt","text":"Field Value aggregation Temporal description This parameter is the temperature of the surface of the Earth. The skin temperature is the theoretical temperature that is required to satisfy the surface energy balance. It represents the temperature of the uppermost surface layer, which has no heat capacity and so can respond instantaneously to changes in surface fluxes. Skin temperature is calculated differently over land and sea. This parameter has units of kelvin (K). Temperature measured in kelvin can be converted to degrees Celsius (\u00b0C) by subtracting 273.15 downloaded from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-pressure-levels?tab=overview long_name Skin Temperature provider ERA5 units Kelvin (K)"},{"location":"datasets/SeasFireCube_v3-zarr/#ssr","title":"ssr","text":"Field Value aggregation Temporal description This parameter is the amount of solar (shortwave) radiation reaching the surface of the Earth (both direct and diffuse) minus the amount reflected by the Earth's surface (which is governed by the albedo), assuming clear-sky (cloudless) conditions. It is the amount of radiation passing through a horizontal plane, not a plane perpendicular to the direction of the Sun downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Surface net solar radiation provider ERA5 units MJ m-2"},{"location":"datasets/SeasFireCube_v3-zarr/#ssrd","title":"ssrd","text":"Field Value aggregation Temporal description This parameter is the amount of solar radiation (also known as shortwave radiation) that reaches a horizontal plane at the surface of the Earth. This parameter comprises both direct and diffuse solar radiation. Radiation from the Sun (solar, or shortwave, radiation) is partly reflected back to space by clouds and particles in the atmosphere (aerosols) and some of it is absorbed. The rest is incident on the Earth's surface (represented by this parameter). To a reasonably good approximation, this parameter is the model equivalent of what would be measured by a pyranometer (an instrument used for measuring solar radiation) at the surface. However, care should be taken when comparing model parameters with observations, because observations are often local to a particular point in space and time, rather than representing averages over a model grid box. This parameter is accumulated over a particular time period which depends on the data extracted. For the reanalysis, the accumulation period is over the 1 hour ending at the validity date and time. For the ensemble members, ensemble mean and ensemble spread, the accumulation period is over the 3 hours ending at the validity date and time. The units are joules per square metre (J m-2 ). To convert to watts per square metre (W m-2 ), the accumulated values should be divided by the accumulation period expressed in seconds. The ECMWF convention for vertical fluxes is positive downwards. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Surface net solar radiation downwards provider ERA5-Land units MJ m-2"},{"location":"datasets/SeasFireCube_v3-zarr/#sst","title":"sst","text":"Field Value description This parameter (SST) is the temperature of sea water near the surface. In ERA5, this parameter is a foundation SST, which means there are no variations due to the daily cycle of the sun (diurnal variations). SST, in ERA5, is given by two external providers. Before September 2007, SST from the HadISST2 dataset is used and from September 2007 onwards, the OSTIA dataset is used. This parameter has units of kelvin (K). Temperature measured in kelvin can be converted to degrees Celsius (\u00b0C) by subtracting 273.15. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Sea surface temperature provider ERA5 units K"},{"location":"datasets/SeasFireCube_v3-zarr/#swvl1","title":"swvl1","text":"Field Value aggregation Temporal description This parameter is the volume of water in soil layer 1 (0 - 7cm, the surface is at 0cm). The ECMWF Integrated Forecasting System (IFS) has a four-layer representation of soil: Layer 1: 0 - 7cm, Layer 2: 7 - 28cm, Layer 3: 28 - 100cm, Layer 4: 100 - 289cm. Soil water is defined over the whole globe, even over ocean. Regions with a water surface can be masked out by only considering grid points where the land-sea mask has a value greater than 0.5 (lsm variable). The volumetric soil water is associated with the soil texture (or classification), soil depth, and the underlying groundwater level. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Volumetric soil water layer 1 provider ERA5-Land units m3 m-3"},{"location":"datasets/SeasFireCube_v3-zarr/#swvl2","title":"swvl2","text":"Field Value aggregation Temporal description This parameter is the volume of water in soil layer 2 (7 - 28cm, the surface is at 0cm). The ECMWF Integrated Forecasting System (IFS) has a four-layer representation of soil: Layer 1: 0 - 7cm, Layer 2: 7 - 28cm, Layer 3: 28 - 100cm, Layer 4: 100 - 289cm. Soil water is defined over the whole globe, even over ocean. Regions with a water surface can be masked out by only considering grid points where the land-sea mask has a value greater than 0.5. The volumetric soil water is associated with the soil texture (or classification), soil depth, and the underlying groundwater level. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Volumetric soil water layer 2 provider ERA5-Land standard_name swvl2 units m3 m-3"},{"location":"datasets/SeasFireCube_v3-zarr/#swvl3","title":"swvl3","text":"Field Value aggregation Temporal description This parameter is the volume of water in soil layer 3 (28 - 100cm, the surface is at 0cm). The ECMWF Integrated Forecasting System (IFS) has a four-layer representation of soil: Layer 1: 0 - 7cm, Layer 2: 7 - 28cm, Layer 3: 28 - 100cm, Layer 4: 100 - 289cm. Soil water is defined over the whole globe, even over ocean. Regions with a water surface can be masked out by only considering grid points where the land-sea mask has a value greater than 0.5. The volumetric soil water is associated with the soil texture (or classification), soil depth, and the underlying groundwater level. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Volumetric soil water layer 3 provider ERA5-Land standard_name swvl3 units m3 m-3"},{"location":"datasets/SeasFireCube_v3-zarr/#swvl4","title":"swvl4","text":"Field Value aggregation Temporal description This parameter is the volume of water in soil layer 4 (100 - 289cm, the surface is at 0cm). The ECMWF Integrated Forecasting System (IFS) has a four-layer representation of soil: Layer 1: 0 - 7cm, Layer 2: 7 - 28cm, Layer 3: 28 - 100cm, Layer 4: 100 - 289cm. Soil water is defined over the whole globe, even over ocean. Regions with a water surface can be masked out by only considering grid points where the land-sea mask has a value greater than 0.5. The volumetric soil water is associated with the soil texture (or classification), soil depth, and the underlying groundwater level. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Volumetric soil water layer 4 provider ERA5-Land standard_name swvl4 units m3 m-3"},{"location":"datasets/SeasFireCube_v3-zarr/#t2m_max","title":"t2m_max","text":"Field Value aggregation Temporal description This parameter is the temperature of air at 2m above the surface of land, sea or inland waters. 2m temperature is calculated by interpolating between the lowest model level and the Earth's surface, taking account of the atmospheric conditions. This parameter has units of kelvin (K). Temperature measured in kelvin can be converted to degrees Celsius (\u00b0C) by subtracting 273.15. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name 2 meters temperature - Maximum value provider ERA5-Land units Kelvin (K)"},{"location":"datasets/SeasFireCube_v3-zarr/#t2m_mean","title":"t2m_mean","text":"Field Value aggregation Temporal description This parameter is the temperature of air at 2m above the surface of land, sea or inland waters. 2m temperature is calculated by interpolating between the lowest model level and the Earth's surface, taking account of the atmospheric conditions. This parameter has units of kelvin (K). Temperature measured in kelvin can be converted to degrees Celsius (\u00b0C) by subtracting 273.15. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name 2 meters temperature - Mean value provider ERA5 units Kelvin (K)"},{"location":"datasets/SeasFireCube_v3-zarr/#t2m_min","title":"t2m_min","text":"Field Value aggregation Temporal description This parameter is the temperature of air at 2m above the surface of land, sea or inland waters. 2m temperature is calculated by interpolating between the lowest model level and the Earth's surface, taking account of the atmospheric conditions. This parameter has units of kelvin (K). Temperature measured in kelvin can be converted to degrees Celsius (\u00b0C) by subtracting 273.15. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name 2 meters temperature - Minimum Value provider ERA5 units Kelvin (K)"},{"location":"datasets/SeasFireCube_v3-zarr/#tp","title":"tp","text":"Field Value aggregation Temporal description This parameter is the accumulated liquid and frozen water, comprising rain and snow, that falls to the Earth's surface. It is the sum of large-scale precipitation and convective precipitation. Large-scale precipitation is generated by the cloud scheme in the ECMWF Integrated Forecasting System (IFS). The cloud scheme represents the formation and dissipation of clouds and large-scale precipitation due to changes in atmospheric quantities (such as pressure, temperature and moisture) predicted directly by the IFS at spatial scales of the grid box or larger. Convective precipitation is generated by the convection scheme in the IFS, which represents convection at spatial scales smaller than the grid box. This parameter does not include fog, dew or the precipitation that evaporates in the atmosphere before it lands at the surface of the Earth. This parameter is accumulated over a particular time period which depends on the data extracted. For the reanalysis, the accumulation period is over the 1 hour ending at the validity date and time. For the ensemble members, ensemble mean and ensemble spread, the accumulation period is over the 3 hours ending at the validity date and time. The units of this parameter are depth in metres of water equivalent. It is the depth the water would have if it were spread evenly over the grid box. Care should be taken when comparing model parameters with observations, because observations are often local to a particular point in space and time, rather than representing averages over a model grid box. downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Total precipitation provider ERA5 units mm"},{"location":"datasets/SeasFireCube_v3-zarr/#vpd","title":"vpd","text":"Field Value aggregation Temporal creator_notes This variable is calculated in-house,with Tetens equation (es) and relative humidity(ea = (es*relat_humid)/100). Vapour_pres_def=(es-ea)*10 description Vapour-pressure deficit, or VPD, is the difference (deficit) between the amount of moisture in the air and how much moisture the air can hold when it is saturated. Once air becomes saturated, water will condense out to form clouds, dew or films of water over leaves. long_name Vapour Pressure Deficit units hPa"},{"location":"datasets/SeasFireCube_v3-zarr/#ws10","title":"ws10","text":"Field Value aggregation Temporal description This variable is the combination of the eastward (U) and northward (V) component of wind at 10 meters, and gives the speed and direction of the horizontal wind. A negative sign indicated air moving towards south-west downloaded_from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview long_name Windspeed of the 10m wind [sqrt(u10^2+v10^2)] provider ERA5 units m*s-2"},{"location":"datasets/SeasFireCube_v3-zarr/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value crs EPSG:4326 description The SeasFire Cube is a scientific datacube for seasonal fire forecasting around the globe. It has been created for the SeasFire project, that adresses 'Earth System Deep Learning for Seasonal Fire Forecasting' and is funded by the European Space Agency (ESA)  in the context of ESA Future EO-1 Science for Society Call. It contains almost 20 years of data (2001-2021) in an 8-days time resolution and 0.25 degrees grid resolution. It has a diverse range of seasonal fire drivers. It expands from atmospheric and climatological ones to vegetation variables, socioeconomic and the target variables related to wildfires such as burned areas, fire radiative power, and wildfire-related CO2 emissions. title SeasFire Cube: A Global Dataset for Seasonal Fire Modeling in the Earth System"},{"location":"datasets/black-sea/","title":"Black Sea Cube","text":""},{"location":"datasets/black-sea/#black-sea-data-cube","title":"Black Sea Data Cube","text":""},{"location":"datasets/black-sea/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\n# The cube is saved as a multilevel cube, the level 0 is the base layer with \n# the highest resolution\nml_dataset = store.open_data('black-sea-1x1024x1024.levels')\n# Chek how many levels are present\nml_dataset.num_levels\n# Open dataset at a certain level, here level 0\nds = ml_dataset.get_dataset(0)\n</code></pre>"},{"location":"datasets/black-sea/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/black-sea/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) 26.0 to 41.998999999999725 Bounding box latitude (\u00b0) 39.000000000000156 to 48.0 Time range 2015-12-31 to 2017-12-31 Time period 1D Publisher Brockmann Consult GmbH <p>Click here for full dataset metadata.</p>"},{"location":"datasets/black-sea/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units VHM0 Spectral Significant Wave Height (Hm0) m chl Chlorophyll Concentration mg m^-3 sla Sea Level Anomaly m sss Sea Surface Salinity psu sst Sea Surface Temperature K ugos Absolute Geostrophic Velocity: Zonal Component m s^-1 ugosa Geostrophic Velocity Anomalies: Zonal Component m s^-1 vgos Absolute Geostrophic Velocity: Meridian Component m s^-1 vgosa Geostrophic Velocity Anomalies: Meridian Component m s^-1"},{"location":"datasets/black-sea/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/black-sea/#vhm0","title":"VHM0","text":"Field Value grid_mapping crs long_name Spectral Significant Wave Height (Hm0) processing_level L4 references https://resources.marine.copernicus.eu/product-detail/BLKSEA_MULTIYEAR_WAV_007_006/DOCUMENTATION source CMEMS, Black Sea Waves Reanalysis standard_name sea_surface_wave_significant_height units m valid_max 6.0 valid_min 0.0"},{"location":"datasets/black-sea/#chl","title":"chl","text":"Field Value grid_mapping crs long_name Chlorophyll Concentration processing_level L3 references http://www.eo4sibs.uliege.be/doc/EO4SIBS_DUM_ATBD_OceanColour.pdf source EO4SIBS, Level 3 Chl-a, 300m, daily and monthly standard_name chlorophyll_concentration units mg m^-3 valid_max 31.0 valid_min 0.0"},{"location":"datasets/black-sea/#crs","title":"crs","text":"Field Value crs_wkt GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]] geographic_crs_name WGS 84 grid_mapping_name latitude_longitude inverse_flattening 298.257223563 longitude_of_prime_meridian 0.0 prime_meridian_name Greenwich reference_ellipsoid_name WGS 84 semi_major_axis 6378137.0 semi_minor_axis 6356752.314245179"},{"location":"datasets/black-sea/#sla","title":"sla","text":"Field Value grid_mapping crs long_name Sea Level Anomaly processing_level L4 references http://www.eo4sibs.uliege.be/doc/EO4SIBS_D4.4_AltimetryL4_DUM_v1.1.pdf source EO4SIBS, Level 4, geostrophic currents, multi-mission gridded merged products for a period of 1 year, 0.0625\u00b0*0.0625\u00b0, daily standard_name sea_surface_height_above_sea_level units m valid_max 0.5 valid_min -0.5"},{"location":"datasets/black-sea/#sss","title":"sss","text":"Field Value grid_mapping crs long_name Sea Surface Salinity processing_level L3 references http://www.eo4sibs.uliege.be/doc/EO4SIBS_DUM_ATBD_Salinity.pdf source EO4SIBS, Level 3 SSS, 0.25\u00b0*0.25\u00b0, 9-day averaged produced daily standard_name sea_surface_salinity units psu valid_max 29.0 valid_min 1.0"},{"location":"datasets/black-sea/#sst","title":"sst","text":"Field Value grid_mapping crs long_name Sea Surface Temperature processing_level L3 references https://resources.marine.copernicus.eu/product-detail/SST_BS_SST_L3S_NRT_OBSERVATIONS_010_013/DOCUMENTATION source CMEMS, Black Sea - High Resolution and Ultra High Resolution L3S Sea Surface Temperature standard_name sea_surface_temperature units K valid_max 305.0 valid_min 270.0"},{"location":"datasets/black-sea/#ugos","title":"ugos","text":"Field Value grid_mapping crs long_name Absolute Geostrophic Velocity: Zonal Component processing_level L4 references http://www.eo4sibs.uliege.be/doc/EO4SIBS_D4.4_AltimetryL4_DUM_v1.1.pdf source EO4SIBS, Level 4, geostrophic currents, multi-mission gridded merged products for a period of 1 year, 0.0625\u00b0*0.0625\u00b0, daily standard_name surface_geostrophic_eastward_sea_water_velocity units m s^-1 valid_max 2.0 valid_min -2.0"},{"location":"datasets/black-sea/#ugosa","title":"ugosa","text":"Field Value grid_mapping crs long_name Geostrophic Velocity Anomalies: Zonal Component processing_level L4 references http://www.eo4sibs.uliege.be/doc/EO4SIBS_D4.4_AltimetryL4_DUM_v1.1.pdf source EO4SIBS, Level 4, geostrophic currents, multi-mission gridded merged products for a period of 1 year, 0.0625\u00b0*0.0625\u00b0, daily standard_name surface_geostrophic_eastward_sea_water_velocity_assuming_sea_level_for_geoid units m s^-1 valid_max 2.0 valid_min -2.0"},{"location":"datasets/black-sea/#vgos","title":"vgos","text":"Field Value grid_mapping crs long_name Absolute Geostrophic Velocity: Meridian Component processing_level L4 references http://www.eo4sibs.uliege.be/doc/EO4SIBS_D4.4_AltimetryL4_DUM_v1.1.pdf source EO4SIBS, Level 4, geostrophic currents, multi-mission gridded merged products for a period of 1 year, 0.0625\u00b0*0.0625\u00b0, daily standard_name surface_geostrophic_northward_sea_water_velocity units m s^-1 valid_max 2.0 valid_min -2.0"},{"location":"datasets/black-sea/#vgosa","title":"vgosa","text":"Field Value grid_mapping crs long_name Geostrophic Velocity Anomalies: Meridian Component processing_level L4 references http://www.eo4sibs.uliege.be/doc/EO4SIBS_D4.4_AltimetryL4_DUM_v1.1.pdf source EO4SIBS, Level 4, geostrophic currents, multi-mission gridded merged products for a period of 1 year, 0.0625\u00b0*0.0625\u00b0, daily standard_name surface_geostrophic_northward_sea_water_velocity_assuming_sea_level_for_geoid units m s^-1 valid_max 2.0 valid_min -2.0"},{"location":"datasets/black-sea/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.9 acknowledgment EO4SIBS, CMEMS, DeepESDL project contributor_name Brockmann Geomatics Sweden AB contributor_url www.brockmann-geomatics.se creator_email info@brockmann-consult.de creator_name Brockmann Consult GmbH creator_url www.brockmann-consult.de date_modified 2022-08-19 16:19:15.359970 geospatial_lat_max 47.9985 geospatial_lat_min 39.001500000000156 geospatial_lat_resolution 0.0030000000000001137 geospatial_lon_max 41.997499999999725 geospatial_lon_min 26.0015 geospatial_lon_resolution 0.0030000000000001137 id black-sea-256x256x256 institution Brockmann Consult GmbH license Terms and conditions of the DeepESDL data distribution project DeepESDL publisher_email info@brockmann-consult.de publisher_name Brockmann Consult GmbH publisher_url www.brockmann-consult.de source EO4SIBS, CMEMS time_coverage_end 2017-12-31T00:00:00.000000000 time_coverage_start 2016-01-01T00:00:00.000000000 title Black Sea Data Cube"},{"location":"datasets/datasets/","title":"List of datasets","text":""},{"location":"datasets/datasets/#datasets","title":"Datasets","text":"<p>Data access is possible in two main manners within DeepESDL. xcube  data stores provide on-the-fly access to datasets via the data stores framework. Other datasets are made anaylsis-ready and persisted in object storage for fastest access. </p> <ul> <li>xcube Data Stores</li> <li>Earth System Data Cube</li> <li>Black Sea Cube</li> <li>Land Cover Cube</li> <li>Ocean Cube</li> <li>SMOS freeze/thaw Cube</li> <li>SMOS ocean salinity Cube</li> <li>SMOS soil moistrue Cube</li> <li>Polar Cube</li> <li>Permafrost Cube</li> <li>Hydrology Cube</li> </ul>"},{"location":"datasets/datastores/","title":"xcube Data Stores","text":""},{"location":"datasets/datastores/#xcube-data-stores","title":"xcube Data Stores","text":"<p>The xcube data stores Framework is described in detail in the xcube  documentation. In this section, the currently available data stores are presented with an  example for each of them. For more detailed examples, please refer to the  getting started notebook section of the jupyterlab  user guide.</p> <ul> <li>DeepESDL public datasets</li> <li>xcube CMEMS data store</li> <li>xcube ESA CCI data store</li> <li>xcube Sentinel Hub data store</li> <li>xcube Copernicus Climate Change Service data store</li> </ul>"},{"location":"datasets/datastores/#deepesdl-datasets-in-public-object-storage","title":"DeepESDL datasets in public object storage","text":"<p>The DeepESDL Consortium provides a growing number of datasets which are made  analysis-ready and persisted in public object storage for easy access.  An overview of available persisted datasets is given section  datasets. </p> <p>To access the DeepESDL persisted datasets, please follow the example: </p> <p>Initializing the xcube datastore for s3 object storage: <pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", \n                       root=\"deep-esdl-public\", \n                       storage_options=dict(anon=True))\n</code></pre> List all available datasets: </p> <pre><code>store.list_data_ids()\n</code></pre> <p>To open a certain dataset, please replace the path with the path to the  desired dataset, which was listed in <code>store.list_data_ids()</code></p> <pre><code>dataset = store.open_data('black-sea-1x1024x1024.zarr')\n</code></pre>"},{"location":"datasets/datastores/#contribute-datacubes-to-the-public","title":"Contribute datacubes to the public","text":"<p>If you have created your own dataset and wish to make it available to the  public in the deep-esdl-public cloud bucket, that's fantastic! For onboarding new data cubes to deep-esdl-public, you need to make sure  your dataset creation is documented and reproducible, and that the  documentation is accessible. This can be e.g. your own GitHub repository, or  you could contribute to the DeepESDL cube-gen repository.</p>"},{"location":"datasets/datastores/#xcube-cmems-data-store","title":"xcube CMEMS data store","text":"<p>The xcube Copernicus Marine Data Store (CMEMS) data store allows for reading and  exploring data from the  Copernicus Marine Data Store.</p> <p>A user login is required to access the data provided by CMEMS. If you do not have cmems user yet, you can register for an  account.  For DeepESDL Jupyter Lab default credentials are configured, but due to  bandwidth  limitation by CMEMS performance may impair when used by several people simultaneously. </p> <p>To use your own CMEMS account: <pre><code># replace with your user name and pwd\n#cmemsuser='your-user-name'\n#cmemspwd='your-user-password'\n</code></pre> Initializing the xcube data store for CMEMS:</p> <pre><code>from xcube.core.store import new_data_store\nstore = new_data_store('cmems')\n</code></pre> <p>If you use your own CMEMS account: </p> <pre><code># store = new_data_store('cmems', \n#                        cmems_user=cmemsuser, \n#                        cmems_user_password = cmemspwd)\n# store = new_data_store('cmems')\n</code></pre> <p>List all available datasets: </p> <pre><code>store.list_data_ids()\n</code></pre> <p>To open a certain dataset, please replace the path with the path to the  desired dataset, which was listed in <code>store.list_data_ids()</code></p> <pre><code>dataset = store.open_data('dataset-bal-reanalysis-wav-hourly',\n                          'dataset:zarr:cmems')\n</code></pre>"},{"location":"datasets/datastores/#xcube-esa-cci-data-store","title":"xcube ESA CCI data store","text":"<p>The xcube ESA Climate Change Initiative (CCI) data store allows for reading and  exploring data from the  ESA Climate Change Initiative. More information on the data sets offered can be found in the Open Data Portal.</p> <p>Initializing the xcube data store for CCI:</p> <pre><code>from xcube.core.store import new_data_store\nstore = new_data_store('cciodp')\n</code></pre> <p>The cci data store offers many datasets, therefore listing all available ones  will reveal a long list of results. The store can thus be searched by specific  parameters, which can be listed:</p> <pre><code>store.get_search_params_schema()\n</code></pre> <p>A target data set can then be be opened following the below schema example below:</p> <pre><code>dataset = store.open_data('esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.sst',\n                          variable_names=['analysed_sst'],\n                          time_range=['1981-08-31','2016-12-31'])\n</code></pre>"},{"location":"datasets/datastores/#xcube-sentinel-hub-data-store","title":"xcube Sentinel Hub data store","text":"<p>The xcube Sentinel Hub (SH) data store allows for reading and exploring data provided by the  Sentinel Hub cloud API.</p> <p>Please note: In order to access data from the commercial Sentinel Hub service, you need Sentinel  Hub API credentials. </p> <p>DeepESDL users may apply for sponsored Sentinel Hub subscriptions -  please contact the DeepESDL team or directly appy via the Network of Resources portal.</p> <p>Initializing the xcube data store for Sentinel Hub:</p> <pre><code>from xcube.core.store import new_data_store\nstore = new_data_store('sentinelhub', num_retries=400)\n</code></pre> <p>The Sentinel Hub xcube data store offers different datasets which are  accessible via data ids:</p> <pre><code>list(store.get_data_ids())\n</code></pre> <p>For requesting Sentinel Hub data subsetting is crucial. In the below example  a Sentinel-2 L2A is requested.</p> <pre><code>dataset = store.open_data('S2L2A', \n                          variable_names=['B04'], \n                          bbox=[9.7, 53.4, 10.2, 53.7], \n                          spatial_res=0.00018, \n                          time_range=('2020-08-10','2020-08-20'), \n                          time_period='1D',\n                          tile_size= [1024, 1024])\n</code></pre>"},{"location":"datasets/datastores/#xcube-copernicus-climate-change-service-c3s-data-store","title":"xcube Copernicus Climate Change Service (C3S) data store","text":"<p>The xcube Climate Data Store (CDS) allows to read and explore temperature data from the Copernicus Climate  Change Service (C3S). </p> <p>To access data from the Climate Data Store, you need a CDS API key. You can  obtain the UID and API key as follows:</p> <ol> <li>Create a user account on the CDS Website.</li> <li>Log in to the website with your user name and password.</li> <li>Navigate to your user profile on the website. Your API key is shown        at the bottom of the page.</li> </ol> <p>Then export the <code>CDSAPI_URL</code> and <code>CDSAPI_KEY</code> environment variables,  replacing <code>[UID]</code> and <code>[API-KEY]</code> with the actual values from your account:</p> <pre><code>export CDSAPI_URL=https://cds.climate.copernicus.eu/api/v2\nexport CDSAPI_KEY=[UID]:[API-KEY]\n</code></pre> <p>For DeepESDL Jupyter Lab default credentials are configured, but due to  bandwidth limitation by CDS performance may impair when used by several  people simultaneously. </p> <p>Initializing the xcube data store for C3S:</p> <pre><code>from xcube.core.store import new_data_store\nstore = new_data_store('cds')\n</code></pre> <p>List all available datasets: </p> <pre><code>store.list_data_ids()\n</code></pre> <p>Get more info about a specific dataset. This includes a description of the  possible open formats:</p> <p><pre><code>store.describe_data('reanalysis-era5-single-levels-monthly-means:monthly_averaged_reanalysis')\n</code></pre> There are 4 required parameters, so we need to provide them to open a dataset:</p> <pre><code>dataset = store.open_data('reanalysis-era5-single-levels-monthly-means:monthly_averaged_reanalysis', \n                          variable_names=['2m_temperature'], \n                          bbox=[-10, 45, 40, 65], \n                          spatial_res=0.25, \n                          time_range=['2001-01-01', '2010-12-31'])\n</code></pre>"},{"location":"datasets/datastores/#contribute-a-xcube-plugin-for-a-new-data-store","title":"Contribute a xcube plugin for a new data store","text":"<p>Do you have an API or a data source which you wish to make available via the  xcube data stores framework? We would be very happy if you like to  contribute to the open source xcube software ecosystem!  The xcube data stores framework is described in detail in the xcube  documentation.  There you can have a look at the details of what is mandatory for a data  store. Furthermore, you can get inspiration from existing data store plugins: - xcube-cds - xcube-cci - xcube-cmems - xcube-sh</p> <p>In case of questions, please feel free to reach out to us! We will give  our best to support you :)</p>"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/","title":"Permafrost Cube","text":""},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#esa-cci-permafrost","title":"ESA CCI PERMAFROST","text":""},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\n# The cube is saved as a multilevel cube, the level 0 is the base layer with \n# the highest resolution\nml_dataset = store.open_data('esa-cci-permafrost-1x1151x1641-0.0.2.levels')\n# Chek how many levels are present\nml_dataset.num_levels\n# Open dataset at a certain level, here level 0\nds = ml_dataset.get_dataset(0)\n</code></pre>"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180 to 180 Bounding box latitude (\u00b0) 0 to 90 Time range 2000-01-01 to 2020-01-01 <p>Click here for full dataset metadata.</p>"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units GST GST surface temperature degrees celsius T10m T10m solid earth subsurface temperature degrees celsius T1m T1m solid earth subsurface temperature degrees celsius T2m T2m solid earth subsurface temperature degrees celsius T5m T5m solid earth subsurface temperature degrees celsius polar_stereographic [none] [none]"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#gst","title":"GST","text":"Field Value color_bar_name coolwarm color_value_max 30 color_value_min -30 grid_mapping polar_stereographic long_name GST surface temperature orig_data_type uint16 standard_name surface_temperature units degrees celsius valid_max 100 valid_min -100"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#t10m","title":"T10m","text":"Field Value color_bar_name coolwarm color_value_max 30 color_value_min -30 grid_mapping polar_stereographic long_name T10m solid earth subsurface temperature orig_data_type uint16 standard_name solid_earth_subsurface_temperature units degrees celsius valid_max 100 valid_min -100"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#t1m","title":"T1m","text":"Field Value color_bar_name coolwarm color_value_max 30 color_value_min -30 grid_mapping polar_stereographic long_name T1m solid earth subsurface temperature orig_data_type uint16 standard_name solid_earth_subsurface_temperature units degrees celsius valid_max 100 valid_min -100"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#t2m","title":"T2m","text":"Field Value color_bar_name coolwarm color_value_max 30 color_value_min -30 grid_mapping polar_stereographic long_name T2m solid earth subsurface temperature orig_data_type uint16 standard_name solid_earth_subsurface_temperature units degrees celsius valid_max 100 valid_min -100"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#t5m","title":"T5m","text":"Field Value color_bar_name coolwarm color_value_max 30 color_value_min -30 grid_mapping polar_stereographic long_name T5m solid earth subsurface temperature orig_data_type uint16 standard_name solid_earth_subsurface_temperature units degrees celsius valid_max 100 valid_min -100"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#polar_stereographic","title":"polar_stereographic","text":"Field Value GeoTransform -8679599.425969256 926.6254331383326 0 4120958.560533902 0 -926.6254331383326 chunk_sizes crs_wkt PROJCRS[\"WGS 84 / Arctic Polar Stereographic\",BASEGEOGCRS[\"WGS 84\",ENSEMBLE[\"World Geodetic System 1984 ensemble\",MEMBER[\"World Geodetic System 1984 (Transit)\"],MEMBER[\"World Geodetic System 1984 (G730)\"],MEMBER[\"World Geodetic System 1984 (G873)\"],MEMBER[\"World Geodetic System 1984 (G1150)\"],MEMBER[\"World Geodetic System 1984 (G1674)\"],MEMBER[\"World Geodetic System 1984 (G1762)\"],MEMBER[\"World Geodetic System 1984 (G2139)\"],ELLIPSOID[\"WGS 84\",6378137,298.257223563,LENGTHUNIT[\"metre\",1]],ENSEMBLEACCURACY[2.0]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433]],ID[\"EPSG\",4326]],CONVERSION[\"Arctic Polar Stereographic\",METHOD[\"Polar Stereographic (variant B)\",ID[\"EPSG\",9829]],PARAMETER[\"Latitude of standard parallel\",71,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8832]],PARAMETER[\"Longitude of origin\",0,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8833]],PARAMETER[\"False easting\",0,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",0,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]]],CS[Cartesian,2],AXIS[\"easting (X)\",south,MERIDIAN[90,ANGLEUNIT[\"degree\",0.0174532925199433]],ORDER[1],LENGTHUNIT[\"metre\",1]],AXIS[\"northing (Y)\",south,MERIDIAN[180,ANGLEUNIT[\"degree\",0.0174532925199433]],ORDER[2],LENGTHUNIT[\"metre\",1]],USAGE[SCOPE[\"Polar research.\"],AREA[\"Northern hemisphere - north of 60\u00b0N onshore and offshore, including Arctic.\"],BBOX[60,-180,90,180]],ID[\"EPSG\",3995]] data_type int64 dimensions esri_wkt PROJCS[\\\"WGS_84_Arctic_Polar_Stereographic\\\",GEOGCS[\\\"GCS_WGS_1984\\\",DATUM[\\\"D_WGS_1984\\\",SPHEROID[\\\"WGS_1984\\\",6378137,298.257223563]],PRIMEM[\\\"Greenwich\\\",0],UNIT[\\\"Degree\\\",0.017453292519943295]],PROJECTION[\\\"Stereographic_North_Pole\\\"],PARAMETER[\\\"standard_parallel_1\\\",71],PARAMETER[\\\"central_meridian\\\",0],PARAMETER[\\\"false_easting\\\",0],PARAMETER[\\\"false_northing\\\",0],UNIT[\\\"Meter\\\",1]] false_easting 0.0 false_northing 0.0 file_chunk_sizes 1 file_dimensions fill_value 9223372036854775807 geographic_crs_name WGS 84 grid_mapping_name polar_stereographic horizontal_datum_name World Geodetic System 1984 ensemble inverse_flattening 298.257223563 latitude_of_projection_origin 90.0 longitude_of_prime_meridian 0.0 orig_data_type int32 prime_meridian_name Greenwich projected_crs_name WGS 84 / Arctic Polar Stereographic reference_ellipsoid_name WGS 84 semi_major_axis 6378137.0 semi_minor_axis 6356752.314245179 shape size 1 spatial_ref GEOGCS[\\\"WGS 84\\\",DATUM[\\\"WGS_1984\\\",SPHEROID[\\\"WGS 84\\\",6378137,298.257223563,AUTHORITY[\\\"EPSG\\\",\\\"7030\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"6326\\\"]],PRIMEM[\\\"Greenwich\\\",0],UNIT[\\\"degree\\\",0.0174532925199433],AUTHORITY[\\\"EPSG\\\",\\\"4326\\\"]],PROJECTION[\\\"Polar_Stereographic\\\"],PARAMETER[\\\"latitude_of_origin\\\",45],PARAMETER[\\\"central_meridian\\\",-170],PARAMETER[\\\"scale_factor\\\",1],PARAMETER[\\\"false_easting\\\",0],PARAMETER[\\\"false_northing\\\",0],UNIT[\\\"metre\\\",1,AUTHORITY[\\\"EPSG\\\",\\\"9001\\\"]] standard_parallel 71.0 straight_vertical_longitude_from_pole 0.0"},{"location":"datasets/esa-cci-permafrost-1x1151x1641-0-0-2-zarr/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.7 date_created 2023-05-08 history cube_params: time_range: 2019-01-01T00:00:00, 2019-12-31T00:00:00, variable_names: polar_stereographic, GST, T1m, T2m, T5m, T10m, program: xcube_cci.chunkstore.CciChunkStore id esa-cci-permafrost-1x1151x1641-0.0.2 processing_level L4 project DeepESDL source esacci.PERMAFROST.yr.L4.GTD.multi-sensor.multi-platform.ERA5_MODISLST_BIASCORRECTED.03-0.r1, esacci.PERMAFROST.yr.L4.GTD.multi-sensor.multi-platform.MODISLST_CRYOGRID.03-0.r1 time_coverage_duration P365DT0H0M0S time_coverage_end 2019-07-02T12:00:00 time_coverage_start 2000-07-02T12:00:00 title ESA CCI PERMAFROST"},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/","title":"extrAIM merged cube 1x86x179 zarr","text":""},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#merged-extraim-dataset","title":"Merged extrAIM dataset","text":""},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('extrAIM-merged-cube-1x86x179.zarr')\n</code></pre>"},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -6.375 to 38.375 Bounding box latitude (\u00b0) 27.625 to 49.125 Time range 2007-01-01 to 2021-10-01 Time period 1D <p>Click here for full dataset metadata.</p>"},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units precip [none] mm"},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#precip","title":"precip","text":"Field Value color_bar_name coolwarm color_value_max 40 color_value_min 0 units mm"},{"location":"datasets/extrAIM-merged-cube-1x86x179-zarr/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value history The extrAIM consortium, Sat Feb 24 12:41:25 2024 institution extrAIM title Merged extrAIM dataset"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/","title":"Hydrology Cube","text":""},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#hydrology-cube","title":"Hydrology Cube","text":""},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('hydrology-1D-0.009deg-100x60x60-3.0.2.zarr')\n</code></pre>"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -5.70080002975464 to 37.76919997024536 Bounding box latitude (\u00b0) 28.339799316406264 to 48.17579931640626 Time range 2014-12-31 to 2022-10-06 Time period 1D Publisher DeepESDL Team <p>Click here for full dataset metadata.</p>"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units E Evaporation mm d^-1 SM Soil Moisture % Relative Saturation SWE Snow Water Equivalent 1 precip Precipitation mm d^-1"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#e","title":"E","text":"Field Value acknowledgement Hydrology 4D color_bar_name plasma color_value_max 10 color_value_min 0 description Evaporation long_name Evaporation original_add_offset 0.0 original_name E original_scale_factor 1.0 processing_steps Gridding nc datasets source 4dmed_data.eodchosting.eu/4dmed_data/GLEAM_openloop_V1.1 standard_name evaporation units mm d^-1"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#sm","title":"SM","text":"Field Value acknowledgement Hydrology 4D color_bar_name plasma_r color_value_max 1 color_value_min 0 description TU Wien RT1-Sentinel-1 soil moisutre retrievals long_name Soil Moisture original_add_offset 0.0 original_name SM original_scale_factor 1.0 processing_steps Gridding nc datasets, daily aggregates source 4dmed_data.eodchosting.eu/4dmed_data/TUWien_RT1_SM standard_name soil_moisture units % Relative Saturation untis % relative saturation"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#swe","title":"SWE","text":"Field Value acknowledgement Hydrology 4D color_bar_name Blues_alpha color_value_max 2000 color_value_min 0 description Snow Water Equivalent long_name Snow Water Equivalent original_add_offset 0.0 original_name SWE original_scale_factor 1.0 processing_steps Gridding nc datasets source 4dmed_data.eodchosting.eu/4dmed_data/SWE/SWE_CPC_GPM_ERA5downT_RadGhent_filter5mm standard_name snow_water_equivalent units 1"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#precip","title":"precip","text":"Field Value acknowledgement Hydrology 4D color_bar_name viridis_alpha color_value_max 100 color_value_min 0 description Precipitation long_name Precipitation original_add_offset 0.0 original_name precip original_scale_factor 1.0 processing_steps Gridding nc datasets source 4dmed_data.eodchosting.eu/4dmed_data/CNR_products/precipitation_GPM_CPC_SM2RAIN-ASCAT standard_name precipitation units mm d^-1"},{"location":"datasets/hydrology-1D-0-009deg-100x60x60-3-0-2-zarr/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.10 acknowledgment All data providers are acknowledged inside each variable contributor_name University of Leipzig, Brockmann Consult GmbH contributor_url https://www.uni-leipzig.de/, https://www.brockmann-consult.de/ creator_name University of Leipzig, Brockmann Consult GmbH creator_url https://www.uni-leipzig.de/, https://www.brockmann-consult.de/ date_modified 2023-12-21T11:50:17.830496 geospatial_lat_max 48.17579932 geospatial_lat_min 28.33979932 geospatial_lat_resolution 0.009 geospatial_lat_units degrees_north geospatial_lon_max 37.76919997 geospatial_lon_min -5.70080003 geospatial_lon_resolution 0.009 geospatial_lon_units degrees_east id hydrology-1D-0.009deg-100x60x60-3.0.2.zarr license Terms and conditions of the DeepESDL data distribution project DeepESDL publisher_name DeepESDL Team publisher_url https://www.earthsystemdatalab.net/ time_coverage_end 2022-10-06T12:00:00.000000000 time_coverage_start 2015-01-01T12:00:00.000000000 title Hydrology Cube"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/","title":"Ocean Cube","text":""},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#bicep-pools-and-fluxes-of-the-ocean-biological-carbon-pump","title":"BICEP Pools and Fluxes of the Ocean Biological Carbon Pump","text":""},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\n# The cube is saved as a multilevel cube, the level 0 is the base layer with \n# the highest resolution\nml_dataset = store.open_data('ocean-1M-9km-1x1080x1080-1.4.0.levels')\n# Chek how many levels are present\nml_dataset.num_levels\n# Open dataset at a certain level, here level 0\nds = ml_dataset.get_dataset(0)\n</code></pre>"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -180.00001525878906 to 180.00001525878906 Bounding box latitude (\u00b0) -89.99999491138114 to 89.99999491138114 Time range 1997-09-01 to 2020-12-01 Publisher DeepESDL Team <p>Click here for full dataset metadata.</p>"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units C_microphyto Microphytoplankton Carbon mg C m^-3 C_nanophyto Nanophytoplankton Carbon mg C m^-3 C_phyto Total Phytoplankton Carbon mg C m^-3 C_picophyto Picophytoplankton Carbon mg C m^-3 DOC Dissolved Organic Carbon \u00b5mol kg^-1 EP_Dunne Export Production based on Dunne et al 2005 mg C m^-2 d^-1 EP_Henson Export Production based on Henson et al 2011 mg C m^-2 d^-1 EP_Li Export Production based on Li et al 2016 mg C m^-2 d^-1 PIC Particulate Inorganic Carbon \u00b5mol C m^-3 POC Particulate Organic Carbon mg C m^-3 chl_a Chlorophyll-a mg C m^-3 mld Mixed Layer Depth m pp Phytoplankton Primary Production mg C m^-2 d^-1"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#c_microphyto","title":"C_microphyto","text":"Field Value acknowledgement BICEP/NCEO color_bar_name YlGnBu_r color_value_max 200 color_value_min 0 description Microphytoplankton Carbon in Sea Water long_name Microphytoplankton Carbon original_add_offset 0.0 original_name C_microphyto original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/PC_1-month_9km/ standard_name mass_concentration_of_microphytoplankton_expressed_as_carbon_in_sea_water units mg C m^-3"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#c_nanophyto","title":"C_nanophyto","text":"Field Value acknowledgement BICEP/NCEO color_bar_name YlGnBu_r color_value_max 200 color_value_min 0 description Nanophytoplankton Carbon in Sea Water long_name Nanophytoplankton Carbon original_add_offset 0.0 original_name C_nanophyto original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/PC_1-month_9km/ standard_name mass_concentration_of_nanophytoplankton_expressed_as_carbon_in_sea_water units mg C m^-3"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#c_phyto","title":"C_phyto","text":"Field Value acknowledgement BICEP/NCEO color_bar_name YlGnBu_r color_value_max 200 color_value_min 0 description Total Phytoplankton Carbon in Sea Water long_name Total Phytoplankton Carbon original_add_offset 0.0 original_name C_phyto original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/PC_1-month_9km/ standard_name mass_concentration_of_phytoplankton_expressed_as_carbon_in_sea_water units mg C m^-3"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#c_picophyto","title":"C_picophyto","text":"Field Value acknowledgement BICEP/NCEO color_bar_name YlGnBu_r color_value_max 200 color_value_min 0 description Picophytoplankton Carbon in Sea Water long_name Picophytoplankton Carbon original_add_offset 0.0 original_name C_picophyto original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/PC_1-month_9km/ standard_name mass_concentration_of_picophytoplankton_expressed_as_carbon_in_sea_water units mg C m^-3"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#doc","title":"DOC","text":"Field Value Derived from In-situ DOC, Ocean Colour, SST, Primary Production, Salinity acknowledgement BICEP/NCEO color_bar_name RdPu_r color_value_max 150 color_value_min 0 description Dissolved organic carbon estimated using EO data and random forest algorithm long_name Dissolved Organic Carbon original_add_offset 0.0 original_name DOC original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/DOC_1-month_9km/ standard_name dissolved_organic_carbon unit \u00b5mol/kg units \u00b5mol kg^-1"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#ep_dunne","title":"EP_Dunne","text":"Field Value Algorithm description DOI 10.1029/2004gb002390 Derived from SST, Chl, Primary Production, Z_eu acknowledgement BICEP/NCEO color_bar_name Blues color_value_max 300 color_value_min 0 data_bins 3954469 data_maximum 0.0 data_minimum 0.0 description Export Production based on Dunne et al 2005 long_name Export Production based on Dunne et al 2005 original_add_offset 0.0 original_name EP_Dunne original_scale_factor 1.0 processing_steps Gridding nc datasets references https://doi.org/10.1029/2004gb002390, https://catalogue.ceda.ac.uk/uuid/a6fc730d88fd4935b59d64903715d891 source https://data.ceda.ac.uk/neodc/bicep/data/oceanic_export_production/v1.0/monthly/ standard_name export_production_dunne unit mg C m-2 d-1 units mg C m^-2 d^-1"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#ep_henson","title":"EP_Henson","text":"Field Value Algorithm description DOI 10.1029/2011gl046735 Derived from SST, Primary Production acknowledgement BICEP/NCEO color_bar_name Blues color_value_max 300 color_value_min 0 data_bins 3954469 data_maximum 0.0 data_minimum 0.0 description Export Production based on Henson et al 2011 long_name Export Production based on Henson et al 2011 original_add_offset 0.0 original_name EP_Henson original_scale_factor 1.0 processing_steps Gridding nc datasets references https://doi.org/10.1029/2011gl046735, https://catalogue.ceda.ac.uk/uuid/a6fc730d88fd4935b59d64903715d891 source https://data.ceda.ac.uk/neodc/bicep/data/oceanic_export_production/v1.0/monthly/ standard_name export_production_henson unit mg C m-2 d-1 units mg C m^-2 d^-1"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#ep_li","title":"EP_Li","text":"Field Value Algorithm description DOI 10.1002/2015gb005314 Derived from SST, Primary Production acknowledgement BICEP/NCEO color_bar_name Blues color_value_max 300 color_value_min 0 data_bins 3954469 data_maximum 0.0 data_minimum 0.0 description Export Production based on Li et al 2016 long_name Export Production based on Li et al 2016 original_add_offset 0.0 original_name EP_Li original_scale_factor 1.0 processing_steps Gridding nc datasets references https://doi.org/10.1002/2015gb005314, https://catalogue.ceda.ac.uk/uuid/a6fc730d88fd4935b59d64903715d891 source https://data.ceda.ac.uk/neodc/bicep/data/oceanic_export_production/v1.0/monthly/ standard_name export_production_li unit mg C m-2 d-1 units mg C m^-2 d^-1"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#pic","title":"PIC","text":"Field Value acknowledgement BICEP/NCEO color_bar_name Blues_r color_value_max 0.001 color_value_min 0 description Particulate Inorganic Carbon long_name Particulate Inorganic Carbon max 0.0015023552358215274 min 7.649861515675783e-05 original_add_offset 0.0 original_name PIC original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/PIC_1-month_9km/ standard_name particulate_inorganic_carbon units \u00b5mol C m^-3"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#poc","title":"POC","text":"Field Value acknowledgement BICEP/NCEO color_bar_name plasma color_value_max 600 color_value_min 0 description Particulate Organic Carbon long_name Particulate Organic Carbon original_add_offset 0.0 original_name POC original_scale_factor 1.0 processing_steps Gridding nc datasets reference(s) see https://bicep-project.org/Deliverables source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/POC_1-month_9km/ standard_name particulate_organic_carbon units mg C m^-3"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#chl_a","title":"chl_a","text":"Field Value acknowledgement BICEP/NCEO color_bar_name viridis color_value_max 50 color_value_min 0 description Chlorophyll-a long_name Chlorophyll-a original_add_offset 0.0 original_name chl_a original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/PC_1-month_9km/ standard_name mass_concentration_of_chlorophyll_a_in_sea_water units mg C m^-3"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#mld","title":"mld","text":"Field Value acknowledgement BICEP/NCEO color_bar_name viridis color_value_max 500 color_value_min 0 description Mixed Layer Depth long_name Mixed Layer Depth original_add_offset 0.0 original_name mld original_scale_factor 1.0 processing_steps Gridding nc datasets source https://rsg.pml.ac.uk/shared_files/gku/ESA_animation/PC_1-month_9km/ standard_name mixed_layer_depth units m"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#pp","title":"pp","text":"Field Value acknowledgement BICEP/NCEO color_bar_name Spectral_r color_value_max 1000 color_value_min 0 description Phytoplankton Primary Production long_name Phytoplankton Primary Production original_add_offset 0.0 original_name pp original_scale_factor 1.0 processing_steps Gridding nc datasets references https://catalogue.ceda.ac.uk/uuid/69b2c9c6c4714517ba10dab3515e4ee6 source https://data.ceda.ac.uk/neodc/bicep/data/marine_primary_production/v4.2/monthly/ standard_name gross_primary_production_of_carbon units mg C m^-2 d^-1"},{"location":"datasets/ocean-1M-9km-1x1080x1080-1-4-0-zarr/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.10 acknowledgment All data providers are acknowledged inside each variable contributor_name University of Leipzig, Brockmann Consult GmbH contributor_url https://www.uni-leipzig.de/, https://www.brockmann-consult.de/ creator_name University of Leipzig, Brockmann Consult GmbH creator_url https://www.uni-leipzig.de/, https://www.brockmann-consult.de/ date_modified 2023-04-05 12:02:59.047482 geospatial_lat_max 89.95832824707031 geospatial_lat_min -89.95832824707031 geospatial_lat_resolution 0.0833282470703125 geospatial_lon_max 179.95834350585938 geospatial_lon_min -179.95834350585938 geospatial_lon_resolution 0.083343505859375 id ocean-1M-9km-1x1080x1080-v1.4.0.zarr license Terms and conditions of the DeepESDL data distribution project DeepESDL publisher_name DeepESDL Team publisher_url https://www.earthsystemdatalab.net/ time_coverage_end 2020-12-01T00:00:00.000000000 time_coverage_start 1997-09-01T00:00:00.000000000 title BICEP Pools and Fluxes of the Ocean Biological Carbon Pump"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/","title":"Polar Cube","text":""},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#polar-data-cube-v101","title":"Polar Data Cube v1.0.1","text":""},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#how-to-open-this-dataset-in-deepesdl-jupyterlab","title":"How to open this dataset in DeepESDL JupyterLab","text":"<pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\nds = store.open_data('polar-100m-1x2048x2048-1.0.1.zarr')\n</code></pre>"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#bounding-box-map","title":"Bounding box map","text":"<p> Map tiles and data from OpenStreetMap, under the ODbL.</p>"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#basic-information","title":"Basic information","text":"Parameter Value Bounding box longitude (\u00b0) -109.3 to -100 Bounding box latitude (\u00b0) -70 to -77 Time range 2013-01-01 to 2017-01-01 Time period 1461D Publisher DeepESDL Team <p>Click here for full dataset metadata.</p>"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#variable-list","title":"Variable list","text":"<p>Click on a variable name to jump to the variable\u2019s full metadata.</p> Variable Long name Units curie_depth_200km Curie Depth Estimates 200x200 km m curie_depth_300km Curie Depth Estimates 300x300 km m geothermal_heat_flow_200km Geothermal Heat Flow 200x200 km m geothermal_heat_flow_300km Geothermal Heat Flow 300x300 km m geothermal_heat_flow_uncertainty_200km Geothermal Heat Flow Uncertainty 200x200 km m geothermal_heat_flow_uncertainty_300km Geothermal Heat Flow Uncertainty 300x300 km m ice_thickness Ice Thickness m magnetic_anomaly Magnetic Anomaly nT thw_124 Subglacial Lake Thw 124 1 thw_142 Subglacial Lake Thw 142 1 thw_170 Subglacial Lake Thw 170 1 thw_70 Subglacial Lake Thw 70 1"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#full-variable-metadata","title":"Full variable metadata","text":""},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#crs","title":"crs","text":"Field Value crs_wkt PROJCS[\"WGS 84 / Antarctic Polar Stereographic\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Polar_Stereographic\"],PARAMETER[\"latitude_of_origin\",-71],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",NORTH],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3031\"]] false_easting 0.0 false_northing 0.0 geographic_crs_name WGS 84 grid_mapping_name polar_stereographic horizontal_datum_name World Geodetic System 1984 inverse_flattening 298.257223563 longitude_of_prime_meridian 0.0 prime_meridian_name Greenwich projected_crs_name WGS 84 / Antarctic Polar Stereographic reference_ellipsoid_name WGS 84 semi_major_axis 6378137.0 semi_minor_axis 6356752.314245179 spatial_ref PROJCS[\"WGS 84 / Antarctic Polar Stereographic\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Polar_Stereographic\"],PARAMETER[\"latitude_of_origin\",-71],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",NORTH],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3031\"]] standard_parallel -71.0 straight_vertical_longitude_from_pole 0.0"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#curie_depth_200km","title":"curie_depth_200km","text":"Field Value acknowledgement 4DAntarctica description Curie depth estimates using a 200x200km window grid_mapping crs long_name Curie Depth Estimates 200x200 km original_add_offset 0.0 original_name z original_scale_factor 1.0 processing_steps Gridding nc dataset, Spatial resampling to 100 m using linear interpolation references https://doi.org/10.1594/PANGAEA.932452, https://doi.org/10.1038/s43247-021-00242-3 source https://download.pangaea.de/dataset/932452/files/200km_Dziadek_etal_CurieDepth_Results.nc standard_name curie_depth_estimates_200km units m"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#curie_depth_300km","title":"curie_depth_300km","text":"Field Value acknowledgement 4DAntarctica description Curie depth estimates using a 300x300km window grid_mapping crs long_name Curie Depth Estimates 300x300 km original_add_offset 0.0 original_name z original_scale_factor 1.0 processing_steps Gridding nc dataset, Spatial resampling to 100 m using linear interpolation references https://doi.org/10.1594/PANGAEA.932452, https://doi.org/10.1038/s43247-021-00242-3 source https://download.pangaea.de/dataset/932452/files/300km_Dziadek_etal_CurieDepth_Results.nc standard_name curie_depth_estimates_300km units m"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#geothermal_heat_flow_200km","title":"geothermal_heat_flow_200km","text":"Field Value acknowledgement 4DAntarctica description Geothermal Heat Flow from a 200x200km window grid_mapping crs long_name Geothermal Heat Flow 200x200 km original_add_offset 0.0 original_name z original_scale_factor 1.0 processing_steps Gridding nc dataset, Spatial resampling to 100 m using linear interpolation references https://doi.org/10.1594/PANGAEA.932452, https://doi.org/10.1038/s43247-021-00242-3 source https://download.pangaea.de/dataset/932452/files/200km_GHF_windowCenters.nc standard_name geothermal_heat_flow_200km units m"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#geothermal_heat_flow_300km","title":"geothermal_heat_flow_300km","text":"Field Value acknowledgement 4DAntarctica description Geothermal Heat Flow from a 300x300km window grid_mapping crs long_name Geothermal Heat Flow 300x300 km original_add_offset 0.0 original_name z original_scale_factor 1.0 processing_steps Gridding nc dataset, Spatial resampling to 100 m using linear interpolation references https://doi.org/10.1594/PANGAEA.932452, https://doi.org/10.1038/s43247-021-00242-3 source https://download.pangaea.de/dataset/932452/files/300km_GHF_windowCenters.nc standard_name geothermal_heat_flow_300km units m"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#geothermal_heat_flow_uncertainty_200km","title":"geothermal_heat_flow_uncertainty_200km","text":"Field Value acknowledgement 4DAntarctica description Geothermal Heat Flow Uncertainty from a 200x200km window grid_mapping crs long_name Geothermal Heat Flow Uncertainty 200x200 km original_add_offset 0.0 original_name z original_scale_factor 1.0 processing_steps Gridding nc dataset, Spatial resampling to 100 m using linear interpolation references https://doi.org/10.1594/PANGAEA.932452, https://doi.org/10.1038/s43247-021-00242-3 source https://download.pangaea.de/dataset/932452/files/200km_GHF_uncertainty.nc standard_name geothermal_heat_flow_uncertainty_200km units m"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#geothermal_heat_flow_uncertainty_300km","title":"geothermal_heat_flow_uncertainty_300km","text":"Field Value acknowledgement 4DAntarctica description Geothermal Heat Flow Uncertainty from a 300x300km window grid_mapping crs long_name Geothermal Heat Flow Uncertainty 300x300 km original_add_offset 0.0 original_name z original_scale_factor 1.0 processing_steps Gridding nc dataset, Spatial resampling to 100 m using linear interpolation references https://doi.org/10.1594/PANGAEA.932452, https://doi.org/10.1038/s43247-021-00242-3 source https://download.pangaea.de/dataset/932452/files/300km_GHF_uncertainty.nc standard_name geothermal_heat_flow_uncertainty_300km units m"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#ice_thickness","title":"ice_thickness","text":"Field Value acknowledgement CryoSMOS description Ice Thickness long_name Ice Thickness original_add_offset 0.0 original_name est original_scale_factor 1.0 processing_steps Masking out no-data values, Slicing dataset to the polar region (&lt; -60 degrees), Reprojecting to EPSG:3031, Spatial resampling to 100 m using linear interpolation references https://opensciencedata.esa.int/products/bedrock-topography-antarctica-cryosmos, http://www.ifac.cnr.it/cryosmos/products/CryoSMOS_D8_EDUM_V2.0.pdf source http://www.ifac.cnr.it/cryosmos/products/SMOS_IceThickness_2015_300.nc standard_name ice_thickness units m"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#magnetic_anomaly","title":"magnetic_anomaly","text":"Field Value acknowledgement 4DAntarctica description Magnetic Anomaly long_name Magnetic Anomaly original_add_offset 0.0 original_name original_scale_factor 1.0 processing_steps Spatial resampling to 100 m using linear interpolation references https://doi.org/10.1594/PANGAEA.932452, https://doi.org/10.1038/s43247-021-00242-3 source https://download.pangaea.de/dataset/932452/files/ASE_MagneticCompilation_Dziadeketal_250m.tif standard_name magnetic_anomaly units nT"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#thw_124","title":"thw_124","text":"Field Value acknowledgement 4DAntarctica color_bar_name GnBu description Subglacial Lake Thw 124 grid_mapping crs long_name Subglacial Lake Thw 124 original_add_offset 0.0 original_name original_scale_factor 1.0 processing_steps Spatial resampling to 100 m using nearest neighbor references https://doi.org/10.1029/2020GL089658 source https://4d-antarctica.org/wp-content/uploads/2021/01/Malczyk_etal_2020_data_v2.tar standard_name thw_124 time_coverage_end 2017-01-01 time_coverage_start 2013-01-01 units 1"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#thw_142","title":"thw_142","text":"Field Value acknowledgement 4DAntarctica color_bar_name GnBu description Subglacial Lake Thw 142 grid_mapping crs long_name Subglacial Lake Thw 142 original_add_offset 0.0 original_name original_scale_factor 1.0 processing_steps Spatial resampling to 100 m using nearest neighbor references https://doi.org/10.1029/2020GL089658 source https://4d-antarctica.org/wp-content/uploads/2021/01/Malczyk_etal_2020_data_v2.tar standard_name thw_142 time_coverage_end 2017-01-01 time_coverage_start 2013-01-01 units 1"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#thw_170","title":"thw_170","text":"Field Value acknowledgement 4DAntarctica color_bar_name GnBu description Subglacial Lake Thw 170 grid_mapping crs long_name Subglacial Lake Thw 170 original_add_offset 0.0 original_name original_scale_factor 1.0 processing_steps Spatial resampling to 100 m using nearest neighbor references https://doi.org/10.1029/2020GL089658 source https://4d-antarctica.org/wp-content/uploads/2021/01/Malczyk_etal_2020_data_v2.tar standard_name thw_170 time_coverage_end 2017-01-01 time_coverage_start 2013-01-01 units 1"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#thw_70","title":"thw_70","text":"Field Value acknowledgement 4DAntarctica color_bar_name GnBu description Subglacial Lake Thw 70 grid_mapping crs long_name Subglacial Lake Thw 70 original_add_offset 0.0 original_name original_scale_factor 1.0 processing_steps Spatial resampling to 100 m using nearest neighbor references https://doi.org/10.1029/2020GL089658 source https://4d-antarctica.org/wp-content/uploads/2021/01/Malczyk_etal_2020_data_v2.tar standard_name thw_70 time_coverage_end 2017-01-01 time_coverage_start 2013-01-01 units 1"},{"location":"datasets/polar-100m-1x2048x2048-1-0-1-zarr/#full-dataset-metadata","title":"Full dataset metadata","text":"Field Value Conventions CF-1.10 acknowledgment All ESDC data providers are acknowledged inside each variable contributor_name University of Leipzig, Brockmann Consult GmbH contributor_url https://www.uni-leipzig.de/, https://www.brockmann-consult.de/ creator_name University of Leipzig, Brockmann Consult GmbH creator_url https://www.uni-leipzig.de/, https://www.brockmann-consult.de/ id polar-100m-1x2048x2048-1.0.1 license Terms and conditions of the DeepESDL data distribution project DeepESDL publisher_name DeepESDL Team publisher_url https://www.earthsystemdatalab.net/ spatial_ref EPSG:3031 time_coverage_end 2017-01-01 time_coverage_start 2013-01-01 title Polar Data Cube v1.0.1"},{"location":"design/","title":"Overview","text":""},{"location":"design/#deepesdl-design-overview","title":"DeepESDL Design Overview","text":"<ol> <li>Context</li> <li>Architecture</li> <li>Hub</li> <li>Services and Tools</li> </ol>"},{"location":"design/architecture/","title":"Architecture","text":""},{"location":"design/architecture/#deepesdl-architecture","title":"DeepESDL Architecture","text":"<p>The following figure depicts the high-level concepts and the architecture  of the DeepESDL project. It comprises the internal DeepESDL Hub  and user projects and the publicly visible parts. Both are served by a  common infrastructure and common resources.</p> <p></p> <p>Applications available to users within DeepESDL comprise:</p> <ul> <li>An xcube Catalogue to browse available data, including a web page    that lists available datasets as a low-barrier entry point for users.    (In development, will be similar to the    ESA EuroDataCube Collections.) </li> <li>Visualisation tools such as adapted versions of the xcube Viewer    and a 4D Viewer.</li> <li>A Jupyter Notebook (JNB) service so that users can run    Jupyter Notebooks with project-specific Machine Learning (ML)    environments. </li> </ul> <p>Services available to users within DeepESDL comprise:</p> <ul> <li>An xcube Server to browse, access, and publish gridded data cubes.</li> <li>A geoDB instance to browse, access, and publish vector datasets. </li> <li>A Workspace so projects can store and share any other data,    such as ML workflows. </li> <li>Access to all shared and project-specific Data Cubes in object storage.</li> </ul> <p>Each project has access to the DeepESDL common resources that comprise  a cluster of worker nodes as well as a common  xcube ARDC Service, that can access a variety of data  access services and turns provided data into analysis-ready data cubes (ARDC). DeepESDL data cubes share a consistent structure and use a uniform format. </p> <p>To enable efficient machine learning on data cubes, the ARDC Generator  utilizes a configurable ML sampling module for training and validation.  Likewise, the worker nodes offer GPU-acceleration for demanding ML training.  Core libraries include Keras/Tensorflow and PyTorch and advanced  tools for model evaluation like TensorBoard or MLflow are made available. </p> <p>The DeepESDL Published Project Data is backed by a subset of the  applications and services granted to each  project, that is, a common xcube Server and geoDB is provided to serve the public catalogue and visualisation tools;  a public REST API allows accessing the services programmatically.  Public services will later also comprise a Jupyter Book  (JB)  or Notebook Viewer  (NBviewer) service, so that selected Project-JNBs  can be elaborated into published, story-telling books.  This service together with catalogue and visualisations tools  are important parts of the DeepESDL scientific information dissemination.</p> <p>The numerous distributed services and applications of the DeepESDL system  are containerized and are executed in a common cloud environment.  To orchestrate, monitor, maintain, and scale the variety of  DeepESDL services and applications, we use today\u2019s most popular  container orchestrator Kubernetes (K8s).  This supports the idea that the DeepESDL system is deployable on any Cloud  environment that can run a K8s service (this is, AWS, Google,  Microsoft Azure, and many others including all DIAS instances). The  initial system is deployed on Amazon Web Services (AWS) in the EU-Central-1  region, which is physically located in Frankfurt, Germany, Europe using the  managed K8s service EKS, which is critical for cost-efficiency and  reliability of DeepESDL. </p> <p>To be able to meet the ambitious requirements for the service,  we must base the solution mainly on existing technologies.  Many of the DeepESDL system components have been developed in former  (mostly ESA) projects and are thus reused, adapted, extended, and branded  for DeepESDL. Examples are</p> <ul> <li>DeepESDL Hub is based on EOxHub developed by EOX and used for   example in the ESA EuroDataCube project;</li> <li>The xcube cube generator tools, xcube Server, xcube Viewer    are derived from already available components in the   xcube Toolkit    developed and used in several activities by the development team;</li> <li>The Cube 4D Viewer will be an adopted version of Earthwave\u2019s 4D Viewer.</li> </ul>"},{"location":"design/context/","title":"Context","text":""},{"location":"design/context/#context","title":"Context","text":"<p>The design of DeepESDL is focused on the idea of on-boarding new  DeepESDL user projects with the help of the  DeepESDL Hub which can be seen as an implementation of the  ESA Earth Science Hub.  Accordingly, a DeepESDL user project may support specific  ESA EO for Society  research projects and activities that are aggregated in Science Clusters  such as</p> <ul> <li>ESA Atmosphere Science Cluster.</li> <li>ESA Carbon Science Cluster.</li> <li>ESA Ocean Science Cluster.</li> <li>ESA Polar Science Cluster.</li> </ul> <p>The DeepESDL Hub manages user projects and their users.  The Hub provides for each project a workspace or tenant,  assigns project resources, hosts project artifacts, provides a dashboard  for result dissemination, and integrates team collaboration tools.  It also controls visibility of results of individual user projects so  that they can be made part of a DeepESDL public portrayal.</p> <p>A DeepESDL user project comprises numerous own applications, interfaces,  services, and data available in the provided workspace.  Service access and application deployments are managed by the  Control Plane of the DeepESDL Hub. Actual workloads are  scaled on the Worker Plane.</p>"},{"location":"design/hub/","title":"Hub","text":""},{"location":"design/hub/#deepesdl-hub","title":"DeepESDL Hub","text":"<p>The DeepESDL Hub is based on the existing software named EOxHub,  which is also used to power, e.g., the  EuroDataCube (EDC) Marketplace as  well as the  EDC EOxHub Workspace.</p> <p>EOxHub is a platform and workflow management runtime for Earth Observation  services and apps. EOxHub can be branded to provide the DeepESDL Hub &amp;  Marketplace and be deployed on any cloud offering a managed Kubernetes service. The designated cloud for DeepESDL, Amazon Web Services (AWS) in  the Europe Frankfurt region, fulfils this requirement with the managed Elastic Kubernetes Service (EKS). Technically EOxHub is split into the Control Plane and the Worker Plane. The Worker Plane is where all workloads from users are run at runtime.  The Control Plane is configured to provide the following:</p> <ul> <li>User Management</li> <li>Access control</li> <li>User Workspaces (Tenants)</li> <li>Workspace Dashboard</li> <li>Service subscription management</li> <li>Marketplace</li> <li>Allocation functions for cloud resources and Data Services</li> <li>Deployment service</li> <li>Workload management functions</li> <li>Docker Image administration/assignment</li> <li>Example notebook catalogue supporting user contributions</li> <li>Automated verification of example notebooks</li> <li>Accounting and billing (voucher handling)</li> </ul> <p>Deploying user workloads on the Worker Plane is performed on configured  autoscaling groups using the managed Elastic Container Service (ECS) of AWS.  This setup ensures, that only actually required resources are used and  thus need to be paid.</p> <p></p> <p>The figure above shows the App deployment in user Workspaces.  The sequence of steps is: The App or Notebook Developer pushes the  App source code to the Code Management tool, adds the App as Docker image,  and registers the App at the Marketplace. The App Consumer discovers and requests the App and triggers the deployment of the App to use it to their  workspace to be run on the cloud infrastructure. The App is now available to be used by the Consumer within the resources provided in their  workspace subscription.</p> <p>EOxHub is extended in two ways to provide the DeepESDL  Collaborative Development Tools:</p> <ul> <li>Extended support for teams as part of the multi-user plan.    Allow for easier sharing of versioned notebooks and other artifacts    within the team but not necessarily the public.</li> <li>Integrated support for Machine Learning (ML) workflows.    This includes the versioning, sharing, and collaborative using of    all ML artifacts like code, data, models, results, etc.    Based on user feedback, the readily available Open-Source tools    like Data Version Control (DVC), MLflow, Kubeflow, or similar, will   be integrated during the project.</li> </ul>"},{"location":"design/services-and-tools/","title":"Services and Tools","text":""},{"location":"design/services-and-tools/#deepesdl-services-and-tools","title":"DeepESDL Services and Tools","text":"<p>The Python xcube Toolkit plays a core role  in the DeepESDL\u2019s system design. Here we briefly introduce the key  components of xcube to provide an overview.</p> <p>The xcube generators are command-line tools used  to generate analysis-ready data cubes (ARDC) with inputs from one or  more data stores. </p> <p>The xcube Server provides a web service that offers several RESTful APIs.  It publishes collections of ARDCs which are provided by configurable data  stores but also provides an OGC WMTS and time-series service. </p> <p>The xcube Viewer is a simple and very easy-to-use single-page web  application that fully exploits the APIs offered by xcube Server. </p> <p>xcube Data Stores are used by the xcube Generator and the xcube Server  represent different data cube sources. </p> <p>Finally, the xcube Python API  provides various high- and low-level functions for  data analysis that operate on data cubes and comprises programmatic  access to all other xcube Components mentioned above.  Users call the Python API from their own programs, scripts, JNBs,  or from user code executed in the <code>xcube gen2</code> generator.  During the project, generic functions will be added to the Python API  to support machine learning using the data cubes (see ML Toolkit below),  as well as new use case specific functions as desired in the  different DeepESDL user projects. </p>"},{"location":"design/services-and-tools/#xcube-generators","title":"xcube Generators","text":"<p>The command-line tools  <code>xcube gen</code> and  <code>xcube gen2</code> offer flexible data cube generation  that is made available via a dedicated JupyterLab  profile. The tool reads data streams from one or more data stores,  then it resamples and combines them. The merged cube can then be  manipulated by user-provided Python code before the resulting  cube is written into another data store, for example AWS S3.</p> <p></p> <p>The current features of the xcube generation service are:</p> <ul> <li>Read data streams from a variety of xcube data stores   (see xcube Toolkit).</li> <li>Perform spatial resampling of raw and rectified data streams to    a common spatial grid using standard coordinate reference systems, e.g.,    EU LAEA (EPSG:3035).</li> <li>Perform temporal resampling by aggregating observations to equally    sized time periods, e.g., 8-day averages. </li> <li>Merge resampled data streams into a single cube, optionally    passing it to user Python code for further value adding or transformation.</li> <li>Write chunked data cube into remote object storage or the    local file system.</li> </ul>"},{"location":"design/services-and-tools/#xcube-server","title":"xcube Server","text":"<p>The xcube Server  (<code>xcube serve</code>) offers a web service that publishes collections of  data cubes which are provided by configurable xcube data stores.  It offers a RESTful API for browsing the published cubes (catalogue),  for multi-resolution image tiles (OCG-compliant WMTS, later WCS),  for time-series retrieval, and for direct data access.  Moreover, it will be equipped with a STAC-compatible catalogue API that  supports spatial OpenSearch requests. </p> <p>The xcube Server will be extended to fully support any new  xcube Viewer features, for example linking the geoDB with  xcube Server and Viewer.</p>"},{"location":"design/services-and-tools/#xcube-viewer","title":"xcube Viewer","text":"<p>The xcube Viewer  (<code>xcube-viewer</code>) is a simple and very easy-to-use single-page web application  that fully exploits the APIs offered by xcube Server. It displays the  spatial images of the data cubes on a map at given time steps, it can show a variable\u2019s time series with error bars for any geometry and  show the details of a data cubes and its variables.</p> <p></p> <p>The xcube Viewer provides several useful tools to explore data cubes.  For example:</p> <ul> <li>Open any number of data cubes and display spatial images of dataset    variables on a map.</li> <li>Open places (vector data) associated with data cubes and display them.</li> <li>Show detailed information about selected data cubes, data variables    and places.</li> <li>Select places or let users draw shapes and display time series with    error bars.</li> <li>Click into time series to show corresponding images in map.</li> <li>Start a \u201cplayer\u201d that steps through time and animates the map and time   indicator in time series diagrams accordingly.</li> <li>Download extracted time series data.</li> </ul> <p>During the DeepESDL project, the xcube Viewer will be enhanced by  new features such as:</p> <ul> <li>Switching between 2D map display and 3D globe displays.</li> <li>Multiple, possibly synchronized 2D/3D displays and split-view displays.</li> <li>Generate new data variables on-the-fly from user-supplied Python functions.   Code may originate from shared locations on GitHub or may be provided   inline by a simple code editor.</li> </ul>"},{"location":"design/services-and-tools/#xcube-data-stores","title":"xcube Data Stores","text":"<p>Data store implementations are  dynamically registered in the xcube Data Store Framework.  The following data stores are already available</p> <ul> <li>ESA CCI Open Data Portal (from xcube plugin xcube-cci).</li> <li>C3S Climate Data Store (from xcube plugin xcube-cds).</li> <li>CMEMS Data Store (from xcube plugin xcube-cmems).</li> <li>Sentinel Hub (from xcube plugin xcube-sh).</li> <li>Generic data stores such as S3-compatible object storage (<code>s3</code>),    local file system (<code>file</code>), and in-memory (<code>memory</code>).</li> </ul> <p>Its is planned to develop data stores for the geoDB service,  so users can retrieve vector datasets and rasterized vector data  sources as gridded ARDCs.  Other data stores that are needed by DeepESDL Projects will be added  as required by specific use cases during the project.</p>"},{"location":"design/services-and-tools/#ml-toolkit","title":"ML Toolkit","text":"<p>The DeepESDL ML Toolkit is a small and handy  Python package that provides useful functions for machine learning tasks with DeepESDL data cubes.</p> <p>The toolkit is made available as a DeepESDL JupyterLab environment (profile),  and includes popular libraries such as scikit learn, TensorFlow,  Keras, and PyTorch.</p> <p>To support model evaluation during training, the ready-to-use processing environment is also extended by a TensorBoard to support the tracking of individual experiments and training runs. This tool can be used with PyTorch and TensorFlow, and it provides  a state-of-the-art toolset for data scientist to inspect the tuning  and training process and compare metrics.</p> <p>The toolkit offers the following:</p> <ul> <li> <p>Adapters are provided to existing data loading and transformation    mechanisms from Keras and PyTorch (<code>DataGenerator</code>, <code>DataLoader</code>)    to be usable for the DeepESDL data cubes. </p> </li> <li> <p>Implementations of sampling mechanisms and online repartitioning methods    suited for the data cube files which are stored in chunks.    This element is essential, as it enables deep learning that respects    the basic principles of geo data way beyond naive applications of   machine learning in the Earth system context. </p> </li> </ul> <p>To ease adoption by scientist we plan to also develop script templates  for common deep learning tasks such as autoencoder on time-series data,  in particular physics informed autoencoder, transfer learning and change  detection on time series.</p> <p>The toolkit is currently for Python only, but Julia will also be  considered to address a small but growing community of data scientists.</p>"},{"location":"guide/datacube-generation/","title":"Datacube generation","text":""},{"location":"guide/datacube-generation/#deepesdl-datacube-generation","title":"DeepESDL datacube generation","text":"<p>The data cubes already provided by DeepESDL might not be sufficient  for your application. However, this should not stop you from creating the  resources you need from source input data and enable you to do your research.  DeepESDL carefully adheres to the reproducibility of dataset resources. Therefore,  there are two approaches to generate datasets.  In the simpler one, the data is retrieved from an existing datastore without persisting the  dataset, usually using the DeepESDL JupyterLab.  If a dataset shall be persisted, maybe even re-published,and is furthermore based on input data that  needs to be e.g. downloaded beforehand or other preprocessing steps are  performed, then the cube generation recipe approach is recommended. Note that the Cube Gen team follows this approach for all cubes generated  and published by DeepESDL. </p>"},{"location":"guide/datacube-generation/#cube-generation-recipe-approach-for-static-data-cubes","title":"Cube generation recipe approach for static data cubes","text":""},{"location":"guide/datacube-generation/#cube-specification-format","title":"Cube specification format","text":"<p>For each data cube, a unique specification is created that describes  the cubes spatio-temporal dimensions, resolutions, coverages, and the data  sources used to generate the cube\u2019s target data variables. To specify each  cube, a dedicated JSON format is created that fully describes the target  cube\u2019s characteristics. We use a special GeoJSON Feature format for this  purpose so each cube definition is both human- and machine-readable and can  be ingested and rendered by many existing tools. The GeoJSON  Feature\u2019s geometry represents the geographical coverage of the cube, while the  GeoJSONFeature\u2019s properties provide numerous further details. To describe many  similar cubes, e.g., for using multiple spatial resolutions for same  variables, a GeoJSON FeatureCollection may be used instead. To validate the  JSON cube specification files, we provide a dedicated online JSON Schema  in the DeepESDL dataset-spec GitHub repository.</p>"},{"location":"guide/datacube-generation/#cube-generation-recipe","title":"Cube generation recipe","text":"<p>The cube generation recipe ensures, that an existing dataset provided by  DeepESDL can be reproduced by following the documented steps and specifications. Here we describe the  recipe structure of the provided datasets within DeepESDL.</p> <p>Each predefined DeepESDL data cube is fully described in a transparent and  comprehensive way by a dedicated sub-folder in the DeepESDL GitHub repository  cube-gen.  Such sub-folder is what we call a cube generation recipe: <code>cube-gen/${cube-name}/</code></p> <p>It contains the machine-readable GeoJSON file that fully specifies the data  cube <code>cube-gen/${cube-name}/cube.geojson</code> and provides the  detailed human-readable information about the cube including how to generate  it from sources in <code>cube-gen/${cube-name}/README.md</code>. After a cube has been released and published, we record the changes in  <code>cube-gen/${cube-name}/CHANGES.md</code>. In each data cube sub-folder, further sub-folders may exist that contain resources and sources such as  configuration files and Python modules. We have defined the following common sub-folder structure, but others may be used  too:</p> <ul> <li> <p><code>cube-gen/${cube-name}/input-collect/</code>        Fetch, download,or copy inputs.</p> </li> <li> <p><code>cube-gen/${cube-name}/input-preprocess/</code>        Transform, concatenate, convert to interm. Zarr.</p> </li> <li> <p><code>cube-gen/${cube-name}/output-merge/</code>       Merge interm. Zarrs to target cube.</p> </li> <li> <p><code>cube-gen/${cube-name}/output-postprocess/</code>        Apply any postprocessing.  </p> </li> </ul> <p>Cube generation recipes are designed to be comprehensive, transparent,  reproducible, and relocatable. That is, with very little configuration  changes, they should be executable in different environments.</p>"},{"location":"guide/datacube-generation/#dynamic-data-cubes-from-data-stores","title":"Dynamic data cubes from Data Stores","text":"<p>Persistently stored data cubes have the advantage that they have a physical  representation, i.e. they \u201cexist\u201d at a given storage location. That way  the data can be \u201cfrozen\u201d and can be assigned a version and/or a DOI. In  addition, access to static data cubes persisted in cloud object storage  is usually fast and scalable. </p> <p>However, there are potential issues and challenges with static cubes. Data  sources of data cubes can become outdated, or are simply updated, like it is the case for new EO data observations  added to an existing product archive. In such cases, related data cubes should  be updated too, hereby creating considerable maintenance effort and risks for the integrity of the data cube. In addition, the generation of static data cubes is in many cases a plain duplication of  data that is actually defined and described elsewhere. The data cube must  ensure to stay in sync with original data sources and metadata. Finally, static cubes can only satisfy requirements of one user or use  case in an optimal way. By definition, they do not allow for streamlined and  tailored datasets. </p> <p>A possible solution to mitigate these issues and address  the challenges are dynamic data cubes. Dynamic data cubes exist in-memory  only and will provide data in a \u201clazy\u201d way. That is, chunks of a data cube  are fetched on-demand, hence computed on-the-fly, including all the  required transformation steps starting with the ingestion of source data.  Dynamic cubes are generated for a given configuration that describes the  data cube to be generated. </p> <p>Dynamic data cubes for a given single data source can be easily retrieved  using xcube data stores. The following Python code opens a data cube  representing a Sentinel-2 L2A data cube with the SentinelHubAPI as data source:</p> <pre><code>from xcube.core.store import new_data_store\nstore = new_data_store(\"sentinelhub\", **credentials)\ncube = store.open_data(\"S2L2A\", \n                       variable_names=[\"B03\",\"B06\",\"B8A\"],\n                       bbox=..., \n                       spatial_res=...,\n                       time_range=..., \n                       time_period=...)\n</code></pre> <p>Dynamic cubes are application-specific and configured by individual users.</p> <p>Below is an overview of the possible xcube data stores that can be  used to create dynamic cubes together with the title of an example notebook,  if there is one available. A description of how the example notebooks can be  accessed is in section DeepESDL JupyterLab.</p> Data store ID Content Example Notebook Access s3 Any Zarr dataset on AWS S3 or similar 01 Access public cubes Depends on permissions cmems CMEMS datasets 02 Generate CMEMS cubes Requires registration, free cciodp All ESA CCI datasets 03 Generate CCI Cubes Free cds Climate data store Requires registration, free sentinelhub Sentinel 1 to 3, Landsat, ... Requires registration, with costs"},{"location":"guide/datacubes/","title":"Datacubes","text":""},{"location":"guide/datacubes/#deepesdl-datacubes","title":"DeepESDL datacubes","text":"<p>DeepESDL provides a growing list of relevant variables for Earth System Science.  Most of them have been derived from Earth Observation, but the compilation also  includes model or re-analysis data if deemed useful. DeepESDL is very grateful to all data owners for kindly providing the data sets  and allowing us to process, and redistribute them free of charge.  All datacubes generated and distributed by DeepESDL come without any warranty,  neither from the owners, from the DeepESDL, nor from ESA.</p> <p>During ingestion into the DeepESDL, data sets are typically transformed in  space and  time to fit to the common grid of the data cube, a process that necessarily  modifies the original data. If you are looking for the original data, please  follow the links within the dataset attributes for each variable and contact  the data owners.</p> <p>To access the documentation of available datasets, please have a look in the  datasets section.</p>"},{"location":"guide/further-information/","title":"Further information","text":""},{"location":"guide/further-information/#further-information","title":"Further information","text":"<ul> <li>The JupyterLab documentation:    an in-depth user guide for the JupyterLab interface.</li> <li>How to Use JupyterLab:    a short introductory video tutorial.</li> <li>The xcube documentation: user    guide and API reference for the xcube libraries.</li> </ul>"},{"location":"guide/jupyterlab/","title":"JupyterLab","text":""},{"location":"guide/jupyterlab/#deepesdl-jupyterlab","title":"DeepESDL JupyterLab","text":""},{"location":"guide/jupyterlab/#basic-usage","title":"Basic usage","text":"<p>This section provides a brief introduction for users to the basic features of the JupyterLab environment as offered by DeepESDL. For more in-depth documentation on the various components, see the links in the section Further Information.</p>"},{"location":"guide/jupyterlab/#logging-in-and-starting-the-jupyterlab-profile","title":"Logging in and starting the JupyterLab profile","text":"<p>To use the DeepESDL JupyterLab environment, navigate to https://deep.earthsystemdatalab.net/ with a web browser (a recent version of Firefox, Chrome, or Safari is recommended).</p> <p>Before first usage, we will have to register you with the system. Currently, we are not operational yet and still in testing phase. There is the possibility to register already as an Early Adopter. To this, we kindly ask you to write as an email at <code>esdl-support@brockmann-consult.de</code> and we will see if we can already onboard you.</p> <p>DeepESDL uses a GitHub to authenticate, so if you are already registered as a DeepESDL user, please use your GitHub account to log in. If your Jupyter server is not already running, you may be presented with a menu of user JupyterLab profiles to use for your session; there might be one or more JupyterLab profiles to choose from, depending on the computational resources needs of your team. Please select a suitable profile for your current task; it might not always require the profile with the strongest computational resources available. After choosing your environment, you will see a progress bar appearing for a few moments while it is started for you. The JupyterLab interface will then appear in your web browser, ready for use.</p>"},{"location":"guide/jupyterlab/#changing-a-jupyterlab-profile","title":"Changing a JupyterLab profile","text":"<p>If you have already started your session and need to change the JupyterLab profile, you can do this by selecting Hub control panel from the File menu within JupyterLab. Then click the <code>Stop my server</code> button and wait for your current server to shut down. When the <code>Start my server</code> button appears, you can click on it to return to the user JupyterLab profiles menu.</p>"},{"location":"guide/jupyterlab/#logging-out","title":"Logging out","text":"<p>To log out, select Log out from the File menu within JupyterLab.</p> <p>Note that your JupyterLab session will continue in the background even after you have logged out, but will eventually be terminated due to inactivity. If you wish to stop your session explicitly, you can use the hub control panel as described in the Changing a JupyterLab profile section above.</p>"},{"location":"guide/jupyterlab/#python-environment-selection-of-the-jupyter-kernel","title":"Python environment selection of the Jupyter Kernel","text":"<p>If you wish to use a special set of python packages, you can adjust it in the top right corner of the notebook. Next, a drop-down menu will appear, and you can select the desired kernel environment from it.</p> <p></p> <p>If the selected kernel seems not to load, it could be due to caching of kernels which do not exist anymore. To remove cached non-existing environment kernels, follow these steps:</p> <ol> <li>Open the terminal within the jupyter lab</li> <li><code>$ rm -r .local/*</code></li> <li>it is alright to get a message like: \"rm: cannot remove    '.local/share/jupyter': Directory not empty\" because you might have    notebooks open, which are in the cache. Make therefor sure not to force    the <code>rm</code> command!</li> <li>Restart your jupyterlab server by selecting Hub control panel from the    File menu within JupyterLab. Then click the <code>Stop my server</code> button and    wait for your current server to shut down. Select the <code>Start my server</code>    button once it appears to return to the user JupyterLab profiles menu and    restart your session.</li> </ol> <p>To get a custom environment which suits your needs, please contact the DeepESDL team directly.</p>"},{"location":"guide/jupyterlab/#creating-custom-team-python-environment","title":"Creating custom team python environment","text":"<p>Up to two team members may create a custom python conda environment for a team. Please inform the DeepESDL Team who should be granted these permissions.</p> <p>Steps to create custom team conda environments:</p> <ol> <li>Head over to https://deep.earthsystemdatalab.net/conda-store</li> <li>Login with your GitHub Account which you also use to access the DeepESDL     JupyterLab</li> <li>If you have never created a custom environment, there will be none listed.</li> <li>Click on the Plus-sign next to Environments</li> <li>In the top section, select the namespace for which to create the custom     environment. There might be more than one if you are part of several     teams. If you are unsure which namespace you should use, have a look     at the Server Options overview of the DeepESDL JupyterLab.</li> <li> <p>You may either choose an environment.yml file to upload or paste your     environment configuration     into the window directly.     It should look something like this example:</p> <pre><code>channels:\n    - conda-forge\ndependencies:\n    - xcube=1.1.1\n    - xcube-cds\n    - xcube-sh\n    - xcube-cmems\n    - xcube-cci\n    - xcube_geodb\n    - boto3\n    - rasterio&gt;=1.3.6\n    - cartopy\n    - ipykernel\nname: xcube-1.1.1\n</code></pre> </li> <li> <p>Make sure to set a meaningful value to the environment's <code>name</code> property,     so also your teammates will know what it is about.</p> </li> <li>Once you are happy with your environment hit submit and grab a coffee. It     will take some time to create your custom environment.</li> <li>After submission, it will appear in the overview on     https://deep.earthsystemdatalab.net/conda-store/</li> <li>You can click on the name of your newly created environment and see its     status. There are three different statuses: Building, Completed, Failed</li> <li>If you click on the build number you can see the logs. This might be     useful if the build has failed.</li> <li>Once the build is completed, you need to refresh your browser window     to make it available in the kernel selection. Instructions how to change     the kernel to your custom environment are provided     in the section python environment selection of the jupyter kernel.</li> </ol> <p>You can also modify an existing environment and rebuild it; the conda-store will keep all the builds' logs. It will look similar to the screenshot below. </p> <p>The conda-build highlighted in green is the one, which you will use in you DeepESDL JupyterLab, per default it is the latest successful build. If you wish to make a different build the one to be used in DeepESDL JupyterLab, select the checkmark in the blue button panel of the desired conda-build. The reload button in the blue button panel will trigger a rebuild of the conda-environment specification of the selected conda-build. The bin button deletes the conda-build of the selected conda-build.</p>"},{"location":"guide/jupyterlab/#getting-started-notebooks","title":"Getting-started notebooks","text":"<p>You can find example notebooks in DeepESDL JupyterLab to help you to get started.</p> <p>To access them:</p> <ol> <li>Head to the JupyterLab <code>Launcher</code>      If your <code>Launcher</code> is not visible right away, you can open it via the <code>plus</code>     button in the top left corner, which is highlighted in blue in the     screenshot.</li> <li>On the bottom of the Launcher you see a tile called <code>CATALOG DeeESDL</code>.     Please select this tile.</li> <li>Once selected you see several example notebooks:     </li> <li>Select one of them, and you will see a preview of the notebook, to execute     the selected notebook click on <code>EXECUTE NOTEBOOK</code> in the top right corner.     </li> <li>The notebook is copied into your workspace, and you can run it and adjust     it according to your needs.</li> </ol>"},{"location":"guide/jupyterlab/#team-resources","title":"Team resources","text":"<p>If you are using DeepESDL Jupyter Lab with others in a team, you can share content with each other and access the team S3 bucket for data storage.</p>"},{"location":"guide/jupyterlab/#team-shared-directory","title":"Team shared directory","text":"<p>To access the team shared directory, you can mount it into your workspace. This is a one-time action, and once mounted, it will persist in your workspace unless you choose to remove the mount. If you are part of multiple teams, you only need to mount the directory once; there\u2019s no need to do so for each team separately.</p> <p>To mount the team shared directory:</p> <ol> <li>Open the terminal in the DeepESDL Jupyter Lab via the Launcher.</li> <li><code>$ ln -s /extra ~/team-shared</code></li> </ol> <p>That\u2019s it! You will now see the team shared directory in your workspace. Please be aware that anyone in the team can see, add, modify, or delete content in this directory.</p>"},{"location":"guide/jupyterlab/#team-shared-s3-bucket","title":"Team shared S3 bucket","text":"<p>How to make use of the DeepESDL Team shared bucket is demonstrated in the example notebook Save_cube_to_team_storage.ipynb. The notebook is located in the DeepESDL notebook catalog in the \"team-shared\" category.</p>"},{"location":"guide/jupyterlab/#proxy-server-processes-like-dashboards-webserver-uis","title":"Proxy server processes like Dashboards, WebServer UIs,...","text":"<p>If you start a server process on a specific port (e.g. 8080) within your JupyterLab session, you can access it via the url  <code>https://deep.earthsystemdatalab.net/user/&lt;your-username&gt;/proxy/&lt;port&gt;/</code>, e.g. user <code>joe</code> can access his server process on port 8080 at  <code>https://deep.earthsystemdatalab.net/user/joe/proxy/8080/</code>.</p> <p>This url is only accessible as long as the JupyterLab session is running.</p>"},{"location":"guide/ml-toolkits/","title":"Overview","text":""},{"location":"guide/ml-toolkits/#ml-toolkits","title":"ML Toolkits","text":"<p>The ML Toolkits provide an introduction to  a set of best practice Python-based Jupyter Notebooks that showcase the  implementation of the three start-of-the-art Machine Learning libraries (1)  scikit-learn, (2) PyTorch and (3) TensorFlow based on the Earth System Data  Cube. </p>"},{"location":"guide/overview/","title":"Overview","text":""},{"location":"guide/overview/#user-guide","title":"User guide","text":"<p>This user guide helps you to get started with the main components of  DeepESDL.</p> <p>The growing set of DeepESDL's general functionalities comprises:  </p> <ol> <li>DeepESDL JupyterLab </li> <li>DeepESDL xcube Viewer </li> <li>DeepESDL datacubes </li> <li>DeepESDL datacube generation </li> <li>ML Toolkits </li> </ol>"},{"location":"guide/xcube-viewer/","title":"xcube Viewer","text":""},{"location":"guide/xcube-viewer/#deepesdl-xcube-viewer","title":"DeepESDL xcube Viewer","text":"<p>The DeepESDL xcube Viewer is reachable at viewer.earthsystemdatalab.net.</p> <p></p> <p>The viewer contains public datasets only, but will later also provide user/team  cubes when logged in. The login will be the same as for the DeepESDL JupyterLab.</p> <p>For a more detailed description of the viewer functionality, please refer to a dedicated section in the xcube viewer documentation.</p>"},{"location":"guide/xcube-viewer/#publish-a-dataset-in-a-public-deepesdl-viewer","title":"Publish a dataset in a public DeepESDL Viewer","text":"<p>In order to publish a data cube in a public DeepESDL Viewer take the following steps:</p> <ul> <li> <p>Check if the data cube is ready for publication with the following tools</p> <ul> <li>xrlint: to validate the <code>xrarray.Dataset</code> with a set of recommended rules </li> <li>xcube Viewer extension: in the JuypterLab use the dedicated Jupyter Notebook to test the presentation of    the data in the xcube Viewer (see <code>Visualize_data_with_xcube_viewer.ipynb</code>)</li> </ul> </li> <li> <p>Store the data cube in team storage on S3 </p> <ul> <li>for more information see <code>Save_cube_to_team_storage.ipynb</code></li> </ul> </li> <li> <p>For the publication contact the DeepESDL team (<code>esdl-support@brockmann-consult.de</code>) and communicate whether </p> <ul> <li>the data cube should be published in the Viewer and stored in a public DeepESDL bucket, or</li> <li>the data cube should be published in the Viewer (visualisation only)</li> </ul> </li> </ul>"},{"location":"ml-toolkit/example/","title":"Example Use Case","text":""},{"location":"ml-toolkit/example/#example-use-cases","title":"Example Use Cases","text":"<p>This toolkit provides a series of tutorial notebooks designed to support geospatial data processing and machine learning tasks on Earth System Data Cubes (ESDCs). Key use cases include:</p> <ol> <li> <p>Land Surface Temperature Prediction: Demonstrates land surface temperature prediction. This serves as an introductory example for ESDC analysis.</p> </li> <li> <p>Distributed Machine Learning: Showcases efficient preparation and training of large datasets on distributed systems.</p> </li> <li> <p>Transfer Learning: Illustrates how to reuse pre-trained models for related tasks with limited data.</p> </li> <li> <p>Cube Insights: Explores the characteristics of data cubes, helping inform preprocessing and modeling decisions.</p> </li> <li> <p>Gap Filling: Provides techniques to fill missing data in remote sensing datasets using support vector regression (SVR).</p> </li> <li> <p>ML for Multidimensional Samples with missing values: Demonstrates predictions on multidimensional data, utilizing simpler data imputation methods to address gaps.</p> </li> </ol> <p>Each of these use cases is accompanied by a corresponding Jupyter notebook/Python script, providing step-by-step instructions, code examples, and visualizations to guide users through the entire workflow. Whether you're looking to explore your data, fill gaps, or train machine learning models, this toolkit offers a set of resources to help you achieve your goals.</p>"},{"location":"ml-toolkit/example/#1-land-surface-temperature-prediction","title":"1. Land Surface Temperature prediction","text":"<p>This generic use case aims at the prediction of land surface temperature values based on air temperature values derived from the ESDC (Sentinel 3 SLSTR and Terra MODIS sensor, s3 store).</p> <p>Satellite monitoring is highly sensitive to atmospheric conditions, in particular to cloud cover, leading to the loss of a significant part of data, especially at high latitudes. This may even affect some pixels of an image which are not cloudy, but strongly influenced by cloud cover, usually because they were cloudy shortly before the moment of sensing or because of cloud shadows (Sarafanov et al. 2020). Therefore, remotely sensed land surface temperature images are patchy and gaps need to be filled in to complete the data set. Here, we propose a shallow neural network (Linear Regression) to predict missing values of land surface temperature from consistent air temperature values.</p> <p> </p> <p> ML prediction of missing Land Surface Temperature values from Air Temperature values (xcube viewer) </p>"},{"location":"ml-toolkit/example/#demo-notebooks","title":"Demo Notebooks","text":"<p>All Jupyter Notebooks follow the same approach, involving five major sections supported by markdown cells, comments, and plots:</p> <ol> <li>Landsurface Temperature Prediction scikit-learn</li> <li>Landsurface Temperature Prediction PyTorch</li> <li>Landsurface Temperature Prediction TensorFlow</li> </ol>"},{"location":"ml-toolkit/example/#approach","title":"Approach","text":"<ol> <li>Import necessary libraries and mltools</li> <li>Load Earth System Data Cube (s3 object store)</li> <li>Initialize data mask</li> <li>Assign train/test split</li> <li>Preprocessing (filtering NaNs, standardization, normalization)</li> <li>Model set-up (linear regression with 1 node/ shallow neural network)</li> <li>Model training and validation over a number of epochs:</li> <li>Training:<ul> <li>Generate training batches using existing data loading and transformation mechanisms from Keras and PyTorch (DataGenerator, DataLoader)</li> <li>Train model, and compute average epoch training loss</li> </ul> </li> <li>Validation:<ul> <li>Generate testing batches using existing data loading and transformation mechanisms from Keras and PyTorch (DataGenerator, DataLoader)</li> <li>Test model, and compute average epoch validation loss</li> </ul> </li> <li>Use model to make predictions &amp; plot results</li> </ol> <p> Machine Learning workflow on Analysis Ready Data Cubes </p>"},{"location":"ml-toolkit/example/#preliminary-condition","title":"Preliminary Condition","text":"<p>As initially described in the demo cases, the missing values of land surface temperature are predicted from consistent air temperature values.</p> Air Temperature Land Surface Temperature"},{"location":"ml-toolkit/example/#machine-learning-approach","title":"Machine Learning Approach","text":"<p>In this section, the machine learning approach is briefly illustrated based on the TenorFlow notebook. For comprehensive implementations, refer to the demo notebooks to see the full implementations.</p>"},{"location":"ml-toolkit/example/#1-load-earth-system-data-cube","title":"1. Load Earth System Data Cube","text":"<p>First, the <code>zarr</code> data cube is loaded from the s3 data store. The ESDC consists of three dimensions: longitude, latitude, and time. The focus will be on two variables: \"land_surface_temperature\" and \"air_temperature_2m\".</p> <pre><code>from xcube.core.store import new_data_store\n\n# Initialize the data store for accessing the s3 bucket\ndata_store = new_data_store(\"s3\", root=\"esdl-esdc-v2.1.1\", storage_options=dict(anon=True))\n\n# Open the dataset\ndataset = data_store.open_data(\"esdc-8d-0.083deg-184x270x270-2.1.1.zarr\")\n\n# Select a smaller subset of the data for this demo case\nstart_time = \"2002-05-21\"\nend_time = \"2002-08-01\"\nds = dataset[[\"land_surface_temperature\", \"air_temperature_2m\"]].sel(time=slice(start_time, end_time))\n</code></pre>"},{"location":"ml-toolkit/example/#2-add-land-mask-variable","title":"2. Add land mask variable","text":"<p>Fir the prediction of the land surface temperature values only terrestrial regions are relevant. Therefore, a land variable is assigned to the ESDC to exclude the oceanic regions.</p> <pre><code>import numpy as np\nimport dask.array as da\nfrom global_land_mask import globe\nfrom ml4xcube.preprocessing import assign_mask\n\nlon_grid, lat_grid = np.meshgrid(ds.lon,ds.lat)\nlm0                = da.from_array(globe.is_land(lat_grid, lon_grid))\nxdsm               = assign_mask(ds, lm0)\nxdsm\n</code></pre>"},{"location":"ml-toolkit/example/#3-train-test-split-on-geo-data","title":"3. Train-/ Test Split on Geo-Data","text":"<p>The <code>ml4xcube.splits</code> module provides two methods to split the data into training and test sets: random split and block split.</p> <p>1. Random Split</p> <p>The random split is a straight forward procedure in classical machine learning application to divide data in a train and a test set. Every data sample is assigned randomly with a predefined probability either to the train or the test. This approach can lead to issues due to spatio-temporal distances and auto-correlation within chunks.</p> <p>2. Block Split</p> <p>It is therefore mandatory to utilize techniques that respects the basic principles of geo-data way beyond naive random split method in the Earth system context. To avoid auto-correlation during the training phase of the model, data splitting should rather be guided by the block split strategy, which segments data into contiguous blocks based on geographical and temporal proximity, assigning data points from these blocks to either training or test sets with a specific probability. This strategy keeps closely related data points together, reducing information leakage across the train-test divide and enhancing testing integrity.</p> Random Train-Test Assignment Balanced Stratified Train-Test Assignment <p>For this case, the <code>assign_block_split</code> method is employed to allocate each data point to either the training or test set:</p> <pre><code>from ml4xcube.splits import assign_rand_split, assign_block_split\n\n# random splitting\n\"\"\"\nxds = assign_rand_split(\n    ds    = xdsm,\n    split = 0.8\n)\n\"\"\"\n\n# block splitting\nxds = assign_block_split(\n    ds         = xdsm,\n    block_size = [(\"time\", 10), (\"lat\", 100), (\"lon\", 100)],\n    split      = 0.8\n)\nxds\n</code></pre>"},{"location":"ml-toolkit/example/#4-train-and-test-set-creation-and-preprocessing","title":"4. Train-/ and Test Set Creation and Preprocessing","text":"<p>In this step, data is preprocessed for training using the designated sampler. The dataset undergoes standardization and is segmented into manageable samples. The feature scaling strategy can be customized via the <code>scale_fn</code> parameter, which allows for normalization or can be set to None for manual adjustments. If <code>None</code>, a custom feature scaling function can be introduced using the <code>callback parameter</code>, enabling further preprocessing flexibility with costum functions.</p> <p>By default, missing values are omitted from the dataset. To apply alternative imputation strategies, adjust the <code>drop_nan</code> parameter of the <code>XrDataset</code>. For comprehensive guidance on these options, please consult the ml4xcube API description description.</p> <p>Following preprocessing, the data is allocated into training and testing sets based on the previously determined block split strategy, ensuring readiness for the subsequent training phase.</p> <pre><code>import tensorflow as tf\nfrom ml4xcube.datasets.xr_dataset import XrDataset\n\nsampler               = XrDataset(ds=xds, num_chunks=3, rand_chunk=False, to_pred='land_surface_temperature')\ntrain_data, test_data = sampler.get_datasets()\n\n# Create TensorFlow 6-datasets for 7-training and testing\ntrain_ds = tf.data.Dataset.from_tensor_slices(train_data).batch(32)\ntest_ds = tf.data.Dataset.from_tensor_slices(test_data).batch(32)\n</code></pre>"},{"location":"ml-toolkit/example/#5-model-setup-optimizer-and-loss-definition","title":"5. Model Setup, Optimizer and Loss Definition","text":"<p>A simple linear regression model using TensorFlow is defined, followed by the setup of the optimizer and the loss function definition.</p> <pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers as L\n\n# Define epoch and learning rate\nlr     = 0.1\nepochs = 10\n\n# Create model\ninputs      = L.Input(name=\"air_temperature_2m\", shape=(1,))\noutput      = L.Dense(1, activation=\"linear\", name=\"land_surface_temperature\")(inputs)\nmodel       = tf.keras.models.Model(inputs=inputs, outputs=output)\nmodel.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mae\"])\n\nmodel.optimizer.learning_rate.assign(lr)\n</code></pre>"},{"location":"ml-toolkit/example/#6-model-training-and-validation","title":"6. Model Training and Validation","text":"<p>Finally, the model is trained using <code>train_ds</code> and validated with the <code>test_ds</code> dataset. Early stopping is employed to prevent overfitting. The best model weights, according to the validation score, are saved, and the trained model is returned, ready for predictions.</p> <pre><code>from ml4xcube.training.tensorflow import Trainer\n\ntrainer = Trainer(\n    model=model,\n    train_data=train_ds,\n    test_data=test_ds,\n    early_stopping=True,\n    patience=5,\n    model_path=\"best_model.keras\",\n    mlflow_run=mlflow,\n    epochs=epochs,\n    create_loss_plot=True\n)\n\nmodel = trainer.train()\n</code></pre>"},{"location":"ml-toolkit/example/#results","title":"Results","text":"<p>After conducting the entire machine learning approach the trained model can be used to make predictions for the missing land surface temperature values:</p> <p> </p> <p> Land Surface Temperature Filled </p>"},{"location":"ml-toolkit/example/#model-tracking","title":"Model Tracking","text":"<p>Within the land surface temperature use cases model tracking is realized through the usage of TensorBoard and mlflow. These tools offer science teams an easy-to-use platform allowing to run and scale their Machine Learning workloads in a collaborative environment supporting versioning and sharing of parameters, models, artefacts, results, etc. within the team and potentially external users. Mlflow supports the MLOps pipelines particularly to log and evaluate experiment runs as well as to store models in a registry\u200b. Persistent mlflow deployments are made available on team level to allow each team member to compare their experiments with those of the other team members and to use the trained models of others. TensorBoard as another collaborative tool in this MLOPs space is currently evaluated by the science teams and available as part of the TensorFlow conda kernel to individual users within their JupyterLab session.</p> <p> </p> <p> Collaborative Experiment Tracking with mlflow. </p>"},{"location":"ml-toolkit/example/#2-distributed-machine-learning","title":"2. Distributed Machine Learning","text":"<p>Satellites continuously monitor various Earth parameters across, generating vast amounts of data ideal for training sophisticated machine learning models. However, preparing and training with such large datasets can be time-consuming and resource-intensive. The <code>ml4xcube</code> package facilitates efficient handling, preparation, and distributed training of large geospatial datasets, providing tools and workflows designed to optimize these processes. Below are demonstrations on efficient dataset preparation (4) and distributed machine learning (5). For simplicity the previous setup is leveraged to illustrate the functionality.</p>"},{"location":"ml-toolkit/example/#demo-scripts","title":"Demo Scripts","text":"<ol> <li>Distributed Dataset Creation.</li> <li>Distributed Machine Learning.</li> </ol>"},{"location":"ml-toolkit/example/#data-preparation","title":"Data Preparation","text":"<p>Before training machine learning models, data must be preprocessed and organized. This snippet is crucial for understanding how data, particularly large and complex datasets like those from satellites, is preprocessed before being used for machine learning. It demonstrates loading the data, computing statistics necessary for normalization, and applying these statistics to standardize the data with the help of a callback function. The callback function is used to apply transformations on-the-fly to each data chunk, ensuring that all data is processed uniformly. Further custom preprocessing steps can be added accordingly.</p> <pre><code>import xarray as xr\nfrom ml4xcube.preprocessing import get_statistics, standardize\nfrom ml4xcube.datasets.multiproc_sampler import MultiProcSampler\n\n# Load sample data\nds = xr.open_zarr('sample_data.zarr')\nds = ds['temperature']\n\n# Create a train and a test set and save them as train.zarr and test.zarr\ntrain_set, test_set = MultiProcSampler(\n    ds          = ds,\n    train_cube  = 'train.zarr',\n    test_cube   = 'test.zarr',\n    nproc       = 5,\n    chunk_batch = 10,\n).get_datasets()\n</code></pre> <p>In the next step, the environment for training must be prepared by converting datasets to a format compatible with PyTorch, setting up a basic neural network model, and configuring the training process. Since in this example 1D data points are utilized for training, the dimension names assigned correspond to a 1D Tuple as well. If the usage of multidimensional data samples is intended, please define the parameter sample_size of the <code>MultiProcSampler</code> class (e.g. <code>sample_size=[('time', 1), ('lat', 3), ('lon', 3)]</code>). Overlapping samples are also possible (<code>overlap=[('time', 0.), ('lat', 0.33), ('lon', 0.33)]</code>). For further details check out the corresponding definition in the ml4xcube API</p> <pre><code>import zarr\nimport torch\nimport xarray as xr\nimport dask.array as da\nfrom ml4xcube.datasets.pytorch import PTXrDataset\n\ndef load_train_objs():\n    train_store = zarr.open('train.zarr')\n    test_store = zarr.open('test.zarr')\n\n    train_set = xr.Dataset(train_data)\n    test_set  = xr.Dataset(test_data)\n\n    # Create PyTorch data sets\n    train_ds = PTXrDataset(train_set)\n    test_ds  = PTXrDataset(test_set)\n\n    # Initialize model and optimizer\n    model     = torch.nn.Linear(in_features=1, out_features=1, bias=True)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n    loss      = torch.nn.MSELoss(reduction='mean')\n\n    return train_ds, test_ds, model, optimizer, loss\n</code></pre> <p>This final snippet sets up and runs the distributed training process using PyTorch. It includes initializing the distributed data parallel training environment, preparing data loaders with parallel processing capabilities, and defining the training loop. This approach significantly enhances the training efficiency on large-scale datasets by leveraging multiple processing units.</p> <pre><code>from ml4xcube.datasets.pytorch import prepare_dataloader\nfrom ml4xcube.training.pytorch_distributed import ddp_init, Trainer, dist_train\n\n# Initialize distributed data parallel training\nddp_init()\n\n# Load training objects\ntrain_set, test_set, model, optimizer, loss = load_train_objs()\n\n# Prepare data loaders\ntrain_loader, test_loader = prepare_dataloader(train_set, test_set, batch_size, num_workers=5, parallel=True)\n\n# Initialize the trainer and start training\ntrainer = Trainer(\n    model                = model,\n    train_data           = train_loader,\n    test_data            = test_loader,\n    optimizer            = optimizer,\n    save_every           = save_every,\n    model_path           = best_model_path,\n    early_stopping       = True,\n    patience             = 3,\n    loss                 = loss,\n    validate_parallelism = True\n)\n</code></pre>"},{"location":"ml-toolkit/example/#3-transfer-learning","title":"3. Transfer Learning","text":"<p>Transfer learning corresponds to a way to reuse information obtained by previous model training for a second related task. This can be necessary when only a concise amount of data is available. Therefore, a PyTorch based Jupyter Notebook provides the implementation of Transfer Learning. This technique was illustrated for the same setting as the first example, predicting missing land surface temperature values.</p>"},{"location":"ml-toolkit/example/#demo-notebook","title":"Demo Notebook","text":"<ol> <li>Transfer Learning.</li> </ol> <p> The Basic Concept of Transfer Learning. </p>"},{"location":"ml-toolkit/example/#4-cube-insights","title":"4. Cube Insights","text":"<p>In order to decide which preprocessing steps are required by your machine learning application, the <code>insights</code> module offers tools for extracting and analyzing characteristics of an <code>xarray.DataArray</code> object. This module includes functions to assess the completeness and distribution of data within the cube.</p>"},{"location":"ml-toolkit/example/#demo-notebook_1","title":"Demo Notebook","text":"<p>The corresponding Jupyter notebook containing the entire workflow can be accessed here:</p> <ol> <li>Landsurface Temperature Insights</li> </ol> <p>The detailed workflow in order to analyze the specifics of a data cube is demonstrated in the following:</p> <pre><code>import xarray as xr\nfrom ml4xcube.insights import get_insights\n\n# Load sample data\nds = xr.open_zarr('sample_data.zarr')\nds = ds['temperature']\n\n# Get insights from the data cube\nget_insights(ds)\n</code></pre> <p>The <code>get_insights</code> function, prints the following statistics (example for a cube containing dimensions named time, latitude, and longitude):</p> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:09&lt;00:00,  1.10it/s]\nThe data cube has the following characteristics:\n\nVariable:             Land Surface Temperature\nShape:                (time: 10, lat: 2160, lon: 4320)\nTime range:           2002-05-21 - 2002-08-01\nLatitude range:       -89.958\u00b0 - 89.958\u00b0\nLongitude range:      -179.958\u00b0 - 179.958\u00b0\nTotal size:           93312000\nSize of each layer:   9331200\nTotal gap size:       74069847 -&gt; 79 %\nMaximum gap size:     87 % on 2002-06-06\nMinimum gap size:     75 % on 2002-08-01\nValue range:          222.99 - 339.32\n</code></pre> <p>Utiliting the get_gap_heat_map the amount of missing values over time can be computed for every latitude/longitude pixel:</p> <pre><code>import xarray as xr\nfrom ml4xcube.plotting import plot_slice\nfrom ml4xcube.insights import get_gap_heat_map\n\n# Load sample data\nds = xr.open_zarr('sample_data.zarr')\nds = ds['temperature']\n\n# Generate and visualize the gap heat map\ngap_heat_map = get_gap_heat_map(ds)\ndataset   = gap_heat_map.to_dataset(name='temperature')\n\nplot_slice(\n    ds          = dataset,\n    var_to_plot = 'temperature',\n    color_map   = \"plasma\",\n    title       = \"Filled artificial gaps matrix\",\n    label       = \"Number of gaps\",\n    xdim        = \"lon\",\n    ydim        = \"lat\"\n)\n</code></pre> <p>Running this example results in an the following illustration, showing a heatmap of data gaps in the land surface temperature variable over time. The number of available data ranges from 0 to 10, corresponding to the 10 frames in the analyzed cube:</p> <p> </p> <p> Heatmap of available data in the land surface temperature variable over time. </p>"},{"location":"ml-toolkit/example/#5-gapfilling","title":"5. Gapfilling","text":"<p>The gapfilling module provides a method for filling gaps in ESDCs, particularly tailored for remote sensing datasets (Sarafanov et al. 2020). This approach utilizes a support vector regression model to predict missing values based on available data.</p> <p>After examining the amount of missing values in the cube, the module can be applied to fill the corresponding areas in the cube as showcased in the following example:</p>"},{"location":"ml-toolkit/example/#demo-notebook_2","title":"Demo Notebook","text":"<ol> <li>Gap Filling Process.</li> </ol>"},{"location":"ml-toolkit/example/#6-predictions-for-multidimensional-samples","title":"6. Predictions for Multidimensional Samples","text":"<p>An alternative to gap filling can be using simpler methods. For example, missing values can be imputed by replacing them with the mean or a constant placeholder. After exploring the data, it might be evident that gaps are not frequent. In some cases, in environmental modeling for specific regions, missing values may be intentional. For instance, values may appear only in terrestrial regions.</p> <p>In such scenarios, data imputation can enable the effective use of the entire dataset, allowing for model training and analysis without the complications of incomplete data.</p> <p>The following notebooks demonstrate the workflow for land surface temperature prediction using multidimensional data with missing values.</p>"},{"location":"ml-toolkit/example/#demo-notebooks_1","title":"Demo Notebooks","text":"<ol> <li>Machine Learning for Multidimensional Samples (PyTorch).</li> <li>Machine Learning for Multidimensional Samples (TensorFlow).</li> </ol> <p> Filling areas outside the continent with constant value. </p>"},{"location":"ml-toolkit/getting-started/","title":"Getting Started","text":""},{"location":"ml-toolkit/getting-started/#getting-started","title":"Getting Started","text":"<p><code>ml4xcube</code> is a comprehensive Python-based toolkit designed for researchers and developers in the field of machine learning with an emphasis on <code>xarray</code> data cubes. This toolkit is engineered to provide specialized and robust support for data cube management and analysis, operating with the state-of-the-art machine learning libraries (1) <code>scikit-learn</code>, (2) <code>PyTorch</code> and (3) <code>TensorFlow</code>. </p>"},{"location":"ml-toolkit/getting-started/#installation","title":"Installation","text":"<p>Get started with <code>ml4xcube</code> effortlessly by installing it directly through pip: <pre><code>pip install ml4xcube\n</code></pre> or conda: <pre><code>conda install -c conda-forge ml4xcube\n</code></pre></p>"},{"location":"ml-toolkit/getting-started/#features","title":"Features","text":"<ul> <li>Data preprocessing and postprocessing functions</li> <li>Filling masked data and gap filling features</li> <li>Dataset creation and train-/ test splitting techniques</li> <li>Trainer classes for <code>sklearn</code>, <code>TensorFlow</code> and <code>PyTorch</code></li> <li>Distributed training framework compatible with <code>PyTorch</code></li> <li>chunk utilities for working with data cubes</li> </ul>"},{"location":"ml-toolkit/getting-started/#requirements","title":"Requirements","text":"Package Versions dask \u22652023.2.0 numpy \u22651.24 pandas \u22652.2 scikit-learn &gt;1.3.1 xarray &gt;2023.8.0 zarr &gt;2.11 rechunker \u22650.5.1 <p>Make sure you have Python version 3.8 or higher.</p> <p>If you're planning to use <code>ml4xcube</code> with TensorFlow or PyTorch, set up these frameworks properly in your conda environment. </p>"},{"location":"ml-toolkit/introduction/","title":"Introduction","text":""},{"location":"ml-toolkit/introduction/#deepesdl-ml4xcube-machine-learning-toolkits-for-data-cubes","title":"DeepESDL ml4xcube - Machine Learning Toolkits for Data Cubes","text":"<p>AI is becoming increasingly important in Earth observations as most parts  of the Earth system are continuously monitored by sensors and AI is able to  cope  with both the volume of data and the heterogeneous data  characteristics. For instance, satellites monitor the atmosphere, land, and  ocean with unprecedented accuracy. In course of DeepESDL, the Earth System  Data Lab (ESDL) capabilities have been extended to support the application of  machine learning (ML) methods on Earth System Data Cubes (ESDC). </p> <p>Various Python-based Jupyter Notebooks/Scripts illustrate a use cases to  demonstrate the capabilities of <code>ml4xcube</code>, utilizing state-of-the-art machine  learning libraries on ESDCs in the DeepESDL Hub environment. Each Jupyter  Notebook involves a self-contained workflow, markdown cells,  comments and plots for user-friendly application and guidance and is based  on one of the three well established open source ML libraries respectively:</p> <ol> <li>scikit-learn       For classical machine learning such as support vector machines, decision trees, regressions or clustering, scikit-learn provides a broad set of features that fulfils many basic requirements.</li> <li>PyTorch       For larger neural networks and support for Deep Learning additional ML toolchains are necessary, for example the python-based ML stack PyTorch. With PyTorch, experienced users are supported. It       provides low-level API and allows for flexibility to develop and customize deep learning models. It allows for GPU computation and supports transfer learning, domain adaptation, or diverse methods       for fine tuning of models.</li> <li>TensorFlow/Keras       Keras provides a high-level API that can be run on the popular execution backend TensorFlow. Due to its simplicity, it fits well to the requirements of those Earth system scientist that do not           require to newly develop neuronal network architectures. As PyTorch, TensorFlow is python-based, allows for GPU computation and it supports Deep Learning applications including transfer learning         or domain adaptation.</li> </ol>"},{"location":"ml-toolkit/introduction/#overview","title":"Overview","text":"<ol> <li>Getting Started </li> <li>Example Use Cases and Juypter Notebooks</li> <li>ml4xcube API</li> </ol>"},{"location":"ml-toolkit/api-reference/","title":"Overview","text":""},{"location":"ml-toolkit/api-reference/#ml4xcube-api","title":"ml4xcube API","text":""},{"location":"ml-toolkit/api-reference/#1-plotting","title":"1. plotting","text":"<p>The <code>Plotting</code> module provides functionality to visualize slices of data from <code>xarray.DataArray</code>. It facilitates the analysis of multidimensional data. The primary function in this module is <code>plot_slice</code>, which allows users to create visualizations with optional masks to highlight specific regions of interest.</p> <p>Functions:</p> <ul> <li>plot_slice - Renders a 2D slice of an <code>xarray.DataArray</code> with optional emphasis on specific features via masking.</li> </ul>"},{"location":"ml-toolkit/api-reference/#2-insights","title":"2. insights","text":"<p>The <code>insights</code> module offers tools for extracting and analyzing characteristics of multidimensional data cubes from <code>xarray.DataArray</code> objects. This module includes functions to assess the completeness and distribution of data within the cube, helping users understand the dataset's quality and spatial-temporal coverage. The detailed workflow in order to analyze the specifics of a data cube is demonstrated in the following Jupyter Notebook.</p> <p>Functions:</p> <ul> <li>get_insights - Extracts and prints detailed characteristics of a data cube, including dimensions, value ranges, and gaps in the data.</li> <li>get_gap_heat_map - Generates a heat map to visualize the distribution of non-<code>NaN</code> values across selected dimensions, revealing patterns of data availability or missingness.</li> </ul>"},{"location":"ml-toolkit/api-reference/#3-gapfilling","title":"3. gapfilling","text":"<p>The <code>ml4xcube.gapfilling</code> module is designed to address and rectify data gaps in multidimensional geospatial datasets, particularly those represented in <code>xarray.Dataset</code> formats. The gap filling process is divided into three main submodules, each playing a crucial role in the preparation, processing, and application of sophisticated machine learning algorithms to ensure accurate and efficient data imputation. he entire gapfilling process is showcased in the following Jupyter Notebook.</p>"},{"location":"ml-toolkit/api-reference/#31-helperpredictors","title":"3.1. helper.predictors","text":"<p>The <code>HelpingPredictor</code> class within the <code>helper.predictors</code> submodule facilitates the preparation of predictor data for gap filling applications. This submodule focuses on extracting and processing global predictor data, such as land cover classifications, matching them to the corresponding dimensions (e.g., latitude and longitude) of the target data cube. The prepared predictor is then stored in a <code>.zarr</code> dataset, ready to be used across various gap filling applications. If not leveraged during the gap filling process, a Support Vector Machine is trained on artificial gaps within the <code>Gapfiller</code> class.</p> <p>Classes:</p> <ul> <li>HelpingPredictor - Facilitates the preparation of predictor data for gap filling.</li> </ul>"},{"location":"ml-toolkit/api-reference/#32-gap_dataset","title":"3.2. gap_dataset","text":"<p>The <code>GapDataset</code> class in the <code>gap_dataset</code>submodule is designed to prepare data before performing the actual gap filling applications. This submodule focuses on slicing specific dimensions from a data cube, applying optional artificial gaps, and managing datasets for subsequent gap filling operations.</p> <p>Classes:</p> <ul> <li>GapDataset - Prepares data with artificial gaps optionally for training of a regressor and datasets with real gaps before gap filling can be performed.</li> </ul>"},{"location":"ml-toolkit/api-reference/#33-gap_filling","title":"3.3. gap_filling","text":"<p>The <code>Gapfiller</code> class within the <code>gap_filling</code> submodule is an integral part of the <code>ml4xcube.gapfilling</code> module designed to implement and manage the gap filling process using machine learning techniques, specifically focusing on Support Vector Regression (SVR) for now. It allows for the integration of different hyperparameters, and predictors to optimize the gap filling process. A prerequsite before gap filling can be applied, is a specific data preparation step, taken over by the functionalities of the <code>GapDataset</code> class.</p> <p>Classes:</p> <ul> <li>Gapfiller - Optionally trains a predictor to estimate actual values in gaps. Performs gap filling with SVR or a user-provided regressor.</li> </ul>"},{"location":"ml-toolkit/api-reference/#4-splits","title":"4. Splits","text":"<p>The <code>splits</code> module includes functions designed to divide an <code>xarray.Datasets</code> into a train and a test set. These functions use sampling strategies to ensure that data is split in a manner that respects the integrity of spatial and temporal data blocks, facilitating the development of machine learning models. Functions to assign split variables provide structured and random approaches to segmenting the dataset, which are then utilized by the <code>create_split</code> function to generate actual train-test splits.</p> <p>Functions:</p> <ul> <li>assign_block_split - Determines the assignment of data blocks to train or test sets using a deterministic approach based on the Cantor pairing function. This structured random sampling respects data locality and sets up the <code>splits</code> variable used by <code>create_split</code>.</li> <li>assign_rand_split - Randomly assigns a split indicator to each element in the dataset based on a specified proportion. This method provides a randomized approach to setting up the <code>splits</code> variable, which is also used by <code>create_split</code>.</li> <li>create_split - Generates train-test splits for machine learning models by utilizing a predefined <code>splits</code> variable within the dataset. Supports <code>xarray.Dataset</code> or a dictionary of <code>numpy</code> arrays and provides flexibility in specifying feature and target variables, effectively leveraging the split defined by the previous functions.</li> </ul>"},{"location":"ml-toolkit/api-reference/#5-preprocessing","title":"5. preprocessing","text":"<p>The <code>preprocessing</code> module provides a collection of functions for preparing and processing data from <code>xarray.Datasets</code>, particularly focusing on operations commonly required in data science and machine learning workflows. These functions include filtering, filling missing data, calculating statistics, and normalizing or standardizing data.</p> <p>Functions:</p> <ul> <li>apply_filter - Applies a specified filter to the data by setting all values to NaN which do not belong to the mask or dropping the entire sample.</li> <li>assign_mask - Assigns a mask to the dataset for later data division or filtering.</li> <li>drop_nan_values - Filters out samples from a dataset if they contain any <code>NaN</code> values, with an optional mask to determine sample validity. It handles both 1D and multi-dimensional samples.</li> <li>fill_nan_values - Fills <code>NaN</code> values in the dataset using a specified method.</li> <li>get_range - Computes the range (min and max) of the data.</li> <li>get_statistics - Computes the mean and standard deviation of a specified variable.</li> <li>normalize - Normalizes the data to the range [0,1].</li> <li>standardize - Standardizes the data to have a mean of 0 and variance of 1.</li> </ul>"},{"location":"ml-toolkit/api-reference/#6-datasets","title":"6. datasets","text":"<p>The datasets module is a comprehensive suite designed to handle, process, and prepare data cubes for machine learning applications. This module supports various data scales and integrates seamlessly with major deep learning frameworks like PyTorch and TensorFlow, ensuring that data stored in <code>xarray</code> datasets is optimally formatted and ready for training deep learning models.</p>"},{"location":"ml-toolkit/api-reference/#61-multiproc_sampler","title":"6.1. multiproc_sampler","text":"<p>The <code>MultiProcSampler</code> class is designed to process and sample large multidimensional training and testing datasets efficiently using parallel processing, specifically tailored for machine learning model training in the <code>ml4xcube</code> framework.</p> <p>Classes:</p> <ul> <li>MultiProcSampler - Samples train and test data as <code>.zarr</code> datasets.</li> </ul>"},{"location":"ml-toolkit/api-reference/#62-pytorch","title":"6.2. pytorch","text":"<p>The <code>datasets.pytorch</code> module integrates with <code>PyTorch</code> to manage and process large datasets efficiently. This module utilizes the power of <code>PyTorch</code>'s <code>Dataset</code> and <code>DataLoader</code> functionalities to prepare and iterate over chunks of data cubes for deep learning applications, ensuring that data management is scalable and performance-optimized.</p> <p>Classes:</p> <ul> <li>PTXrDataset - Corresponds to a subclass of PyTorch\u2019s Dataset, designed specifically to handle large datasets based on a provided <code>xarray.Dataset</code>.</li> </ul> <p>Functions:</p> <ul> <li>prep_dataloader - Sets up one or two <code>DataLoader</code>s from a PyTorch <code>Dataset</code> which was sampled from an <code>xarray.Dataset</code>. If a test set is provided, two <code>DataLoader</code>s are returned; otherwise, one.</li> </ul>"},{"location":"ml-toolkit/api-reference/#63-tensorflow","title":"6.3. tensorflow","text":"<p>The <code>datasets.tensorflow module</code> is specifically designed to handle and iterate over large <code>xarray</code> datasets and efficiently prepare them for use with TensorFlow models. This module provides a seamless interface to transform data stored in <code>xarray</code> datasets into structured TensorFlow datasets that can be directly utilized in training and inference pipelines. The core functionality is encapsulated in the <code>TFXrDataset</code> class, which leverages TensorFlow's capabilities to manage data flow dynamically, supporting scalable machine learning operations on large datasets.</p> <p>Classes:</p> <ul> <li>TFXrDataset - TensorFlow specific implementation to handle and iterate over large <code>xarray</code> datasets.</li> </ul>"},{"location":"ml-toolkit/api-reference/#64-xr_dataset","title":"6.4. xr_dataset","text":"<p>The <code>XrDataset</code> class within the <code>datasets/xr-dataset</code> module is tailored to efficiently manage and process smaller datasets directly within memory, leveraging in-memory operations to enhance both speed and performance.</p> <p>Classes:</p> <ul> <li>XrDataset - Creates small datasets manageable in memory.</li> </ul>"},{"location":"ml-toolkit/api-reference/#7-training","title":"7. training","text":"<p>The training module serves as a comprehensive suite for training machine learning models across various frameworks, designed to accommodate the unique demands of large-scale and high-dimensional datasets typically encountered in geospatial analysis and beyond. This module streamlines the training process, offering specialized support for PyTorch, TensorFlow, and scikit-learn, enabling users to leverage the strengths of these popular frameworks efficiently.</p>"},{"location":"ml-toolkit/api-reference/#71-pytorch","title":"7.1. pytorch","text":"<p>The <code>training.pytorch</code> module provides tools for training PyTorch models. It includes functionalities such as early stopping, model checkpointing, and performance logging, ensuring efficient training and optimization of models.</p> <p>Classes:</p> <ul> <li>Trainer - Tailored for the training of PyTorch models.</li> </ul>"},{"location":"ml-toolkit/api-reference/#72-pytorch_distributed","title":"7.2. pytorch_distributed","text":"<p>The training.pytorch_distributed module is designed to facilitate efficient distributed training of PyTorch models across multiple GPUs or nodes. This module leverages PyTorch's DistributedDataParallel (DDP) functionality, providing tools to handle complex distributed training tasks with ease, including setup, execution, and synchronization across multiple processes.</p> <p>Classes:</p> <ul> <li>Trainer - Crafted to perform distributed training for PyTorch models.</li> </ul> <p>Functions:</p> <ul> <li>ddp_init - Initializes the distributed process group for GPU-based distributed training.</li> </ul>"},{"location":"ml-toolkit/api-reference/#73-sklearn","title":"7.3. sklearn","text":"<p>The <code>training.sklearn</code> module is tailored to train scikit-learn models efficiently. It supports batch training for handling large datasets and provides tools for evaluating model performance using various metrics, catering to both supervised and unsupervised learning tasks.</p> <p>Classes:</p> <ul> <li>Trainer - Designed for training scikit-learn models.</li> </ul>"},{"location":"ml-toolkit/api-reference/#74-tensorflow","title":"7.4. tensorflow","text":"<p>The <code>training.tensorflow</code> module is specifically designed for training TensorFlow models. This module provides a comprehensive suite of tools for training, evaluating, and monitoring TensorFlow models, particularly those used in processing large datasets typically encountered in fields such as geospatial analysis.</p> <p>Classes:</p> <ul> <li>Trainer - Created to facilitate the training of TensorFlow models.</li> </ul>"},{"location":"ml-toolkit/api-reference/#8-postprocessing","title":"8. postprocessing","text":"<p>The <code>preprocessing</code> module provides functionalities, which are commonly required after machine learning operations to receive the final predictions.</p> <p>Functions:</p> <ul> <li>undo_normalizing - Reverts the normalization process to obtain the original data range.</li> <li>undo_standardizing - Reverts the standardization process to obtain the original data scale.</li> </ul>"},{"location":"ml-toolkit/api-reference/#9-evaluation","title":"9. evaluation","text":"<p>The evaluation module in the <code>ml4xcube</code> API is designed to support comprehensive metric evaluation for machine learning models across various frameworks including PyTorch, TensorFlow, and Scikit-learn. This module supports with assessing model performance during validation or testing phases, providing a range of metrics to evaluate accuracy, error rates, and other critical performance indicators. Providing a unified access to metrics from the different frameworks</p>"},{"location":"ml-toolkit/api-reference/#91-evaluator","title":"9.1. evaluator","text":"<p>The <code>Evaluator</code> class is tailored to handle metric evaluations, allowing users to measure and analyze model performance using metrics suited to their specific framework.</p> <p>Classes:</p> <ul> <li>Evaluator - Facilitates metric evaluation across different machine learning frameworks, enabling the assessment of various performance metrics during model validation or testing.</li> </ul>"},{"location":"ml-toolkit/api-reference/#10-utils","title":"10. utils","text":"<p>The <code>utils</code> module provides a set of utility functions for handling and processing <code>xarray.Datasets</code>. These functions facilitate tasks such as rechunking datasets, retrieving specific data chunks, and iterating over data blocks. They are particularly helpful for optimizing the performance of data operations and preparing datasets for machine learning tasks.</p> <p>Functions:</p> <ul> <li>assign_dims - Assign dimensions to each <code>dask.array</code> or <code>xarray.DataArray</code> within a dictionary.</li> <li>calculate_total_chunks - Compute the number of chunks of an <code>xarray.Dataset</code>.</li> <li>get_chunk_by_index - Retrieve a specific data chunk from an <code>xarray.Dataset</code>.</li> <li>get_chunk_sizes - Determine maximum chunk sizes of all data variables of the <code>xarray.Dataset</code>.</li> <li>get_dim_range - Calculates the dimension range of an <code>xarray.DataArray</code> dimension.</li> <li>iter_data_var_blocks - Create an iterator over chunks of an <code>xarray.Dataset</code>.</li> <li>rechunk_cube - Rechunks an <code>xarray.DataArray</code> to a new chunking scheme and stores the result at a specified path.</li> <li>split_chunk - Split a chunk into data samples for subsequent machine learning training.</li> </ul>"},{"location":"ml-toolkit/api-reference/1-plotting/plot-slice/","title":"Plot slice","text":""},{"location":"ml-toolkit/api-reference/1-plotting/plot-slice/#plot_slice","title":"plot_slice","text":"<pre><code>def plot_slice(\n    ds: xr.DataArray, var_to_plot: str, xdim: str, ydim: str, filter_var: str ='land_mask', title: str ='Slice Plot',\n    label: str ='Cube Slice', color_map: str ='viridis', xlabel: str ='Longitude', ylabel: str ='Latitude',\n    save_fig: bool = False, file_name: str ='plot.png', fig_size: Tuple[int, int] =(15, 10), vmin: float = None,\n    vmax: float = None, ticks: List[float] = None\n) -&gt; None\n</code></pre>"},{"location":"ml-toolkit/api-reference/1-plotting/plot-slice/#description","title":"Description","text":"<p>The <code>plot_slice</code> function plots a slice of data from an <code>xarray.DataArray</code> with an optional mask for context. It visualizes the specified variable using a heatmap and can highlight land borders if a mask is provided (e.g. land mask for ESDC). The function also supports saving the plot to a file.</p>"},{"location":"ml-toolkit/api-reference/1-plotting/plot-slice/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.DataArray</code>): DataArray containing the data to plot.</li> <li>var_to_plot (<code>str</code>): Name of the variable to visualize.</li> <li>xdim (<code>str</code>): Name of the x dimension to plot (e.g., longitude).</li> <li>ydim (<code>str</code>): Name of the y dimension to plot (e.g., latitude).</li> <li>filter_var (<code>str</code>): Name of the variable used for masking relevant areas for plotting. Defaults to <code>'land_mask'</code>.</li> <li>title (<code>str</code>): Title of the plot. Defaults to <code>'Cube Slice Plot'</code>.</li> <li>label (<code>str</code>): Legend label for the plot. Defaults to <code>'Cube Slice'</code>.</li> <li>color_map (<code>str</code>): Color map to use for the plot. Defaults to <code>'viridis'</code>.</li> <li>xlabel (<code>str</code>): Label for the x-axis. Defaults to <code>'Longitude'</code>.</li> <li>ylabel (<code>str</code>): Label for the y-axis. Defaults to <code>'Latitude'</code>.</li> <li>save_fig (<code>bool</code>): If <code>True</code>, saves the figure to a file. Defaults to <code>False</code>.</li> <li>file_name (<code>str</code>): Name of the file to save the plot to, if <code>save_fig</code> is <code>True</code>. Defaults to <code>'plot.png'</code>.</li> <li>fig_size (<code>tuple</code>): Size of the figure to create. Defaults to <code>(15, 10)</code>.</li> <li>vmin (<code>float</code>): Minimum value for the color bar. Defaults to <code>None</code>.</li> <li>vmax (<code>float</code>): MMaximum value for the color bar. Defaults to<code>None</code>.</li> <li>ticks (<code>List[float]</code>): List of tick values for the color bar. Defaults to <code>None</code>.</li> </ul>"},{"location":"ml-toolkit/api-reference/1-plotting/plot-slice/#returns","title":"Returns","text":"<ul> <li><code>None</code>: The function creates a plot but does not return any value.</li> </ul>"},{"location":"ml-toolkit/api-reference/1-plotting/plot-slice/#example","title":"Example","text":"<pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.xr_plots import plot_slice\n\n# Example usage with a sample dataset\nds = xr.Dataset({\n    'temperature': (('time', 'latitude', 'longitude'), np.random.rand(10, 20, 30)),\n    'precipitation': (('time', 'latitude', 'longitude'), np.random.rand(10, 20, 30)),\n    'land_mask': (('latitude', 'longitude'), np.random.choice([True, False], size=(20, 30)))\n})\n\n# Select a specific time slice (valid index within the example data range)\nds_slice = ds.isel(time=0)\n\n# Plot the slice\nplot_slice(\n    ds          = ds_slice,\n    var_to_plot = 'temperature',\n    xdim        = 'longitude',\n    ydim        = 'latitude',\n    filter_var  = 'land_mask',\n    title       = 'Temperature Plot',\n    label       = 'Temperature (\u00b0C)',\n    color_map   = 'coolwarm',\n    xlabel      = 'Longitude',\n    ylabel      = 'Latitude',\n    save_fig    = True,\n    file_name   = 'temperature_plot.png',\n    fig_size    = (12, 8),\n    vmin        = -10,\n    vmax        = 30,\n    ticks       = [-10, 0, 10, 20, 30]\n)\n</code></pre>"},{"location":"ml-toolkit/api-reference/1-plotting/plot-slice/#notes","title":"Notes","text":"<ul> <li>Ensure that the <code>filter_var</code> is present in the dataset to use it as a mask.</li> <li>Adjust the <code>fig_size</code>, <code>vmin</code>, <code>vmax</code>, and <code>ticks</code> parameters as needed to customize the plot appearance.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/assign-dims/","title":"Assign dims","text":""},{"location":"ml-toolkit/api-reference/10-utils/assign-dims/#assign_dims","title":"assign_dims","text":"<pre><code>def assign_dims(data: Dict[str, da.Array|xr.DataArray], dims: Tuple[str]) -&gt; Dict[str, xr.DataArray]\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/assign-dims/#description","title":"Description","text":"<p>The <code>assign_dims</code> function assigns dimension names to each variable in a dataset based on the provided dimension names. This function is useful for standardizing the dimensional metadata of dask arrays or xarray DataArrays within a dictionary and for the creation of xarray Datasets.</p>"},{"location":"ml-toolkit/api-reference/10-utils/assign-dims/#parameters","title":"Parameters","text":"<ul> <li>data (<code>Dict[str, dask.array | xarray.DataArray]</code>): A dictionary where keys are variable names and values are of type <code>dask.array</code> or <code>xarray.DataArray</code>.</li> <li>dims (<code>Tuple[str]</code>): A tuple of dimension names to assign to the arrays.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/assign-dims/#returns","title":"Returns","text":"<ul> <li><code>Dict[str, xarray.DataArray]</code>: A dictionary where keys are variable names and values are of type<code>xarray.DataArray</code> with assigned dimensions.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/assign-dims/#example","title":"Example","text":"<p><pre><code>import dask.array as da\nfrom ml4xcube.utils import assign_dims\n\n# Example data\ndata = {\n    'temperature': da.random.random((10, 20, 30)),\n    'precipitation': da.random.random((10, 20, 30))\n}\n\n# Assign dimensions\ndims = ('time', 'lat', 'lon')\nassigned_dims_data = assign_dims(data, dims)\n\n# Output the data with assigned dimensions\nfor var, dataarray in assigned_dims_data.items():\n    print(f\"{var}: {dataarray.dims}\")\n</code></pre> The dimensions 'time', 'lat', and 'lon' are assigned to the data arrays in the dictionary.</p>"},{"location":"ml-toolkit/api-reference/10-utils/calculate-total-chunks/","title":"Calculate total chunks","text":""},{"location":"ml-toolkit/api-reference/10-utils/calculate-total-chunks/#calculate_total_chunks","title":"calculate_total_chunks","text":"<pre><code>def calculate_total_chunks(ds: xr.Dataset, block_size: List[Tuple[str, int]] = None) -&gt; int\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/calculate-total-chunks/#description","title":"Description","text":"<p>The total number of chunks for an <code>xarray.Dataset</code> is calculated based on specified or default chunk sizes. </p>"},{"location":"ml-toolkit/api-reference/10-utils/calculate-total-chunks/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The dataset for which the total number of chunks will be calculated. The dataset should have dimensions that can be chunked.</li> <li>block_size (<code>Optional[List[Tuple[str, int]]]</code>): A sequence of tuples specifying the block size for each dimension. Each tuple should contain a dimension name and a block size for that dimension. If not provided, the function will use the dataset's default chunk sizes.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/calculate-total-chunks/#returns","title":"Returns","text":"<ul> <li><code>int</code>: The total number of chunks in the dataset based on the specified or default chunk sizes.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/calculate-total-chunks/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.utils import calculate_total_chunks\n\n# Example dataset\ndata = np.random.rand(100, 200, 300)\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), data),\n    'precipitation': (('time', 'lat', 'lon'), data)\n})\n\n# Define the block size (chunk size)\nblock_size = [('time', 10), ('lat', 20), ('lon', 30)]\n\n# Calculate total chunks\ntotal_chunks = calculate_total_chunks(ds, block_size=block_size)\nprint(f\"Total number of chunks: {total_chunks}\")\n</code></pre> Based on the specified blocks the number of chunks is calculated.</p>"},{"location":"ml-toolkit/api-reference/10-utils/calculate-total-chunks/#notes","title":"Notes","text":"<ul> <li>The <code>block_size</code> parameter allows you to specify custom chunk sizes. If not provided, the function will use the dataset's default chunk sizes.</li> <li>Ensure that the provided <code>block_size</code> values are appropriate for the dimensions of the dataset.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-by-index/","title":"Get chunk by index","text":""},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-by-index/#get_chunk_by_index","title":"get_chunk_by_index","text":"<pre><code>def get_chunk_by_index(ds: xr.Dataset, index: int, block_size: List[Tuple[str, int]] = None) -&gt; Dict[str, np.ndarray]\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-by-index/#description","title":"Description","text":"<p><code>get_chunk_by_index</code> retrieves a specific data chunk from an <code>xarray.Dataset</code> based on a given linear index. This way the extraction of subsets is feasible. Further the chunks of a cube can be iterated.</p>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-by-index/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The <code>xarray.Dataset</code> from which to retrieve a chunk.</li> <li>index (<code>int</code>): The index of the chunk to retrieve. This index will be converted to a multi-dimensional index based on the chunk sizes.</li> <li>block_size (<code>List[Tuple[str, int]]</code>): An optional list of tuples specifying the block size for each dimension. Each tuple should contain a dimension name and a block size for that dimension. A chunk with the specified block sizes will be returned. If not provided, the function will use the dataset's default chunk sizes.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-by-index/#returns","title":"Returns","text":"<ul> <li><code>Dict[str, np.ndarray]</code>: A dictionary where keys are variable names from the dataset and values are NumPy arrays containing the data of the specified chunk.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-by-index/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.utils import get_chunk_by_index\n\n# Example dataset\ndata = np.random.rand(100, 200, 300)\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), data),\n    'precipitation': (('time', 'lat', 'lon'), data)\n})\n\n# Define the block size (chunk size)\nblock_size = [('time', 10), ('lat', 20), ('lon', 30)]\n\n# Get the 5th chunk (index starts from 0)\nchunk_data = get_chunk_by_index(ds, index=5, block_size=block_size)\n\n# Output the chunk data\nfor var_name, chunk in chunk_data.items():\n    print(f\"{var_name} chunk shape: {chunk.shape}\")\n</code></pre> The <code>get_chunk_by_index</code> function retrieves the 5th chunk from the dataset, using the specified block sizes for each dimension.</p>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-sizes/","title":"Get chunk sizes","text":""},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-sizes/#get_chunk_sizes","title":"get_chunk_sizes","text":"<pre><code>def get_chunk_sizes(ds: xr.Dataset) -&gt; List[Tuple[str, int]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-sizes/#description","title":"Description","text":"<p>The maximum chunk sizes for all data variables in a given <code>xarray.Dataset</code> is determined. This allows to understand the chunking scheme of the dataset and for setting up consistent chunk sizes for processing.</p>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-sizes/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The dataset for which the maximum chunk sizes are to be determined. The dataset should have dimensions that can be chunked.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-sizes/#returns","title":"Returns","text":"<ul> <li><code>List[Tuple[str, int]]</code>: A list of tuples where each tuple contains a dimension name (str) and its corresponding maximum chunk size (int) over all variables.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-sizes/#example","title":"Example","text":"<pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.utils import get_chunk_sizes\n\n# Example dataset with chunking\ndata = np.random.rand(100, 200, 300)\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), data),\n    'precipitation': (('time', 'lat', 'lon'), data)\n}).chunk({'time': 10, 'lat': 20, 'lon': 30})\n\n# Get maximum chunk sizes\nchunk_sizes = get_chunk_sizes(ds)\nprint(chunk_sizes)\n</code></pre> <p>In this example, the <code>get_chunk_sizes</code> function returns the chunk sizes for each dimension in the dataset.</p>"},{"location":"ml-toolkit/api-reference/10-utils/get-chunk-sizes/#notes","title":"Notes","text":"<ul> <li>The function iterates over all data variables in the dataset and retrieves their chunk sizes.</li> <li>If the variable has chunk sizes, it calculates the maximum chunk size for each dimension.</li> <li>The returned list contains tuples with dimension names and their maximum chunk sizes, which can be used for further processing.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-dim-range/","title":"Get dim range","text":""},{"location":"ml-toolkit/api-reference/10-utils/get-dim-range/#get_dim_range","title":"get_dim_range","text":"<pre><code>def get_dim_range(cube: xr.DataArray, dim: str) -&gt; Union[Tuple[float, float], Tuple[str, str]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/get-dim-range/#description","title":"Description","text":"<p>Calculates and returns the minimum and maximum values of a specified dimension within an <code>xarray.Dataset</code>. This function supports dimensions with numerical or datetime data types, making it versatile for a range of data analysis contexts.</p>"},{"location":"ml-toolkit/api-reference/10-utils/get-dim-range/#parameters","title":"Parameters","text":"<ul> <li>cube (<code>xarray.DataArray</code>): The input data cube from which the dimension range is to be calculated.</li> <li>dim (<code>str</code>): The name of the dimension for which the range is to be determined.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-dim-range/#returns","title":"Returns","text":"<ul> <li><code>Union[Tuple[float, float], Tuple[str, str]]</code>: The minimum and maximum values of the dimension. For datetime dimensions, these values are formatted as strings; for numerical dimensions, they are returned as numbers.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/get-dim-range/#example","title":"Example","text":"<p>Here's how you might use the <code>get_dim_range</code> function to find the range of the 'time' dimension in a dataset: <pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.utils import get_dim_range\n\n# Create a sample DataArray with datetime and numerical data\ntimes = np.array(['2020-01-01', '2020-01-02', '2020-01-03'], dtype='datetime64[D]')\ndata = np.random.rand(3, 2, 2)  # Random data for 3 days, 2 latitudes, and 2 longitudes\ncube = xr.DataArray(data, dims=['time', 'latitude', 'longitude'], coords={'time': times})\n\n# Get the range of the 'time' dimension\ntime_range = get_dim_range(cube, 'time')\nprint(\"Time Dimension Range:\", time_range)\n</code></pre> A simple <code>xarray.DataArray</code> with a 'time' dimension is created and <code>get_dim_range</code> is used to extract and print the date range.</p>"},{"location":"ml-toolkit/api-reference/10-utils/iter-data-var-blocks/","title":"Iter data var blocks","text":""},{"location":"ml-toolkit/api-reference/10-utils/iter-data-var-blocks/#iter_data_var_blocks","title":"iter_data_var_blocks","text":"<pre><code>def iter_data_var_blocks(ds: xr.Dataset, block_size: List[Tuple[str, int]] = None) -&gt; Iterator[Dict[str, np.ndarray]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/iter-data-var-blocks/#description","title":"Description","text":"<p>An iterator that provides all data blocks of all data variables in the given dataset is created. Allows iterating over chunks.</p>"},{"location":"ml-toolkit/api-reference/10-utils/iter-data-var-blocks/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The dataset to iterate over</li> <li>block_size (<code>List[Tuple[str, int]]</code>): A sequence of tuples specifying the block size for each dimension. Each tuple should contain a dimension name and a block size for that dimension. If not provided, the function will use the dataset's default chunk sizes.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/iter-data-var-blocks/#yields","title":"Yields","text":"<ul> <li><code>Iterator[Dict[str, numpy.ndarray]]</code>: An iterator of dictionaries where keys are variable names from the dataset and values are data blocks as NumPy arrays. Each iteration yields a dictionary representing a single data block for all variables.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/iter-data-var-blocks/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.utils import iter_data_var_blocks\n\n# Example dataset\ndata = np.random.rand(100, 200, 300)\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), data),\n    'precipitation': (('time', 'lat', 'lon'), data)\n})\n\n# Define the block size (chunk size)\nblock_size = [('time', 10), ('lat', 20), ('lon', 30)]\n\n# Iterate over data blocks\nfor block in iter_data_var_blocks(ds, block_size=block_size):\n    for var_name, chunk in block.items():\n        print(f\"{var_name} chunk shape: {chunk.shape}\")\n</code></pre> The <code>iter_data_var_blocks</code> function iterates over the dataset, yielding chunks of data according to the specified block sizes.</p>"},{"location":"ml-toolkit/api-reference/10-utils/iter-data-var-blocks/#notes","title":"Notes","text":"<ul> <li>The <code>block_size</code> parameter allows you to specify custom chunk sizes. If not provided, the function will use the dataset's default chunk sizes.</li> <li>Ensure that the provided <code>block_size</code> values are appropriate for the dimensions of the dataset.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/rechunk-cube/","title":"Rechunk cube","text":""},{"location":"ml-toolkit/api-reference/10-utils/rechunk-cube/#rechunk_cube","title":"rechunk_cube","text":"<pre><code>def rechunk_cube(source_cube: xr.DataArray, target_chunks: Dict[str, int] | Tuple[int] | List[int], target_path: str)\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/rechunk-cube/#description","title":"Description","text":"<p>The <code>rechunk_cube</code> function rechunks an xarray <code>DataArray</code> to a new chunking scheme and stores the result at the specified path. </p>"},{"location":"ml-toolkit/api-reference/10-utils/rechunk-cube/#parameters","title":"Parameters","text":"<ul> <li>source_cube (<code>xarray.DataArray</code>): The input <code>DataArray</code> that you want to rechunk. This <code>DataArray</code> should be already chunked or be capable of being chunked.</li> <li>target_chunks (<code>Dict[str, int] | Tuple[int] | List[int]</code>): The desired chunk sizes for the rechunking operation. This can be specified in different formats:</li> <li>Dictionary: Specify sizes for each named dimension, e.g., <code>{'lon': 60, 'lat': 1, 'time': 100}</code>. </li> <li>Tuple or List: Specify sizes by order, corresponding to the array's dimensions, e.g., <code>(60, 1, 100)</code>.</li> <li>target_path (<code>str</code>): The path where the rechunked <code>DataArray</code> should be stored, typically a path to a Zarr store. </li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/rechunk-cube/#returns","title":"Returns","text":"<ul> <li><code>None</code>: The function does not return any value. It prints a message upon successful completion of the rechunking process.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/rechunk-cube/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.utils import rechunk_cube\n\n# Example data\nsource_cube = xr.DataArray(\n    np.random.rand(100, 200, 300),\n    dims=['time', 'lat', 'lon'],\n    name='example_data'\n)\n\n# Desired chunk sizes\ntarget_chunks = {'time': 10, 'lat': 20, 'lon': 30}\n\n# Rechunk the DataArray\nrechunk_cube(source_cube, target_chunks=target_chunks, target_path='rechunked_data.zarr')\n</code></pre> In this example, the <code>source_cube</code> is rechunked according to the specified <code>target_chunks</code> and stored at the given <code>target_path</code>.</p>"},{"location":"ml-toolkit/api-reference/10-utils/split-chunk/","title":"Split chunk","text":""},{"location":"ml-toolkit/api-reference/10-utils/split-chunk/#split_chunk","title":"split_chunk","text":"<pre><code>def split_chunk(chunk: Dict[str, np.ndarray], sample_size: List[Tuple[str, int]] = None,\n                overlap: List[Tuple[str, float]] = None) -&gt; Dict[str, np.ndarray]\n</code></pre>"},{"location":"ml-toolkit/api-reference/10-utils/split-chunk/#description","title":"Description","text":"<p>The <code>split_chunk</code> function splits a given chunk of data into smaller data samples or points based on the provided sample size and optional overlap configurations. This function is useful for data preprocessing, particularly when dealing with large datasets that need to be divided into manageable parts for analysis or model training.</p>"},{"location":"ml-toolkit/api-reference/10-utils/split-chunk/#parameters","title":"Parameters","text":"<ul> <li>chunk (<code>Dict[str, numpy.ndarray]</code>): A dictionary where keys are variable names and values are NumPy arrays representing the data chunk to be split.</li> <li>sample_size (<code>List[Tuple[str, int]]</code>): A list of tuples specifying the sample size for each dimension. Each tuple consists of a dimension name (str) and the corresponding sample size (int). If <code>None</code> the chunk is split into points.</li> <li>overlap (<code>List[Tuple[str, float]]</code>): A list of tuples specifying the overlap for overlapping samples due to chunk splitting. Each tuple consists of a dimension name (str) and the overlap percentage (float) between 0 and 1. If <code>None</code>, the resulting samples don't overlap.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/split-chunk/#returns","title":"Returns","text":"<ul> <li><code>Dict[str, np.ndarray]</code>: A dictionary where keys are variable names and values are NumPy arrays containing the split data samples or points.</li> </ul>"},{"location":"ml-toolkit/api-reference/10-utils/split-chunk/#example","title":"Example","text":"<pre><code>import numpy as np\nfrom ml4xcube.utils import split_chunk\n\n# Example chunk data\nchunk = {\n    'temperature': np.random.rand(10, 10, 10),\n    'precipitation': np.random.rand(10, 10, 10)\n}\n\n# Split the chunk\nsplit_data = split_chunk(chunk, sample_size=[('time', 5), ('lat', 5), ('lon', 5)], overlap=[('time', 0.5), ('lat', 0.5), ('lon', 0.5)])\nprint(split_data)\n</code></pre> <p>The above code splits the chunk of data into smaller samples based on the specified sample size and optional overlap, resulting in a dictionary with the split data for each variable.</p>"},{"location":"ml-toolkit/api-reference/10-utils/split-chunk/#notes","title":"Notes","text":"<ul> <li>The <code>overlap</code> parameter allows you to specify overlapping regions between the samples, which can be useful for certain types of analyses or training machine learning models where context from neighboring samples is important.</li> <li>Ensure that the specified <code>sample_size</code> and overlap values are appropriate for the dimensions and size of the input chunk to avoid errors or unexpected behavior.</li> </ul>"},{"location":"ml-toolkit/api-reference/2-insights/get-count-heat-map/","title":"Get count heat map","text":""},{"location":"ml-toolkit/api-reference/2-insights/get-count-heat-map/#get_gap_heat_map","title":"get_gap_heat_map","text":"<pre><code>def get_gap_heat_map(cube: xr.DataArray, count_dim: str) -&gt; xr.DataArray\n</code></pre>"},{"location":"ml-toolkit/api-reference/2-insights/get-count-heat-map/#description","title":"Description","text":"<p>A heat map of value counts (non-<code>NaN</code> values) for each pixel of dimensions in an <code>xarray.DataArray</code> is genrated.  This heat map helps in visualizing the distribution and density of gaps across the spatial dimensions.</p>"},{"location":"ml-toolkit/api-reference/2-insights/get-count-heat-map/#parameters","title":"Parameters","text":"<ul> <li>cube (<code>xarray.DataArray</code>): The input data cube.</li> <li>count_dim (<code>str</code>): The dimension along which to count non-<code>NaN</code> values, typically spatial dimensions such as 'latitude' or 'longitude'.</li> </ul>"},{"location":"ml-toolkit/api-reference/2-insights/get-count-heat-map/#returns","title":"Returns","text":"<ul> <li><code>xarray.DataArray</code>: Heat map of non-<code>NaN</code> value counts for each pixel across one dimension.</li> </ul>"},{"location":"ml-toolkit/api-reference/2-insights/get-count-heat-map/#example","title":"Example","text":"<p><pre><code>import xarray as xr\nfrom ml4xcube.xr_plots import plot_slice\nfrom ml4xcube.cube_insights import get_gap_heat_map\n\n# Load sample data\nds = xr.open_zarr('sample_data.zarr')\nds = ds['temperature']\n\n# Generate and visualize the gap heat map\ngap_heat_map = get_gap_heat_map(ds)\ndataset   = gap_heat_map.to_dataset(name='temperature')\n\nplot_slice(\n    ds          = dataset,\n    var_to_plot = 'temperature', \n    color_map   = \"plasma\",\n    title       = \"Filled artificial gaps matrix\",\n    label       = \"Number of gaps\",\n    xdim        = \"lon\",\n    ydim        = \"lat\"\n)\n</code></pre> Running this example results in an illustration as the following:</p> <p> </p>"},{"location":"ml-toolkit/api-reference/2-insights/get-insights/","title":"Get insights","text":""},{"location":"ml-toolkit/api-reference/2-insights/get-insights/#get_insights","title":"get_insights","text":"<pre><code>def get_insights(cube: xr.Dataset, variable: str, layer_dim: str = None) -&gt; None\n</code></pre>"},{"location":"ml-toolkit/api-reference/2-insights/get-insights/#description","title":"Description","text":"<p>Various characteristics of a data cube represented by an <code>xarray.DataArray</code> are extracted and printed.  The function provides information such as the variable's name, dimension names and ranges, the size of the data cube, the number and percentage of <code>NaN</code> values, and the range of data values.  Additionally, it identifies the maximum and minimum gap sizes within the data cube layers</p>"},{"location":"ml-toolkit/api-reference/2-insights/get-insights/#parameters","title":"Parameters","text":"<ul> <li>cube (<code>xarray.Dataset</code>): The input data cube.</li> <li>variable (<code>str</code>): variable to extract the <code>DataArray</code> containing the data to receive insights from</li> <li>layer_dim (<code>str</code>): The dimension along which to iterate and extract detailed layer-specific information. First dimension is default if not specified. </li> </ul>"},{"location":"ml-toolkit/api-reference/2-insights/get-insights/#returns","title":"Returns","text":"<ul> <li><code>None</code>: The function prints the extracted characteristics of the data cube.</li> </ul>"},{"location":"ml-toolkit/api-reference/2-insights/get-insights/#example","title":"Example","text":"<p><pre><code>import xarray as xr\nfrom ml4xcube.cube_insights import get_insights\n\n# Load sample data\nds = xr.open_zarr('sample_data.zarr')\nds = ds['temperature']\n\n# Get insights from the data cube\nget_insights(ds)\n</code></pre> The <code>get_insights</code> function, prints the following statistics (example for a cube containing dimensions named Time, Latitude, and Longitude):</p> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:09&lt;00:00,  1.10it/s]\nThe data cube has the following characteristics:\n\nVariable:             Land Surface Temperature\nShape:                (time: 10, lat: 2160, lon: 4320)\nTime range:           2002-05-21 - 2002-08-01\nLatitude range:       -89.958\u00b0 - 89.958\u00b0\nLongitude range:      -179.958\u00b0 - 179.958\u00b0\nTotal size:           93312000\nSize of each layer:   9331200\nTotal gap size:       74069847 -&gt; 79 %\nMaximum gap size:     87 % on 2002-06-06\nMinimum gap size:     75 % on 2002-08-01\nValue range:          222.99 - 339.32\n</code></pre>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/","title":"Gap dataset","text":""},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/#class-gapdataset","title":"Class: GapDataset","text":"<p>The <code>GapDataset</code> class within the <code>helper.predictors</code> submodule is designed to manage and prepare predictor data that aids in the estimation of missing values within a target dataset. It initializes with a specific variable from a dataset and aligns predictor data accordingly, handling both the extraction and storage of the processed data.</p>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/#constructor","title":"Constructor","text":"<pre><code> def __init__(self, ds: xr.DataArray, ds_name: str = 'Test123',\n              dimensions: Dict[str, tuple] = None,\n              artificial_gaps: List[float] = None,\n              actual_matrix: Union[str, datetime.date] = 'Random',\n              predictor_path: str = None, layer_dim: str = 'time'):\n</code></pre>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.DataArray</code>): The input dataset that contains gaps.</li> <li>ds_name (<code>str</code>): The name of the dataset.</li> <li>dimensions (<code>Dict[str, tuple]</code>): Dict containing dimension ranges (e.g., lat, lon, times) in order to extract the subset of <code>ds</code> relevant for the prediction.</li> <li>artificial_gaps (<code>List[float]</code>):  List of artificial gap sizes (floats between 0 and 1) to create ground truth for training and subsequent gapfilling of real gaps. If None no artificial gaps are created. The predictor will estimate real gaps directly when utilizing the Gapfilling class. </li> <li>actual_matrix (<code>Union[str, datetime.date]</code>): Specifies the selection criterion for extracting a specific slice of the dataset based on the dimension defined by layer_dim. This parameter can be used in two ways:<ul> <li>As <code>str</code>: If set to 'Random', a random value from the available values within the specified dimension (layer_dim) is chosen. This is useful for stochastic approaches where randomness is needed for validation or testing.</li> <li>As <code>datetime.date</code> or specific value: Allows precise specification of the slice to be selected. When a date or specific value is provided, the dataset is sliced at this exact point, or the nearest available point if the exact value is not present in the dataset. </li> </ul> </li> <li>predictor_path (<code>str</code>): Path to the directory where predictor data is stored. If <code>None</code> a support vector regression is performed for every gap size defined in <code>artificial_gaps</code>.</li> <li>layer_dim (<code>str</code>): Dimension along which to iterate. First dimension is default if not specified. </li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/#get_data","title":"get_data","text":"<p>This method orchestrates several key operations essential for setting up the dataset for subsequent gap-filling tasks. It manages the data retrieval, processing, and preparation phases, ensuring that all necessary data transformations and setups are completed before the actual gap-filling process begins. <pre><code>def get_data(self) -&gt; None\n</code></pre></p>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/#returns","title":"Returns","text":"<ul> <li><code>None</code>: This method does not return any value.</li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-dataset/#example","title":"Example","text":"<pre><code>import datetime\nimport xarray as xr\nfrom ml4xcube.gapfilling.gap_dataset import GapDataset\n\n# Example data and dimensions\ndata = xr.open_dataarray('path_to_dataarray')\ndimensions = {\n    'lat': (30, 45), \n    'lon': (10, 25), \n    'time': (datetime.date(2020, 1, 1), datetime.date(2020, 12, 31))\n}\n\n# Initialize the GapDataset class\ngap_dataset = GapDataset(data, 'ExampleDataset', dimensions)\n\n# Process data to create a dataset ready for gap filling\ngap_dataset.get_data()\n</code></pre>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/","title":"Gap filling","text":""},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/#class-gapfiller","title":"Class: Gapfiller","text":""},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, ds_name: str = \"Test123\", hyperparameters: str = \"RandomGridSearch\", predictor: str = \"RandomPoints\")\n</code></pre>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/#parameters","title":"Parameters","text":"<ul> <li>ds_name (<code>str</code>): The name used to identify and reference the dataset throughout the gap filling process. This name is utilized for directory naming, which helps in organizing output results such as filled datasets or models.</li> <li>hyperparameters (<code>str</code>): Defines the approach for hyperparameter optimization, with options such as <code>RandomGridSearch</code>, <code>FullGridSearch</code>, or <code>Custom</code>. The selection influences how the underlying machine learning model, such as SVR, will tune its parameters to best fit the data.<ul> <li><code>RandomGridSearch</code>:  Utilizes a randomized search over a predefined grid of hyperparameters. This method is faster but might miss the optimal point, suitable for large datasets or when a good-enough solution is acceptable.</li> <li><code>FullGridSearch</code>: Performs an exhaustive search over the specified grid of hyperparameters. While comprehensive, it can be computationally intensive.</li> <li><code>Custom</code>: Allows for manual specification of hyperparameters, providing full control over the learning process, ideal for use cases where domain knowledge can guide specific settings.</li> </ul> </li> <li>predictor (<code>str</code>): Strategy for selecting predictors used in model training, options include <code>AllPoints</code>, <code>RandomPoints</code>, <code>lccs_class</code>, or any specified extra matrix predictors.<ul> <li><code>AllPoints</code>: Uses all available data points as predictors, maximizing the information available for model training.</li> <li><code>RandomPoints</code>: Randomly selects a subset of data points to be used as predictors, useful for reducing computational load or when data is too large.</li> <li><code>lccs_class</code>: Specifies the use of land cover value estimation, focusing on leveraging spatial or categorical similarities.</li> <li><code>Custom</code>: Utilizes different types of external or derived predictors.</li> </ul> </li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/#gapfill","title":"gapfill","text":"<p>Main method to execute the gap filling process. It orchestrates data retrieval, directory management, model training, and result processing. Prints the directory containing the application results when the processes is completed.</p> <pre><code>def gapfill(self) -&gt; None\n</code></pre>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/#returns","title":"Returns","text":"<ul> <li><code>None</code>: This method does not return any value.</li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/gap-filling/#example","title":"Example","text":"<pre><code>import datetime\nimport xarray as xr\nfrom ml4xcube.gapfilling.gap_filler import Gapfiller\n\n# Example of using the Gapfiller class to fill data gaps\ngap_filler = Gapfiller(\n    ds_name='ExampleDataset', \n    hyperparameters=\"RandomGridSearch\", \n    predictor=\"lccs_class\"\n)\ngap_filler.gapfill()\n</code></pre>"},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/","title":"Helper helpingpredictor","text":""},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/#class-helpingpredictor","title":"Class: HelpingPredictor","text":"<p>The <code>HelpingPredictor</code> class within the <code>helper.predictors</code> submodule is designed to manage and prepare predictor data that aids in the estimation of missing values within a target dataset. It initializes with a specific variable from a dataset and aligns predictor data accordingly, handling both the extraction and storage of the processed data.</p>"},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, ds: xr.Dataset, variable: str, ds_predictor: xr.DataArray, predictor_path: str,\n            predictor: str = 'lccs_class', layer_dim: str = None)\n</code></pre>"},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The dataset containing the target variable.</li> <li>variable (<code>str</code>): The target variable to estimate.</li> <li>ds_predictor (<code>xarray.DataArray</code>): The dataset containing the predictor variable.</li> <li>predictor_path (<code>str</code>): Path to save the processed predictor data.</li> <li>predictor (<code>str</code>): Name of the predictor variable. Defaults to <code>lccs_class</code>.</li> <li>layer_dim (<code>str</code>): Dimension along which to iterate. First dimension is default if not specified. </li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/#methods","title":"Methods","text":"<p>The <code>get_predictor_data</code> method is designed to fetch and prepare the predictor in order to conduct the gap filling process. It extracts relevant data that align with the target dataset, processes this data if necessary, and saves it in a format that is easily accessible for further analysis. <pre><code>def get_predictor_data(self) -&gt; str\n</code></pre></p>"},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/#returns","title":"Returns","text":"<ul> <li><code>str</code>: The file path of the saved <code>.zarr</code> file containing the processed predictor data.</li> </ul>"},{"location":"ml-toolkit/api-reference/3-gapfilling/helper-helpingpredictor/#example","title":"Example","text":"<pre><code>import xarray as xr\nfrom ml4xcube.gapfilling.helper.predictors import HelpingPredictor\n\n# Example data and paths\nds = xr.open_dataset('path_to_dataset')\nds_predictor = xr.open_dataarray('path_to_predictor_data')\npredictor_path = 'path_to_store_processed_data'\n\n# Initialize the HelpingPredictor\npredictor = HelpingPredictor(ds, 'temperature', ds_predictor, predictor_path)\n\n# Get and process predictor data\npredictor_data_path = predictor.get_predictor_data()\nprint(f'Processed predictor data saved to: {predictor_data_path}')\n</code></pre>"},{"location":"ml-toolkit/api-reference/4-splits/assign-block-split/","title":"Assign block split","text":""},{"location":"ml-toolkit/api-reference/4-splits/assign-block-split/#assign_block_split","title":"assign_block_split","text":"<pre><code>def assign_block_split(ds: xr.Dataset, block_size: List[Tuple[str, int]] = None, split: float = 0.8) -&gt; xr.Dataset\n</code></pre>"},{"location":"ml-toolkit/api-reference/4-splits/assign-block-split/#description","title":"Description","text":"<p>Assigns blocks of data to training or testing sets based on a specified split ratio.  This method uses a deterministic random seed generated from the indices of each block, ensuring that the same blocks are consistently assigned to the same subset across different runs, given the same initial conditions.</p>"},{"location":"ml-toolkit/api-reference/4-splits/assign-block-split/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The input dataset.</li> <li>block_size (<code>List[Tuple[str, int]]</code>): List of tuples specifying the dimensions and their respective sizes for block division. If <code>None</code>, chunk sizes are inferred from the dataset.</li> <li>split (<code>float</code>): The fraction of data to assign to the training set. The remainder is assigned to the testing set. Default is <code>0.8</code>.</li> </ul>"},{"location":"ml-toolkit/api-reference/4-splits/assign-block-split/#returns","title":"Returns","text":"<ul> <li><code>xarray.Dataset</code>: The dataset with an added 'split' variable that indicates whether each block belongs to the training set (<code>1.</code>) or the testing set (<code>0.</code>).</li> </ul>"},{"location":"ml-toolkit/api-reference/4-splits/assign-block-split/#example","title":"Example","text":"<pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.data_split import assign_block_split\n\n# Example dataset\ndata = xr.Dataset({'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 2, 3))})\nblock_size = [('time', 5), ('lat', 2), ('lon', 3)]\nsplit_dataset = assign_block_split(data, block_size)\nprint(split_dataset)\n</code></pre> <p> Block Assignment of Train/Test Split </p>"},{"location":"ml-toolkit/api-reference/4-splits/assign-rand-split/","title":"Assign rand split","text":""},{"location":"ml-toolkit/api-reference/4-splits/assign-rand-split/#assign_rand_split","title":"assign_rand_split","text":"<pre><code>def assign_rand_split(ds: xr.Dataset, split: float = 0.8) -&gt; xr.Dataset\n</code></pre>"},{"location":"ml-toolkit/api-reference/4-splits/assign-rand-split/#description","title":"Description","text":"<p>Randomly assigns a training/test split indicator to each element in an <code>xarray.Dataset</code> based on a specified split ratio. This method ensures that the distribution of training and testing data is balanced across the entire dataset.</p>"},{"location":"ml-toolkit/api-reference/4-splits/assign-rand-split/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The input dataset.</li> <li>split (<code>float</code>):  The proportion of the dataset to be used for training. Defaults to <code>0.8</code>.</li> </ul>"},{"location":"ml-toolkit/api-reference/4-splits/assign-rand-split/#returns","title":"Returns","text":"<ul> <li><code>xarray.Dataset</code>: The dataset with an additional 'split' variable indicating the random split, where <code>1.</code> represents training data and <code>0.</code> represents testing data.</li> </ul>"},{"location":"ml-toolkit/api-reference/4-splits/assign-rand-split/#example","title":"Example","text":"<pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.data_split import assign_rand_split\n\n# Example dataset\ndata = xr.Dataset({'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 2, 3))})\nsplit_dataset = assign_rand_split(data, 0.7)\nprint(split_dataset)\n</code></pre> <p>In the example, each point in the dataset is randomly assigned to the train set with a 70% probability or to the test set  with a 30% probability. The random split is demonstrated in the image below:</p> <p> </p> <p> Random Assignment of Train/Test Split </p>"},{"location":"ml-toolkit/api-reference/4-splits/create-split/","title":"Create split","text":""},{"location":"ml-toolkit/api-reference/4-splits/create-split/#create_split","title":"create_split","text":"<pre><code>def create_split(\n        data: Union[xr.Dataset, Dict[str, np.ndarray]], to_pred: Union[List[str], str] = None,\n        exclude_vars: List[str] = list(), feature_vars: List[str] = None, stack_axis: int = -1\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n</code></pre>"},{"location":"ml-toolkit/api-reference/4-splits/create-split/#description","title":"Description","text":"<p>The <code>create_split</code> function efficiently generates train-test splits for machine learning models by using a predefined  <code>split</code> variable within a dataset. This method supports inputs in the form of either <code>xarray.Dataset</code> or a dictionary of <code>numpy</code> arrays.  It allows for flexible specification of feature and target variables, along with optional exclusions and custom stacking  configurations for input dimensions.</p>"},{"location":"ml-toolkit/api-reference/4-splits/create-split/#parameters","title":"Parameters","text":"<ul> <li>data (<code>Union[xr.Dataset, Dict[str, np.ndarray]]</code>): The data set from which to generate the split, provided either as an xarray dataset or a dictionary of variables.</li> <li>to_pred (<code>Union[List[str], str]</code>):  Names of the variables to be used as targets. Can be a single string or a list of strings.</li> <li>exclude_vars (<code>List[str]</code>):  Names of the variables to exclude from the feature set. Defaults to an empty list.</li> <li>feature_vars (<code>List[str]</code>):  Explicit list of variable names to use as features. If <code>None</code>, the function automatically determines which variables to use based on exclusion criteria and target variables.</li> </ul>"},{"location":"ml-toolkit/api-reference/4-splits/create-split/#returns","title":"Returns","text":"<ul> <li><code>Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]</code>: A tuple containing numpy arrays for training features, testing features, training targets, and testing targets.</li> </ul>"},{"location":"ml-toolkit/api-reference/4-splits/create-split/#example","title":"Example","text":"<p><pre><code>import numpy as np\nfrom ml4xcube.splits import create_split\n\n# Example data setup\ndata = {\n    'temperature': np.random.rand(100, 10),\n    'humidity': np.random.rand(100, 10),\n    'split': np.random.choice([0., 1.], size=(100,))\n}\n\n# Specify the variables to predict and the feature set\nto_predict = 'temperature'\nfeatures = ['humidity']\n\n# Create train and test splits\nX_train, X_test, y_train, y_test = create_split(data, to_pred=to_predict, feature_vars=features)\n\nprint('Training features shape:', X_train.shape)\nprint('Training labels shape:', y_train.shape)\n</code></pre> This example illustrates the use of <code>create_split</code> to prepare data arrays for training and testing a model, where  <code>temperature</code> is the target variable and <code>humidity</code> serves as a feature variable.  The <code>split</code> variable within the data cube specifies which entries belong to the training set (1.) and which to the testing set (0.). It can be assigned using the assign_block_split or the assign_rand_split method.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/apply-filter/","title":"Apply filter","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/apply-filter/#apply_filter","title":"apply_filter","text":"<pre><code>def apply_filter(ds: Dict[str, np.ndarray], filter_var: str, drop_sample: bool = False) -&gt; Dict[str, np.ndarray]\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/apply-filter/#description","title":"Description","text":"<p>The <code>apply_filter</code> function applies a filter to a dataset. If <code>drop_sample</code> is True and any value in a sample does not belong to the mask (<code>False</code>), the function drops the entire sample. If <code>drop_sample</code> is <code>False</code>, it sets all values to <code>NaN</code> that do not belong to the mask (<code>False</code>). For lists of points, it keeps the current behavior of dropping single values.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/apply-filter/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>Dict[str, numpy.ndarray]</code>): The dataset to filter. It should be a dictionary where keys are variable names and values are numpy arrays.</li> <li>filter_var (<code>str</code>): The variable name to use as the filter mask, which must be contained in the dataset.</li> <li>drop_sample (<code>bool</code>): A boolean flag to determine whether to drop the entire subarray or set values to <code>NaN</code>.</li> <li></li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/apply-filter/#returns","title":"Returns","text":"<ul> <li><code>Dict[str, numpy.ndarray]</code>: The dataset after filtering NaN values as a dictionary where keys are variable names and values are filtered numpy arrays.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/apply-filter/#example","title":"Example","text":"<p><pre><code>import numpy as np\nfrom ml4xcube.preprocessing import apply_filter\nfrom ml4xcube.cube_utilities import split_chunk\n\n# Example dataset\nchunk = {\n    'temperature': np.random.rand(10, 20, 30),\n    'precipitation': np.random.rand(10, 20, 30),\n    'filter_mask': np.random.choice([True, False], size=(10, 20, 30))\n}\n\n# Split the chunk\nsplit_data = split_chunk(chunk, sample_size=[('time', 1), ('lat', 2), ('lon', 2)], overlap=[('time', 0.5), ('lat', 0.5), ('lon', 0.5)])\n\n# Apply the filter with drop_sample set to True\nfiltered_ds = apply_filter(split_data, filter_var='filter_mask', drop_sample=True)\n\n# Apply the filter with drop_sample set to False\nfiltered_ds_nan = apply_filter(split_data, filter_var='filter_mask', drop_sample=False)\n</code></pre> The <code>apply_filter</code> function applies the filter mask to the dataset. Depending on the value of <code>drop_sample</code>, it either drops entire subarrays or sets invalid values to <code>NaN</code>.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/apply-filter/#notes","title":"Notes","text":"<ul> <li>The <code>filter_var</code> parameter specifies the variable used as the filter mask. Ensure this variable exists in the dataset.</li> <li>If <code>drop_sample</code> is <code>True</code>, the function drops entire subarrays if any value does not belong to the mask.</li> <li>If <code>drop_sample</code> is <code>False</code>, the function sets invalid values to <code>NaN</code>.</li> <li>The function handles lists of points separately by dropping single values if they are <code>NaN</code>.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/assign-mask/","title":"Assign mask","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/assign-mask/#assign_mask","title":"assign_mask","text":"<pre><code>def assign_mask(ds: xr.Dataset, mask: da.Array, mask_name: str = None, stack_dim: str = 'time') -&gt; xr.Dataset\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/assign-mask/#description","title":"Description","text":"<p>The <code>assign_mask</code> function incorporates a mask into an <code>xarray.Dataset</code>, optionally expanding it along a specified  dimension. This is particularly useful when you need to apply the same mask across multiple data points in a dataset, such as across different time steps or other dimensions. The function ensures that the mask is properly aligned with  the dataset's dimensions and chunks, facilitating seamless integration.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/assign-mask/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset]</code>): TThe dataset to which the mask will be assigned.</li> <li>mask (<code>dask.array</code>):  The mask array to be integrated into the dataset. It must be compatible in shape or expandable to the dimensions of the dataset.</li> <li>mask_name (<code>str</code>): The name assigned to the mask variable within the dataset. Defaults to <code>filter_mask</code> if not provided.</li> <li>stack_dim (<code>str</code>): The dimension along which to expand the mask. If not specified, the mask will not be expanded. Defaults to expanding along the 'time' dimension if no value is provided.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/assign-mask/#returns","title":"Returns","text":"<ul> <li><code>xarray.Dataset</code>: The updated dataset containing the new mask variable.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/assign-mask/#example","title":"Example","text":"<pre><code>import xarray as xr\nimport dask.array as da\nfrom ml4xcube.preprocessing import assign_mask\n\n# Example dataset\ndata = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), da.random.random((10, 20, 30), chunks=(5, 10, 10)))\n})\n\n# Example mask\nmask_array = da.ones((10, 20, 30), chunks=(5, 10, 10))\n\n# Assign mask to dataset without expansion\ndataset_with_mask = assign_mask(data, mask_array, mask_name='custom_mask')\n\n# Example mask\nmask_array = da.ones((20, 30), chunks=(10, 10))\n\n# Assign mask to dataset with expansion along 'time'\ndataset_with_expanded_mask = assign_mask(data, mask_array, mask_name='custom_mask_2', stack_dim='time')\n\nprint(dataset_with_mask)\nprint(dataset_with_expanded_mask)\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/assign-mask/#notes","title":"Notes","text":"<ul> <li>If the specified <code>stack_dim</code> is specified but not a dimension within the dataset, a <code>ValueError</code> is raised. </li> <li>This function ensures that the mask is expanded and rechunked appropriately to match the dataset's dimensions and chunk sizes, facilitating efficient computations on large datasets. </li> <li>The mask is added to the dataset as a new data variable using the specified <code>mask_name</code>, or defaults to <code>filter_mask</code> if no name is provided</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/drop-nan-values/","title":"Drop nan values","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/drop-nan-values/#drop_nan_values","title":"drop_nan_values","text":"<pre><code>def drop_nan_values(ds: Dict[str, np.ndarray], vars: List[str], mode: str = 'auto', filter_var: str = 'filter_mask') -&gt; Dict[str, np.ndarray]\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/drop-nan-values/#description","title":"Description","text":"<p>The <code>drop_nan_values</code> function filters out samples containing <code>NaN</code> values from a dataset, using various modes to determine how NaNs affect the data inclusion. This function is applicable to datasets represented as dictionaries of numpy arrays. It can handle both lists of points and multi-dimensional arrays. Additionally, it can utilize a mask variable from the dataset to define valid data points, aligning the filtering process with specific validity conditions dictated by the mask.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/drop-nan-values/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>Dict[str, numpy.ndarray]</code>): The dataset to filter, provided as a dictionary where keys are variable names and values are numpy arrays.</li> <li>vars (<code>List[str]</code>): A list of variable names to check for <code>NaN</code> values.</li> <li>mode (<code>str</code>): If not <code>None</code>, specifies the method for handling areas with missing values:</li> <li>auto: Drops the entire sample if any part contains a <code>NaN</code>.</li> <li>if_all_nan (<code>List[str]</code>): Drops the sample only if it is entirely composed of <code>NaN</code>s.</li> <li>masked (<code>List[str]</code>): Drops valid regions (as defined by <code>filter_var</code>) containing any <code>NaN</code>s. Requires <code>filter_var</code> to be defined.</li> <li>filter_var (<code>str</code>):  An optional argument specifying the name of a mask variable in the dataset. If provided, this mask is used to determine the validity of a sample.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/drop-nan-values/#returns","title":"Returns","text":"<ul> <li><code>Dict[str, numpy.ndarray]</code>:  The dataset dictionary with the NaN-containing entries filtered out according to the specified mode.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/drop-nan-values/#example","title":"Example","text":"<p><pre><code>import numpy as np\nfrom ml4xcube.preprocessing import drop_nan_values\n\n# Creating a dataset with NaN values\ndataset = {\n    'temperature': np.random.rand(10, 20, 30),\n    'precipitation': np.random.rand(10, 20, 30),\n    'filter_mask': np.random.choice([True, False], size=(10, 20, 30))\n}\ndataset['temperature'][0, 0, 0] = np.nan\ndataset['precipitation'][1, 1, 1] = np.nan\n\n# Specifying the variables to check for NaNs\nvariables = ['temperature', 'precipitation']\n\n# Filter the dataset in 'auto' mode\nfiltered_ds_auto = drop_nan_values(dataset, vars=variables, mode='auto')\n\n# Filter the dataset in 'masked' mode using a filter mask\nfiltered_ds_masked = drop_nan_values(dataset, vars=variables, mode='masked', filter_var='filter_mask')\n</code></pre> The <code>drop_nan_values</code> function filters out samples containing <code>NaN</code> values from the dataset. If a <code>filter_var</code> is provided, it also uses this mask to determine the validity of the samples. If values are considered relevant according to the mask (<code>True</code>), samples containing containing <code>NaN</code> values in these specific regions are dropped.  If no relevant values according to the filter mask exist the sample is also dropped.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/drop-nan-values/#notes","title":"Notes","text":"<ul> <li>The function provides flexible handling of NaN values, with the ability to adjust the strictness of filtering through the <code>mode</code> parameter. </li> <li>When the <code>mode</code> is <code>masked</code>, the function checks for NaNs specifically in areas deemed valid by the <code>filter_var</code>. If any valid area contains a NaN, the whole subarray is considered invalid. </li> <li>For datasets with dimensions of 1 (lists of points), NaNs are dropped individually. For higher dimensions (2, 3, or 4), the function can operate by checking across the specified axes, contingent on the selected mode.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/","title":"Fill nan values","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/#fill_masked_data","title":"fill_masked_data","text":"<pre><code>def fill_nan_values(ds: Dict[str, np.ndarray], vars: List[str], method: str = 'mean', const: Union[float, str, bool] = None) -&gt;  Union[Dict[str, np.ndarray], xr.Dataset]\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/#description","title":"Description","text":"<p>The <code>fill_nan_values</code> function fills <code>NaN</code> values in the dataset using a specified method. The methods available are 'mean', 'noise', or 'constant'. Depending on the method, <code>NaN</code> values are replaced with the mean of non-<code>NaN</code> values, random noise within the range of non-<code>NaN</code> values, or a specified constant value. In some cases in certain areas no values are intended (e.g. where mask values <code>False</code>). To incorporate samples containing boundaries (like coastlines in ESDC), the <code>fill_masked_data</code> function can be utilized to prepare the data for masked machine learning. This approach is demonstrated in this jupyter notebook.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>Union[Dict[str, numpy.ndarray], xarray.Dataset]</code>): The dataset to fill. It should be a dictionary or <code>xarray.Dataset</code> where keys are variable names and with the values containing the data to fill.</li> <li>vars (<code>List[str]</code>): The list of variables for which to fill NaN values. These variables should be present in the dataset.</li> <li>method (<code>str</code>): The method to use for filling NaN values. Options are <code>mean</code>, <code>sample_mean</code>, <code>noise</code>, <code>constant</code>, or None.</li> <li>None: <code>NaN</code>s are not filled.</li> <li>mean: <code>NaN</code>s are filled with the mean value of the non-<code>NaN</code> values</li> <li>sample_mean: <code>NaN</code>s are filled with the sample mean value.</li> <li>noise: <code>NaN</code> are filled with random noise within the range of the non-<code>NaN</code> values.</li> <li>constant: <code>NaN</code>s are filled with the specified constant value (<code>const</code>).</li> <li> </li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/#const-unionfloat-str-bool-the-constant-value-to-use-for-filling-nan-values-when-the-method-is-constant-this-parameter-is-required-when-the-method-is-constant","title":"const (<code>Union[float, str, bool]</code>): The constant value to use for filling <code>NaN</code> values when the method is 'constant'. This parameter is required when the method is 'constant'.","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/#returns","title":"Returns","text":"<ul> <li><code>Union[Dict[str, numpy.ndarray], xarray.Dataset]</code>: The dataset with <code>NaN</code> values filled, where keys are variable names and values are NumPy arrays with filled data.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/#example","title":"Example","text":"<pre><code>import numpy as np\nfrom ml4xcube.preprocessing import fill_nan_values\n\n# Example dataset\nds = {\n    'temperature': np.random.rand(10, 20, 30),\n    'precipitation': np.random.rand(10, 20, 30)\n}\n\n# Introduce some NaN values\nds['temperature'][0, 0, 0] = np.nan\nds['precipitation'][1, 1, 1] = np.nan\n\n# Fill NaN values using the mean method\nfilled_ds_mean = fill_nan_values(ds, vars=['temperature', 'precipitation'], method='mean')\n\n# Fill NaN values using the noise method\nfilled_ds_noise = fill_nan_values(ds, vars=['temperature', 'precipitation'], method='noise')\n\n# Fill NaN values using a constant value\nfilled_ds_constant = fill_nan_values(ds, vars=['temperature', 'precipitation'], method='constant', const=0.0)\n</code></pre> <p>In this example, the <code>fill_nan_values</code> function fills the NaN values in the dataset using different methods: 'mean', 'noise', and 'constant'.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/fill-nan-values/#notes","title":"Notes","text":"<ul> <li>The <code>vars</code> parameter specifies the list of variables for which to fill <code>NaN</code> values. Ensure these variables exist in the dataset.</li> <li>When using the 'constant' method, the <code>const</code> parameter must be provided to specify the constant value for filling <code>NaNs</code>.</li> <li>The function handles both single-dimensional and multi-dimensional arrays for filling <code>NaN</code> values.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-range/","title":"Get range","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/get-range/#get_range","title":"get_range","text":"<pre><code>def get_range(ds: Union[xr.Dataset, Dict[str, np.ndarray]], exclude_vars:List[str] = list()) -&gt; Dict[str, List[float]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-range/#description","title":"Description","text":"<p>The <code>get_range</code> function computes the minimum and maximum values for all variables within an <code>xarray.Dataset</code> or a  dictionary of <code>numpy</code> arrays, excluding specified variables. This utility is useful for preprocessing  tasks like normalization where knowing the range of data values is crucial. Users can specify certain variables to  exclude from the range calculations, such as mask variables that do not represent the actual data range of interest.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-range/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>Union[xarray.Dataset, Dict[str, numpy.ndarray]]</code>): The dataset to analyze, provided either as an xarray dataset or a dictionary where keys are variable names and values are numpy arrays.</li> <li>exclude_vars (<code>List[str]</code>): List of variable names to exclude from the range calculations. Useful for ignoring auxiliary or non-data variables like masks or identifiers.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-range/#returns","title":"Returns","text":"<ul> <li><code>Dict[str, List[float]]</code>: A dictionary where each key is a variable name and the value is a list containing the minimum and maximum values of that variable.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-range/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.preprocessing import get_range\n\n# Example dataset\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'precipitation': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'land_mask': (('lat', 'lon'), np.random.randint(0, 2, size=(20, 30)))\n})\n\n# Exclude the 'land_mask' variable from range calculations\nranges = get_range(ds, exclude_vars=['land_mask'])\n\n# Output the calculated ranges\nfor var, r in ranges.items():\n    print(f\"{var} range: {r}\")\n\n# Example of how these ranges might be used for normalization\nnormalized_datasets = {var: (ds[var] - r[0]) / (r[1] - r[0]) for var, r in ranges.items()}\n\n# Display normalized datasets\nfor var, data in normalized_datasets.items():\n    print(f\"Normalized {var}: {data}\")\n</code></pre> In this example, the <code>get_range</code> function is used to retrieve the minimum and maximum values for the <code>temperature</code> and  <code>precipitation</code> variables in an <code>xarray.Dataset</code>. These ranges are then used by the <code>normalize</code> function to scale the  data to the range <code>[0, 1]</code>.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-statistics/","title":"Get statistics","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/get-statistics/#get_statistics","title":"get_statistics","text":"<pre><code>def get_statistics(ds: Union[xr.Dataset, Dict[str, np.ndarray]], exclude_vars:List[str] = list()) -&gt; Dict[str, List[float]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-statistics/#description","title":"Description","text":"<p>The <code>get_statistics</code> function calculates and returns the mean and standard deviation for all variables in an  <code>xarray.Dataset</code> or a dictionary of numpy arrays, except for specified variables to exclude. This function is  useful for statistical analysis and data standardization, where mean and standard deviation are essential for tasks  such as feature scaling.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-statistics/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>Union[xarray.Dataset, Dict[str, numpy.ndarray]]</code>):  The dataset to analyze, which can be provided either as an xarray dataset or a dictionary where keys are variable names and values are numpy arrays.</li> <li>exclude_vars (<code>List[str]</code>): A list of variable names to exclude from the statistical calculations. This can be useful for ignoring non-analytical variables like identifiers or masks.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-statistics/#returns","title":"Returns","text":"<ul> <li><code>Dict[str, List[float]]</code>: A dictionary where each key is a variable name and the value is a list containing the mean and standard deviation of that variable.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/get-statistics/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.preprocessing import get_statistics\n\n# Example dataset\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'precipitation': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'land_mask': (('lat', 'lon'), np.random.randint(0, 2, size=(20, 30)))\n})\n\n# Exclude the 'land_mask' variable from statistical calculations\nstats = get_statistics(ds, exclude_vars=['land_mask'])\n\n# Output the calculated statistics\nfor var, values in stats.items():\n    print(f\"{var} - Mean: {values[0]}, Standard Deviation: {values[1]}\")\n</code></pre> A random dataset is created. Subsequently the mean and standard deviation for the standardization of the variable <code>temperature</code>.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/normalize/","title":"Normalize","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/normalize/#normalize","title":"normalize","text":"<pre><code>def normalize(ds: Union[xr.Dataset, Dict[str, np.ndarray]], range_dict: Dict[str, List[float]], filter_var: str = None) -&gt; Union[xr.Dataset, Dict[str, np.ndarray]]:\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/normalize/#description","title":"Description","text":"<p>The <code>normalize</code>  function applies min-max scaling to each variable within an <code>xarray.Dataset</code> or a dictionary of numpy  arrays, contained within the <code>range_dict</code> dictionary. This dictionary specifies range values for this operation.  This scaling adjusts each data point to a [0, 1] range,  based on the minimum (<code>xmin</code>) and maximum (<code>xmax</code>) values for each variable. This method is essential for scaling input data in  various data processing and machine learning tasks, ensuring that each variable contributes equally without bias due  to different scales. A variable specified as <code>filter_var</code> is excluded from normalization, which can be useful for  variables, like mask indicators or filters.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/normalize/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>Union[xarray.Dataset, Dict[str, np.ndarray]]</code>): The dataset to normalize. It can either be an <code>xarray.Dataset</code> or a dictionary where keys are variable names and values are NumPy arrays.</li> <li>range_dict (<code>Dict[str, List[float]]</code>): A dictionary containing the minimum and maximum values (<code>xmin</code>, <code>xmax</code>) for each variable that requires normalization.</li> <li>filter_var (<code>str</code>): The name of a variable to exclude from normalization. This is useful for excluding non-data variables like mask or index fields.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/normalize/#returns","title":"Returns","text":"<ul> <li><code>numpy.ndarray</code>: The normalized array, with values scaled to the range [0, 1].</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/normalize/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom preprocessing import get_range, normalize\n\n# Creating an example dataset\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'humidity': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'land_mask': (('time', 'lat', 'lon'), np.random.randint(0, 2, size=(10, 20, 30)))\n})\n\n# Calculate the range for 'temperature' and 'humidity'\nranges = get_range(ds, exclude_vars=['land_mask'])\nprint(\"Ranges calculated:\", ranges)\n\n# Normalize the dataset, excluding 'land_mask' from normalization\nnormalized_ds = normalize(ds, ranges, filter_var='land_mask')\nprint(\"Normalized Dataset:\")\nprint(normalized_ds)\n</code></pre> In this example, the <code>get_range</code> function is used to retrieve the minimum and maximum values for the 'temperature' and 'precipitation' variables in an xarray dataset. These ranges are then used by the <code>normalize</code> function to scale the data to the range [0, 1].</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/standardize/","title":"Standardize","text":""},{"location":"ml-toolkit/api-reference/5-preprocessing/standardize/#standardize","title":"standardize","text":"<pre><code>def standardize(ds: Union[xr.Dataset, Dict[str, np.ndarray]], stats_dict: Dict[str, List[float]], filter_var: str = None) -&gt; Union[xr.Dataset, Dict[str, np.ndarray]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/5-preprocessing/standardize/#description","title":"Description","text":"<p>The <code>standardize</code> function performs standardization fpr all variables within an <code>xarray.Dataset</code> or a dictionary of  NumPy arrays, contained in the <code>stats_dict</code> dictionary. This dictionary provides mean and standard deviation values.  This standardization process adjusts each data point  so that the resulting distribution of each variable has a mean of 0 and a standard deviation of 1. This is crucial for  many statistical analyses and machine learning models to ensure that features have comparable scales without biasing  the model due to the variance in magnitude. Variables specified by <code>filter_var</code> are excluded from standardization, which  is beneficial for non-data variables like masks or indices.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/standardize/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>Union[xarray.Dataset, Dict[str, numpy.ndarray]]</code>): The dataset to standardize. It can either be an <code>xarray.Dataset</code> or a dictionary where keys are variable names and values are NumPy arrays.</li> <li>stats_dict (<code>Dict[str, List[float]]</code>): A dictionary containing the minimum and maximum values (<code>xmin</code>, <code>xmax</code>) for each variable that requires normalization.</li> <li>filter_var (<code>str</code>): The name of a variable to exclude from normalization. This is useful for excluding non-data variables like mask or index fields.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/standardize/#returns","title":"Returns","text":"<ul> <li><code>Union[xarray.Dataset, Dict[str, numpy.ndarray]]</code>: The standardized dataset. The data structure returned depends on the input; it will return an <code>xarray.Dataset</code> if provided with one, otherwise it will return a dictionary.</li> </ul>"},{"location":"ml-toolkit/api-reference/5-preprocessing/standardize/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom preprocessing import get_statistics, standardize\n\n# Creating an example dataset\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'humidity': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'land_mask': (('time', 'lat', 'lon'), np.random.randint(0, 2, size=(10, 20, 30)))\n})\n\n# Calculate statistics for 'temperature' and 'humidity'\nstats = get_statistics(ds, exclude_vars=['land_mask'])\nprint(\"Statistics calculated:\", stats)\n\n# Standardize the dataset, excluding 'land_mask' from standardization\nstandardized_ds = standardize(ds, stats, filter_var='land_mask')\nprint(\"Standardized Dataset:\")\nfor var in ['temperature', 'humidity']:\n    print(f\"Standardized {var}: mean={np.mean(standardized_ds[var].values)}, std={np.std(standardized_ds[var].values)}\")\n</code></pre> A random dataset is created. Subsequently the mean and standard deviation are used for the standardization of  <code>temperature</code> and <code>humidity</code> variables.</p>"},{"location":"ml-toolkit/api-reference/5-preprocessing/standardize/#notes","title":"Notes","text":"<ul> <li>Standardization is carried out by subtracting the mean and dividing by the standard deviation. If the standard deviation is zero (indicating no variability within the variable), the variable values are reduced by the mean alone since division by zero is not feasible.</li> <li>This function supports excluding specific variables from the standardization process, which is especially useful for preserving the integrity of certain types of data like binary masks or categorical indices. </li> <li>The function intelligently handles both <code>xarray.Dataset</code> and dictionary formats, making it versatile for different data handling contexts in scientific computing and machine learning.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/","title":"Multiproc sampler","text":""},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/#multiprocsampler","title":"MultiProcSampler","text":"<p>The <code>MultiProcSampler</code> class is a specialized component of the <code>datasets</code> module, designed to efficiently prepare large datasets for machine learning applications.  By leveraging Python's <code>multiprocessing</code> module, this class facilitates parallel processing to handle extensive datasets rapidly and effectively.  This class is particularly beneficial in environments where the handling and transformation of large volumes of data are required before training sophisticated machine learning models. It applies functionality to preprocess, scale, and partition data into training and testing sets, which are then stored in an efficient format for later retrieval.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n   self, ds: xr.Dataset, rand_chunk: bool = False, data_fraq: float = 1.0, nproc: int = 4,\n   apply_mask: bool = True, drop_sample: bool = True, fill_method: str = None, const: float = None,\n   filter_var: str = 'land_mask', chunk_size: Tuple[int, ...] = None, train_cube: str = 'train_cube.zarr',\n   test_cube: str = 'test_cube.zarr', drop_nan: str = 'auto', array_dims: Tuple[str, ...] = ('samples',),\n   data_split: float = 0.8, chunk_batch: int = None, callback: Callable = None,\n   block_size: List[Tuple[str, int]] = None, sample_size: List[Tuple[str, int]] = None,\n   overlap: List[Tuple[str, float]] = None, scale_fn: str = 'standardize'\n)\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The input dataset to extract train and test data from</li> <li>rand_chunk (<code>bool</code>): If True, chunks are selected randomly; otherwise, they are processed sequentially.</li> <li>data_fraq (<code>float</code>):  Fraction of the data to process, allowing for partial dataset processing.</li> <li>nproc (<code>int</code>): Number of processor cores to use for parallel processing.</li> <li>apply_mask (<code>bool</code>): Whether to apply a filtering condition to the data chunks.</li> <li>drop_sample (<code>bool</code>): If true, <code>NaN</code> values are dropped during filter application.</li> <li>fill_method (<code>str</code>): Method used to fill masked or <code>NaN</code> data.<ul> <li>None: <code>NaN</code>s are not filled.</li> <li>mean: <code>NaN</code>s are filled with the mean value of the non-<code>NaN</code> values</li> <li>sample_mean: <code>NaN</code>s are filled with the sample mean value.</li> <li>noise: <code>NaN</code> are filled with random noise within the range of the non-<code>NaN</code> values.</li> <li>constant: <code>NaN</code>s are filled with the specified constant value (<code>const</code>).</li> </ul> </li> <li>const (<code>float</code>): Constant value used when fill_method is 'constant'.</li> <li>filter_var (<code>str</code>): Variable name used for filtering data chunks.</li> <li>chunk_size (<code>Tuple[int, ...]</code>): The size of chunks in the generated training and testing data.</li> <li>train_cube (<code>str</code>): Path where training data are stored as <code>zarr</code> datasets.</li> <li>test_cube (<code>str</code>): Path where test data are stored as <code>zarr</code> datasets.</li> <li>drop_nan (<code>str</code>): If not <code>None</code>, specifies the method for handling areas with missing values:<ul> <li>auto: Drops the entire sample if any part contains a <code>NaN</code>.</li> <li>if_all_nan (<code>List[str]</code>): Drops the sample only if it is entirely composed of <code>NaN</code>s.</li> <li>masked (<code>List[str]</code>): Drops valid regions (as defined by <code>filter_var</code>) containing any <code>NaN</code>s. Requires <code>filter_var</code> to be defined.</li> </ul> </li> <li>array_dims (<code>Tuple[str, ...]</code>): Dimension names of the resulting <code>zarr</code>s with train and test data. </li> <li>data_split (<code>float</code>): Proportion of data allocated to training; remainder goes to testing.</li> <li>chunk_batch (<code>int</code>): Number of chunks to process in each batch during parallel execution.</li> <li>callback (<code>Callable</code>): Optional function applied to each chunk after initial processing.</li> <li>block_size (<code>List[Tuple[str, int]]</code>): Size of the chunks to process, which can define memory usage and performance. If <code>None</code> the chunk size of ds is utilized</li> <li>sample_size (<code>List[Tuple[str, int]]</code>): List of tuples specifying the dimensions and their respective sizes.</li> <li>overlap (<code>List[Tuple[str, float]]</code>): List of tuples specifying the dimensions and their respective overlap proportion.</li> <li>scale_fn (<code>str</code>): Feature scaling function to apply (<code>standardize</code>, <code>normalize</code>, or <code>None</code>).</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/#get_datasets","title":"get_datasets","text":"<p>This method retrieves the processed training and testing datasets, ensuring all data is ready for analysis or machine learning model training.</p> <pre><code>def get_datasets(self) -&gt; Tuple[xr.Dataset, xr.Dataset]\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/#returns","title":"Returns","text":"<ul> <li><code>Tuple[xarray.Dataset, xarray.Dataset]</code>: Tuple containing the training and testing dataset, each in the <code>xarray.Dataset</code> format</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/multiproc-sampler/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.datasets.multiproc_sampler import MultiProcSampler\n\n# Example dataset with chunking\ndata = np.random.rand(100, 200, 300)\ndataset = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), data),\n    'precipitation': (('time', 'lat', 'lon'), data)\n}).chunk({'time': 10, 'lat': 50, 'lon': 50})\n\n# Create an instance of MultiProcSampler\nsampler = MultiProcSampler(\n    ds=dataset, rand_chunk=True, nproc=4, apply_mask=True, \n    drop_sample=True, fill_method='constant', const=0.0,\n    filter_var='land_mask', chunk_size=(100, 100, 10), \n    data_split=0.75, sample_size=[('time', 1),('lat', 5), ('lon', 5)], \n    overlap=[('time', 0.),('lat', 0.5), ('lon', 0.5)]\n)\n\n# Process the dataset and retrieve the 7-training and testing sets\ntrain_ds, test_ds = sampler.get_datasets()\n</code></pre> This documentation reflects the updated functionality and parameters of the <code>MultiProcSampler</code> class, providing a  comprehensive guide for users to utilize its capabilities in data processing workflows.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/prepare-dataloader/","title":"Prepare dataloader","text":""},{"location":"ml-toolkit/api-reference/6-datasets/prepare-dataloader/#prep_dataloader","title":"prep_dataloader","text":"<pre><code>def prep_dataloader(\n    train_ds: Dataset, test_ds: Dataset = None, batch_size: int = 1, callback: Callable = None, num_workers: int = 0, \n    parallel: bool = False, shuffle = True, drop_last=True\n) -&gt; Union[DataLoader, Tuple[DataLoader, DataLoader]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/prepare-dataloader/#description","title":"Description","text":"<p>This function sets up one or two <code>DataLoader</code>s' for PyTorch models, facilitating efficient and configurable data loading  for both training and optional testing phases. This function integrates best practices for data management in deep  learning applications, supporting parallel data loading, optional shuffling, and batch handling. It can handle single  and distributed computing environments, making it versatile for local training sessions or scalable, distributed  training across multiple GPUs or nodes.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/prepare-dataloader/#parameters","title":"Parameters","text":"<ul> <li>train_ds (<code>torch.utils.data.Dataset</code>): The PyTorch dataset for training data loading.</li> <li>test_ds (<code>torch.utils.data.Dataset</code>): The PyTorch dataset for testing data loading. If provided, the function returns a separate <code>DataLoader</code> for testing. Defaults to <code>None</code>.</li> <li>batch_size (<code>int</code>): Number of samples/chunks per batch to load.</li> <li>callback (<code>Callable</code>): A function to collate data into batches, or to perform custom operations during data loading.</li> <li>num_workers (<code>int</code>): Number of subprocesses used for data loading.</li> <li>shuffle (<code>bool</code>):  If <code>True</code>, the dataset is shuffled at every epoch to reduce model bias. Automatically set to <code>False</code> if parallel is <code>True</code>.</li> <li>parallel (<code>bool</code>): If set to True, enables distributed training mode, which is crucial for training across multiple GPUs or nodes.</li> <li>drop_last (<code>bool</code>): Whether to drop the last incomplete batch if the total number of samples isn't divisible by the batch size. This is often useful during training to ensure consistent batch sizes.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/prepare-dataloader/#returns","title":"Returns","text":"<ul> <li>Union[DataLoader, Tuple[DataLoader, DataLoader]]:  A single <code>DataLoader</code> for the training dataset if <code>test_ds</code> is <code>None</code>, or a tuple containing <code>DataLoader</code>s for both training and testing datasets if <code>test_ds</code> is provided.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/prepare-dataloader/#example","title":"Example","text":"<p><pre><code>from datasets.pytorch import prep_dataloader\n\n# Assuming 'MyDataset' is a custom class derived from torch.utils.data.Dataset\ntrain_dataset = MyDataset()\ntest_dataset = MyDataset()  # Optional test dataset\nbatch_size = 32\nnum_workers = 4\n\n# Prepare DataLoader for training, with an optional test DataLoader\ntrain_loader, test_loader = prep_dataloader(\n    train_ds=train_dataset,\n    test_ds=test_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    parallel=False,  # Set to True for distributed training\n    shuffle=True,\n    drop_last=True\n)\n\n# Use the DataLoader in a training loop\nfor data in train_loader:\n    # Training operations go here\n    pass\n</code></pre> In this example, the <code>prep_dataloader</code> function is used to set up <code>DataLoader</code>s for PyTorch datasets, specifying the  batch size and the number of worker subprocesses. This setup is typical for training machine learning models where  efficient data handling and processing are crucial. </p>"},{"location":"ml-toolkit/api-reference/6-datasets/prepare-dataloader/#notes","title":"Notes","text":"<p>The function checks if parallel processing is enabled: - If <code>parallel</code> is <code>True</code>, a <code>DistributedSampler</code> is used, which is essential for distributed training environments. This changes the sampling behavior of the dataset to ensure that each part of the dataset is handled by a different part of the model distributed across several nodes or GPUs. - The <code>DataLoader</code> is then configured with the specified parameters. Notably, <code>pin_memory</code> is set conditionally based on whether CUDA is available, which can enhance data transfer speeds to GPU.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/pt-large-scale-xr-dataset/","title":"Pt large scale xr dataset","text":""},{"location":"ml-toolkit/api-reference/6-datasets/pt-large-scale-xr-dataset/#ptxrdataset","title":"PTXrDataset","text":"<p><code>PTXrDataset</code> extends PyTorch's <code>Dataset</code> to provide specialized handling of <code>xarray.Dataset</code> for training neural  networks in PyTorch. It supports dynamic processing and iteration over chunks of large datasets that cannot be fully  loaded into memory, making it ideal for environments with significant data volumes. The class allows for flexible data  manipulation including optional random chunk selection, dropping of <code>NaN</code> values, application of filtering masks, and  data filling strategies, contained in the preprocessing module.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/pt-large-scale-xr-dataset/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n   self, ds: xr.Dataset, rand_chunk: bool = True, drop_nan: str = 'auto', drop_sample: bool = False,\n  chunk_indices: List[int] = None, apply_mask: bool = True, fill_method: str = None,\n  const: float = None, filter_var: str = 'filter_mask', num_chunks: int = None, callback = None,\n  block_sizes: List[Tuple[str, int]] = None, sample_size: List[Tuple[str, int]] = None,\n  overlap: List[Tuple[str, float]] = None, process_chunks: bool = False\n):\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/pt-large-scale-xr-dataset/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The dataset from which data chunks are processed.</li> <li>rand_chunk (<code>bool</code>): If True, chunks are selected randomly; otherwise, they are processed sequentially.</li> <li>drop_nan (<code>str</code>): If not <code>None</code>, specifies the method for handling areas with missing values:<ul> <li>auto: Drops the entire sample if any part contains a <code>NaN</code>.</li> <li>if_all_nan (<code>List[str]</code>): Drops the sample only if it is entirely composed of <code>NaN</code>s.</li> <li>masked (<code>List[str]</code>): Drops valid regions (as defined by <code>filter_var</code>) containing any <code>NaN</code>s. Requires <code>filter_var</code> to be defined.</li> </ul> </li> <li>drop_sample (<code>bool</code>): If true, <code>NaN</code> values are dropped during filter application.</li> <li>chunk_indices (<code>List[int]</code>): Specifies indices of chunks to be processed if not randomly selected.</li> <li>apply_mask (<code>bool</code>): Whether to apply a filtering condition defined by the <code>filter_var</code> to the data chunks.</li> <li>fill_method (<code>str</code>): Method used to fill masked or NaN data.<ul> <li>None: <code>NaN</code>s are not filled.</li> <li>mean: <code>NaN</code>s are filled with the mean value of the non-<code>NaN</code> values</li> <li>sample_mean: <code>NaN</code>s are filled with the sample mean value.</li> <li>noise: <code>NaN</code> are filled with random noise within the range of the non-<code>NaN</code> values.</li> <li>constant: <code>NaN</code>s are filled with the specified constant value (<code>const</code>).</li> </ul> </li> <li>const (<code>float</code>): Constant value used when <code>fill_method</code> is <code>constant</code>.</li> <li>filter_var (<code>str</code>): Variable used for filtering data chunks.</li> <li>num_chunk (<code>int</code>): Specifies the number of chunks to process if not processing all.</li> <li>block_size (<code>List[Tuple[str, int]]</code>): Size of the chunks to process, which can define memory usage and performance. If <code>None</code> the chunk size of ds is utilized</li> <li>sample_size (<code>List[Tuple[str, int]]</code>): List of tuples specifying the dimensions and their respective sizes.</li> <li>overlap (<code>List[Tuple[str, float]]</code>): List of tuples specifying the dimensions and their respective overlap proportion.</li> <li>process_chunks (<code>bool</code>): Whether to preprocess each chunk before returning.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/pt-large-scale-xr-dataset/#example","title":"Example","text":"<p><pre><code>import xarray as xr\nfrom ml4xcube.datasets.pytorch import PTXrDataset, prep_dataloader  \n\n# Initializing the dataset\ndataset = PTXrDataset(\n    ds=my_xarray_dataset,\n    rand_chunk=True,\n    use_filter=True,\n    filter_var='land_mask',\n    sample_size=[('time', 2), ('lat', 10), ('lon', 10)],\n    overlap=[('time', 0.5), ('lat', 0.5), ('lon', 0.5)]\n)\n\n# Creating a DataLoader for batch processing\ndata_loader = prep_dataloader(dataset, batch_size=10, shuffle=True, callback=map_fn)\n\n# Using DataLoader in a training loop\nfor batch in data_loader:\n    inputs, targets = batch\n    outputs = model(inputs)  # Assuming 'model' is an instance of a PyTorch model\n    # Continue with training steps\n</code></pre> This setup demonstrates how <code>PTXrDataset</code> is integrated into a PyTorch training loop using <code>DataLoader</code>, facilitating  efficient and scalable processing of geospatial datasets for deep learning applications.  This functionality is critical for leveraging high-performance computing resources effectively, ensuring that large  datasets are handled in a manner that optimizes both memory usage and computational speed. The <code>map_fn</code> is a callback function as defined in prep_dataloader.  It allows to define the features as well as the dependent variable for the training process and include further preprocessing steps.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/pt-large-scale-xr-dataset/#notes","title":"Notes","text":"<ul> <li><code>training.pytorch.Trainer</code> is able to handle empty chunks. Therefore raw data can be handed over to the <code>PTXrDataset</code> despite of gaps in the data.</li> <li>Samples obtained from a chunk serve as a batch of data. If a consistent batch size is required leverage the XrDataset or the MultiProcSampler to prepare data accordingly.</li> <li>This class efficiently handles large datasets by enabling the selective loading and processing of manageable data chunks.</li> <li><code>PTXrDataset</code> allows for high customization in how data is processed, which is vital for training deep learning models that require specific data formats or preprocessing steps.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/","title":"Tf large scale xr dataset","text":""},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#largescalexrdataset","title":"LargeScaleXrDataset","text":"<p><code>TFXrDataset</code> is specifically designed to integrate <code>xarray.Dataset</code>s with TensorFlow machine learning workflows. It efficiently processes and streams data chunks for training or inference, making it particularly well-suited for h andling extensive datasets that exceed memory capacities. This class enables dynamic data preprocessing and chunk-based iteration, facilitating performance optimization and seamless integration into TensorFlow pipelines.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, ds: xr.Dataset, rand_chunk: bool = True, drop_nan: str = 'auto', chunk_indices: list = None,\n            apply_mask: bool = True, drop_sample: bool = False, fill_method: str = None, const: float = None,\n            filter_var: str = 'filter_mask', num_chunks: int = None, callback_fn = None,\n            block_size: List[Tuple[str, int]] = None, sample_size: List[Tuple[str, int]] = None,\n            overlap: List[Tuple[str, float]] = None, process_chunks: bool = False):\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The dataset from which data chunks are processed.</li> <li>rand_chunk (<code>bool</code>): If True, chunks are selected randomly; otherwise, they are processed sequentially.</li> <li>drop_nan (<code>str</code>): If not <code>None</code>, specifies the method for handling areas with missing values:</li> <li>auto: Drops the entire sample if any part contains a <code>NaN</code>.</li> <li>if_all_nan (<code>List[str]</code>): Drops the sample only if it is entirely composed of <code>NaN</code>s.</li> <li>masked (<code>List[str]</code>): Drops valid regions (as defined by <code>filter_var</code>) containing any <code>NaN</code>s. Requires <code>filter_var</code> to be defined.</li> <li>chunk_indices (<code>List[int]</code>): Specifies indices of chunks to be processed if not randomly selected.</li> <li>apply_mask (<code>bool</code>): Whether to apply a filtering condition defined by the <code>filter_var</code> to the data chunks.</li> <li>drop_sample (<code>bool</code>): If True, drops any samples that doesn't contain relevant values according to the <code>filter_var</code> criteria completely.</li> <li>fill_method (<code>str</code>): Method used to fill masked or NaN data.</li> <li>None: <code>NaN</code>s are not filled.</li> <li>mean: <code>NaN</code>s are filled with the mean value of the non-<code>NaN</code> values</li> <li>sample_mean: <code>NaN</code>s are filled with the sample mean value.</li> <li>noise: <code>NaN</code> are filled with random noise within the range of the non-<code>NaN</code> values.</li> <li>constant: <code>NaN</code>s are filled with the specified constant value (<code>const</code>).</li> <li>const (<code>float</code>): Constant value used when fill_method is 'constant'.</li> <li>filter_var (<code>str</code>): Filter mask name used for filtering data chunks.</li> <li>num_chunk (<code>int</code>): Specifies the number of chunks to process if not processing all.</li> <li>callback (<code>int</code>): Optional function applied to each chunk after initial processing.</li> <li>block_size (<code>List[Tuple[str, int]]</code>): Size of the chunks to process, which can define memory usage and performance. If <code>None</code> the chunk size of ds is utilized</li> <li>sample_size (<code>List[Tuple[str, int]]</code>): List of tuples specifying the dimensions and their respective sizes.</li> <li>overlap (<code>List[Tuple[str, float]]</code>): List of tuples specifying the dimensions and their respective overlap proportion.</li> <li>process_chunks (<code>bool</code>): Whether to preprocess each chunk before returning.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#len","title":"len","text":"<p>Returns the number of chunks, providing insights into the volume of data being processed.</p> <pre><code>def __len__(self) -&gt; int\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#returns","title":"Returns","text":"<ul> <li><code>int</code>: number of chunks</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#get_datasets","title":"get_datasets","text":"<p>Creates a <code>TensorFlow</code> dataset from the generator.</p> <pre><code>get_dataset(self) -&gt; tf.data.Dataset\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#parameters_2","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#returns_1","title":"Returns","text":"<ul> <li><code>tf.data.Dataset</code>: <code>TensorFlow</code> dataset object, yielding chunks of data.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#example","title":"Example","text":"<pre><code># Create an instance of LargeScaleXrDataset\ndataset_processor = LargeScaleXrDataset(\n    xr_dataset=my_xarray_dataset,\n    rand_chunk=True,\n    drop_nan=True,\n    use_filter=True,\n    filter_var='land_mask',\n    sample_size=[('time', 24)],\n    overlap=[('time', 1)]\n)\n\n# Use the generator to feed data into a TensorFlow model\nfor data_chunk in dataset_processor.generate():\n    # Assuming 'model' is an instance of a TensorFlow model\n    predictions = model.predict(data_chunk['input_data'])\n    print(\"Processed predictions:\", predictions)\n</code></pre> <p>This example demonstrates how to initialize the <code>LargeScaleXrDataset</code> class and use its generate method to continuously supply data to a TensorFlow model. The class is essential for applications where large datasets are common, and chunks must be loaded to memory and processed successively.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/tf-large-scale-xr-dataset/#notes","title":"Notes","text":"<ul> <li>If instances of <code>LargeScaleXrDataset</code> are handed over to the <code>training.tensorflow.Trainer</code> every processed chunk must contain valid data</li> <li>If validity of data samples can not be guaranteed after preprocessing a chunk, prepare an appropriate datasets using the XrDataset or the MultiProcSampler.</li> <li>Samples obtained from a chunk serve as a batch of data. If a consistent batch size is required leverage the [XrDataset]xr-dataset.md) or the MultiProcSampler to prepare data accordingly.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/","title":"Xr dataset","text":""},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#xrdataset","title":"XrDataset","text":"<p>The <code>XrDataset</code> class within the <code>datasets.xr_dataset</code> module is specifically tailored to sample smaller datasets that are manageable in memory.  It is created for scenarios where direct, rapid manipulation of data in memory is feasible and preferable over the more complex, disk-based operations typical of very large datasets.  This capability makes it ideal for machine learning applications where dataset size allows for quick iterations and immediate feedback on preprocessing and modeling efforts. This class aims provides tools to selectively process data chunks based on specified criteria and apply data cleaning operations such as NaN value handling and optional filtering based on specific variables. It efficiently concatenates processed chunks into a single dataset for further analysis or model training.</p>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, ds: xr.Dataset, chunk_indices: List[int] = None, rand_chunk: bool = True, drop_nan: str = 'auto',\n             apply_mask: bool = True, drop_sample: bool = False, fill_method: str = None, const: float = None,\n             filter_var: str = 'filter_mask', patience: int = 500, block_size: List[Tuple[str, int]] = None,\n             sample_size: List[Tuple[str, int]] = None, overlap: List[Tuple[str, float]] = None, callback: Callable = None,\n             num_chunks: int = None, to_pred: Union[str, List[str]] = None, scale_fn: str = 'standardize'):\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>xarray.Dataset</code>): The input dataset to extract the training samples from.</li> <li>chunk_indices (<code>List[int]</code>): List of indices specifying which chunks to process.</li> <li>rand_chunk (<code>bool</code>): If True, chunks are selected randomly; otherwise, they are processed sequentially.</li> <li>drop_nan (<code>str</code>): If not <code>None</code>, specifies the method for handling areas with missing values:<ul> <li>auto: Drops the entire sample if any part contains a <code>NaN</code>.</li> <li>if_all_nan (<code>List[str]</code>): Drops the sample only if it is entirely composed of <code>NaN</code>s.</li> <li>masked (<code>List[str]</code>): Drops valid regions (as defined by <code>filter_var</code>) containing any <code>NaN</code>s. Requires <code>filter_var</code> to be defined.</li> </ul> </li> <li>apply_mask (<code>bool</code>): Whether to apply a filtering condition to the data chunks.</li> <li>drop_sample (<code>bool</code>): If True, drops any samples that doesn't contain relevant values according to the <code>filter_var</code> criteria completely.</li> <li>fill_method (<code>str</code>): Method used to fill masked or NaN data.<ul> <li>None: <code>NaN</code>s are not filled.</li> <li>mean: <code>NaN</code>s are filled with the mean value of the non-<code>NaN</code> values</li> <li>sample_mean: <code>NaN</code>s are filled with the sample mean value.</li> <li>noise: <code>NaN</code> are filled with random noise within the range of the non-<code>NaN</code> values.</li> <li>constant: <code>NaN</code>s are filled with the specified constant value (<code>const</code>).</li> </ul> </li> <li>const (<code>float</code>): Constant value used when fill_method is 'constant'.</li> <li>filter_var (<code>str</code>): Filter mask name used for filtering data chunks.</li> <li>patience (<code>int</code>): The number of consecutive iterations without a valid chunk before stopping.</li> <li>block_size (<code>List[Tuple[str, int]]</code>): Size of the chunks to process, which can define memory usage and performance. If <code>None</code> the chunk size of ds is utilized.</li> <li>sample_size (<code>List[Tuple[str, int]]</code>): List of tuples specifying the sizes of the resulting dataset's samples</li> <li>overlap (<code>List[Tuple[str, float]]</code>): List of tuples specifying the overlap proportion of the resulting dataset's samples</li> <li>callback (<code>Callable</code>): Optional function applied to each chunk after initial processing.</li> <li>num_chunk (<code>int</code>):  Specifies the number of chunks to process if not processing all.</li> <li>to_pred (<code>Union[str, List[str]]</code>): Variable or list of variables to construct the dependent variable or sample to predict.</li> <li>scale_fn (<code>str</code>): Feature scaling function to apply (<code>standardize</code>, <code>normalize</code>, or <code>None</code>).</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#get_datasets","title":"get_datasets","text":"<p>Retrieves the fully processed dataset, ready for use in applications or further analysis. This method ensures that all preprocessing steps are applied and the data is returned in a manageable format.</p> <pre><code>def get_datasets(self) -&gt; Union[Dict[str, np.ndarray], Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]]\n</code></pre>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#returns","title":"Returns","text":"<ul> <li><code>Union[Dict[str, np.ndarray], Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]]</code>: Returns the processed dataset.</li> <li>If <code>to_pred</code> is <code>None</code>: Returns a dictionary where keys are variable names and values are concatenated numpy arrays representing the dataset.</li> <li>If <code>to_pred</code> is provided: Returns a tuple containing:       (X_train, y_train): Training features and targets.       (X_test, y_test): Testing features and targets.</li> </ul>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.splits import assign_block_split\nfrom ml4xcube.datasets.xr_dataset import XrDataset\n\n# Example dataset with chunking\ndata = np.random.rand(100, 200, 300)\ndataset = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), data),\n    'precipitation': (('time', 'lat', 'lon'), data)\n}).chunk({'time': 10, 'lat': 50, 'lon': 50})\n\n# Initialize the XrDataset\nxr_dataset = XrDataset(\n    ds          = dataset,\n    rand_chunk  = True,\n    drop_nan    = 'if_all_nan',\n    fill_method = 'mean',\n    sample_size = [('time', 1),('lat', 10), ('lon', 10)], \n    overlap     = [('time', 0.),('lat', 0.8), ('lon', 0.8)],\n    to_pred     = 'precipitation',\n    num_chunks  = 3\n)\n\n# Retrieve the processed dataset\ntrain_data, test_data = xr_dataset.get_datasets()\n</code></pre> This example demonstrates initializing the <code>XrDataset</code> class with a dataset, where chunks are randomly selected.  <code>NaN</code> values are managed by replacing them with the sample mean, and specific sample sizes and overlaps are defined to  create the datasets. precipitation is predicted based on temperature. The processed train and test data is then retrieved, showcasing the class's capability to efficiently manage and  preprocess smaller datasets suitable for machine learning models and other analytical tasks. </p>"},{"location":"ml-toolkit/api-reference/6-datasets/xr-dataset/#notes","title":"Notes","text":"<ul> <li><code>XrDataset</code> is designed in order to obtain data samples from <code>num_chunks</code> unique chunks.</li> <li>If <code>num_chunks</code> is not provided it will be set automatically determined, using <code>chunk_indices</code> if assigned or the number of total chunks in the dataset</li> <li>until <code>num_chunks</code> chunks are found containing valid data samples the <code>patience</code> parameter is used. It manages the number of attempts to find the next valid chunk before stopping.</li> <li>Sampling with <code>XrDataset</code> provides the following options:</li> <li>Use specific chunks if <code>chunk_indices</code>are set.</li> <li>Limit <code>num_chunks</code> to the total chunks if exceeded.</li> <li>Default <code>num_chunks</code> to total chunks if unspecified.</li> <li>Ensure all chunks in <code>num_chunks</code> contain valid, non-<code>NaN</code> data.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/ddp-init/","title":"Ddp init","text":""},{"location":"ml-toolkit/api-reference/7-training/ddp-init/#ddp_init","title":"ddp_init","text":"<pre><code>def ddp_init() -&gt; None \n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/ddp-init/#description","title":"Description","text":"<p><code>ddp_init</code> initializes the distributed process group for GPU-based distributed training using NCCL (NVIDIA Collective Communications Library).  It configures the environment to ensure each process operates on its designated GPU.</p>"},{"location":"ml-toolkit/api-reference/7-training/ddp-init/#parameters","title":"Parameters","text":"<ul> <li>(<code>None</code>): This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/ddp-init/#returns","title":"Returns","text":"<ul> <li><code>None</code>: The function does not return any value. It initializes the distributed training process</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/ddp-init/#example","title":"Example","text":"<pre><code>ddp_init()  # Initialize distributed environment\nmodel = MyModel()\ntrainer = Trainer(model=model, ...)\ntrainer.train()  # Manage distributed training\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/ddp-init/#notes","title":"Notes","text":"<ul> <li>NCCL Backend: Optimizes GPU communication in multi-GPU settings, enhancing the speed and efficiency of model training.</li> <li>Environment Configuration: Automatically sets the CUDA device to the local rank provided by the environment, aligning the process-to-GPU mapping.</li> <li>Usage Scenario: This function should be called at the beginning of your script to set up the necessary environment for distributed training.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/","title":"Ddp trainer","text":""},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/#trainer","title":"Trainer","text":"<p>The <code>Trainer</code> class in the <code>training.pytorch_distributed</code> module is meticulously crafted to optimize the distributed training of PyTorch models on systems equipped with multiple GPUs.  It supports a range of functionalities such as distributed data parallel processing, early stopping, metrics evaluation, snapshot saving, and optional loss plotting in a distributed environment.</p>"},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self, model: torch.nn.Module, train_data: DataLoader, test_data: DataLoader,\n    optimizer: torch.optim.Optimizer, save_every: int, best_model_path: str,\n    snapshot_path: str = None, early_stopping: bool = True, patience: int = 10,\n    loss: Callable = None, metrics: Dict[str, Callable] = None, epochs: int = 10,\n    validate_parallelism: bool = False, create_loss_plot: bool = False\n)\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/#parameters","title":"Parameters","text":"<ul> <li>model (<code>torch.nn.Module</code>): The model to be trained, which will be wrapped within a DistributedDataParallel (DDP) container.</li> <li>train_data (<code>DataLoader</code>): The DataLoader for the training dataset, appropriately set up to work in a distributed manner.</li> <li>test_data (<code>DataLoader</code>): The DataLoader for the validation dataset, also configured for distributed usage.</li> <li>optimizer (<code>torch.optim.Optimizer</code>): Optimizer used for training the model.</li> <li>save_every (<code>int</code>): Epoch frequency at which to save training snapshots.</li> <li>best_model_path (<code>str</code>): Path where the best model according to validation loss is saved.</li> <li>snapshot_path (<code>str</code>): Path to save periodic training snapshots; helpful for long training sessions.</li> <li>early_stopping (<code>bool</code>): Indicates whether training should stop early if there's no improvement, with a default setting of True</li> <li>patience (<code>int</code>): Number of epochs to wait for improvement in validation loss before early stopping. Defaults to <code>10</code>.</li> <li>loss (<code>Callable</code>): Loss function used during training. Must be specified.</li> <li>metrics (<code>Dict[str, Callable]</code>): Dictionary containing metrics to be evaluated during validation.</li> <li>epochs (<code>int</code>): The number of maximum training epochs.</li> <li>validate_parallelism (<code>bool</code>): If set to True, prints loss information from each GPU, useful for debugging and performance tuning.</li> <li>create_loss_plot (<code>bool</code>): Enables the creation of a plot that displays the training and validation loss progress over epochs.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/#train","title":"train","text":"<p>Manages the distributed training process across all epochs, handles early stopping, and loads the best model state at the end. It encapsulates the training process within a recorded session to handle potential errors and ensure proper cleanup of resources.</p> <pre><code>train(self) -&gt; torch.nn.Module\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/#returns","title":"Returns","text":"<ul> <li><code>torch.nn.Module</code>: The trained model with the best performance on validation data after distributed training.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/ddp-trainer/#example","title":"Example","text":"<p><pre><code>import torch\nimport xarray as xr\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\nfrom ml4xcube.splits import assign_block_split\nfrom ml4xcube.datasets.xr_dataset import XrDataset\nfrom ml4xcube.datasets.pytorch import prep_dataloader\nfrom ml4xcube.training.pytorch_distributed import ddp_init, Trainer\n\n# Example model\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.linear = nn.Linear(...)\n\n    def forward(self, x):\n        return self.linear(x)\n\n# Setting up distributed 7-training environment\nddp_init()\n\n# Load sample data\nxds = xr.open_zarr('sample_data.zarr')\n\n# Assign a train test split\nxds = assign_block_split(ds=xds, block_size=[(\"time\", 10), (\"lat\", 100), (\"lon\", 100)], split=0.8)\n\n# Extract a subset for training and testing, \ntrain_data, test_data = XrDataset(ds=xds, num_chunks=5, rand_chunk=False, to_pred='variable1').get_datasets()\n\n# Extract X_train and y_train, as well as X_test and y_test from train_data and test_data tuples and potentially reshape\n# ...\n\n# Load data and prepare it for parallel training\ntrain_ds     = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\ntest_ds      = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n\ntrain_loader, test_loader = prep_dataloader(train_ds, test_ds, batch_size=64, parallel=True)\n\n# Model, optimizer, and loss\nmodel = SimpleModel()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\nloss_fn = nn.CrossEntropyLoss()\n\n# Trainer\ntrainer = Trainer(model, train_loader, test_loader, optimizer, model_path=\"best_model.pth\", loss=loss_fn, epochs=10)\n\n# Train the model\ntrained_model = trainer.train()\n</code></pre> This setup demonstrates the workflow of the of handling a distributed training tasks within <code>ml4xcube</code>. This module provides a tool for training with large datasets, requiring high computational resources.</p>"},{"location":"ml-toolkit/api-reference/7-training/pytorch/","title":"Pytorch","text":""},{"location":"ml-toolkit/api-reference/7-training/pytorch/#class-trainer","title":"Class: Trainer","text":"<p>The <code>Trainer</code> class in the <code>training.pytorch</code> module is designed to facilitate efficient and effective training of PyTorch models, particularly on single or no GPU systems. It incorporates various functionalities such as early stopping, model saving, metrics evaluation, and optional loss plotting to streamline the training process.</p>"},{"location":"ml-toolkit/api-reference/7-training/pytorch/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self, model: torch.nn.Module, train_data: DataLoader, test_data: DataLoader,\n    optimizer: torch.optim.Optimizer, best_model_path: str,\n    early_stopping: bool = True, patience: int = 10, loss: Callable = None,\n    metrics: Dict[str, Callable] = None, epochs: int = 10, mlflow_run=None,\n    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    create_loss_plot: bool = False,\n)\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/pytorch/#parameters","title":"Parameters","text":"<ul> <li>model (<code>torch.nn.Module</code>): The PyTorch model to be trained.</li> <li>train_data (<code>DataLoader</code>): DataLoader for the training dataset.</li> <li>test_data (<code>DataLoader</code>): DataLoader for the validation/test dataset.</li> <li>optimizer (<code>torch.optim.Optimizer</code>): Optimizer used for training the model.</li> <li>best_model_path (<code>str</code>): Path where the best model according to validation loss is saved.</li> <li>early_stopping (<code>bool</code>): Indicates whether training should stop early if there's no improvement, with a default setting of True</li> <li>patience (<code>int</code>): Number of epochs to wait for improvement in validation loss before early stopping. Defaults to <code>10</code>.</li> <li>loss (<code>Callable</code>): Loss function used during training. Must be specified.</li> <li>metrics (<code>Dict[str, Callable]</code>): Dictionary containing metrics to be evaluated during validation.</li> <li>epochs (<code>int</code>): Total number of epochs to train. Defaults to <code>10</code>.</li> <li>mlflow_run (optional): Optional MLflow run instance to log training parameters, metrics, and models. Default is <code>None</code>.</li> <li>device (<code>torch.device</code>): Device on which to train the model (<code>cuda</code> or <code>cpu</code>). Automatically set based on availability.</li> <li>create_loss_plot (<code>bool</code>): If <code>True</code>, generates a plot for training and validation losses after training. Defaults to <code>False</code>.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/pytorch/#train","title":"train","text":"<p>Conducts the training process across all epochs, handles early stopping, and loads the best model state at the end.</p> <pre><code>train(self) -&gt; torch.nn.Module\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/pytorch/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/pytorch/#returns","title":"Returns","text":"<ul> <li><code>torch.nn.Module</code>: The trained model with the best performance on validation data.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/pytorch/#example","title":"Example","text":"<p><pre><code># Assuming model, train_loader, and test_loader are predefined:\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ntrainer = Trainer(model, train_loader, test_loader, optimizer, \"path/to/save/best_model.pth\")\ntrained_model = trainer.train()\n</code></pre> This class is integral to the ml4xcube framework, providing a structured and efficient way to train PyTorch models, especially suited for handling large-scale datasets typically used in geospatial analysis.</p>"},{"location":"ml-toolkit/api-reference/7-training/sklearn/","title":"Sklearn","text":""},{"location":"ml-toolkit/api-reference/7-training/sklearn/#class-trainer","title":"Class: Trainer","text":"<p>The Trainer class is designed for training scikit-learn models using versatile data inputs, such as PyTorch DataLoaders or numpy arrays.  This class is particularly useful for handling large datasets that may not fit into memory, as well as for leveraging the speed of batch training.</p>"},{"location":"ml-toolkit/api-reference/7-training/sklearn/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n        self,\n        model: BaseEstimator, train_data: Union[Any, Tuple[np.ndarray, np.ndarray]],\n        test_data: Union[Any, Tuple[np.ndarray, np.ndarray]] = None, metrics: Dict[str, Callable] = None,\n        model_path: str = None, batch_training: bool = False, mlflow_run=None, task_type: str = 'supervised'\n    )\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/sklearn/#parameters","title":"Parameters","text":"<ul> <li>model (<code>sklearn.base.BaseEstimator</code>): A scikit-learn estimator capable of partial_fit for incremental learning or fit for standard full-batch training.</li> <li>train_data (<code>Union[DataLoader, Tuple[numpy.ndarray, numpy.ndarray]]</code>):  Training data can be provided as a PyTorch DataLoader for batch training or a tuple of numpy arrays (X_train, y_train).</li> <li>test_data (<code>Union[DataLoader, Tuple[numpy.ndarray, numpy.ndarray]]</code>): Similar to train_data, validation/testing data can also be provided either as a DataLoader or a tuple of numpy arrays (X_test, y_test).</li> <li>metrics (<code>Dict[str, Callable]</code>): Dictionary containing metric functions that compute a performance score between predictions and true labels.</li> <li>model_path (<code>str</code>): File path to save the trained model.</li> <li>batch_training (<code>bool</code>): : Specifies whether to train the model using batches (if True) or on the complete dataset at once (if False).</li> <li>mlflow_run (optional): Optional MLflow run instance to log training parameters, metrics, and models. Default is <code>None</code>.</li> <li>task_type (<code>str</code>):  Specifies whether the training is 'supervised' or 'unsupervised'. Default is 'supervised'.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/sklearn/#train","title":"train","text":"<p>Conducts the training process, using batched training or on the complete dataset at once and returns the model. <pre><code>train(self) -&gt; BaseEstimator:\n</code></pre></p>"},{"location":"ml-toolkit/api-reference/7-training/sklearn/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/sklearn/#returns","title":"Returns","text":"<ul> <li><code>sklearn.base.BaseEstimator</code>: The trained model.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/sklearn/#example","title":"Example","text":"<p><pre><code>import numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import SGDClassifier\nfrom ml4xcube.training.sklearn import Trainer\n\n# Define a simple model and metrics\nmodel = SGDClassifier()\nmetrics = {'Accuracy': accuracy_score}\n\n# Dummy data\nX_train = np.random.rand(100, 10)\ny_train = np.random.randint(0, 2, 100)\nX_test = np.random.rand(20, 10)\ny_test = np.random.randint(0, 2, 20)\n\n# Initialize trainer\ntrainer = Trainer(\n    model=model,\n    train_data=(X_train, y_train),\n    test_data=(X_test, y_test),\n    metrics=[(\"Accuracy\", accuracy_score)],\n    model_path=\"best_model.pkl\",\n    batch_training=False\n)\n\n# Train the model\ntrained_model = trainer.train()\n\n# Evaluate the model\npredictions = trained_model.predict(X_test)\nprint(\"Test Accuracy:\", accuracy_score(y_test, predictions))\n</code></pre> This setup offers an approach to train scikit-learn models, accommodating both large-scale and in-memory datasets effectively. The Trainer class not only facilitates extensive training configurations but also integrates model evaluation and saving mechanisms, making it a robust tool for machine learning training in diverse data-intensive environments.</p>"},{"location":"ml-toolkit/api-reference/7-training/tensorflow/","title":"Tensorflow","text":""},{"location":"ml-toolkit/api-reference/7-training/tensorflow/#class-trainer","title":"Class: Trainer","text":"<p>The Trainer class in the training.tensorflow module is tailored to facilitate the efficient training of TensorFlow models, especially on systems equipped with a single GPU or none. It provides comprehensive support for training session management, including early stopping, model checkpointing, and integration with TensorBoard for monitoring.</p>"},{"location":"ml-toolkit/api-reference/7-training/tensorflow/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self, model: tf.keras.Model, train_data: tf.data.Dataset, test_data: tf.data.Dataset, \n    best_model_path: str, early_stopping: bool = True, patience: int = 10, \n    tf_log_dir: str = './logs', mlflow_run=None, epochs: int = 100, \n    train_epoch_steps: int = None, val_epoch_steps: int = None, create_loss_plot: bool = False,\n)\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/tensorflow/#parameters","title":"Parameters","text":"<ul> <li>model (<code>tensorflow.keras.Model</code>): The TensorFlow model to be trained.</li> <li>train_data (<code>tensorflow.data.Dataset</code>): TensorFlow Dataset containing the training data.</li> <li>test_data (<code>tensorflow.data.Dataset</code>): TensorFlow Dataset containing the validation data.</li> <li>best_model_path (<code>str</code>): Path where the best model according to validation loss is saved.</li> <li>early_stopping (<code>bool</code>): Indicates whether training should stop early if there's no improvement, with a default setting of True</li> <li>patience (<code>int</code>): Number of epochs to wait for improvement in validation loss before early stopping. Defaults to <code>10</code>.</li> <li>tf_log_dir (<code>str</code>): Directory path for saving TensorBoard logs, defaulted to './logs'.</li> <li>mlflow_run (optional): Optional MLflow run instance to log training parameters, metrics, and models. Default is <code>None</code>.</li> <li>epochs (<code>int</code>): Total number of epochs to train. Defaults to <code>10</code>.</li> <li>train_epoch_steps (<code>int</code>): Number of steps to run each training epoch, calculated dynamically if not set.</li> <li>val_epoch_steps (<code>int</code>): Number of steps to run each validation epoch, calculated dynamically if not set.</li> <li>create_loss_plot (<code>bool</code>): If <code>True</code>, generates a plot for training and validation losses after training. Defaults to <code>False</code>.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/tensorflow/#train","title":"train","text":"<p>Conducts the training process across all epochs, handles early stopping, and loads the best model state at the end.</p> <pre><code>train(self) -&gt; tf.keras.Model\n</code></pre>"},{"location":"ml-toolkit/api-reference/7-training/tensorflow/#parameters_1","title":"Parameters","text":"<ul> <li><code>None</code>: This method has no input parameters.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/tensorflow/#returns","title":"Returns","text":"<ul> <li><code>torch.nn.Module</code>:  The trained model, equipped with the best weights found during the training if early stopping was triggered.</li> </ul>"},{"location":"ml-toolkit/api-reference/7-training/tensorflow/#example","title":"Example","text":"<p><pre><code>import tensorflow as tf\nfrom ml4xcube.training.tensorflow import Trainer\n\n# Define the model\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(10, activation='relu', input_shape=(feature_size,)),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Prepare data\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n\n# Create a trainer instance\ntrainer = Trainer(\n    model=model,\n    train_data=train_dataset,\n    test_data=test_dataset,\n    best_model_path='path/to/save/best_model.h5',\n    tf_log_dir='path/to/save/logs',\n    epochs=50,\n    create_loss_plot=True\n)\n\n# Train the model\ntrained_model = trainer.train()\n</code></pre> This class offers a robust solution for training complex TensorFlow models with high efficiency, providing tools necessary for handling large-scale data and optimizing computational resources.</p>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-normalizing/","title":"Undo normalizing","text":""},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-normalizing/#undo_normalizing","title":"undo_normalizing","text":"<pre><code>def undo_normalizing(x: np.ndarray, xmin: float, xmax: float) -&gt; np.ndarray\n</code></pre>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-normalizing/#description","title":"Description","text":"<p>This function performs the inverse operation of normalization, transforming normalized data back to its original scale.</p>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-normalizing/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>numpy.ndarray</code>): The normalized array.</li> <li>xmin (<code>str</code>): The minimum value used for the original normalization.</li> <li>xmax (<code>bool</code>): The maximum value used for the original normalization.</li> <li></li> </ul>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-normalizing/#returns","title":"Returns","text":"<ul> <li><code>numpy.ndarray</code>:  The denormalized array.</li> </ul>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-normalizing/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.postprocessing import undo_normalization\nfrom ml4xcube.preprocessing import get_range, normalize\n\n# Example dataset\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n    'precipitation': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30))\n})\n\n# Get the range of the 'temperature' variable\ntemperature_range = get_range(ds, 'temperature')\nprint(f\"Temperature range: {temperature_range}\")\n\n# Normalize the 'temperature' variable\nnormalized_temperature = normalize(ds['temperature'].values, *temperature_range)\nprint(f\"Normalized temperature: {normalized_temperature}\")\n\n# Revert the normalization\noriginal_temperature = undo_normalization(normalized_temperature, *temperature_range)\n</code></pre> This example demonstrates how to revert normalized data back to its original scale using the <code>undo_normalizing</code> function.</p>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-standardizing/","title":"Undo standardizing","text":""},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-standardizing/#undo_normalizing","title":"undo_normalizing","text":"<pre><code>def undo_standardizing(x: np.ndarray, xmean: float, xstd: float) -&gt; np.ndarray\n</code></pre>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-standardizing/#description","title":"Description","text":"<p>This function performs the inverse operation of standardization, transforming standardized data back to its original scale.</p>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-standardizing/#parameters","title":"Parameters","text":"<ul> <li>ds (<code>numpy.ndarray</code>): The standardized array.</li> <li>xmean (<code>str</code>): The mean value used for the original standardization.</li> <li>xstd (<code>bool</code>): The standard deviation value used for the original standardization.</li> <li></li> </ul>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-standardizing/#returns","title":"Returns","text":"<ul> <li><code>numpy.ndarray</code>:  The destandardized array.</li> </ul>"},{"location":"ml-toolkit/api-reference/8-postprocessing/undo-standardizing/#example","title":"Example","text":"<p><pre><code>import numpy as np\nimport xarray as xr\nfrom ml4xcube.postprocessing import undo_standardization\nfrom ml4xcube.preprocessing import get_statistics, standardize\n\n# Example dataset\nds = xr.Dataset({\n    'temperature': (('time', 'lat', 'lon'), np.random.rand(10, 20, 30)),\n})\n\n# Calculate statistics\nstatistics = get_statistics(ds, 'temperature')\n\nprint(f\"Mean: {statistics[0]}, Standard Deviation: {statistics[1]}\")\n\n# Standardize the 'temperature' variable\nstandardized_temperature = standardize(ds['temperature'].values, *statistics)\nprint(f\"Normalized temperature: {standardized_temperature}\")\n\n# Revert the standardization\noriginal_temperature = undo_standardization(standardized_temperature, *statistics)\n</code></pre> This example demonstrates how to revert standardized data back to its original scale using the <code>undo_standardizing</code> function.</p>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/","title":"Evaluation","text":""},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#evaluator","title":"Evaluator","text":"<p>The <code>Evaluator</code> class in the <code>ml4xcube.evaluation.metrics</code> module is designed to handle metric evaluation  for machine learning frameworks (PyTorch, TensorFlow, and Scikit-learn), allowing users to evaluate various  metrics during model validation or testing.</p>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, framework: str):\n</code></pre>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#parameters","title":"Parameters","text":"<ul> <li>framework (<code>str</code>): The deep learning framework being used. Supported values are:</li> <li><code>'pytorch'</code></li> <li><code>'tensorflow'</code></li> <li><code>'sklearn'</code></li> </ul>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#get_metrics","title":"get_metrics","text":"<pre><code>def get_metrics(self, metric_names: List[str], average: str = 'macro', delta: float = 1.0) -&gt; Dict[str, Callable]:\n</code></pre> <p>This method returns a dictionary of metric functions based on the selected framework, metric names, and  optional parameters. <code>average</code> is </p>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#parameters_1","title":"Parameters","text":"<ul> <li>metric_names (<code>List[str]</code>): A list of metric names to retrieve. These names should correspond to the keys in the <code>metric_functions</code> attribute.</li> <li>average (<code>str</code>): The averaging method for precision, recall, and F1 score. Default is <code>'macro'</code>. Other possible values include <code>'micro'</code> and <code>'weighted'</code>.</li> <li>delta (<code>float</code>): The delta parameter for Huber loss. Default is <code>1.0</code>.</li> </ul>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#returns","title":"Returns","text":"<ul> <li>metrics (<code>Dict[str, Callable]</code>): A dictionary where the keys are metric names and the values are the corresponding metric functions.</li> </ul>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#supported-metrics","title":"Supported Metrics","text":"<ul> <li>'mae': Mean Absolute Error</li> <li>'mse': Mean Squared Error</li> <li>'rmse': Root Mean Squared Error</li> <li>'r2': R-squared</li> <li>'huber_loss': Huber Loss</li> <li>'mape': Mean Absolute Percentage Error</li> <li>'med_ae': Median Absolute Error</li> <li>'explained_variance': Explained Variance</li> <li>'accuracy': Accuracy</li> <li>'roc_auc': ROC AUC score</li> <li>'cross_entropy': Cross-Entropy Loss</li> <li>'precision': Precision score</li> <li>'recall': Recall score</li> <li>'f1_score': F1 Score</li> </ul>"},{"location":"ml-toolkit/api-reference/9-evaluation/evaluation/#example-usage","title":"Example Usage","text":"<pre><code>from ml4xcube.training.sklearn import Trainer\nfrom ml4xcube.evaluation.evaluator import Evaluator\n\nevaluator = Evaluator(framework='sklearn')\nmetrics = evaluator.get_metrics(metric_names=['recall', 'accuracy', 'precision'])\n\n# Access a specific metric function and use it\nmae_fn = metrics['recall']\n\n# Use the dictionary for model validation during training\ntrainer = Trainer(\n  model   = model,\n  ...\n  metrics = metrics\n)\n\ntrained_model = trainer.train()\n</code></pre>"},{"location":"science_cases/drought_effects/","title":"Drought Effects","text":""},{"location":"science_cases/drought_effects/#linear-estimation-of-drought-effects-on-productivity","title":"Linear estimation of drought effects on productivity","text":"<p>Authors: Chaonan Ji</p> <p>The rationale for the demonstration case on climate extremes is the following: especially excess drought and heatwave events (DHEs) are increasingly co-occurring, severely reducing the productivity of both semi-natural vegetation and crops and, therefore, carbon sequestration. Although the impacts of individual extreme events are partly very well investigated, we still lack a general understanding of the impacts of compound event years on ecosystems. Specifically, it remains unclear, firstly, whether DHEs systematically reduce vegetation\u2019s productivity potential of the entire growing season, if temporal compensations undo these effects, and secondly, how such responses differ among vegetation types. In a first exploratory phase (Ji et al. in prep \u2013 figures below), we analysed European climate conditions from 2001 to 2022 and used a rank-based method to identify the hottest and driest compound event per year for the entire continent. We used vegetation greenness data per vegetation classes to assess the integral growing season greenness anomalies across events and climate zones. Our results clearly show the large-scale signatures of DHEs on the annual growing conditions for vegetation, which lead to clear impacts on vegetation growing conditions that cannot be compensated. However, the effect is much more pronounced for grasslands than forests, which seem to have much higher seasonal resilience to DHE events. In the case of subsequent DHE years, the effects on forests are, however, much more pronounced indicating the risks of clustered extremes as we expect them in the near future. Given that this study investigated growing-season integrals, it corroborates earlier findings that individual extreme events have the potential to affect the inter-annual variability of the terrestrial carbon cycle. Future land-management strategies should consider such effects in landscape planning for buffering the impacts of climate extremes, reducing the volatility of the carbon sequestration potential of ecosystems, and regulating regional climate feedback.</p>          Ranking results of CHD years for Europe identifies regions experiencing         sweltering summers, defined by the top three temperature years in the ranking,         shown in reddish tones and highlighted with green edges. Similar analyses were         done for all relevant climate variales.               kNDVI value reductions by temperature but in different vegetation types. Clear         reductions are seen in grasslands for very high temperature regimes, showing the         susceptibility of these ecosystems to DHS.               Vegetation responses in climate space.       <p>Figures 4-6 aim to investigate the responses of different vegetation types (e.g., grasslands, conifers) in hot and dry years worldwide. Specifically, we would like to address (1) the rapid detection of extreme years, (2) the spatial trend of vegetation responses, and (3) the different responses of different vegetation types. We used ERA5 T, P, and soil moisture data, spectral indices, and land cover maps to investigate these points. Based on these preliminary results, we embarked with developing deep learning frameworks that can effectively predict the responses especially in ecosystems that do not show obvious results i.e., forest ecosystems. One first methodological idea towards deep learning was using Echo State Networks, an advanced variant of recurrent neural networks. The results by Martinuzzi et al. (accepted) has not been as conclusive in the sense of gaining significant improvements over existing RNNs as expected. In Echo State Networks (ESNs) only the last layer is trained through linear regression. The absence of derivatives guarantees no vanishing or exploding gradients, offering an alternative solution to gating. To ensure a comprehensive comparison, we also investigate the performance of other RNN architectures. The comparison of these models has a strong focus on the extreme responses of vegetation indices to climate drivers. The primary focus of this model comparison lies in understanding vegetation\u2019s extreme responses to climate drivers. We conducted a comprehensive analysis of recurrent neural networks in the context of modeling biosphere dynamics in response to climate factors. By using daily data, we assessed the effectiveness of these network architectures in capturing extreme events within vegetation dynamics. To discern variations in performance across different scenarios, we employed various metrics. Echo State Networks (ESNs) slightly outperformed other RNNs, but the improvements are relatively minor, despite multiple theoretical arguments in favour of ESN. This is why we still started to explore other methodological avenues before going to a continent-wide deep analysis of extremes in DeepESDL.</p>"},{"location":"science_cases/land_biosphere/","title":"Land-Biosphere-Society","text":""},{"location":"science_cases/land_biosphere/#science-demonstration-case-land-biosphere-society","title":"Science Demonstration Case \u201cLand-Biosphere-Society\u201d","text":"<p>Authors: Wanton Li, Gregory Duveiller, Fabian Gans, Jeroen Smits, Guido Kraemer, Dorothea Frank, Miguel Mahecha, Ulrich Weber, Mirco Migliavacca, Andrey Ceglar, Markus Reichstein: Diagnosing syndroms of biosphere-atmosphere-socioeconomic change.</p> <p>While previous work (Kraemer et al. 2020) to create an index of the Earth System focussed on describing the different spheres (Atmosphere - Biosphere - Society) separately this use case focuses on describing the interaction between these individual spheres. To obtain a first understanding on the main signal that can be extracted from the ESDC we applied canonical correlation analysis (CCA) on annually aggregated time series per country. Applying a linear method first\u202f can be seen as creating a baseline to understand the relationships between variables before applying nonlinear deep-learning methods. The main difference of using CCA in comparison to PCA is that instead of maximizing the explanation of variance in a dataset itself, the CCA tries to explain as much variance as possible for an independent dataset. This method can be applied as a 3-way CCA to sub-datasets of the ESDC from the biosphere and atmosphere as well as to a compiled dataset based on World-Bank socioeconomic indicators. In order to remove confounding spatial patterns, we spatially detrended the input data to concentrate the analysis on the temporal evolution of the country-based data.</p> <p>Finally, the result of our analysis is a time-dependent interaction index for each country and every pair of variables that encodes the possible interaction between these spheres. This can be interesting from two viewpoints. First one can examine certain known events for single countries and test if there is a signal in multiple of the interaction data streams. This can be an indication that an event had an effect on multiple spheres and hypotheses can be generated about the possible interactions and causal effects.</p> <p></p> <p>We use canonical correlation analysis (CCA) to construct interactive socio-biosphere-atmosphere indices and monitor their temporal changes across different countries. The left plot shows a 3d scatter plot of the first component of every sphere for all countries and years. Outliers points are marked in red color. For two of the outliers (Niger and Vanuatu) the time evolution of these indices is shown and the outliers can be related to known events (2017 Niger soil drought and 2015 Cyclone in Vanuatu).</p> <p>Another approach to investigate the data is to summarize the long-term trajectories different countries take on decadal time scales. For example, it is possible to define clusters of countries with similar co-evolution of different indices based on\u202f trend and standard deviation of their index time series.</p> <p></p> <p>The upper figure (a) shows that global countries are distinguished into seven common groups based on clustering on the CCA constructed components. The button figure (b) shows the mean trajectories of CCA constructed socio-biosphere-atmosphere indices across the groups.  </p> <p>We conclude that our results demonstrate the possibilities to explore the interactions of different Earth System components by using dimensionality reduction techniques that aim to summarize the interaction between different data domains. Since these interactions can be very complex and nonlinear there will be future possibilities to explore nonlinear Deep-Learning based extensions of our methods to improve the robustness of our results and provide more capabilities of diagnosing data-driven trends and generating hypotheses on interactions in the Earth System. Work is also foreseen in collaboration with the EU funded Open-Earth-Monitor (OEMC) project, in which the concept is being further developed with a specific use case involving the European Central Bank to diagnose interactions between the financial sector and the natural system.</p>"},{"location":"science_cases/ocean/","title":"Ocean Carbon Cycle","text":""},{"location":"science_cases/ocean/#science-demonstration-case-ocean","title":"Science Demonstration Case \u201cOcean\u201d","text":"<p>Authors: Julia Klima, Jannes Kruse, Jonas Neumann</p> <p>A novel approach was explored to understanding the ocean carbon cycle through the utilization of a recently developed, uniformly structured data cube encompassing key ocean carbon cycle variables. The methodology involves a combination of dimensionality reduction and feature selection techniques, enabling to unravel the intricacies of the carbon cycle across various geographic locations. The study spans a period from 1998 to 2020, during which the complexity of the ocean's carbon cycle was analysed. We find that the originally eight-dimensional feature space can be effectively condensed into three to four dimensions. This reduction not only simplifies the representation but also enhances our understanding of the underlying processes. A significant aspect of our analysis is the identification of geographical patterns in the carbon cycle's complexity. These patterns are closely linked to the dynamics of thermohaline circulation and the movement of water masses. Specifically, areas of intense upwelling and the warm, surface-near currents of the Global Conveyor Belt are highlighted, along with their anomalies and regional extreme events.</p> <p>This research pinpoints several key variables, such as Particulate Inorganic Carbon and Mixed Layer Depth, as critical in both a global and regional context for representing the simplified dimensions of the carbon cycle. These findings are pivotal in deepening our understanding of the carbon cycle's regional behaviours. We will now iterate the findings with the data providers in order to understand how to enhance the predictability and effectiveness of future research in carbon modelling and oceanic pathways.</p>          Study design using a data cube of oceanic carbon indicators.               Steps performed to obtain the dimensionally reduced and clustered new         data cubes               Results of the Principal Component Analysis per pixel.       <p>The study presented here is still in preliminatry state. However, we find that using Principal Component Analysis (PCA) and Principal Feature Analysis (PFA) is very helpful to analyze ocean carbon cycle variables from a data cube. The key findings are summarized as follows:</p> <ul> <li>Dimensionality Reduction: The PCA results indicated that most of the ocean's   carbon cycle can be described by three to four dimensions, significantly   reducing the original eight-dimensional feature space. This suggests that many   ocean carbon pump variables are interrelated and exhibit coordinated   variations over time.</li> <li>Geographical Patterns: The study revealed distinct geographical patterns in   the number of dimensions per pixel. These patterns are associated with ocean   currents, upwelling regions, wind zones, and differences between coastal and   open-sea areas. In open-sea regions, the data cube variables often require   four dimensions for description, while near coastlines, three dimensions are   generally sufficient.</li> <li>Regional Variations: The analysis identified specific regions with unique   dimensional patterns. For example, large areas above 30\u00b0 N and 30\u00b0 S in the   westerly wind zone predominantly show three dimensions. Notable regions like   the equatorial-subtropical zone, the Pacific-Indian Ocean transition, and   areas around the Global Conveyor Belt also exhibit distinct patterns.</li> <li>El Ni\u00f1o-Southern Oscillation Area: This area is mostly represented by four   dimensions, with patterns influenced by trade winds, upwelling regions, and   warm ocean currents. The equatorial-subtropical zone is the only area where   five dimensions are occasionally present.</li> <li>Principal Component Variability: The first principal component (PC1) explains   a significant portion of the variability in many regions, especially around   the equator. However, in areas like the East Pacific Rise, the variability is   spread across multiple dimensions.</li> <li>Principal Feature Analysis (PFA) Results: PFA identified key variables that   describe the reduced dimensionality of the ocean carbon cycle. Particulate   Inorganic Carbon (PIC) is a major variable, present in almost all pixels,   indicating its importance in describing the variability of the ocean carbon   pump.</li> <li>Phytoplankton Variability: Different types of phytoplankton (micro, pico, and   nano) show varying presence across different regions, indicating their role in   the ocean carbon cycle's variability.</li> <li>Mixed Layer Depth (MLD) and Other Variables: MLD is generally important for   describing variability, but its presence varies geographically. Other   variables like Primary Production (PP) and Particulate Organic Carbon (POC)   also show varying importance across different locations.</li> <li>Cluster Analysis: The clustered PFA results reveal patterns in variable   importance across different ocean regions. These clusters help in   understanding the regional differences in the ocean carbon cycle</li> <li>Implications and Limitations: The study provides insights into the   interconnected nature of oceanic processes affecting the carbon cycle.   However, it also acknowledges limitations such as the exclusion of certain   variables, the lack of a comprehensive theoretical framework, and the absence   of data in certain latitudes.</li> </ul> <p>Overall, the study offers a nuanced understanding of the ocean carbon cycle's complexity and its geographical variability, highlighting the interconnectedness of various oceanic processes and their impact on carbon cycling.</p>"}]}